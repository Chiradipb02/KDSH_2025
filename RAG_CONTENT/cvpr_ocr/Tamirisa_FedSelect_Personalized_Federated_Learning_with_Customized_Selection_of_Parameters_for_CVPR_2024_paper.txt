FedSelect: Personalized Federated Learning with Customized Selection of
Parameters for Fine-Tuning
Rishub Tamirisa†, Chulin Xie†,‡, Wenxuan Bao†,‡, Andy Zhou†, Ron Arel†, Aviv Shamsian§
†Lapis Labs‡University of Illinois Urbana-Champaign§Bar-Ilan University
{rishubt2,chulinx2,wbao4,andyz3,ronarel2 }@illinois.edu aviv.shamsian@live.biu.ac.il
Abstract
Standard federated learning approaches suffer when
client data distributions have sufficient heterogeneity. Re-
cent methods addressed the client data heterogeneity issue
via personalized federated learning (PFL) - a class of FL
algorithms aiming to personalize learned global knowledge
to better suit the clients’ local data distributions. Exist-
ing PFL methods usually decouple global updates in deep
neural networks by performing personalization on partic-
ular layers (i.e. classifier heads) and global aggregation
for the rest of the network. However, preselecting network
layers for personalization may result in suboptimal stor-
age of global knowledge. In this work, we propose FED-
SELECT , a novel PFL algorithm inspired by the iterative
subnetwork discovery procedure used for the Lottery Ticket
Hypothesis. FEDSELECT incrementally expands subnet-
works to personalize client parameters, concurrently con-
ducting global aggregations on the remaining parameters.
This approach enables the personalization of both client
parameters and subnetwork structure during the training
process. Finally, we show that FEDSELECT outperforms
recent state-of-the-art PFL algorithms under challenging
client data heterogeneity settings and demonstrates robust-
ness to various real-world distributional shifts. Our code is
available at https://github.com/lapisrocks/fedselect.
1. Introduction
Federated Learning (FL) enables distributed clients or de-
vices to jointly learn a shared global model while keeping
the training data local [31], which is particularly beneficial
for privacy-critical applications [37]. Despite its potential,
the efficacy of FL is challenged by the inherent heterogene-
ity of data across different clients. As the local updates of
clients can be remarkably diverse given their heterogeneous
local data, the aggregated global model can diverge under
the standard FL paradigms [23].
To address this issue, personalized federated learning(PFL) emerges as a promising approach. PFL allows indi-
vidual clients to maintain their unique, personalized mod-
els, with parameters tailored to their local data distribu-
tions, while knowledge sharing across different clients. One
representative type of PFL algorithm is “parameter decou-
pling”. It decomposes the FL model into two distinct com-
ponents: a global subnetwork shared among all clients
(e.g., the feature extractor), and a personalized subnetwork
adapted to each client’s local distribution (e.g., the predic-
tion head). By personalizing a subset of parameters, param-
eter decoupling strikes a balance between knowledge shar-
ing among clients and personalization to each client.
A natural follow-up question would be how to determine
which parameters to personalize . Previous works typically
selected specific layers for personalization, e.g., the predic-
tion head [5, 7], and the feature extractor [27, 33]. However,
recent studies [11, 12, 35] suggest that even within the same
layer, the importance of parameters for prediction can vary
significantly. The coarse-grained layer-wise selection of
personalized subnetwork may not fully balance knowledge
sharing among clients and personalization to each client.
Furthermore, current methods typically pre-defining the ar-
chitecture of the personalized subnetwork, i.e., which layers
to personalize, and the architecture of the personalized sub-
network is shared for all clients. These designs limit the per-
formance of parameter decoupling, as the optimal person-
alized subnetwork often varies depending on each client’s
specific local data distribution.
In this paper, we delve into these challenges, exploring a
novel strategy named F EDSELECT for parameter selection
and subnetwork personalization in PFL to unlock the full
potential of parameter decoupling. Our method in enlight-
ened by the Lottery Ticket Hypothesis (LTH): deep neu-
ral networks contain subnetworks (i.e., winning tickets) that
can reach comparable accuracy as the original network. We
believe that FL model also contain a subnetwork that is cru-
cial for personalization to a specific client’s local distribu-
tion, and personalizing that subnetwork can achieve opti-
mal balance between global knowledge sharing and local
personalization. Specifically, we believe that parameters
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
23985
that change the most over the course of a local client up-
date should be personalized, while parameters changing the
least should be globally aggregated. This is achieved by
comparing the element-wise magnitude difference between
the state of the client model before and after local training.
Notably, our method is different from the original LTH in
twofold: in terms of purpose, while LTH seeks a sparse net-
work for efficiency, our goal is to enhance personalized FL
performance by finding the optimal subnetwork of individ-
ual clients for personalization, based on the characteristics
of local data distribution. Methodologically, LTH prunes
less important parameters (i.e., non-winning tickets) to zero
value, whereas we assign them the global aggregated value
for storing global information, aligning with the collabora-
tive nature of FL. We summarize our contributions below:
• We introduce a hypothesis for selecting parameters during
training time, aimed at enhancing client personalization
performance in FL.
• We propose F EDSELECT , a novel personalized FL algo-
rithm based on our hypothesis that automatically identi-
fies the customized subnetwork for each client’s person-
alization, guided by the updating magnitude of each pa-
rameter.
• We evaluate F EDSELECT across a range of benchmark
datasets: CIFAR10, CIFAR10-C, OfficeHome, and Mini-
ImageNet. Our method outperforms state-of-the-art per-
sonalized FL baselines in various FL scenarios, encom-
passing feature and label shifts, as well as different lo-
cal data sizes. Moreover, our visualizations verify that
the learned personalized subnetworks successfully cap-
ture the distributional information among clients.
2. Related Work
2.1. Federated Learning
There has been extensive work in federated learning on en-
hancing the training of a global model from various clients
with non-independent and identically distributed (non-IID)
data [4, 22]. The original approach, FedAvg [31], aims to
develop a single global model computed through the aggre-
gation of local updates from each client without requiring
raw client data storage at a central server. However, chal-
lenges arise with non-IID data, leading to innovations in
optimizing local learning algorithms through objective reg-
ularization [1], local bias correction [24], and data handling
techniques such as class-balanced resampling [16] or loss
reweighting [43]. Different from these works, we adapt
each client to its local data.
2.2. Personalized Federated Learning
Personalized federated learning aims to address the issue
of data heterogeneity by adapting clients to their local data
distribution [40]. Common approaches include multitasklearning [3, 38], clustering [9, 13, 29], transfer learning
[46, 47], meta learning [6, 10, 17], hypernetworks [36], and
gaussian processes [2]. We focus on partial model person-
alization, which seeks to improve the performance of client
models by altering a subset of their structure or weights to
better suit their local tasks. It also addresses the issue of
“catastrophic forgetting” [30], an issue in personalized FL
where global information is lost when fine-tuning a client
model on its local distribution from a global initialization
[18, 34]. It does this by forcefully preserving a subset of pa-
rameters, u, to serve as a fixed global representation for all
clients. However, existing methods [7, 34, 44] introduced
for partial model personalization require hand-selected par-
titioning of these shared and personalized parameters and
choose uas only the input or output layers.
LotteryFL [21] learns a shared global model via Fe-
dAvg [31] and personalizes client models by pruning the
global model via the vanilla LTH. Importantly, parameters
are pruned to zero according to their magnitude after an iter-
ation of batched stochastic gradient updates. However, due
to a low final pruning percentage in LotteryFL, the lottery
tickets found for each client share many of the same param-
eters, and lack sufficient personalization [32].
3. Background
Preliminaries We start by introducing the standard FL
setting. Let the set of clients be C={c1, . . . , c N}, where
the total number of clients N=|C|. For the k-th client
ck∈C, the corresponding local dataset is defined as
Di={xk
i, yk
i}Nk
i=1, where Nkis the number of local data
points for client ck. Letθgbe the FL global model, and the
local loss function (e.g. cross-entropy loss) for client kas
fk(θg, x), we can define the canonical FL objective:
min
θg1
NNX
k=1NkX
i=1fk(θg, xk
i) (1)
FEDAVG[31] optimizes the objective in (1) by locally
training each of the client models θt
kforLnumber of local
epochs at each communication round t. The updated local
client model θt+
kis aggregated into the global model θt+1
g,
resulting in the global model update for round tin F EDAVG
being θt+1
g←1
NPN
kθt+
k. At the next round, the aggre-
gated global model θt+1
gis then redistributed to all clients,
resulting in θt+1
k←θt+1
g,∀k.
The goal of personalized federated learning, on the other
hand, is to find personalized models θkfor each client k, ei-
ther adapted from θgor discovered independently, resulting
in a modified objective:
min
θ1,θ2,...,θ N1
NNX
k=1NkX
i=1fk(θk, xk
i) (2)
23986
Partial model personalization refers to the procedure in
which model parameters are partitioned into shared and per-
sonalized parameters, denoted uandv, for averaging and
personalization. We consequently define θk= (uk, vk),
where udenotes a set of shared global parameters (e.g.,
aggregated via F EDAVG), and vkthe personalized client
parameters. Substituting the tuple (uk, vk)for each client
model θkyields the partial model personalization objective
as in [34].
4. FedSelect
In this section we discuss the motivation for our algo-
rithm, followed by a description of the top-level procedure
(Algorithm 1). We then provide detailed overviews of 2
sub-procedures within F EDSELECT (Algorithms 2, 3). We
present an analogy between F EDSELECT and the algorithm
used by [21] for finding winning lottery ticket networks, and
note important distinctions in our application to FL.
4.1. Motivation
Prior works in FL selectively personalize specific layers of
the model to better learn the heterogeneous distributions of
local clients [7, 20, 26, 27, 34], which are referred to as per-
forming “parameter-decoupling”. For example, they rely on
the conventional wisdom that the final linear layer of deep
neural networks (DNNs) for classification contains a more
enriched semantic representation with respect to the output
prediction neurons [5, 45], which could be more suitable for
personalization.
In this work, we propose a novel hypothesis describing
that only parameters that change the most during training
are necessary as personalized parameters for client mod-
els during FL; the rest can be aggregated as global parame-
ters. Analogous to the sparse winning ticket networks found
via the LTH, F EDSELECT aims to elicit an optimal subnet-
work for fine-grained personalization of individual client
model. Our primary intuition is that drastic distributional
changes in the client personalization task may be better ac-
commodated by preserving the accumulated global knowl-
edge and personalized knowledge in a parameter-wise gran-
ularity, rather than layer-wise . Following this we state our
hypothesis:
FL Gradient-based Lottery Ticket Hypothesis. When
training a client model on its local distribution during fed-
erated learning, parameters exhibiting minimal variation
are suitable for encoding shared knowledge, while param-
eters demonstrating significant change are optimal for fine-
tuning on the local distribution and encoding personalized
knowledge.
Now we introduce some basic notations regarding neural
subnetwork and vanilla LTH. Given a neural network withparameters θ, we define a subnetwork via a binary mask m.
For convenience, we use the Hadamard operator ⊙to be an
indexing operator for m, rather than an elementwise multi-
ply operator. For example, θ⊙massumes θandmhave the
same dimensions and returns a reference to the set of param-
eters in θwhere mis not equal to zero. We also define the
operator ¬to invert the binary masks. To identify winning
tickets via the vanilla LTH, the iterative magnitude search
(IMS) procedure is proposed [11], which finds a mask m
for network parameters θsuch that θ⊙mcan be trained in
isolation to match the performance of θon a given dataset.
For clarity, we restate the IMS procedure as follows. Given
a pruning rate p, an initial model θis trained for jiterations,
yielding θ+. Next, the smallest p%of parameters in θ+are
identified via a binary mask m. The original model θis then
pruned, given by θ⊙¬m←0, and the process repeats until
the desired sparsity of mis achieved.
While we draw inspiration from the winning ticket IMS,
there are three distinct and notable differences in our ap-
proach that set it apart, which will be covered in greater
detail in Sections 4.2 and 4.4. First, rather than iteratively
finding smaller subnetworks, our approach iteratively grows
a subnetwork within each client. Second, we do not per-
form any parameter pruning; instead, we use mkfor each
client θkto obtain the partition of global and personalized
parameters as (uk, vk) = ( θk⊙ ¬mk, θk⊙mk). Third,
the mask pruning update to mkis computed based on the
element-wise magnitude of parameter update ∇vkfrom lo-
cal client training rather than the magnitude of parameter vk
itself. Regarding the winning ticket found via our IMS, the
set of parameters selected in F EDSELECT will permanently
remain personalized (i.e., always kept local and not sent to
server); this set will grow in size over the course of FL.
There are two additional considerations when designing
an algorithm that selects parameters using our hypothesis
for personalization while training during FL: (1) computa-
tional efficiency for subnetwork discovery and (2) a suitable
mechanism for performing a local update on both ukand
vk. A possible algorithm for selecting vkfor each commu-
nication round could involve performing federated averag-
ing, followed by a gradient-based subnetwork search using
a modified version of the IMS that computes musing∇vk.
However, this would require performing j×Literations of
local training, which would incur an undesirable computa-
tional overhead during FL. Next, we discuss how F EDSE-
LECT addresses both points (1) and (2).
FEDSELECT (Algorithm 1) takes the following as input:
the set of clients C, an initial model θI, the number of com-
munication rounds T, the personalization rate p, and the
personalization limit α. We reuse the notation from Sec-
tion 3 to refer to global and personalized parameters as ei-
theruandv, or positions in the client masks mkequal to 0
or1, respectively.
23987
Update Personal / Global Partition
Update Personal / Global PartitionFigure 1. Illustration of the F EDSELECT algorithm. An example subnetwork update for communication round t=iintot=i+ 1 is
depicted for Nclients, where 2 clients are shown. There are 4 key steps: (1) the local update / new partition via GradSelect (Algorithm 3),
(2) the aggregation of global parameters vt
k, (3) broadcast of global parameters to the updated clients, and (4) application of the new mask
mt+1
kas a partition for the global/personalized parameters of each client in the subsequent round t+ 1. In our algorithm, ut
kdenotes the
global parameters for each client at round t;vkdenotes the personalized parameters for each client at round t;ut+
kdenotes the updated
global parameters after GradSelect; vt+
kdenotes the updated personalized parameters after GradSelect; mt+1
kis the binary mask with “0”
denoting global parameter and “1” denoting personalized parameter; θt
gis the aggregate global parameters materialized each round.
4.2. Client Update Overview
At the beginning of FL, all client models are initialized with
the same global model θ0
g, denoted via binary client masks
set as m0
k= 0 applied to each client model θ0
k. Here, we
use GradSelect as a black-box procedure for obtaining a
personalized subnetwork update, which will be described
in further detail in Section 4.5. First, given the current
communication round t, GradSelect updates each clients’
current global parameters ( ut
k) and personalized parame-
ters (vt
k); these are identified using the current mask mt
k
as an index into θt
k. Then, the next set of parameters to
include for personalization in vt
kare chosen according to
a scalar personalization rate p∈[0,1]. This is notated
via values at the corresponding indices for vt
kinmt
kbe-
ing set to 1. Importantly, in each communication round,
the new parameters in vt
kare selected solely from the set
of global parameters that have been aggregated such that
Idx(v0
k)∈Idx(v1
k)∈...∈Idx(vT
k). We note a subtlety
in our formulation in Eq. 2 that differs from the original
PFL objective proposed in [34]. Namely, in the original
partial model personalization framework, it is expected that
all client models share the same global parameters denoted
asut. In F EDSELECT , clients may learn different person-alized parameters vt
k, resulting in differing partitions of the
shared global parameters ut
k.
4.3. Global Aggregation
After each client performs its update, the global parame-
tersukfor each client are aggregated. Since mkis likely
to be heterogeneous across clients in typical personalized
FL settings, averaging the global parameters ukfor each
client requires careful handling. This is because the lo-
cations of global parameters (values of 0inmk) may be
non-overlapping for multiple parameter indices. In F ED-
SELECT , global averaging for a given index in the current
global model θt
gonly occurs across parameters ukin each
client for which the corresponding mask entry in mk= 0.
We use ωtas a tool of implementation to store the num-
ber of clients contributing to each global parameter in θt
g
element-wise. By this construction, different subsets of
clients ck∈Ccan contribute to different global parameters
inθt
g. We provide a visual description of this procedure in
Figure 1. As a result, the first communication round in F ED-
SELECT where all clients masks are set to 0performs pure
federated averaging. A consequence of this formulation is
that in the extreme case where global parameters ukis com-
23988
Algorithm 1 FedSelect
Input: C={c1, . . . , c N}, θI, T, L
Server Executes:
Initialize all client models {θ0
i}N
i=1withθI
Initialize all client masks {m0
i}N
i=1with0s
foreach round tin0,1, . . . , R −1do
foreach client ck∈Cin parallel do
# Executed locally on client ck
ut
k←θt
k⊙ ¬mt
k
vt
k←θt
k⊙mt
k
ut+
k, vt+
k, mt+1
k←GradSelect (ut
k, vt
k)
# Averaging occurs only across clients where the
mask is 0for a given parameter’s position
θt
g←0
ωt←0
fork= 1toNdo
θt
g⊙ ¬mk←(θt
g⊙ ¬mt
k) +ut+
k
ωt⊙ ¬mk←(ωt⊙ ¬mt
k) +¬mt
k
mt
g←Binary mask for ωt̸= 0
θt
g⊙mt
g←θt
g⊙mt
g
ωt⊙mtg
fork= 1toNdo
# Distribute global params to clients’ non-
selected params, located via ¬mt
k
θt+1
k⊙ ¬mt
k←θt
g⊙ ¬mt
k
θt+1
k⊙mt
k←vt+
k
pletely disjoint across all clients, each client will effectively
undergo purely local training. However, full disjointedness
across ukis unlikely in typical FL settings where clients
are likely to have similar data distributions. We hypothe-
size that clients with similar data distribution trained using
FEDSELECT will learn similar personalized subnetworks;
we show visualizations of these correlations in Section 5.2.
4.4. Subnetwork Representation
A key property of F EDSELECT , mentioned in Section 4.1,
is that the personalized subnetwork representation gradu-
ally grows in size as the communication rounds progress.
Specifically, the subnetwork size grows by a factor of p%
until the corresponding client mask mkreaches the maxi-
mum subnetwork mask sparsity α, a scalar defined within
[0,1]. We also refer to αas the personalization limit, the
central hyperparameter of our proposed algorithm, where
larger αindicates greater personalization. Therefore, when
α= 1.0, FEDSELECT computes local-only training of each
client after mkfor each client reaches sparsity α. Con-
versely, when α= 0, FEDSELECT reduces exactly to F E-
DAVG. We consider the behavior of our proposed algorithm
as enabling a “rough interpolation” between these two ex-
tremes for personalization.Algorithm 2 LocalAlt (uk,0, vk,0)[34]
Input: Global/personalized parameters uk,0, vk,0,
# of steps τ, global/local learning rates γv, γu,
Batched data D={b0, b2, . . . , b τ−1}
fori= 0,1, . . . , τ −1do
vk,i+1←vk,i−γv∇vkfk((uk, vk,i), bi)
v+
k←vk,τ
fori= 0,1, . . . , τ −1do
uk,i+1←uk,i−γu∇ukfk((uk, v+
k), bi)
u+
k←uk,τ
Return v+
k, u+
k
Algorithm 3 GradSelect (uk, vk)
Input: Global/personalized parameters uk,vk, Per. rate
p, # local epochs L, Per. bound α
uk,0←uk
vk,0←vk
fori= 0,1, . . . , L −1do
uk,i+1, vk,i+1←LocalAlt( uk,i, vk,i)
ifsparsity of ¬mk< αthen
∆uk← |uk,L−uk,0|
m+
k←binary mask for largest p%values in ∆uk
# Element-wise Binary OR of m+
kandmk
m+
k←m+
k∨mk
else
m+
k←mk
Return uk,L, vk,L, m+
k
Furthermore, for α > 0.50and sufficiently many com-
munication rounds for the chosen personalization rate p, a
majority of parameters in all client models will be selected
for personalization, resulting in decreased communication
costs over time within FedSelect.
By our design of evolving masks over the course of FL
rather than within a single communication round, F EDSE-
LECT achieves a similar time complexity to other alternat-
ing minimization-based personalized FL methods like Fe-
dRep [7] and FedPAC [44], while performing a first-of-its-
kind selection of personalized parameters.
4.5. GradSelect
We now describe GradSelect in isolation. GradSelect takes
as input a partition of the current global parameters ut
kand
personalized parameters vt
kat communication round t, a
personalization rate p, and personalization limit α. The
goal is to compute an update to the client model that grows
the current subnetwork by a factor of p%while also train-
ing the current client model. To this end, we apply our
FL Gradient-based Lottery Ticket Hypothesis to both train
the client model’s parameters and discover new parameters
23989
for personalization. First, to update both ukandvkfor
each client, we use LocalAlt (Algorithm 2) from the par-
tial model personalization framework [34]. LocalAlt was
introduced to update a predefined set of shared and person-
alized parameters, ukandvk, by alternating full passes of
stochastic gradient descent between both sets of parameters.
The LocalAlt training epochs within GradSelect are
analogous to the training iterations used to compute lottery
ticket updates in the aforementioned IMS procedure. Con-
tinuing the analogy, we store the state of the client parame-
ters before LocalAlt θk, and compute the absolute value of
the change of all global parameters after LocalAlt in ∆uk.
Taking the largest p%of values in ∆uk, we create a the
new mask m+
k, which finishes the subnetwork update. Be-
cause m+
kis the mask to be used for local updates in the
subsequent communication round, GradSelect also returns
the newly-trained global/personalized parameters from the
current mask partition mkgiven by uk,L, vk,L. The param-
eterpcontrols the rate at which personalized subnetworks
grow, and αcontrols the maximum size of the subnetwork.
We include the result of varying αin Section 5.2, as well as
the effect of changing pin Appendix B.
5. Experiments
In this section, we compare FedSelect with different ap-
proaches from FL and personalized FL. We use a variety of
datasets and learning setups to demonstrate the superiority
of FedSelect. We will make our source code publicly avail-
able to encourage reproducibility. Additional experimental
results and details are provided in Appendix B.
5.1. Experimental Setup
Models & Datasets. We show results for our experimen-
tal setting across a breadth of benchmark datasets: CIFAR-
10 [19], CIFAR10-C [15], Mini-ImageNet [42], and the Of-
ficeHome dataset [41]. These settings additionally cover
label shifts and feature shifts, which are common distri-
butional shifts in real-world data. We use a ResNet18
[14] backbone with random initialization on CIFAR-10,
CIFAR10-C, and Mini-ImageNet. For OfficeHome, we fol-
low [39] to use a ResNet18 backbone pretrained on Ima-
geNet [8].
Baselines. We compare our method to the full model per-
sonalization methods, including local-only training, Fe-
dAvg [31] with local fine-tuning (FedAvg + FT), Ditto [25],
as well as partial model personalization methods, including
FedPAC [44], FedBABU [33], FedRep [7], FedPer [5], and
LG-FedAvg [27].
FL Settings & Hyperparameters. We consider the typ-
ical cross-silo setting [28] with dozens of clients and full-
client participation. For the CIFAR-10 and CIFAR-10C ex-Method CIFAR10 CIFAR10-C Mini-ImageNet Officehome
Local Only 74.60 69.50 34.98 75.60
FedAvg 27.70 21.35 8.06 74.35
FedAvg + FT 75.30 66.65 30.78 76.84
Ditto 72.75 63.70 32.30 77.99
FedPAC 77.20 69.50 37.72 66.61
FedRep 67.60 62.50 34.71 80.07
FedPer 75.40 65.95 24.90 69.01
FedBABU 75.45 64.95 6.99 65.63
LG-FedAvg 77.65 68.55 36.68 78.75
FedSelect 82.25 72.05 38.69 80.51
Table 1. Personalized accuracy of different methods on four
datasets. F EDSELECT achieves the highest personalized perfor-
mance.
periments, the number of clients N=|C|= 10 , with each
client assigned s= 2classes. In the Mini-ImageNet experi-
ment, we set N= 20 , with s= 10 classes assigned to each
client. Finally, the OfficeHome experiment involves N= 4
clients, one for each of the 4domain shifts in the dataset;
each client is allocated all 65 classes. We vary the personal-
ization limit, α, within [0.05,0.30,0.50,0.80]. Further de-
tails on hyperparameter tuning for both F EDSELECT and
the compared baselines are provided in Appendix A.
Evaluation Metric. For all methods, the mean accuracy
of the final model(s) across individual client data distri-
butions calculated at the final communication round is re-
ported. For FedAvg, accuracy is reported for a single global
model. However, for other methods that learn personalized
client models, the final average accuracy is reported by av-
eraging individual clients’ personalized model accuracies.
Training Settings. We use standard SGD for performing
local client optimization for all methods based on their re-
spective training objective. To fairly compare the perfor-
mance of these methods, we fix the number of local train-
ing epochs across all methods to 3. The number of com-
munication rounds Tis set to 200for all experiments ex-
cept OfficeHome, where T= 30 . For both CIFAR-10 and
CIFAR10-C, each client is given 100 training samples. For
Mini-ImageNet, we use 20% of the total training data, sam-
pled for 20 clients in a non-iid manner. The 4 clients in the
OfficeHome experiment were given the full training parti-
tion for each of their corresponding domains, resulting in
about 2,000training samples per client. Further details on
our data partition are provided in Appendix A.
5.2. Results
Personalization Performance. We report the results on
four datasets in Table 1. In all cases, F EDSELECT achieves
state-of-the-art results, surpassing baseline methods with an
average improvement of over 2.4%. It is also clear that
23990
Figure 2. Test accuracy across communication rounds of F EDSELECT and baselines under the experimental settings in Table 1. F EDSE-
LECT outperforms all baselines and exhibits more stable convergence.
Method CIFAR10 CIFAR10-C Mini-ImageNet Officehome
FedSelect ( α= 0.05) 80.10 68.30 37.60 78.46
FedSelect ( α= 0.30)82.25 70.60 37.84 80.51
FedSelect ( α= 0.50) 82.20 72.05 38.69 76.08
FedSelect ( α= 0.80) 81.40 71.40 34.51 76.65
Table 2. Performance of F EDSELECT when varying the personal-
ization limit α.
Variant CIFAR-10 CIFAR10-C Mini-ImageNet OfficeHome
Personalize Least 79.87 64.20 35.65 78.23
Layer A 79.96 65.76 35.43 77.06
Layer B 78.36 62.81 32.74 79.08
Layer C 79.24 64.82 33.33 79.64
Layer D 78.08 62.50 34.91 79.11
Random 79.73 61.12 33.28 76.33
FedSelect ( α= 0.30) 82.25 70.60 37.84 80.51
FedSelect ( α= 0.50) 82.20 72.05 38.69 76.08
Table 3. Ablation study for three variants: Personalize Least refers
to the inverse of our hypothesis (personalize parameters with the
least variation); Layer A/B/C/D refers to personalizing specific in-
ternal layers of ResNet18; Random refers to choosing a random
partition between global and personal parameters.
our method is scalable to various feature/label shifts, evi-
denced by consistent performance across CIFAR10-C and
Mini-ImageNet experiments. In contrast, the performance
of methods such as FedBABU, FedPer, FedRep, and Ditto
degrades significantly for these label/feature-shift experi-
ments. We also note that FedAvg with fine-tuning (Fe-
dAvg + FT) performs competitively with other PFL base-
lines, which has also been previously observed in [7, 44].
The consistent superiority of F EDSELECT demonstrates the
benefits of learning which parameters to personalize, while
fine-tuning them. We also observe in the test accuracy con-
vergence plots in Figure 2 that our method converges more
smoothly to its final test accuracy than the other baselines.
Early convergence in FL is useful for enabling early stop-
ping and preventing further communication costs.
Effect of Personalization Limit α.In Table 2, we find
that adjusting the personalization limit αenables the per-
formance of F EDSELECT to be tuned under different client
Figure 3. Personalized performance on CIFAR-10 with different
local training data size and shard s= 2. FEDSELECT outperforms
prior methods.
data distributions. Recall from Section 4.4 that the two ex-
tremes of F EDSELECT ,α= 0 andα= 1.0, perform Fe-
dAvg and eventual local training, respectively. The results
in Table 2 suggest that choosing a middle-ground between
personalizing and globally averaging parameters is benefi-
cial. In particular, we recommend α∈[0.3,0.5]as suitable
for most heterogenous client distributions. We present re-
sults for other values of αin Appendix B.
Effect of Training Data Size. We conduct an additional
set of experiments on CIFAR-10 to demonstrate the per-
formance of F EDSELECT and the aforementioned baseline
methods as the size of training data increases. The purpose
of this experiment is to stress-test the performance of the al-
gorithms under significantly limited data settings, as well as
consistency in performance as the client training sets scale
in size. The results shown in Figure 3 showcase the robust-
ness of F EDSELECT to a setting with significantly fewer
data samples, whereas other baselines like FedBABU and
FedPer degrade to as little as 33.1% and 55.7%, respec-
tively. Full table results for this experiment are included
in Appendix B.
23991
T extFigure 4. Normalized intersection-over-union (IoU) overlap of the subnetwork masks mkin the final round in the ResNet18 final linear
layer for each client in the CIFAR10 experiments from Table 2. Increasing αis shown from left-to-right. Each client was assigned 2
classes; the class labels are shown along the rows and columns of each matrix. Clients with similar labels develop similar subnetworks;
increasing αresults in more personalized parameters, but less distinct subnetworks.
Ablation Study. We perform ablation experiments to ver-
ify the design choices behind our proposed algorithm. De-
picted in Table 3, we test the following three variants of
parameter-wise FL algorithms. First, we address the in-
verse of our hypothesis, which is to personalize parameters
that change the least (Personalize Least). Second, we test 4
different layers (denoted A, B, C, & D) in ResNet18 for in-
dividual personalization. In the case of the layer-wise study,
we also note that baseline algorithms like FedRep and Fed-
BABU are versions of parameter-wise PFL where the final
linear layer is decoupled. We provide further details on the
set of layers studied in Appendix B. Third, we test a ran-
dom partition of personal vs. global parameters. In Table
3, we observe that all ablations perform worse than F EDS-
ELECT , which further supports our FL Gradient-based Lot-
tery Ticket Hypothesis.
Subnetwork Development During FL. While raw test
accuracy of the final communication rounds provides use-
ful insights into the performance of F EDSELECT , we also
seek to visualize the behavior of the clients’ collaboration
of both parameters and subnetworks. In Figure 4, we show-
case the similarity of subnetwork structures across clients
based on their label distributions. We observe that clients
that share at least one label have exhibit significant overlap
in their subnetwork masks. We also note that the increased
overlap in subnetwork parameters due to increasing αre-
sults in less distinct but more locally trained parameters.
6. Limitations
In this work, we mainly focus on improving the personal-
ization performance of FL. Nevertheless, personalized fed-
erated learning faces challenges in balancing personaliza-
tion and model generalization (e.g., test-time distribution
shifts), as local updates may lead to overfitting and biased
outcomes. Heterogeneous datasets contribute to diverse
knowledge for FL, while security risks such as adversarialattacks and model poisoning persist. The decentralized na-
ture introduces communication overhead and resource de-
mands, impacting scalability and real-time responsiveness.
Ongoing research is crucial to address these limitations and
strike a balance between personalization, efficiency, model
robustness, and privacy in federated learning systems.
7. Conclusion
In this work, we propose FedSelect, an approach that
adaptively selects model parameters for personalization
while concurrently conducting global aggregations on the
remaining parameters in personalized federated learning.
FedSelect represents a significant stride towards achieving a
harmonious balance between individual customization and
collective model performance while reducing communica-
tion costs. By dynamically tailoring specific parameters to
local data characteristics, FedSelect mitigates the risk of
overfitting and enhances personalization. Simultaneously,
its global aggregation mechanism ensures the model main-
tains robust and generalized performance across the entire
federated network. Finally, we evaluated FedSelect on
multiple datasets with different learning setups and showed
that it outperforms previous approaches by a significant
margin. The impressive performance of FedSelect paves
the way for intriguing future research directions in this
domain.
Acknowledgements. We thank Bo Li for the initial dis-
cussion and constructive suggestions. We also thank the
National Center for Supercomputing Applications (NCSA)
and Illinois Campus Cluster Program (ICCP) for supporting
our computing needs. This work used NVIDIA GPUs at
NCSA Delta through allocations CIS230117 from the Ad-
vanced Cyberinfrastructure Coordination Ecosystem: Ser-
vices & Support (ACCESS) program, which is supported
by NSF Grants #2138259, #2138286, #2138307, #2137603,
and #2138296.
23992
References
[1] Durmus Alp Emre Acar, Yue Zhao, Ramon Matas, Matthew
Mattina, Paul Whatmough, and Venkatesh Saligrama. Fed-
erated learning based on dynamic regularization. In Interna-
tional Conference on Learning Representations , 2021. 2
[2] Idan Achituve, Aviv Shamsian, Aviv Navon, Gal Chechik,
and Ethan Fetaya. Personalized federated learning with gaus-
sian processes. Advances in Neural Information Processing
Systems , 34:8392–8406, 2021. 2
[3] Alekh Agarwal, John Langford, and Chen-Yu Wei. Feder-
ated residual learning. ArXiv , abs/2003.12880, 2020. 2
[4] Mohammed Aledhari, Rehma Razzak, Reza M Parizi, and
Fahad Saeed. Federated learning: A survey on enabling
technologies, protocols, and applications. IEEE Access , 8:
140699–140725, 2020. 2
[5] Manoj Ghuhan Arivazhagan, Vinay Aggarwal, Aaditya Ku-
mar Singh, and Sunav Choudhary. Federated learning with
personalization layers, 2019. 1, 3, 6
[6] Fei Chen, Mi Luo, Zhenhua Dong, Zhenguo Li, and Xi-
uqiang He. Federated meta-learning with fast convergence
and efficient communication, 2019. 2
[7] Liam Collins, Hamed Hassani, Aryan Mokhtari, and Sanjay
Shakkottai. Exploiting shared representations for personal-
ized federated learning. In International Conference on Ma-
chine Learning , pages 2089–2099. PMLR, 2021. 1, 2, 3, 5,
6, 7
[8] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In 2009 IEEE Conference on Computer Vision and
Pattern Recognition , pages 248–255, 2009. 6
[9] Moming Duan, Duo Liu, Xinyuan Ji, Yu Wu, Liang Liang,
Xianzhang Chen, and Yujuan Tan. Flexible clustered fed-
erated learning for client-level data distribution shift. IEEE
Transactions on Parallel and Distributed Systems , 33:2661–
2674, 2021. 2
[10] Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. Per-
sonalized federated learning: A meta-learning approach,
2020. 2
[11] Jonathan Frankle and Michael Carbin. The lottery ticket hy-
pothesis: Finding sparse, trainable neural networks. In Inter-
national Conference on Learning Representations , 2019. 1,
3
[12] Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M.
Roy, and Michael Carbin. Linear mode connectivity and the
lottery ticket hypothesis, 2020. 1
[13] Avishek Ghosh, Jichan Chung, Dong Yin, and Kannan Ram-
chandran. An efficient framework for clustered federated
learning. IEEE Transactions on Information Theory , 68:
8076–8091, 2020. 2
[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition, 2015. 6
[15] Dan Hendrycks and Thomas Dietterich. Benchmarking neu-
ral network robustness to common corruptions and perturba-
tions. Proceedings of the International Conference on Learn-
ing Representations , 2019. 6[16] Tzu-Ming Harry Hsu, Hang Qi, and Matthew Brown. Fed-
erated visual classification with real-world data distribution,
2020. 2
[17] Yihan Jiang, Jakub Kone ˇcn´y, Keith Rush, and Sreeram Kan-
nan. Improving federated learning personalization via model
agnostic meta learning, 2023. 2
[18] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel
Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran
Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-
Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Ku-
maran, and Raia Hadsell. Overcoming catastrophic for-
getting in neural networks. Proceedings of the National
Academy of Sciences , 114(13):3521–3526, 2017. 2
[19] Alex Krizhevsky. Learning multiple layers of features from
tiny images. 2009. 6
[20] Yoonho Lee, Annie S. Chen, Fahim Tajwar, Ananya Kumar,
Huaxiu Yao, Percy Liang, and Chelsea Finn. Surgical fine-
tuning improves adaptation to distribution shifts, 2023. 3
[21] Ang Li, Jingwei Sun, Binghui Wang, Lin Duan, Sicheng
Li, Yiran Chen, and Hai Li. Lotteryfl: Personalized
and communication-efficient federated learning with lottery
ticket hypothesis on non-iid datasets, 2020. 2, 3
[22] Q. Li, Zeyi Wen, Zhaomin Wu, and Bingsheng He. A survey
on federated learning systems: Vision, hype and reality for
data privacy and protection. IEEE Transactions on Knowl-
edge and Data Engineering , 35:3347–3366, 2019. 2
[23] Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia
Smith. Federated learning: Challenges, methods, and future
directions. IEEE Signal Processing Magazine , 37(3):50–60,
2020. 1
[24] Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi,
Ameet Talwalkar, and Virginia Smith. Federated optimiza-
tion in heterogeneous networks, 2020. 2
[25] T. Li, S. Hu, A. Beirami, and V . Smith. Ditto: Fair and robust
federated learning through personalization. In International
Conference on Machine Learning , pages 6357–6368. PMLR,
2021. 6
[26] Xiaoxiao Li, Meirui JIANG, Xiaofei Zhang, Michael Kamp,
and Qi Dou. Fedbn: Federated learning on non-iid features
via local batch normalization. In International Conference
on Learning Representations , 2021. 3
[27] Paul Pu Liang, Terrance Liu, Liu Ziyin, Nicholas B Allen,
Randy P Auerbach, David Brent, Ruslan Salakhutdinov, and
Louis-Philippe Morency. Think locally, act globally: Fed-
erated learning with local and global representations. arXiv
preprint arXiv:2001.01523 , 2020. 1, 3, 6
[28] Ziyu Liu, Shengyuan Hu, Zhiwei Steven Wu, and Virginia
Smith. On privacy and personalization in cross-silo federated
learning, 2022. 6
[29] Yishay Mansour, Mehryar Mohri, Jae Ro, and
Ananda Theertha Suresh. Three approaches for per-
sonalization with applications to federated learning, 2020.
2
[30] Michael McCloskey and Neal J. Cohen. Catastrophic inter-
ference in connectionist networks: The sequential learning
problem. Psychology of Learning and Motivation , 24:109–
165, 1989. 2
23993
[31] Brendan McMahan, Eider Moore, Daniel Ramage, Seth
Hampson, and Blaise Aguera y Arcas. Communication-
efficient learning of deep networks from decentralized data.
InProc. of Int’l Conf. Artificial Intelligence and Statistics
(AISTATS) , 2017. 1, 2, 6
[32] Vaikkunth Mugunthan, Eric Lin, Vignesh Gokul, Christian
Lau, Lalana Kagal, and Steve Pieper. Fedltn: Federated
learning for sparse and personalized lottery ticket networks.
InComputer Vision – ECCV 2022 , pages 69–85, Cham,
2022. Springer Nature Switzerland. 2
[33] Jaehoon Oh, SangMook Kim, and Se-Young Yun. Fed-
BABU: Toward enhanced representation for federated image
classification. In International Conference on Learning Rep-
resentations , 2022. 1, 6
[34] K. Pillutla, K. Malik, A. Mohamed, M. Rabbat, M. Sanjabi,
and L. Xiao. Federated learning with partial model personal-
ization. In International Conference on Machine Learning ,
2022. 2, 3, 4, 5, 6
[35] Alex Renda, Jonathan Frankle, and Michael Carbin. Com-
paring rewinding and fine-tuning in neural network pruning,
2020. 1
[36] Aviv Shamsian, Aviv Navon, Ethan Fetaya, and Gal Chechik.
Personalized federated learning using hypernetworks. In In-
ternational Conference on Machine Learning , pages 9489–
9502. PMLR, 2021. 2
[37] M. J. Sheller, B. Edwards, G. A. Reina, J. Martin, and S.
Bakas. Federated learning in medicine: facilitating multi-
institutional collaborations without sharing patient data. Sci-
entific Reports , 10(12598), 2020. 1
[38] Virginia Smith, Chao-Kai Chiang, Maziar Sanjabi, and
Ameet Talwalkar. Federated multi-task learning. ArXiv ,
abs/1705.10467, 2017. 2
[39] Benyuan Sun, Hongxing Huo, Yi Yang, and Bo Bai. Par-
tialfed: Cross-domain personalized federated learning via
partial initialization. Advances in Neural Information Pro-
cessing Systems , 34:23309–23320, 2021. 6
[40] Alysa Ziying Tan, Han Yu, Lizhen Cui, and Qiang Yang.
Towards personalized federated learning. IEEE Transactions
on Neural Networks and Learning Systems , 2022. 2
[41] Hemanth Venkateswara, Jos ´e Eus ´ebio, Shayok Chakraborty,
and Sethuraman Panchanathan. Deep hashing network for
unsupervised domain adaptation. 2017 IEEE Conference on
Computer Vision and Pattern Recognition (CVPR) , 2017. 6
[42] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray
Kavukcuoglu, and Daan Wierstra. Matching networks for
one shot learning. In Neural Information Processing Sys-
tems, 2017. 6
[43] Lixu Wang, Shichao Xu, Xiao Wang, and Qi Zhu. Address-
ing class imbalance in federated learning, 2020. 2
[44] Jian Xu, Xinyi Tong, and Shao-Lun Huang. Personalized
federated learning with feature alignment and classifier col-
laboration. In The Eleventh International Conference on
Learning Representations , 2023. 2, 5, 6, 7
[45] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson.
How transferable are features in deep neural networks? In
Neural Information Processing Systems , 2014. 3
[46] Tao Yu, Eugene Bagdasaryan, and Vitaly Shmatikov. Sal-
vaging federated learning by local adaptation, 2022. 2[47] Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon
Civin, and Vikas Chandra. Federated learning with non-iid
data. 2018. 2
23994
