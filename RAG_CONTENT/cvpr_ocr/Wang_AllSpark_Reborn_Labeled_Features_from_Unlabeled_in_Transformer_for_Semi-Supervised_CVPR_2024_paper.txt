AllSpark : Reborn Labeled Features from Unlabeled in Transformer for
Semi-Supervised Semantic Segmentation
Haonan Wang*, Qixiang Zhang*, Yi Li, Xiaomeng LiB
The Hong Kong University of Science and Technology
{hwanggr, qzhangcq, ylini }@connect.ust.hk, eexmli@ust.hk
Abstract
Semi-supervised semantic segmentation (SSSS) has been
proposed to alleviate the burden of time-consuming pixel-
level manual labeling, which leverages limited labeled data
along with larger amounts of unlabeled data. Current
state-of-the-art methods train the labeled data with ground
truths and unlabeled data with pseudo labels. However,
the two training flows are separate, which allows labeled
data to dominate the training process, resulting in low-
quality pseudo labels and, consequently, sub-optimal re-
sults. To alleviate this issue, we present AllSpark1, which
reborns the labeled features from unlabeled ones with the
channel-wise cross-attention mechanism. We further in-
troduce a Semantic Memory along with a Channel Se-
mantic Grouping strategy to ensure that unlabeled fea-
tures adequately represent labeled features. The AllSpark
shed new light on the architecture level designs of SSSS
rather than framework level, which avoids increasingly
complicated training pipeline designs. It can also be re-
garded as a flexible bottleneck module that can be seam-
lessly integrated into a general transformer-based segmen-
tation model. The proposed AllSpark outperforms ex-
isting methods across all evaluation protocols on Pas-
cal, Cityscapes and COCO benchmarks without bells-
and-whistles. Code and model weights are available at:
https://github.com/xmed-lab/AllSpark .
1. Introduction
Semantic segmentation is a dense prediction task that aims
to enable deep learning models to have class awareness [5,
30, 33, 48]. However, the success of such approaches
heavily relies on labor-intensive manual labeling. Semi-
Supervised Semantic Segmentation (SSSS) methods [6, 36,
*Equal contribution.
BCorresponding author.
1TheAllSpark is a powerful Cybertronian artifact in the film of Trans-
formers: Revenge of the Fallen , which can be used to reborn the Trans-
formers. It aligns well with our core idea.
Unlabeled data flowLabeled data flow
Unlabeled data flowLabeled data flowSeparatedDominance of labeled flow(a) Previous(b) Ours‚Ä¶‚Ä¶
Intervene‚Ä¶‚Ä¶
Label-Unlabeled margin
SupOnlyPreviousOursErrorRate (%)
0.1112.469.320.255.445.540.804.635.11
Labeled SetUnlabeled SetValidation Set(c) Illustration of the dominance of labeled data issue
Label-Val marginGT (Precise)
Pseudo label (Coarse)Figure 1. (a)(b) Comparison between the training data flows of
previous methods and ours. Previous methods separate the labeled
and unlabeled data training flows thus leading to dominance of the
labeled data. (c) Dominance of labeled data issue of the previous
method ( e.g., UniMatch [50]). The red margin denotes how the la-
beled data overwhelm the unlabeled; Larger gray margin indicates
the model may over-fit to labeled data.
40, 41, 45, 50, 59], which are built upon the principles of
semi-supervised learning [1, 22, 34, 54], are well-studied
to train models using a limited amount of labeled data in
conjunction with a larger amount of unlabeled data.
Most state-of-the-art SSSS methods [23, 24, 26, 31, 50]
are based on the pseudo labeling [22, 34] scheme, where the
unlabeled data is assigned pseudo labels based on model
predictions. The model is then iteratively trained using
these pseudo labeled data as if they were labeled examples.
Based on the training flows of the labeled and unlabeled
data, the current state-of-the-arts can be mainly divided into
two categories: (1) successive , the most representative one
is the teacher-student-based framework [17, 20, 36, 44, 45,
56], where a student model is first trained with labeled data,
then a teacher model updated by the exponential moving av-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
3627
EncoderQueryingReborn
‚Ä¶
‚Ä¶
DecoderCchannels
‚Ä¶AllSparkFigure 2. Illustration of the core idea of AllSpark , which leverages
the unlabeled features to reborn the labeled ones. The regenerated
labeled features exhibit a high level of precision, yet they also pos-
sess diversity compared to the original features.
eraging (EMA) of the student is used to generate the pseudo
labels; (2) parallel , represented by CPS (Cross Pseudo Su-
pervision) [6, 23, 46, 50], where the labeled data are con-
catenated with the unlabeled data within a mini-batch, and
the model optimize the two flows simultaneously through
consistency regularization.
One key characteristic of these state-of-the-art methods
is the separation of training flows for labeled and unlabeled
data, as shown in Figure 1(a). The separation of training
flows allows labeled data to dominate the training process
since they have labels; see Figure 1(c). However, these ap-
proaches have a common significant drawback: the model
used to generate pseudo labels for unlabeled images is heav-
ily influenced by a limited set of labeled samples, resulting
in low-quality pseudo labels and, consequently, sub-optimal
results. Unfortunately, previous research has overlooked
this crucial observation.
To this end, we present a novel approach: rather than
training directly on the labeled samples, we leverage the
unlabeled ones to generate labeled features (as depicted in
Figure 1(b)). The underlying rationale is that the labeled
data is far less than the unlabeled data, by reborning labeled
features from unlabeled data, we introduce diversity into
the labeled flow, creating a more challenging learning en-
vironment. Instead of relying solely on self-reconstruction,
the labeled branch learns to extract valuable channels from
the unlabeled features. The generic semantic information
within channels provides feasibility to this idea, for exam-
ple, as illustrated in Figure 3, the labeled sample of a ‚Äúboat‚Äù
and the unlabeled sample of ‚Äúairplanes‚Äù exhibit a similar
metal material texture.
The subsequent step is to determine an effective method
to reborn the labeled data. Fortunately, the Cross-Attention
mechanism [39], initially utilized in the transformer de-
coder, provides a suitable approach as it is designed to
reconstruct a target sequence using the source sequence.
Therefore, we develop our method based on the concept
of cross-attention, adapting it to operate in a channel-wise
manner [9, 42] in order to extract channel-wise semanticinformation. This method, named AllSpark , utilizes the la-
beled data features as queries and the unlabeled data fea-
tures as keys and values, as depicted in Figure 2. Specifi-
cally, AllSpark calculates a similarity matrix between each
channel of the labeled features and the unlabeled features.
The unlabeled channels with the highest similarities are
then emphasized to reconstruct the labeled features. How-
ever, a critical challenge in implementing this idea is the
limited unlabeled features within a mini-batch, which may
not be sufficient to reconstruct the correct labeled features.
To address this, we introduce a Semantic Memory (S-Mem)
that gathers channels from previous unlabeled features.
S-Mem are updated iteratively with a proposed channel-
wise semantic grouping strategy which first groups together
channels that represent the same semantics and then en-
queue them into according class slot of S-Mem.
Furthermore, naively plugging AllSpark into a CNN-
based backbone is not applicable, due to the smaller recep-
tion field of the convolution operation, as shown in Figure 8,
compared with that of the Vision Transformers (ViTs) [10],
which have made remarkable progress in semantic segmen-
tation [7, 18, 19, 35, 43, 48] with the well-known property
of capturing long-range dependencies. Thus, we focus on
building a pure-transformer-based SSSS framework.
Our contributions can be summarized in three folds:
‚Ä¢ We notice the separate training approach in current SSSS
frameworks will cause the dominance of labeled data and
thus lead to sub-optimal solutions.
‚Ä¢ We propose AllSpark module to address the labeled data
dominance issue and build a SOTA pure-transformer-
based SSSS framework, which can further be bene-
fit for efficiently training the foundation models, e.g.,
SAM [21], with less labeled data.
‚Ä¢ Extensive experiments have been conducted to validate
the effectiveness of our proposed AllSpark . The results of
these experiments have demonstrated solid performance
gains across three widely recognized benchmarks: PAS-
CAL VOC 2012 [11], Cityscapes [8], and COCO [27].
2. Related Work
2.1. Semantic Segmentation
Semantic segmentation is a dense prediction tasks, and has
been significantly advanced in many real-world applica-
tions [4, 5, 30, 33, 55]. Most recently, Vision Transformers
(ViTs) [2, 3, 10, 13, 29, 43, 47, 51, 53] have remarkably
reformed this field. Concretely, SETR [57] adopts ViT as
a backbone to extract features, achieving impressive perfor-
mance. PVT [43] is the first work to introduce a pyramid
structure in Transformer. SegFormer [48], with hierarchical
MiT encoder and a lightweight All-MLP decoder, serves
as a solid baseline for semantic segmentation. These ViT-
based segmentation approaches demonstrate the potential of
3628
EncoderDecoderKeyChannel-wiseCross-AttnValueQueryLabeledImage
‚Ñí!‚Ñí"
UnlabeledImageGround Truth(Very Limited)
Pseudo LabelUpdating Semantic MemoryStop gradientLoss supervisionForward & back propagation
AllSpark
Probability maps‚Ä¶argmax
‚Ä¶
‚Ä¶
‚Ä¶
‚Ä¶ùíâùíçùíâùíñùíâ"ùíçùíâ"ùíñSemantic Memory (Figure 4)Replace(Train Only)
Self-Attn
Figure 3. Illustration of the proposed AllSpark , which can be regarded as a flexible bottleneck plugged in the middle of a general segmen-
tation model. In the training stage, the unlabeled features are replaced by the Semantic Memory (¬ß 3.3 & Figure 4 bottom). Moreover,
the probability maps is used for Channel Semantic Grouping strategy (¬ß 3.4 & Figure 4 top). In the inference stage, the cross-attention is
degraded to self-attention with the inputs as the hidden features of the test images.
a pure Transformer framework in dense prediction tasks.
However, when it comes to semi-supervised setting, it is
hard for such a framework to achieve the same advances.
The reason is that these Transformer-based methods have
weaker inductive bias compared with CNN-based methods
and heavily rely on large amount of training data [10]. Thus,
appropriate designs are desirable to train these methods in
low-data regime.
2.2. Semi-supervised Semantic Segmentation
Conventional semi-supervised semantic segmentation
(SSSS) methods focus on how to better leverage the
unlabeled data with framework level designs, and use
basic C NNs-based segmentation model as backbone.
Two most remarkable basic frameworks are the teacher-
student [20, 36, 44, 45, 56] and CPS (Cross Pseudo
Supervision) [6, 46, 50]. Specifically, in the teacher-
student-based framework, a student model is first trained
with labeled data, then a teacher model updated by the
exponential moving averaging (EMA) of the student is used
to generate the pseudo labels. In the CPS-based framework,
the labeled data are concatenated with the unlabeled data
within a mini-batch, and the model optimize the two flows
simultaneously with consistency regularization.
Recently, several works have explored Transformer-
based models to extend the border of SSSS. Specifically,
SemiCVT [17] presents an inter-model class-wise consis-
tency to complement the class-level statistics of CNNsand Transformer in a cross-teaching manner. Other meth-
ods [23, 25] utilize ViTs as a diverse model to CNNs in
CPS-based framework [6]. These methods can be summa-
rized as CNN-Transformer-based methods and the way they
utilize transformer are naive, hence restrict the full potential
of ViT. Unlike these methods, we shed new light on a pure-
transformer SSSS method with architecture level designs.
3.AllSpark : Reborn Labeled Features from
Unlabeled Features
3.1. Baseline and Overview
General semi-supervised semantic segmentation dataset
comprises limited labeled data and plenty of unlabeled
data. Assume that the entire dataset comprises of NL
labeled samples {(xl
i, yi)}NL
i=1andNUunlabeled samples
{xu
i}NU
i=1, where xi‚ààR3√óH√óWis the input volume and
yi‚ààRK√óH√óWis the ground-truth annotation with K
classes. Pseudo-labeling-base methods train the model with
ground truth supervision first, then generate pseudo labels
ÀÜywith the pretrained model as supervision to the unlabeled
data. The most basic objective function is formulated as:
L=Ls+Lu=1
NLNLX
i=0LCE(pl
i, yi)+1
NUNUX
j=0LCE(pu
j,ÀÜyj)
(1)
whereLCEis the cross-entropy loss.
Increasing methods have been proposed to extend this
3629
formulation, especially for the unsupervised loss, making
this formulation and the training pipeline complicated. In
this work, we focus on introducing architecture-level mod-
ifications to enhance the performance of pseudo-labeling in
semi-supervised semantic segmentation. We start with the
basic formulation of pseudo-labeling as our baseline and
then propose the AllSpark module, which is integrated be-
tween the encoder and decoder of the ViT-based segmenta-
tion model. The AllSpark module, as illustrated in Fig. 3
and Algorithm 1, consists a Channel-wise Cross-Attention
module (¬ß3.2), a Semantic Memory to store unlabeled fea-
tures for better reconstruction of the labeled features (¬ß3.3),
and a Channel-wise Semantic Grouping strategy to better
update the Semantic Memory (¬ß3.4).
3.2. Channel-wise Cross-attention: the Core of
AllSpark
To alleviate the dominance of the biased labeled data
brought by the separate training scheme, we propose
AllSpark to directly intervene in the labeled data training
flow with unlabeled data. Typically, different feature chan-
nels encode distinct semantic information. Compared to
patch tokens, these channel-wise features encompass richer
contextual information that is more general across various
input images (refer to Figure 7). We leverage this contextual
information from unlabeled data to reconstruct the features
of labeled data as a robust regularization using the channel-
wise cross-attention mechanism [9, 42]. Within this mecha-
nism, labeled data features serve as queries, while unlabeled
features serve as keys and values, as depicted in Figure 3.
Specifically, we calculate the similarity between each chan-
nel of the labeled features and the unlabeled features, and
the unlabeled channels with the highest similarities play a
more significant role in reconstructing the labeled features.
The underlying rationale is that even though the unlabeled
features may originate from different classes compared to
the labeled features, they still possess generic information
with channel wise that can be shared, such as textures.
Given the hidden features [hl, hu]of labeled and unla-
beled data after the encoder stage, we split them then set hl
asquery andhuaskeyandvalue in multi-head manner:
q=hlwq, k=huwk, v=huwv (2)
where wq, wk, wv‚ààRC√ó2Care transformation weights,
hl, hu‚ààRC√ód,dis the sequence length (number of
patches) and Cis the channel dimension. The channel-wise
attention is defined as:
ÀÜhl=Mv‚ä§=œÉ[œà(q‚ä§k)]v‚ä§wout (3)
where œà(¬∑)andœÉ(¬∑)denote the instance normalization [37]
and the softmax function, wout‚ààR2C√óC. In contrast to
the traditional self-attention mechanism, channel-wise at-
Semantic Memory (FIFO)‚Ä¶
‚Ñé!‚Ñé"‚Ñé#‚Ñé$
‚Ä¶
ùëù!ùëù"ùëù#ùëù%
‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶
ùëÜùëñùëö$,%
ùëÜùëñùëö!,!‚Ä¶
‚Ä¶
‚Ä¶
‚Ä¶
‚Ä¶
ùëÜùëñùëö!,%Probability maps
Enqueue
Class KClass 0Class 1Class 2
Channel-wise Semantic Grouping
resizeresizeresizeresizeSimilarity matrix
‚Ä¶Unlabeled tokensCchannels‚Ñé!‚Ñé"‚Ñé#‚Ñé$‚Ä¶ùëÜùëñùëö$,!ùëÜùëñùëö!,"Figure 4. Illustration of the Class-wise Semantic Memory Bank
(¬ß3.3) and the Channel-wise Semantic Grouping (¬ß3.4). Sim i,j
denotes the similarity between ithchannel of the unlabeled hidden
feature and the jthprobability map. The dash lines give some
visual examples of some channels. Take the column with red box
as an example, h0has the largest similarity with p1(Sim 0,1), so
it should be added to Class 1 slot of the Semantic Memory.
tention can capture long-range dependencies among chan-
nels. To further refine the hidden features of the unlabeled
data, we also apply a channel-wise self-attention. The for-
mulation to obtain the refined unlabeled feature ÀÜhu
iis sim-
ilar to Eq. 2 & 3, with the only difference being that qis
replaced with q=huwq. The reconstructed ÀÜhl
iandÀÜhu
iare
subsequently fed into the decoder to generate the final pre-
dictions.
3.3. Semantic Memory for Enlarging the Feature
Space of AllSpark
Directly utilizing the unlabeled features within a single
mini-batch is not enough to effectively reborn the labeled
features. To overcome this limitation, we need to expand
3630
Algorithm 1 PyTorch-style pseudocode for
AllSpark module.
# Take Pascal dataset as an example, where the
crop size is 513√ó513
# channel num = 512, token num = 289, class num=21
# pu‚ààR289√ó21: probability token
pu = flatten(decoder.forward(h u).resize(17, 17))
# hl‚ààR289√ó512: labeled hidden feature map
# hu‚ààR289√ó512: unlabeled hidden feature map
def cross attn((h l,y), h u, p u):
# channel2class similarity: 21 √ó512
simprob chl = mm(p u.T, h u) # see eq (4)
# channel pseudo label: 1 √ó512
chlpseu = argmax(sim prob chl, dim=0)
for cls in range(21):
# group channels belonging to class cls
index = nonzero(where(chl pseu==cls, 1, 0))
chls = h u(:, index)
Nu = len(index)
# enqueue channels into semantic memory
enqueue(S-Mem, chls, cls)
dequeue(S-Mem, cls)
# queries: 289 √ó512
q = l q.forward(h l)
# Keys: 289 √ó512
k = l k.forward(flatten(S-Mem(:,:,:N u))
# values: 289 √ó512
v = l v.forward(flatten(S-Mem(:,:,N u:))
# channel similarity: 512 √ó512
m = mm(q.T, k)
# reborn labeled feature: 289 √ó512
qre = mm(m, v.T).T
return q re
the unlabeled feature space. To achieve this, we introduce
a first-in-first-out queue, Class-wise Semantic Memory (S-
Mem), to store a significant number of unlabeled features,
as shown in Figure 4 bottom. This memory allows us to
efficiently access a broader range of unlabeled features for
the reconstruction process.
S-Mem has a shape of RK√óC√ód, where Krepresents the
number of classes. For each class, S-Mem stores Cchan-
nels, and each channel consists of dpatches. During the
training process, we utilize the semantic memory to replace
the original keyandvalue components of the unlabeled fea-
tures ( kTandvTin Eq. 3). In the next sub-section ¬ß3.4,
we demonstrate how to update each class slot with accurate
channels containing class-specific semantic information.
3.4. Channel-wise Semantic Grouping
Storing previous unlabeled features in a naive manner is not
suitable for semantic segmentation tasks, as classes are of-
ten imbalanced. To ensure sufficient semantic information
for each class, it is necessary to establish a class-balanced
semantic memory. Thus, we introduce a Channel-wise Se-
mantic Grouping strategy to determine the semantic repre-
sentation of each channel in the unlabeled feature, and then
group them and add into the corresponding class slot of S-
Mem, as illustrated in Figure 4. Specifically, we calculate
the similarity between the unlabeled feature hu‚ààRC√ód
and the probability token ÀÜp‚ààRK√ód. The probability to-ken is resized and reshaped vector obtained from the prob-
ability map p‚ààRK√óH√óW, which is the soft prediction
of the segmentation network and contains the overall se-
mantic information of hu. Once the semantic represen-
tation is determined for each channel, we can store them
in the memory according to their respective semantic cat-
egories. The similarity matrix Sim‚ààRK√óCis defined
as:Sim i,j=œà(ÀÜp‚ä§
jhu
i). In this way, by comparing the se-
mantic information of ith, i‚àà[0, C]channel and the over-
all semantic context of the image, we could determine the
most likely semantic category that the ithchannel repre-
sents. Then, we group together the channels that represent
the same semantic category and enqueue them into the ap-
propriate class slot within the semantic memory.
In the inference stage , although we can still utilize
the cross-attention mechanism with S-Mem, it often comes
with a computational burden and provides only marginal
improvements in performance. To enhance efficiency, we
opt to remove S-Mem and CSG in the inference stage. As
a result, the two inputs of cross-attention become identical,
which can be simplified to self-attention.
4. Experiments
4.1. Experimental Setup
Datasets. We evaluate our AllSpark on three widely-used
datasets: PASCAL VOC 2012 [11] is a dataset consisting
of around 4,000 samples. This dataset is partitioned into
three subsets: train, validation, and test. The train set com-
prises 1,464 images, the validation set contains 1,449 im-
ages, and the test set consists of 1,456 images. This dataset
includes pixel-level annotations for a total of 21 categories,
including the background. In accordance with the con-
ventions established in [6, 45], an additional set of 9,118
coarsely labeled images from the SBD dataset [14] is incor-
porated as a complement to the training data, which is re-
ferred to as the augmented set. The Cityscapes dataset [8],
is specifically designed for the purpose of urban scene un-
derstanding. It comprises a collection of 5,000 images that
have been meticulously annotated with fine-grained pixel-
level details. These images are divided into three sets: train,
validation, and test. The train set contains 2,975 images,
the validation set consists of 500 images, and the test set
includes 1,524 images. In the Cityscapes dataset, there
are 19 semantic categories that are organized into 6 super-
classes. COCO [27] stands out for its extensive annotations
and wide variety of object categories. The dataset includes
118,000 training images and 5,000 validation images. It
covers 80 object categories and encompasses both indoor
and outdoor scenes.
Implementation Details. For the three datasets, we all uti-
lized SGD as the optimizer, and the poly scheduling to ad-
just the learning rate as lr=lrinit¬∑(1‚àíi
I)0.9, where lrinit
3631
PASCAL original 1/16 (92) 1/8 (183) 1/4 (366) 1/2 (732) Full(1464)
SupBaseline-CNN 45.1 55.3 64.8 69.7 73.5
SupBaseline-ViT 50.65 63.62 70.76 75.44 77.01
CutMixSeg [12] [BMVC‚Äô20] 55.58 63.20 68.36 69.84 76.54
PseudoSeg [59] [ICLR‚Äô21] 57.60 65.50 69.14 72.41
CPS [6] [CVPR‚Äô21] 64.07 67.42 71.71 75.88 -
PC2Seg [58] [ICCV‚Äô21] 57.00 66.28 69.78 73.05 74.15
PS-MT [28] [CVPR‚Äô22] 65.80 69.58 76.57 78.42 80.01
ST++ [49] [CVPR‚Äô22] 65.20 71.00 74.60 77.30 79.10
U2PL [45] [CVPR‚Äô22] 67.98 69.15 73.66 76.16 79.49
GTA-Seg [20] [NeurIPS‚Äô22] 70.02 73.16 75.57 78.37 80.47
SemiCVT [17] [CVPR‚Äô23] 68.56 71.26 74.99 78.54 80.32
FPL [32] [CVPR‚Äô23] 69.30 71.72 75.73 78.95 -
CCVC [46] [CVPR‚Äô23] 70.2 74.4 77.4 79.1 80.5
AugSeg [56] [CVPR‚Äô23] 71.09 75.45 78.8 80.33 81.36
DGCL [44] [CVPR‚Äô23] 70.47 77.14 78.73 79.23 81.55
UniMatch [50] [CVPR‚Äô23] 75.2 77.19 78.8 79.9 -
ESL [31] [ICCV‚Äô23] 70.97 74.06 78.14 79.53 81.77
LogicDiag [26] [ICCV‚Äô23] 73.25 76.66 77.93 79.39 -
AllSpark (Ours) 76.07 78.41 79.77 80.75 82.12
Table 1. Comparison with SOTAs on Pascal original dataset.
Cityscapes 1/16 (186) 1/8 (372) 1/4 (744) 1/2 (1488)
SupBaseline-CNN 66.3 72.8 75.0 78.0
SupBaseline-ViT 65.69 70.52 74.01 78.14
CPS [6] [CVPR‚Äô21] 69.78 74.31 74.58 76.81
AEL [16] [NeurIPS‚Äô21] 74.45 75.55 77.48 79.01
ST++ [49] [CVPR‚Äô22] 67.64 73.43 74.64 77.78
U2PL [45] [CVPR‚Äô22] 74.90 76.48 78.51 79.12
GTA-Seg [20] [NeurIPS‚Äô22] 69.38 72.02 76.08 -
SemiCVT [17] [CVPR‚Äô23] 72.19 75.41 77.17 79.55
FPL [32] [CVPR‚Äô23] 75.74 78.47 79.19 -
CCVC [46] [CVPR‚Äô23] 74.9 76.4 77.3 -
AugSeg [56] [CVPR‚Äô23] 75.22 77.82 79.56 80.43
DGCL [44] [CVPR‚Äô23] 73.18 77.29 78.48 80.71
UniMatch [50] [CVPR‚Äô23] 76.6 77.9 79.2 79.5
ESL [31] [ICCV‚Äô23] 75.12 77.15 78.93 80.46
LogicDiag [26] [ICCV‚Äô23] 76.83 78.90 80.21 81.25
CFCG [24] [ICCV‚Äô23] 77.28 79.09 80.07 80.59
AllSpark (Ours) 78.33 79.24 80.56 81.39
Table 2. Comparison with SOTAs on Cityscapes dataset.
represents the initial learning rate, iis the current iteration,
andIis the max number of iterations. For the PASCAL VOC
2012 dataset , we set the initial learning rate as 0.001. Please
note that the learning rate for the AllSpark and the decoder
is 5 times of the backbone network. We train the model for
80 epochs. For the Cityscapes dataset , the initial learning
rate is set as 0.005, and we train the model for 240 epochs.
It is worth noting that for the Cityscapes dataset, we employ
two approaches used in previous works [6, 16, 28, 45, 50],
i.e., online hard example mining (OHEM) and sliding win-
dow evaluation. For the COCO dataset , we set the initial
learning rate of 0.001, and train the model for 10 epochs.
Baselines. We report two supervised baselines among dif-
ferent ratios of labeled data: a CNN-based model, RestNet-
101 [15] with DeepLabV3+ [5], referred to as SupBaseline-
CNN , and a ViT-based model, SegFormer-B5 [48], referred
to as SupBaseline-ViT . Additionally, we employ a sim-
ple pseudo-labeling scheme outlined in section ¬ß3.1 withPASCAL augmented 1/16 (662) 1/8 (1323) 1/4 (2646) 1/2 (5291)
SupBaseline-CNN 67.5 71.1 74.2 -
SupBaseline-ViT 72.01 73.20 76.62 77.61
CutMixSeg [12] [BMVC‚Äô20] 72.56 72.69 74.25 75.89
PseudoSeg [59] [ICLR‚Äô21] - - - -
AEL [16] [NeurIPS‚Äô21] 77.20 77.57 78.06 80.29
CPS [6] [CVPR‚Äô21] 72.18 75.83 77.55 78.64
PC2Seg [58] [ICCV‚Äô21] - - - -
PS-MT [28] [CVPR‚Äô22] 75.50 78.20 78.72 79.76
ST++ [49] [CVPR‚Äô22] 74.70 77.90 77.90 -
FPL [32] [CVPR‚Äô23] 74.98 77.75 78.30 -
CCVC [46] [CVPR‚Äô23] 77.2 78.4 79.0 -
AugSeg [56] [CVPR‚Äô23] 77.01 78.20 78.82 -
DGCL [44] [CVPR‚Äô23] 76.61 78.37 79.31 80.96
UniMatch [50] [CVPR‚Äô23] 78.1 78.4 79.2 -
ESL [31] [ICCV‚Äô23] 76.36 78.57 79.02 79.98
CFCG [24] [ICCV‚Äô23] 76.82 79.10 79.96 80.18
DLG [23] [ICCV‚Äô23] 77.75 79.31 79.14 79.54
AllSpark (Ours) 78.32 79.98 80.42 81.14
U2PL‚Ä†[45] [CVPR‚Äô22] 77.21 79.01 79.30 80.50
GTA-Seg‚Ä†[20] [NeurIPS‚Äô22] 77.82 80.47 80.57 81.01
SemiCVT‚Ä†[17] [CVPR‚Äô23] 78.20 79.95 80.20 80.92
UniMatch‚Ä†[50] [CVPR‚Äô23] 80.94 81.92 80.41 -
LogicDiag‚Ä†[26] [ICCV‚Äô23] 79.65 80.24 80.62 81.00
AllSpark‚Ä†(Ours) 81.56 82.04 80.92 81.13
Table 3. Comparison with SOTAs on the Pascal augmented
dataset. ‚Ä†means using the same split as U2PL [45].
COCO 1/512 (232) 1/256 (463) 1/128 (925) 1/64 (1849)
SupBaseline-CNN 22.94 27.96 33.60 37.80
SupBaseline-ViT 19.66 26.72 35.90 40.89
PseudoSeg [59] [ICLR‚Äô21] 29.78 37.11 39.11 41.75
PC2Seg [58] [ICCV‚Äô21] 29.94 37.53 40.12 43.67
MKD [52] [ACM MM‚Äô23] 30.24 38.04 42.32 45.50
UniMatch [50] [CVPR‚Äô23] 31.86 38.88 44.35 48.17
LogicDiag [26] [ICCV‚Äô23] 33.07 40.28 45.35 48.83
AllSpark (Ours) 34.10 41.65 45.48 49.56
Table 4. Comparison with SOTAs on different partitions of the
COCO dataset.
SegFormer-B5 as our semi-supervised baseline, denoted as
SemiBaseline .
4.2. Comparison with Existing Methods
PASCAL VOC 2012 original. Table 1 presents a sum-
mary of the quantitative comparisons across different label
amounts. From this analysis, we make an important obser-
vation: our method consistently outperforms the previous
state-of-the-art approaches in all scenarios. This highlights
the effectiveness of AllSpark and establishes it as the new
state-of-the-art.
PASCAL VOC 2012 augmented. Table 3 showcases the
comparison results using the augmented training set. Once
again, our method establishes new state-of-the-art perfor-
mance across all partition protocols. It achieves an average
gain of 0.47. Furthermore, we conducted a comparison us-
ing the same setting as U2PL [45], where all labeled data is
of high quality. In this scenario, AllSpark demonstrates an
3632
ImageBefore AllSparkAfter AllSparkFigure 5. Visualization of labeled feature channels before and after
theAllSpark with the same indexes. The features before AllSpark
focus on more similar regions, while those after AllSpark focus on
different objects or context.
even greater performance gain.
Cityscapes. Table 2 quantitatively compares our method
against the competitors on Cityscapes val. In spite of the
presence of complex street scenes, our method still delivers
a solid overtaking trend across different partitions.
COCO. In Table 4, we showcase the model performance on
the COCO validation set. Notably, leveraging the extensive
semantic hierarchy present in COCO, AllSpark achieves
significant performance gains over the leading method in
all partitions. These experimental results further validate
the effectiveness of AllSpark .
SemiBaselineAllSpark 1/8 1/2
CCA S-Mem CSG (183) (732)
‚úì 68.71 74.40
‚úì ‚úì 76.77 78.93
‚úì ‚úì ‚úì 77.62 79.71
‚úì ‚úì ‚úì ‚úì 78.41 80.75
Table 5. Effectiveness of the proposed components on PASCAL
dataset. We leverage a basic pseudo-labeling scheme (¬ß3.1) with
the SegFormer [48] as our baseline, AllSpark consists of 3 hierar-
chy components: CCA (Channel-wise Cross-Attention), S-Mem
(Semantic Memory), and CSG (Channel Semantic Grouping).
4.3. Ablation Studies
Effectiveness of the AllSpark Module. We analyze the ef-
fectiveness of different components in AllSpark ,i.e., the
channel-wise cross attention mechanism (CCA), seman-
tic memory (S-Mem) and channel-wise similarity-guided
grouping strategy (CSG) as in Table 5. The three com-
020406080100120backgroundaeroplanebicyclebirdboatbottlebuscarcatchaircowtabledoghorsemotorbikepersonpotted plantsheepsofatrainTVChannel NumberCategory
020406080100120140160backgroundaeroplanebicyclebirdboatbottlebuscarcatchaircowtabledoghorsemotorbikepersonpotted plantsheepsofatrainTVChannel NumberCategorydogboat
Figure 6. Ablation on the effectiveness of the similarity-guided
channel grouping strategy. The y-axis of the bar chart denotes the
number of channels to be selected and added to the corresponding
slot of the Semantic Memory.
ponent are hierarchy, i.e., S-Mem is built upon CCA, the
core of AllSpark , and CSG strategy is built for the update
of S-Mem. According to the results, the core of AllSpark ,
channel-wise cross-attention mechanism (the second row),
helps our method outperform the previous methods, push-
ing the performance about 8.06%/4.53% higher than the
baseline (the first row). Further, the semantic mem-
ory brings about 0.85%/0.78% performance improvements.
With all of these components, our method outperforms the
baseline by over 9.70%/5.95% in mIoU. Furthermore, we
also provide visual verification of AllSpark module, as
shown in Figure 5. The reconstructed labeled features af-
terAllSpark are quite diverse compared with the original
features, which is useful for alleviating the dominance of
labeled data. Moreover, the features tend to be closer to
the target due to the larger feature space compared with a
self-attention with the features of itself.
The necessity of the Semantic Memory for enlarging un-
labeled feature space. As shown in Figure 7, we visualize
part of the Semantic Memory. Overall, the Semantic Mem-
ory stores quite accurate features related to the class slot
and indeed enlarges the unlabeled feature space not only in
the same classes with the current labeled data, but also the
features from other classes as negative samples when calcu-
lating the similarity matrix.
Ablation on the effectiveness of the Channel-wise Se-
mantic Grouping strategy. As shown in Table 5, com-
pared with blindly adding all channels to the semantic mem-
ory, using semantic grouping strategy obtains 0.79%/0.64%
performance gain. Figure 6 also verifies that this strategy
can assign accurate semantic labels to most channels. Fur-
3633
AeroplaneBicycleBirdBoatBottleBusCarCatChairDogPerson
‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶
‚Ä¶‚Ä¶
‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶Figure 7. Visualization of part of the the Semantic Memory. Each image denotes one channel. For better visualization, we run inference
on unlabeled training data, so that we can put the corresponding raw image under each channel.
Backbone CCVC [46] DGCL [44] UniMatch [50] Ours
R101+DeepLabV3+ 74.40 77.14 77.19 73.70
SegFormer-B4 71.07‚Üì 76.33‚Üì 76.28‚Üì 77.92‚Üë
SegFormer-B5 73.77‚Üì 76.72‚Üì 76.56‚Üì 78.41‚Üë
Table 6. Ablations on different Segmentation Backbones on orig-
inalPASCAL VOC 2012 dataset - 1/8 (183).
thermore, the examples in Figure 6 indicate that the pro-
posed strategy may add similar features from other classes,
e.g., some channels of the boat are added to some similar
classes, aeroplane ,bus,etc., which offers richer fea-
tures as references to reform other classes.
Why tailored for transformer-based method? It might be
argued that our improvements are due to a stronger back-
bone, SegFormer [48]. Considering this concern, we fur-
ther carry out a comparison with the previous SOTAs with
transformer backbones. As shown in Table 6, when switch-
ing to SegFormer-B4/B5, previous methods all have per-
formance drops, since the training of transformer requires
more annotated data to avoid over-fitting [38]. However, our
AllSpark also exhibits poorer performance with ResNet101
and DeepLabV3+. This can be attributed to the limited re-
ceptive fields of CNNs. As shown in Fig. 8, when analyzing
large objects ( e.g., car), the attention maps often prioritize
a few salient local parts ( e.g., wheels). Consequently, when
these features processed by AllSpark , there is a risk of erro-
neously querying channels that correspond to similar parts
of different objects ( e.g., wheels of an aeroplane landing
gear), leading to unnecessary noise and performance decay.
5. Conclusions
We notice the separate training approach in current SSSS
frameworks will cause the dominance of labeled data and
ImageTransformerConvolutional Neural Network
Figure 8. Comparison of Transformer and CNN feature channels.
thus lead to sub-optimal solutions. This separation hin-
ders the effective integration of ViT backbones with SSSS
frameworks. We propose AllSpark to alleviate this issue by
directly reborning the labeled features with the unlabeled
features. To further augment the unlabeled feature space,
we introduce a Semantic Memory (S-Mem) to store pre-
vious unlabeled features from different classes. Then, we
can leverage these comprehensive features from all classes
to reconstruct more accurate labeled features. To establish
the class-balanced S-Mem, we propose Channel Semantic
Grouping strategy, which assigns semantic information to
each channel based on its similarity with the probability
map. We present that equipped with the proposed AllSpark ,
a naive pseudo-labeling scheme can outperform the SOTAs.
Finally, AllSpark improves previous results remarkably on
three well-established benchmarks.
Acknowledgement. This work is partially supported
by the National Natural Science Foundation of China
under Grant 62306254, the Hong Kong Innovation
and Technology Fund under Grant ITS/030/21, and
grants from Foshan HKUST Projects under Grants
FSUST21-HKUST10E and FSUST21-HKUST11E.
3634
References
[1] David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas
Papernot, Avital Oliver, and Colin A Raffel. Mixmatch: A
holistic approach to semi-supervised learning. NeurIPS , 32,
2019. 1
[2] Han Cai, Chuang Gan, and Song Han. Efficientvit: Enhanced
linear attention for high-resolution low-computation visual
recognition. In ICCV , 2023. 2
[3] Chun-Fu Richard Chen, Quanfu Fan, and Rameswar Panda.
Crossvit: Cross-attention multi-scale vision transformer for
image classification. In ICCV , pages 357‚Äì366, 2021. 2
[4] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,
Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image
segmentation with deep convolutional nets, atrous convolu-
tion, and fully connected crfs. IEEE TPAMI , 40(4):834‚Äì848,
2017. 2
[5] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian
Schroff, and Hartwig Adam. Encoder-decoder with atrous
separable convolution for semantic image segmentation. In
ECCV , pages 801‚Äì818, 2018. 1, 2, 6
[6] Xiaokang Chen, Yuhui Yuan, Gang Zeng, and Jingdong
Wang. Semi-supervised semantic segmentation with cross
pseudo supervision. In CVPR , pages 2613‚Äì2622, 2021. 1, 2,
3, 5, 6
[7] Bowen Cheng, Alex Schwing, and Alexander Kirillov. Per-
pixel classification is not all you need for semantic segmen-
tation. NeurIPS , 34:17864‚Äì17875, 2021. 2
[8] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo
Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe
Franke, Stefan Roth, and Bernt Schiele. The cityscapes
dataset for semantic urban scene understanding. In CVPR ,
pages 3213‚Äì3223, 2016. 2, 5
[9] Mingyu Ding, Bin Xiao, Noel Codella, Ping Luo, Jingdong
Wang, and Lu Yuan. Davit: Dual attention vision transform-
ers. In ECCV , pages 74‚Äì92. Springer, 2022. 2, 4
[10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale. ICLR , 2021. 2, 3
[11] Mark Everingham, SM Ali Eslami, Luc Van Gool, Christo-
pher KI Williams, John Winn, and Andrew Zisserman. The
pascal visual object classes challenge: A retrospective. IJCV ,
111:98‚Äì136, 2015. 2, 5
[12] Geoff French, Timo Aila, Samuli Laine, Michal Mackiewicz,
and Graham Finlayson. Semi-supervised semantic segmen-
tation needs strong, high-dimensional perturbations. BMVC ,
2020. 6
[13] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu,
and Yunhe Wang. Transformer in transformer. NeurIPS , 34:
15908‚Äì15919, 2021. 2
[14] Bharath Hariharan, Pablo Arbel ¬¥aez, Lubomir Bourdev,
Subhransu Maji, and Jitendra Malik. Semantic contours from
inverse detectors. In 2011 international conference on com-
puter vision , pages 991‚Äì998. IEEE, 2011. 5
[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.Deep residual learning for image recognition. In CVPR ,
pages 770‚Äì778, 2016. 6
[16] Hanzhe Hu, Fangyun Wei, Han Hu, Qiwei Ye, Jinshi Cui,
and Liwei Wang. Semi-supervised semantic segmentation
via adaptive equalization learning. NeurIPS , 34:22106‚Äì
22118, 2021. 6
[17] Huimin Huang, Shiao Xie, Lanfen Lin, Ruofeng Tong, Yen-
Wei Chen, Yuexiang Li, Hong Wang, Yawen Huang, and
Yefeng Zheng. Semicvt: Semi-supervised convolutional vi-
sion transformer for semantic segmentation. In CVPR , pages
11340‚Äì11349, 2023. 1, 3, 6
[18] Jitesh Jain, Jiachen Li, Mang Tik Chiu, Ali Hassani, Nikita
Orlov, and Humphrey Shi. Oneformer: One transformer to
rule universal image segmentation. In CVPR , pages 2989‚Äì
2998, 2023. 2
[19] Jitesh Jain, Anukriti Singh, Nikita Orlov, Zilong Huang, Ji-
achen Li, Steven Walton, and Humphrey Shi. Semask: Se-
mantically masked transformers for semantic segmentation.
InICCV , pages 752‚Äì761, 2023. 2
[20] Ying Jin, Jiaqi Wang, and Dahua Lin. Semi-supervised se-
mantic segmentation via gentle teaching assistant. NeurIPS ,
35:2803‚Äì2816, 2022. 1, 3, 6
[21] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,
Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-
head, Alexander C Berg, Wan-Yen Lo, et al. Segment any-
thing. arXiv preprint arXiv:2304.02643 , 2023. 2
[22] Dong-Hyun Lee et al. Pseudo-label: The simple and efficient
semi-supervised learning method for deep neural networks.
InICML, Workshops , page 896, 2013. 1
[23] Peixia Li, Pulak Purkait, Thalaiyasingam Ajanthan, Ma-
jid Abdolshah, Ravi Garg, Hisham Husain, Chenchen Xu,
Stephen Gould, Wanli Ouyang, and Anton van den Hengel.
Semi-supervised semantic segmentation under label noise
via diverse learning groups. In ICCV , pages 1229‚Äì1238,
2023. 1, 2, 3, 6
[24] Shuo Li, Yue He, Weiming Zhang, Wei Zhang, Xiao Tan,
Junyu Han, Errui Ding, and Jingdong Wang. Cfcg: Semi-
supervised semantic segmentation via cross-fusion and con-
tour guidance supervision. In ICCV , pages 16348‚Äì16358,
2023. 1, 6
[25] Yijiang Li, Xinjiang Wang, Lihe Yang, Litong Feng, Wayne
Zhang, and Ying Gao. Diverse cotraining makes strong semi-
supervised segmentor. In ICCV , 2023. 3
[26] Chen Liang, Wenguan Wang, Jiaxu Miao, and Yi Yang.
Logic-induced diagnostic reasoning for semi-supervised se-
mantic segmentation. In ICCV , pages 16197‚Äì16208, 2023.
1, 6
[27] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll ¬¥ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context. In
ECCV , pages 740‚Äì755. Springer, 2014. 2, 5
[28] Yuyuan Liu, Yu Tian, Yuanhong Chen, Fengbei Liu,
Vasileios Belagiannis, and Gustavo Carneiro. Perturbed and
strict mean teachers for semi-supervised semantic segmenta-
tion. In CVPR , pages 4258‚Äì4267, 2022. 6
[29] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng
Zhang, Stephen Lin, and Baining Guo. Swin transformer:
3635
Hierarchical vision transformer using shifted windows. In
ICCV , pages 10012‚Äì10022, 2021. 2
[30] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully
convolutional networks for semantic segmentation. In
CVPR , pages 3431‚Äì3440, 2015. 1, 2
[31] Jie Ma, Chuan Wang, Yang Liu, Liang Lin, and Guanbin Li.
Enhanced soft label for semi-supervised semantic segmenta-
tion. In ICCV , pages 1185‚Äì1195, 2023. 1, 6
[32] Pengchong Qiao, Zhidan Wei, Yu Wang, Zhennan Wang,
Guoli Song, Fan Xu, Xiangyang Ji, Chang Liu, and Jie Chen.
Fuzzy positive learning for semi-supervised semantic seg-
mentation. In CVPR , pages 15465‚Äì15474, 2023. 6
[33] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:
Convolutional networks for biomedical image segmentation.
InMICCAI , pages 234‚Äì241. Springer, 2015. 1, 2
[34] Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao
Zhang, Han Zhang, Colin A Raffel, Ekin Dogus Cubuk,
Alexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying
semi-supervised learning with consistency and confidence.
NeurIPS , 33:596‚Äì608, 2020. 1
[35] Robin Strudel, Ricardo Garcia, Ivan Laptev, and Cordelia
Schmid. Segmenter: Transformer for semantic segmenta-
tion. In ICCV , pages 7262‚Äì7272, 2021. 2
[36] Antti Tarvainen and Harri Valpola. Mean teachers are better
role models: Weight-averaged consistency targets improve
semi-supervised deep learning results. NeurIPS , 30, 2017.
1, 3
[37] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. In-
stance normalization: The missing ingredient for fast styliza-
tion. arXiv preprint arXiv:1607.08022 , 2016. 4
[38] Dusan Varis and Ond Àárej Bojar. Sequence length is a domain:
Length-based overfitting in transformer models. In Proceed-
ings of the 2021 Conference on Empirical Methods in Natu-
ral Language Processing , pages 8246‚Äì8257, 2021. 8
[39] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia
Polosukhin. Attention is all you need. NeurIPS , 30, 2017. 2
[40] Haonan Wang and Xiaomeng Li. Towards generic semi-
supervised framework for volumetric medical image seg-
mentation. In NeurIPS , 2023. 1
[41] Haonan Wang and Xiaomeng Li. Dhc: Dual-debiased
heterogeneous co-training framework for class-imbalanced
semi-supervised medical image segmentation. In MICCAI ,
pages 582‚Äì591. Springer, 2023. 1
[42] Haonan Wang, Peng Cao, Jiaqi Wang, and Osmar R Zaiane.
Uctransnet: rethinking the skip connections in u-net from a
channel-wise perspective with transformer. In AAAI , pages
2441‚Äì2449, 2022. 2, 4
[43] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao
Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao.
Pyramid vision transformer: A versatile backbone for dense
prediction without convolutions. In ICCV , pages 568‚Äì578,
2021. 2
[44] Xiaoyang Wang, Bingfeng Zhang, Limin Yu, and Jimin
Xiao. Hunting sparsity: Density-guided contrastive learn-
ing for semi-supervised semantic segmentation. In CVPR ,
pages 3114‚Äì3123, 2023. 1, 3, 6, 8[45] Yuchao Wang, Haochen Wang, Yujun Shen, Jingjing Fei,
Wei Li, Guoqiang Jin, Liwei Wu, Rui Zhao, and Xinyi
Le. Semi-supervised semantic segmentation using unreliable
pseudo-labels. In CVPR , pages 4248‚Äì4257, 2022. 1, 3, 5, 6
[46] Zicheng Wang, Zhen Zhao, Xiaoxia Xing, Dong Xu, Xi-
angyu Kong, and Luping Zhou. Conflict-based cross-view
consistency for semi-supervised semantic segmentation. In
CVPR , pages 19585‚Äì19595, 2023. 2, 3, 6, 8
[47] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu,
Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt: Introducing
convolutions to vision transformers. In ICCV , pages 22‚Äì31,
2021. 2
[48] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar,
Jose M Alvarez, and Ping Luo. Segformer: Simple and ef-
ficient design for semantic segmentation with transformers.
NeurIPS , 34:12077‚Äì12090, 2021. 1, 2, 6, 7, 8
[49] Lihe Yang, Wei Zhuo, Lei Qi, Yinghuan Shi, and Yang Gao.
St++: Make self-training work better for semi-supervised se-
mantic segmentation. In CVPR , pages 4268‚Äì4277, 2022. 6
[50] Lihe Yang, Lei Qi, Litong Feng, Wayne Zhang, and
Yinghuan Shi. Revisiting weak-to-strong consistency in
semi-supervised semantic segmentation. In CVPR , pages
7236‚Äì7246, 2023. 1, 2, 3, 6, 8
[51] Weihao Yu, Mi Luo, Pan Zhou, Chenyang Si, Yichen Zhou,
Xinchao Wang, Jiashi Feng, and Shuicheng Yan. Metaformer
is actually what you need for vision. In CVPR , pages 10819‚Äì
10829, 2022. 2
[52] Jianlong Yuan, Jinchao Ge, Zhibin Wang, and Yifan Liu.
Semi-supervised semantic segmentation with mutual knowl-
edge distillation. In ACM MM , pages 5436‚Äì5444, 2023. 6
[53] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi,
Zi-Hang Jiang, Francis EH Tay, Jiashi Feng, and Shuicheng
Yan. Tokens-to-token vit: Training vision transformers from
scratch on imagenet. In ICCV , pages 558‚Äì567, 2021. 2
[54] Bowen Zhang, Yidong Wang, Wenxin Hou, Hao Wu, Jin-
dong Wang, Manabu Okumura, and Takahiro Shinozaki.
Flexmatch: Boosting semi-supervised learning with curricu-
lum pseudo labeling. NeurIPS , 34:18408‚Äì18419, 2021. 1
[55] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang
Wang, and Jiaya Jia. Pyramid scene parsing network. In
CVPR , pages 2881‚Äì2890, 2017. 2
[56] Zhen Zhao, Lihe Yang, Sifan Long, Jimin Pi, Luping Zhou,
and Jingdong Wang. Augmentation matters: A simple-yet-
effective approach to semi-supervised semantic segmenta-
tion. In CVPR , pages 11350‚Äì11359, 2023. 1, 3, 6
[57] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu,
Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao
Xiang, Philip HS Torr, et al. Rethinking semantic segmen-
tation from a sequence-to-sequence perspective with trans-
formers. In CVPR , pages 6881‚Äì6890, 2021. 2
[58] Yuanyi Zhong, Bodi Yuan, Hong Wu, Zhiqiang Yuan, Jian
Peng, and Yu-Xiong Wang. Pixel contrastive-consistent
semi-supervised semantic segmentation. In ICCV , pages
7273‚Äì7282, 2021. 6
[59] Yuliang Zou, Zizhao Zhang, Han Zhang, Chun-Liang Li,
Xiao Bian, Jia-Bin Huang, and Tomas Pfister. Pseudoseg:
Designing pseudo labels for semantic segmentation. ICLR ,
2020. 1, 6
3636
