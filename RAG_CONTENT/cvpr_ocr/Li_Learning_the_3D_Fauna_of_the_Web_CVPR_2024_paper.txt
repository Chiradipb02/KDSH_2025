Learning the 3D Fauna of the Web
Zizhang Li1*Dor Litvak1,2*Ruining Li3Yunzhi Zhang1Tomas Jakab3Christian Rupprecht3
Shangzhe Wu1†Andrea Vedaldi3†Jiajun Wu1†
1Stanford University2UT Austin3University of Oxford
kyleleey.github.io/3DFauna/
Figure 1. Learning Diverse 3D Animals from the Internet. Our method, 3D-Fauna , learns a pan-category deformable 3D model of more
than 100 different animal species using only 2D Internet images as training data. At test time, the model can turn a single image of an
quadruped instance into an articulated, textured 3D mesh in a feed-forward manner, ready for animation and rendering.
Abstract
Learning 3D models of all animals in nature requires
massively scaling up existing solutions. With this ultimate
goal in mind, we develop 3D-Fauna, an approach that
learns a pan-category deformable 3D animal model for
more than 100 animal species jointly. One crucial bottle-
neck of modeling animals is the limited availability of train-
ing data, which we overcome by learning our model from
2D Internet images. We show that prior approaches, which
are category-specific, fail to generalize to rare species with
limited training images. We address this challenge by in-
troducing the Semantic Bank of Skinned Models (SBSM),
which automatically discovers a small set of base animal
shapes by combining geometric inductive priors with se-
mantic knowledge implicitly captured by an off-the-shelf
self-supervised feature extractor. To train such a model,
we also contribute a new large-scale dataset of diverse an-
imal species. At inference time, given a single image of any
quadruped animal, our model reconstructs an articulated
3D mesh in a feed-forward manner in seconds.
1. Introduction
Computer vision models can nowadays reconstruct humans
in monocular images and videos robustly and accurately, re-
*Equal contribution
†Equal advisingcovering their 3D shape, articulated pose, and even appear-
ance [3, 11, 12, 14, 21, 34]. However, humans are but a tiny
fraction of the animals that exist in nature, and 3D models
remain essentially blind to the vast majority of biodiversity.
While in principle the same approaches that work for hu-
mans could work for many other animal species, in prac-
tice scaling it to each of the 2.1 million different animal
species on Earth is nearly hopeless. In fact, building a hu-
man model such as SMPL [34] and a corresponding pose
predictor [3, 14] requires collecting 3D scans of many peo-
ple in laboratory [21], crafting a corresponding articulated
deformable model semi-automatically, and collecting ex-
tensive manual labels to train corresponding pose regres-
sors. Of all animals, only humans are currently of sufficient
importance in applications to justify the costs.
A technically harder but much more practical approach
is to learn animal models automatically from images and
videos readily available on the Internet. Several authors
have demonstrated that at least rough models can be learned
from such uncontrolled image collections [22, 60, 70]. Even
so, many limitations remain, starting from the fact that these
methods can only reconstruct one or a few specific animal
exemplars [70], or at most a single class of animals at a
given time [22, 60]. The latter restriction is particularly
glaring, as it defeats the purpose of using the Internet as
a vast data source for modeling biodiversity.
We introduce 3D-Fauna , a method that learns a pan-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
9752
category deformable model for a large number ( >100) of
different quadruped animal species, such as dogs, antelopes,
and hedgehogs, as shown in Fig. 1. For the approach to be as
automated and thus as scalable as possible, we assume that
only Internet images of the animals are provided as train-
ing data and only consider as prerequisites a pre-trained 2D
object segmentation model and off-the-shelf unsupervised
visual features. 3D-Fauna is designed as a feed-forward
network that deforms and poses the deformable model to
reconstruct any animal given a single image as input. The
ability to perform monocular reconstruction is necessary for
training on (single-view) Internet images, and is also useful
in many real-world applications.
Crucial to 3D-Fauna is to learn a single joint model of
all animals in one go. Despite posing a challenge, model-
ing many animals jointly is essential for reconstructing rarer
species, for which we often have only a small number of im-
ages to train on. This allows us to exploit the structural sim-
ilarity of different animals that results from evolution, and
maximize statistical efficiency. Here, we focus our atten-
tion on animals that share a given body plan, in particular,
quadrupeds, and share the structure of the underlying skele-
tal model, which would otherwise be difficult to pin down.
Learning such a model from only unlabeled single-view
images requires several technical innovations. The most
important is to develop a 3D representation that is suf-
ficiently expressive to model the diverse shape variations
of the animals, and at the same time tight enough to be
learned from single-view images without overfitting indi-
vidual views. Prior work partly achieved this goal by us-
ing skinned models, which consider small shape variations
around a base template followed by articulation [60]. We
found that this approach does not provide sufficient induc-
tive biases to learn diverse animal species from Internet
images alone. Hence, we introduce the Semantic Bank of
Skinned Models (SBSM), which uses off-the-shelf unsuper-
vised features, such as DINO [5, 39], to hypothesize how
different animals may relate semantically, and automati-
cally learns a low-dimensional base shape bank.
Lastly, Internet images, which are not captured with the
purpose of 3D reconstruction in mind, are characterized by
a strong photographer bias, skewing the viewpoint distribu-
tion to mostly frontal, which significantly hinders the sta-
bility of 3D shape learning. To mitigate this issue, 3D-
Fauna further encourages the predicted shapes to look re-
alistic from all viewpoints, by introducing an efficient mask
discriminator that enforces the silhouettes rendered from a
random viewpoint to stay within the distribution of the sil-
houettes of the real images.
Combining these ideas, 3D-Fauna is an end-to-end
framework that learns a pan-category model of 3D
quadruped animals from online image collections. To train
3D-Fauna, we collected a large-scale animal dataset of over100 quadruped species, dubbed the Fauna Dataset , as part
of the contribution. After training, the model can turn a
single test image of any quadruped instance into a fully
articulated 3D mesh in a feed-forward fashion, ready for
animation and rendering. Extensive quantitative and qual-
itative comparisons demonstrate significant improvements
over existing methods. Code and data will be released.
2. Related Work
Optimization-Based 3D Reconstruction of Animals.
Due to the lack of explicit 3D data for the vast majority
of animals, reconstruction has mostly relied on pre-defined
shape models or multi-view images. Initially, efforts fo-
cus on fitting a parametric 3D shape model obtained form
3D scans, e.g., SMAL [74], to animal images using anno-
tated 2D keypoints and segmentation masks, which is fur-
ther extended to multi-view images [75]. Other works aim
to optimize the 3D shape [6, 55, 65–67, 70–72] directly
from image or video collections of a smaller scale using
various forms of supervision in addition to masks, such
as keypoints [6, 55], self-supervised semantic correspon-
dences [70–72], optical flow [64–67], surface normals [67],
category-specific template shapes [6, 55].
Learning 3D from Internet Images and Videos. Re-
cently, authors have attempted to learn 3D priors from In-
ternet images and videos at a larger scale [1, 13, 20, 22,
28, 29, 52, 57–60, 73], mostly focusing on a single cate-
gory at a time. Reconstructing animals presents additional
challenges due to their highly deformable nature, which of-
ten necessitates stronger supervisory signals for training,
similar to the ones used in optimization-based methods.
Some methods have, in particular, learned to model ar-
ticulated animals, such as horses, from single-view image
collections without any 3D supervision, adopting a hierar-
chical shape model that factorizes a category-specific prior
shape from instance-specific shape deformation and articu-
lation [20, 59, 60]. However, these models are trained in a
category-specific manner and fail to generalize to less com-
mon animal species as shown in Sec. 5.3.
Attempts to model diverse animal species again resort to
pre-defined shape models, e.g., SMAL. Ruegg et al. [42,
43] model multiple dog breeds and regularize the learn-
ing process by encouraging intra-breed similarities using a
triplet loss, which requires breed labels for training, in addi-
tion to keypoint annotations and template shape models. In
contrast, our approach reconstructs a significantly broader
set of animals and is trained in a category-agnostic fashion,
without relying on existing 3D shape models or keypoints.
Another related work [19] aims to learn a category-agnostic
3D shape regressor by exploiting pre-trained CLIP features
and an off-the-shelf normal estimator, but does not model
deformation and produces coarse shapes. Concurrent work
9753
!"!Semantic Base Shape Bank⊕≈$!!!"#$!!%&'……⇒*⇒*⇒*⇒*⇒learned memory bankLosses
Instance-specific Appearance & DeformationsIns.Pred.recon.GTrandom viewBaseShape⇒
⇒DifferentiableRendererinput viewEnc
ℒ()	≈
Dis.⇒real /fake?!"+%
Internet Images
ℒ,#&-	≈
ℒ)	≈
albedodeformationposingshadingFigure 2. Training Pipeline. 3D-Fauna is trained using only single-view images from the Internet. Given each input image, it first extracts
a feature vector ϕusing a pre-trained unsupervised image encoder [5]. This is then used to query a learned memory bank to produce a base
shape and a DINO feature field in the canonical pose. The model also predicts the albedo, instance-specific deformation, articulated pose
and lighting, and is trained via image reconstruction losses on RGB, DINO feature map and mask, as well as a mask discriminator loss.
SAOR [2] also trains one model to reconstruct diverse ani-
mal categories, but obtains less realistic results and tends to
suffer from strong photographer bias.
Another line of research attempts to distill 3D recon-
structions from 2D generative models trained on large-
scale datasets of Internet images, which can be GAN-
based [7, 8, 15, 38] or more recently, diffusion-based mod-
els [9, 18, 35, 47] using Score Distillation Sampling [40]
and its variants. This idea has been extended to learn
image-conditional multi-view generator networks [25, 30–
33, 41, 45, 48, 49, 56, 63, 68]. However, most of these
methods optimize one single shape at a time, whereas our
model learns a pan-category deformable model that can re-
construct any animal instance in a feed-forward fashion.
Animal Datasets. Learning 3D models often requires
high-quality images without blur or occlusion. Exist-
ing high-quality datasets were only collected for a small
number of categories [46, 54, 59, 66], and more diverse
datasets [37, 61, 62, 69] often contain many noisy im-
ages unsuitable for training off the shelf. To train our
pan-category model for a wide range of quadruped animal
species, we aggregate these existing datasets after substan-
tial filtering, and additionally source more images from the
Internet to create a large-scale object-centric image dataset
spanning over 100quadruped species, as detailed in Sec. 4.
3. Method
Our goal is to learn a deformable model of a large variety
of different animals using only Internet images for supervi-
sion. Formally, we learn a function f:I7→Othat maps
any image I∈R3×H×Wof an animal to a corresponding
3D reconstruction O, capturing the animal’s shape, defor-
mation and appearance.3D reconstruction is greatly facilitated by using multi-
view data [17], but this is not available at scale, or at all,
for most animals. Instead, we wish to reconstruct animals
from weak single-view supervision obtained from the Inter-
net. Compared to prior works [60, 70–72], which focused
on reconstructing a single animal type at a time, here we tar-
get a large number of animal species at once, which is sig-
nificantly more difficult. We show in the next section how
solving this problem requires carefully exploiting the se-
mantic similarities and geometric correspondences between
different animals to regularize their 3D geometry.
3.1. Semantic Bank of Skinned Models
Given an image I, consider the problem of estimating the
3D shape (V, F)of the animal contained in it, where V∈
RK×3is a list of vertices of a 3D mesh with face connec-
tivity given by triplets F⊂ {1, . . . , K }3. While recovering
a 3D shape from a single image is ill-posed, as we train the
model fon a large dataset, we can ultimately observe ani-
mals from a variety of viewpoints. However, different im-
ages show different animals with different 3D shapes. Non-
Rigid Structure-from-Motion [4, 50, 51] shows that recon-
struction is still possible, but only if one makes the space of
possible 3D shapes sufficiently tight to remove the recon-
struction ambiguity. At the same time, the space must be
sufficiently expressive to capture all animals.
Skinned Models (SM). Following SMPL [34], many
works [20, 59, 60, 67] have adopted a Skinned Model (SM)
to model the shape of deformable objects when learning
from single-view image collections or videos. An SM starts
from a base shape Vbaseof the object (e.g., human or animal)
at ‘rest’, applies as a small deformation Vins=fins(Vbase, ϕ)
to capture instance-specific details, and then applies a larger
9754
Enc
0.42similaritySemantic Base Shape Bank
Enc
Enc
0.06similarity𝜙Figure 3. Queries from the Semantic Base Shape Bank. With-
out requiring any category labels, the Semantic Bank (Sec 3.1)
automatically learns diverse base shapes for various animals and
preserves the semantic similarities across different instances.
deformation via a skinning function V=fpose(Vins, ϕ),
controlled by the articulation of the underlying skeleton.
We assume that deformations are predicted by neural net-
works that receive as input image features ϕ=fϕ(I)ex-
tracted from a powerful self-supervised image encoder.
In our case, a single SM is insufficient to capture the very
large shape variations between different animals, which in-
clude horses, dogs, antelopes, hedgehogs, etc. Na ¨ıvely at-
tempting to capture this diversity using the network fins
means that the resulting deformations cannot be small any
longer, which throws off the tightness of the model.
Semantic Bank of Skinned Models. In order to increase
the expressiveness of the model while still avoiding overfit-
ting individual images, we propose to exploit the fact that
different animals often have similar 3D shapes as a result of
evolution. We can thus reduce the shape variation to a small
number of shape bases Vbase, and interpolate between them.
To do so, we introduce a Semantic Bank of Skinned Mod-
elsthat automatically discovers a set of latent shape bases
and learns to project each image into a linear combination
of these bases. Key to this method is to use pre-trained un-
supervised image features [5, 39] to automatically and im-
plicitly identify similar animals. This is realized by means
of a small memory bank with Klearned key-value pairs
{(ϕkey
k, ϕval
k)}K
k=1. Specifically, given an image embedding
ϕ, we query the memory bank to obtain a latent shape em-
bedding ˜ϕas a linear combination of the value tokens {ϕval
k}
via a mechanism similar to attention [53]:
˜ϕ=KX
k=1wkϕval
k,where wk=cossim( ϕ, ϕkey
k)
PK
j=1cossim( ϕ, ϕkey
j),
(1)
andcossim denotes cosine similarity between two feature
vectors. This embedding ˜ϕis then used as a condition to thebase shape predictor (Vbase, F) =fs(˜ϕ), which produces
semantically-adaptive base shapes without relying on any
category labels or being bound to a hard categorization.
In practice, the image features ϕare obtained from a
well-trained feature extractor like DINO-ViT [5, 39]. Defin-
ing the weights based on the cosine similarities between the
image features ϕand a small number of bases {ϕkey
k}cap-
tures the semantic similarities across different animal in-
stances. For instance, as illustrated in Fig. 3, the cosine
similarity between the image features of a zebra and a horse
is0.42, whereas the similarity between a zebra and an arctic
fox is only 0.06. Ablations in Fig. 6 further verify the im-
portance of this Semantic Bank, without which the model
easily overfits each training image and fails to reconstruct
plausible 3D shapes.
Implementation Details. The base shape is predicted us-
ing a hybrid SDF-mesh representation [44, 60] parameter-
ized by a coordinate MLP, with a conditioning vector ˜ϕin-
jected via layer weight modulation [23, 24]. Since extract-
ing meshes from SDFs using DMTet [44] is memory and
compute intensive, in practice, we only compute it once for
each iteration, by assuming the batched images contain the
same animal species, and simply averaging out the embed-
dings ˜ϕ. The instance-specific deformation is predicted us-
ing another coordinate MLP that outputs the displacement
∆Vins,i=f∆V(Vbase,i, ϕ)for each vertex Vbase,iof the base
mesh conditioned on the image feature ϕ, resulting in the
deformed shape Vins= ∆Vins+Vbase. We enforce a bilateral
symmetry on both the base shape and the instance deforma-
tion by mirroring the query locations for the MLPs. Given
the instance mesh Vins, we initialize a quadrupedal skele-
ton using a simple heuristic [60], and predict the rigid pose
ξ1∈SE(3)and bone rotations ξb∈SO(3), b= 2, . . . , B
using a pose network. These posing parameters are then ap-
plied to the instance mesh via a linear blend skinning equa-
tion [34]. Refer to the sup. mat. for more details.
Appearance. Assuming a Lambertian illumination model,
we model the appearance of the object using an albedo
fielda(x) = fa(x, ϕ)∈[0,1]3and a dominant direc-
tional light. The final shaded color of each pixel is com-
puted as ˆI(u) = ( ka+kd·max{0,⟨l,n⟩})·a(x), where
nis the normal direction of the posed mesh at pixel u, and
ka, kd∈[0,1]andl∈S2are respectively the ambient in-
tensity, diffuse intensity and dominant light direction pre-
dicted by the lighting network (ka, kd,l) =fl(ϕ).
3.2. Learning Formulation
The entire pipeline is trained in an unsupervised fashion,
using only self-supervised image features [5, 39] and object
masks obtained from off-the-shelf segmenters [26, 27].
Reconstruction Losses. Given the final predicted posed
shape Vand appearance of the object, we use a differen-
9755
tiable renderer Rto obtain an RGB image ˆIas well as a
mask image ˆM, which are compared to the input image I
and the pseudo-ground-truth object mask M:
Lm=∥ˆM−M∥2
2+λdt∥ˆM⊙dt(M)∥1, (2)
Lim=∥˜M⊙(ˆI−I)∥1, (3)
where dt(·)is distance transform for more effective gradi-
ents [22, 58], ⊙denotes the Hadamard product, λdtspecifies
the balancing weight, and ˜M=ˆM⊙Mis the intersection
of the predicted and ground-truth masks.
Correspondences from Self-Supervised Features. Self-
supervised feature extractors are notoriously good at estab-
lishing semantic correspondences between objects, which
can be distilled to facilitate 3D reconstruction [60]. To do
so, we extract a patch-based feature map Φ∈RD×H×W
from each training image. These raw feature maps can
be noisy and may preserve image-specific information ir-
relevant to other images. To distill more effective seman-
tic correspondences across different images, we perform
a Principal Component Analysis (PCA) across all feature
maps [60], reducing the dimension to D′= 16 . We then
task the model to also learn a feature field in the canon-
ical frame ψ(x,˜ϕ)∈RD′that is rendered into a fea-
ture image ˆΦgiven predicted posed shape using the same
renderer R. Training then encourages the rendered fea-
ture images ˆΦto match the pre-extracted PCA features Φ′:
Lfeat=∥˜M⊙(ˆΦ−Φ′)∥2
2.Note that although the space
of the PCA features Φ′is shared across different animal in-
stances, the feature field ψstill receives the latent embed-
ding˜ϕas a condition. This is because different animals vary
in shape, resulting in different feature fields.
Mask Discriminator. In practice, despite exploiting these
semantic correspondences, we still find that the viewpoint
prediction may easily collapse to only frontal viewpoints,
due to the heavy photographer bias in Internet photos. This
can lead to overly elongated shapes as shown in Fig. 6, and
further deteriorates the viewpoint predictions. To mitigate
this, we further encourage the shape to look realistic from
arbitrary viewpoints. Specifically, we introduce a mask dis-
criminator Dthat encourages the mask images ˆMrvren-
dered from a random viewpoint to stay within the distribu-
tion of the ground-truth masks M. The discriminator also
receives the base embedding ˜ϕ(with gradients detached)
as a condition to make this adversarial guidance tailored to
specific types of animals and thus more effective. Formally,
this is achieved via an adversarial loss [15]:
Ladv=EM∼M[logD(M;˜ϕ)]
+EˆMrv∼M rv[log(1 −D(ˆMrv;˜ϕ))].(4)
Note that we do not use a discriminator on the rendered
RGB images, as the predicted texture is often much less re-alistic when compared to real images, which gives the dis-
criminator a trivial task. Moreover, the distribution of mask
images is less susceptible to viewpoint bias than RGB im-
ages, and hence we can simply sample random viewpoints
uniformly, without requiring a precise viewpoint distribu-
tion of the training images.
Overall Loss. We further enforce the Eikonal constraint
REikon the SDF network as well as the viewpoint hypothe-
sis loss Lhypand the magnitude regularizers Rdefon vertex
deformations and Rarton articulation parameters ξ. See the
supplementary materials for details.
The final training objective Lis thus
L=Lrec+λhypLhyp+λadvLadv+R, (5)
where Lrec=λmLm+λimLim+λfeatLfeatsummarizes the
three reconstruction losses, R=λEikREik+λartRart+
λdefRdefsummarizes the regularizers, and λ’s balance the
contribution of each term.
Training Schedule. We design a robust training schedule
that comprises three stages. First, we train the base shapes
and the viewpoint network without articulation or deforma-
tion. This significantly improves the stability of the training
and allows the model to roughly register the rigid pose of
all instances and learn the coarse base shapes.
As the viewpoint prediction stabilizes after 20k itera-
tions, in the second stage, we instantiate the bones and en-
able the articulation, allowing the shapes to gradually grow
legs and fit the articulated pose in each image. Meanwhile,
we also turn on the mask discriminator to prevent view-
point collapse and shape elongation. In the final stage, we
optimize the instance shape deformation field to allow the
model to capture the fine-grained geometric details of indi-
vidual instances, with the discriminator disabled, as it may
corrupt the shape if overused.
4. Dataset Collection
In order to train this pan-category model for all types of
quadruped animals, we create a new animal image dataset,
dubbed the Fauna Dataset , spanning 128 quadruped
species from dogs, antelopes to minks and platypuses, with
a total of 78,168 images. We first aggregate the training
sets of existing animal image datasets, including Animals-
with-Attributes [61], APT-36K [69], Animal3D [62] and
DOVE [59]. Many of these images are blurry or contain
heavy occlusions, which will impact the stability of the
training. We thus filter the images using automatic scripts
first, followed by manual inspection. This results in 8,378
images covering approximately 70animal species. To fur-
ther increase the size as well as the diversity of the dataset,
we additionally collect 69,790images from the Internet, in-
cluding 63,115video frames and 2,358images for 7com-
mon animals (bear, cow, elephant, giraffe, horse, sheep, ze-
9756
bra) as well as 4,317images for another 51less common
species. We use off-the-shelf segmentation models [26, 27]
to detect and segment the instances in the images. Out of the
121few-shot categories, we hold out 5as novel categories
unused at training. For validation, we randomly select 5im-
ages in each of the rest 116few-shot categories, and 2,462
images for the 7common species. To reduce the viewpoint
bias in the few-shot categories, we manually identify a few
(1–10) backward-facing instances in the training set and du-
plicate them to match the size of the rest.
5. Experiments
5.1. Technical Details
We base our architecture on MagicPony [60], adding the
new SBSM and mask discriminator. For the Semantic Bank,
we use K= 60 key-value pairs. The dimension of keys is
384 (same as DINO-ViT) and the dimension of values is
128. As the texture network tends to struggle to predict de-
tailed appearance in one go, partially due to limited capac-
ity, for all the visualizations, we follow [60] and fine-tune
(only) the texture network for 50iterations, which takes
<10seconds. Refer to the sup. mat. for further details.
5.2. Qualitative Results
After training, 3D-Fauna takes in a single test image of any
quadruped animal and produces an articulated and textured
3D mesh in a feed-forward manner, as visualized in Fig. 4.
The model can reconstruct very different animals, such as
antelopes, armadillos, and fishers, without requiring any
category labels. All the input images in Fig. 4 have not been
seen during training. In particular, the model also performs
well on held-out categories, e.g. the wolf in the third row.
5.3. Comparisons with Prior Work
Baselines. To the best of our knowledge, ours is the first
deformable model designed to handle 100+ quadruped
species, learned purely from 2D Internet data. We carry
out quantitative and qualitative comparisons to methods that
are at least in principle applicable to this setting. The base-
line is MagicPony [60], which however is category-specific
(they first train on horses, and fine-tune on giraffes, cows
and zebras). We also compare with two popular deformable
models that can work in the wild, namely UMR [29] and
A-CSM [28]. However, they require weakly-supervised
part segmentations and shape templates, respectively. Other
works, such as LASSIE [70] and its follow-ups [71, 72], op-
timize a deformable model on a small set of about 20 images
covering a single animal category at a time. More recently,
image-to-3D methods based on distilling 2D diffusion mod-
els and/or large 3D datasets [31] have also demonstrated
plausible 3D reconstructions of animals from a single im-
age. In contrast, our model predicts an articulated meshPASCAL APT-36K Animal3D
KT-PCK@0.1 PCK@0.1 PCK@0.1 PCK@0.1
UMR [29] 0.284 - - -
A-CSM [28] 0.329 0.687 0.649 0.822
MagicPony [60] 0.429 - 0.756 0.867
Ours 0.539 0.782 0.841 0.901
Table 1. Quantitative Comparisons on PASCAL VOC [10],
APT-36K [69] and Animal3D [62]. When compared to baselines
including the competitive MagicPony [60], our method demon-
strates significantly improved performance on all datasets.
from a single image within seconds. Although it is difficult
to establish a fair numerical comparison given these differ-
ent settings, in Sec. 5.3, we provide a side-by-side quali-
tative comparison against baselines [31, 70, 71]. We use
the publicly released code [31, 60, 70, 71] and report num-
bers [28, 29] included in MagicPony [60].
Quantitative Comparisons. We conduct quantitative eval-
uation across three different datasets, APT-36K [69], Ani-
mal3D [62], and PASCAL VOC [10], which contain images
of various animals with 2D keypoint annotations. Follow-
ing MagicPony [60], we first evaluate on horses in PAS-
CAL VOC [10] using the widely used Keypoint Transfer
metric [22, 28, 29]. We use the same protocol as in A-
CSM [28] and randomly sample 20k source-target image
pairs. For each source image, we project the visible vertices
of the predicted mesh onto the image and map each anno-
tated 2D keypoint to its nearest vertex. We then project that
vertex to the target image and check if it lies within a small
distance (10% of image size) to the corresponding keypoint
in the target image. We summarize the results using the
Percentage of Correct Keypoints (KT-PCK@0.1) in Tab. 1.
In Tab. 1, we follow CMR [22] to evaluate the three
datasets on more species, optimizing a linear mapping from
mesh vertices to desired keypoints for each category, and
reporting PCK@0.1 between the predicted and annotated
2D keypoints. Our model demonstrates significant improve-
ment over existing methods on all datasets. A performance
breakdown for each category is provided in the sup. mat.
Qualitative Comparisons. Figure 5 compares 3D-Fauna
qualitatively to several recent works [31, 60, 70, 71]. To
establish a fair comparison with MagicPony [60], for cate-
gories demonstrated in their paper (e.g. horse), we simply
run inference using the released model. For each of the
other categories, we use their public code to train a per-
category model on our dataset from scratch (which con-
tains less than 100 images for some rare categories). For
LASSIE [70] and Hi-LASSIE [71], which optimize over a
small set of images, we train their models on the testimage
together with additional 29images randomly selected from
the training set of that category. Hi-LASSIE [71] is further
9757
Input Reconstruction Other Views Articulated
Figure 4. Single Image 3D Reconstruction. Given a single image of any quadruped animal at test time, our model reconstructs an
articulated and textured 3D mesh in a feed-forward manner without requiring category labels, which can be readily animated.
9758
OursMagicPonyLASSIEHi-LASSIE
Zero-1-to-3
Input ImageInput ViewNovel ViewInput ViewNovel ViewInput ViewNovel ViewInput ViewNovel ViewInput ViewNovel ViewFigure 5. Qualitative Comparisons against MagicPony [60], LASSIE [70], Hi-LASSIE [71] and Zero-1-to-3 [31]. Compared to all
baselines, our method predicts more stable poses and higher-fidelity reconstructions. Note that our method is learning-based and predicts
3D meshes in a feed-forward fashion (as opposed to [70, 71] that optimize on test images), which is orders of magnitude faster.
Full ModelCategory-conditionedw/o ℒ!"#	w/o Semantic Bank
Input ViewInput Image
Side View
Input ViewSide View
Input Image
Figure 6. Ablation Studies. Both the Semantic Bank and the mask
discriminator improve the results as discussed in Sec. 5.4.
fine-tuned on the test image after training. To compare with
Zero-1-to-3 [31], we use the implementation in threestu-
dio [16] to first distill a NeRF [36] using Score Distillation
Sampling [40] given the masked test image, and then extract
a 3D mesh for fair comparison. Note that our model predicts
3D meshes within seconds, whereas the optimization takes
at least 10–20 mins for the other methods [31, 70, 71].
As shown in Fig. 5, MagicPony is sensitive to the size of
the training set. When trained on rare categories with fewer
(<100) images, such as the puma in Fig. 5, it fails to learn
meaningful shapes and produces severe artifacts. Despite
optimizing on the test images, LASSIE and Hi-LASSIE
produce coarser reconstructions, partially due to the part-
based representation that struggles in capturing the detailed
geometry and articulation, as well as unstable viewpoint
prediction. Zero-1-to-3, on the other hand, often fails to
correctly reconstruct the legs, and does not explicitly model
the articulated pose. On the contrary, our method predicts
accurate viewpoint and reconstructs fine-grained articulated
shapes for all different animals, with only one single model.5.4. Ablation Study
In Fig. 6, we present ablation results on three key design
choices in our pipeline: SBSM, category-agnostic training,
and mask discriminator. If we remove the SBSM and di-
rectly condition the base shape network on each individual
image embedding ϕ, the model tends to overfit each training
views without learning meaningful canonical 3D shapes and
pose. Alternatively, we can simply condition the base shape
on an explicit (learned) category-specific embedding and
train the model in a category-conditioned manner. This also
leads to sub-optimal reconstructions, in particular on rare
categories with few training images. Lastly, training with-
out the mask discriminator results in biased viewpoint pre-
diction (towards frontal) and produces elongated shapes.
6. Conclusions
We have presented 3D-Fauna, a deformable model for 100
animal categories learned using only Internet images. 3D-
Fauna can reconstruct any quadruped image by instantiat-
ing in seconds a posed version of the deformable model to
match the input image. Despite capable of modeling diverse
animals, the current model is still limited to quadruped
species that share a same skeletal structure. Furthermore,
the training images still need to be lightly curated. Never-
theless, 3D-Fauna still presents a significant leap compared
to prior works and moves us closer to models that will be
able to understand and reconstruct all animals in nature.
Acknowledgments. We thank Cristobal Eyzaguirre, Kyle
Sargent, and Yunhao Ge for their insightful discussions and
Chen Geng for proofreading. The work is in part supported
by the Stanford Institute for Human-Centered AI (HAI),
NSF RI #2211258, #2338203, ONR MURI N00014-22-1-
2740, ONR YIP N00014-24-1-2117, the Samsung Global
Research Outreach (GRO) program, Amazon, Google, and
EPSRC VisualAI EP/T028572/1.
9759
References
[1] Kalyan Vasudev Alwala, Abhinav Gupta, and Shubham Tul-
siani. Pre-train, self-train, distill: A simple recipe for super-
sizing 3d reconstruction. In CVPR , 2022. 2
[2] Mehmet Ayg ¨un and Oisin Mac Aodha. Saor: Single-view
articulated object reconstruction. In CVPR , 2024. 3
[3] Federica Bogo, Angjoo Kanazawa, Christoph Lassner, Peter
Gehler, Javier Romero, and Michael J. Black. Keep it SMPL:
Automatic estimation of 3D human pose and shape from a
single image. In ECCV , 2016. 1
[4] Christoph Bregler, Aaron Hertzmann, and Henning Bier-
mann. Recovering non-rigid 3d shape from image streams.
InCVPR , 2000. 3
[5] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv ´e J´egou,
Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-
ing properties in self-supervised vision transformers. In
ICCV , 2021. 2, 3, 4
[6] Thomas J. Cashman and Andrew W. Fitzgibbon. What shape
are dolphins? building 3d morphable models from 2d im-
ages. IEEE TPAMI , 2012. 2
[7] Eric Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu, and
Gordon Wetzstein. pi-GAN: Periodic implicit generative ad-
versarial networks for 3d-aware image synthesis. In CVPR ,
2021. 3
[8] Eric R. Chan, Connor Z. Lin, Matthew A. Chan, Koki
Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo,
Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, Tero
Karras, and Gordon Wetzstein. Efficient geometry-aware 3D
generative adversarial networks. In CVPR , 2022. 3
[9] Congyue Deng, Chiyu ”Max” Jiang, Charles R. Qi, Xinchen
Yan, Yin Zhou, Leonidas Guibas, and Dragomir Anguelov.
Nerdi: Single-view nerf synthesis with language-guided dif-
fusion as general image priors. In CVPR , 2023. 3
[10] Mark Everingham, SM Ali Eslami, Luc Van Gool, Christo-
pher KI Williams, John Winn, and Andrew Zisserman. The
pascal visual object classes challenge: A retrospective. IJCV ,
2015. 6
[11] Pedro F Felzenszwalb and Daniel P Huttenlocher. Efficient
matching of pictorial structures. In CVPR , 2000. 1
[12] Martin A. Fischler and Robert A. Elschlager. The represen-
tation and matching of pictorial structures. IEEE Trans. on
Computers , 1973. 1
[13] Shubham Goel, Angjoo Kanazawa, and Jitendra Malik.
Shape and viewpoints without keypoints. In ECCV , 2020.
2
[14] Shubham Goel, Georgios Pavlakos, Jathushan Rajasegaran,
Angjoo Kanazawa, and Jitendra Malik. Humans in 4d: Re-
constructing and tracking humans with transformers. In
ICCV , 2023. 1
[15] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets. NeurIPS , 2014.
3, 5
[16] Yuan-Chen Guo, Ying-Tian Liu, Ruizhi Shao, Christian
Laforte, Vikram V oleti, Guan Luo, Chia-Hao Chen, Zi-
Xin Zou, Chen Wang, Yan-Pei Cao, and Song-Hai Zhang.threestudio: A unified framework for 3d content generation.
https://github.com/threestudio-project/
threestudio , 2023. 8
[17] Richard Hartley and Andrew Zisserman. Multiple View Ge-
ometry in Computer Vision . Cambridge University Press,
ISBN: 0521540518, second edition, 2004. 3
[18] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-
sion probabilistic models. NeurIPS , 2020. 3
[19] Zixuan Huang, Varun Jampani, Anh Thai, Yuanzhen Li, Ste-
fan Stojanov, and James M Rehg. Shapeclipper: Scalable 3d
shape learning from single-view images via geometric and
clip-based consistency. In CVPR , 2023. 2
[20] Tomas Jakab, Ruining Li, Shangzhe Wu, Christian Rup-
precht, and Andrea Vedaldi. Farm3d: Learning articulated
3d animals by distilling 2d diffusion. In 3DV, 2024. 2, 3
[21] Hanbyul Joo, Tomas Simon, Xulong Li, Hao Liu, Lei Tan,
Lin Gui, Sean Banerjee, Timothy Godisart, Bart Nabbe, Iain
Matthews, et al. Panoptic studio: A massively multiview
system for social interaction capture. IEEE TPAMI , 2019. 1
[22] Angjoo Kanazawa, Shubham Tulsiani, Alexei A. Efros, and
Jitendra Malik. Learning category-specific mesh reconstruc-
tion from image collections. In ECCV , 2018. 1, 2, 5, 6
[23] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,
Jaakko Lehtinen, and Timo Aila. Analyzing and improving
the image quality of stylegan. In CVPR , 2020. 4
[24] Tero Karras, Miika Aittala, Samuli Laine, Erik H ¨ark¨onen,
Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Alias-free
generative adversarial networks. NeurIPS , 2021. 4
[25] Subin Kim, Kyungmin Lee, June Suk Choi, Jongheon Jeong,
Kihyuk Sohn, and Jinwoo Shin. Collaborative score dis-
tillation for consistent visual synthesis. arXiv preprint
arXiv:2307.04787 , 2023. 3
[26] Alexander Kirillov, Yuxin Wu, Kaiming He, and Ross Gir-
shick. Pointrend: Image segmentation as rendering. In
CVPR , 2020. 4, 6
[27] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,
Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-
head, Alexander C Berg, Wan-Yen Lo, et al. Segment any-
thing. In ICCV , 2023. 4, 6
[28] Nilesh Kulkarni, Abhinav Gupta, David F Fouhey, and Shub-
ham Tulsiani. Articulation-aware canonical surface map-
ping. In CVPR , 2020. 2, 6
[29] Xueting Li, Sifei Liu, Kihwan Kim, Shalini De Mello, Varun
Jampani, Ming-Hsuan Yang, and Jan Kautz. Self-supervised
single-view 3d reconstruction via semantic consistency. In
ECCV , 2020. 2, 6
[30] Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, Zexiang
Xu, Hao Su, et al. One-2-3-45: Any single image to 3d mesh
in 45 seconds without per-shape optimization. NeurIPS ,
2023. 3
[31] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-
makov, Sergey Zakharov, and Carl V ondrick. Zero-1-to-3:
Zero-shot one image to 3d object. In ICCV , 2023. 6, 8
[32] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie
Liu, Taku Komura, and Wenping Wang. SyncDreamer: Gen-
erating multiview-consistent images from a single-view im-
age. arXiv preprint arXiv:2309.03453 , 2023.
9760
[33] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu,
Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang,
Marc Habermann, Christian Theobalt, et al. Wonder3d: Sin-
gle image to 3d using cross-domain diffusion. arXiv preprint
arXiv:2310.15008 , 2023. 3
[34] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard
Pons-Moll, and Michael J Black. SMPL: A skinned multi-
person linear model. ACM TOG , 2015. 1, 3, 4
[35] Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, and
Andrea Vedaldi. Realfusion: 360deg reconstruction of any
object from a single image. In CVPR , 2023. 3
[36] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,
Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF:
Representing scenes as neural radiance fields for view syn-
thesis. In ECCV , 2020. 8
[37] Xun Long Ng, Kian Eng Ong, Qichen Zheng, Yun Ni,
Si Yong Yeo, and Jun Liu. Animal kingdom: A large and
diverse dataset for animal behavior understanding. In CVPR ,
2022. 3
[38] Thu Nguyen-Phuoc, Chuan Li, Lucas Theis, Christian
Richardt, and Yong-Liang Yang. HoloGAN: Unsupervised
learning of 3d representations from natural images. In ICCV ,
2019. 3
[39] Maxime Oquab, Timoth ´ee Darcet, Th ´eo Moutakanni, Huy
V o, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,
Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al.
Dinov2: Learning robust visual features without supervision.
arXiv preprint arXiv:2304.07193 , 2023. 2, 4
[40] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-
hall. Dreamfusion: Text-to-3d using 2d diffusion. ICLR ,
2023. 3, 8
[41] Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren,
Aliaksandr Siarohin, Bing Li, Hsin-Ying Lee, Ivan Sko-
rokhodov, Peter Wonka, Sergey Tulyakov, et al. Magic123:
One image to high-quality 3d object generation using both
2d and 3d diffusion priors. arXiv preprint arXiv:2306.17843 ,
2023. 3
[42] Nadine R ¨uegg, Silvia Zuffi, Konrad Schindler, and Michael J
Black. Barc: Learning to regress 3d dog shape from images
by exploiting breed information. In CVPR , 2022. 2
[43] Nadine R ¨uegg, Shashank Tripathi, Konrad Schindler,
Michael J Black, and Silvia Zuffi. Bite: Beyond priors for
improved three-d dog pose estimation. In CVPR , 2023. 2
[44] Tianchang Shen, Jun Gao, Kangxue Yin, Ming-Yu Liu, and
Sanja Fidler. Deep marching tetrahedra: a hybrid representa-
tion for high-resolution 3d shape synthesis. NeurIPS , 2021.
4
[45] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li,
and Xiao Yang. Mvdream: Multi-view diffusion for 3d gen-
eration. arXiv preprint arXiv:2308.16512 , 2023. 3
[46] Samarth Sinha, Roman Shapovalov, Jeremy Reizenstein, Ig-
nacio Rocco, Natalia Neverova, Andrea Vedaldi, and David
Novotny. Common pets in 3d: Dynamic new-view synthesis
of real-life deformable categories. In CVPR , 2023. 3
[47] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-
hishek Kumar, Stefano Ermon, and Ben Poole. Score-based
generative modeling through stochastic differential equa-
tions. In ICLR , 2021. 3[48] Jingxiang Sun, Bo Zhang, Ruizhi Shao, Lizhen Wang, Wen
Liu, Zhenda Xie, and Yebin Liu. Dreamcraft3d: Hierarchi-
cal 3d generation with bootstrapped diffusion prior. arXiv
preprint arXiv:2310.16818 , 2023. 3
[49] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang
Zeng. Dreamgaussian: Generative gaussian splatting for effi-
cient 3d content creation. arXiv preprint arXiv:2309.16653 ,
2023. 3
[50] Lorenzo Torresani, Aaron Hertzmann, and Christoph Bre-
gler. Learning non-rigid 3d shape from 2d motion. NeurIPS ,
2004. 3
[51] Edith Tretschk, Navami Kairanda, Mallikarjun BR, Rishabh
Dabral, Adam Kortylewski, Bernhard Egger, Marc Haber-
mann, Pascal Fua, Christian Theobalt, and Vladislav
Golyanik. State of the art in dense monocular non-rigid 3d
reconstruction. In Comput. Graph. Forum , pages 485–520,
2023. 3
[52] Shubham Tulsiani, Nilesh Kulkarni, and Abhinav Gupta. Im-
plicit mesh reconstruction from unannotated image collec-
tions. arXiv preprint arXiv:2007.08504 , 2020. 2
[53] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. NeurIPS , 2017. 4
[54] Catherine Wah, Steve Branson, Peter Welinder, Pietro Per-
ona, and Serge Belongie. The Caltech-UCSD Birds-200-
2011 Dataset. Technical Report CNS-TR-2011-001, Cali-
fornia Institute of Technology, 2011. 3
[55] Yufu Wang, Nikos Kolotouros, Kostas Daniilidis, and Marc
Badger. Birds of a feather: Capturing avian shape models
from images. In CVPR , 2021. 2
[56] Haohan Weng, Tianyu Yang, Jianan Wang, Yu Li, Tong
Zhang, CL Chen, and Lei Zhang. Consistent123: Improve
consistency for one image to 3d object synthesis. arXiv
preprint arXiv:2310.08092 , 2023. 3
[57] Shangzhe Wu, Christian Rupprecht, and Andrea Vedaldi.
Unsupervised learning of probably symmetric deformable 3d
objects from images in the wild. In CVPR , 2020. 2
[58] Shangzhe Wu, Ameesh Makadia, Jiajun Wu, Noah Snavely,
Richard Tucker, and Angjoo Kanazawa. De-rendering the
world’s revolutionary artefacts. In CVPR , 2021. 5
[59] Shangzhe Wu, Tomas Jakab, Christian Rupprecht, and An-
drea Vedaldi. DOVE: Learning deformable 3d objects by
watching videos. IJCV , 2023. 2, 3, 5
[60] Shangzhe Wu, Ruining Li, Tomas Jakab, Christian Rup-
precht, and Andrea Vedaldi. Magicpony: Learning articu-
lated 3d animals in the wild. In CVPR , 2023. 1, 2, 3, 4, 5, 6,
8
[61] Yongqin Xian, Christoph H. Lampert, Bernt Schiele, and
Zeynep Akata. Zero-shot learning—a comprehensive eval-
uation of the good, the bad and the ugly. IEEE TPAMI , 2019.
3, 5
[62] Jiacong Xu, Yi Zhang, Jiawei Peng, Wufei Ma, Artur Jesslen,
Pengliang Ji, Qixin Hu, Jiehua Zhang, Qihao Liu, Jiahao
Wang, et al. Animal3d: A comprehensive dataset of 3d ani-
mal pose and shape. In ICCV , 2023. 3, 5, 6
[63] Yinghao Xu, Hao Tan, Fujun Luan, Sai Bi, Wang Peng, Jihao
Li, Zifan Shi, Kaylan Sunkavalli, Wetzstein Gordon, Zexiang
9761
Xu, and Zhang Kai. DMV3D: Denoising multi-view dif-
fusion using 3d large reconstruction model. arXiv preprint
arXiv:2311.09217 , 2023. 3
[64] Gengshan Yang, Deqing Sun, Varun Jampani, Daniel Vlasic,
Forrester Cole, Huiwen Chang, Deva Ramanan, William T.
Freeman, and Ce Liu. LASR: Learning articulated shape
reconstruction from a monocular video. In CVPR , 2021. 2
[65] Gengshan Yang, Deqing Sun, Varun Jampani, Daniel Vla-
sic, Forrester Cole, Ce Liu, and Deva Ramanan. ViSER:
Video-specific surface embeddings for articulated 3d shape
reconstruction. In NeurIPS , 2021. 2
[66] Gengshan Yang, Minh V o, Neverova Natalia, Deva Ra-
manan, Vedaldi Andrea, and Joo Hanbyul. BANMo: Build-
ing animatable 3d neural models from many casual videos.
InCVPR , 2022. 3
[67] Gengshan Yang, Chaoyang Wang, N. Dinesh Reddy, and
Deva Ramanan. Reconstructing animatable categories from
videos. In CVPR , 2023. 2, 3
[68] Jiayu Yang, Ziang Cheng, Yunfei Duan, Pan Ji, and Hong-
dong Li. Consistnet: Enforcing 3d consistency for multi-
view images diffusion. arXiv preprint arXiv:2310.10343 ,
2023. 3
[69] Yuxiang Yang, Junjie Yang, Yufei Xu, Jing Zhang, Long
Lan, and Dacheng Tao. Apt-36k: A large-scale benchmark
for animal pose estimation and tracking. NeurIPS , 2022. 3,
5, 6
[70] Chun-Han Yao, Wei-Chih Hung, Yuanzhen Li, Michael Ru-
binstein, Ming-Hsuan Yang, and Varun Jampani. Lassie:
Learning articulated shapes from sparse image ensemble via
3d part discovery. NeurIPS , 2022. 1, 2, 3, 6, 8
[71] Chun-Han Yao, Wei-Chih Hung, Yuanzhen Li, Michael Ru-
binstein, Ming-Hsuan Yang, and Varun Jampani. Hi-lassie:
High-fidelity articulated shape and skeleton discovery from
sparse image ensemble. In CVPR , 2023. 6, 8
[72] Chun-Han Yao, Amit Raj, Wei-Chih Hung, Yuanzhen Li,
Michael Rubinstein, Ming-Hsuan Yang, and Varun Jampani.
Artic3d: Learning robust articulated 3d shapes from noisy
web image collections. NeurIPS , 2023. 2, 3, 6
[73] Yufei Ye, Shubham Tulsiani, and Abhinav Gupta. Shelf-
supervised mesh prediction in the wild. In CVPR , 2021. 2
[74] Silvia Zuffi, Angjoo Kanazawa, David W Jacobs, and
Michael J Black. 3d menagerie: Modeling the 3d shape and
pose of animals. In CVPR , 2017. 2
[75] Silvia Zuffi, Angjoo Kanazawa, and Michael J Black. Li-
ons and tigers and bears: Capturing non-rigid, 3d, articulated
shape from images. In CVPR , 2018. 2
9762
