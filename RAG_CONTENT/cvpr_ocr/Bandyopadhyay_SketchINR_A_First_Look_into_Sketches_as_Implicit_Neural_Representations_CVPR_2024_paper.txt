SketchINR : A First Look into Sketches as Implicit Neural Representations
Hmrishav Bandyopadhyay1Ayan Kumar Bhunia1Pinaki Nath Chowdhury1Aneeshan Sain1
Tao Xiang1,2Timothy Hospedales3Yi-Zhe Song1,2
1SketchX, CVSSP, University of Surrey, United Kingdom.
2iFlyTek-Surrey Joint Research Centre on Artificial Intelligence.
3University of Edinburgh, United Kingdom
{h.bandyopadhyay, a.bhunia, p.chowdhury, a.sain, t.xiang, y.song }@surrey.ac.uk
t.hospedales@ed.ac.uk
Chamfer Distance
Sketch-RNN
Vector SketchRaster Sketch
Self-ReconstructionRecon. GT
Ours
FS-COCOOurs
Sketch Complexity
 Sketch ComplexitySketchy
(a)
FS-COCO Sketchy
Complexity Scaling(b)Compressed Sketch
Storage (Bytes)
Figure 1. SketchINR is an implicit neural representation for sequential vector sketches. It is the first neural representation with sufficient
fidelity to be a drop-in replacement for the raw sketch data (a). The representation is significantly higher fidelity than existing learned
representations such as SketchRNN [28], especially for more complex sketches (b, left). SketchINR also provides state-of-the-art sketch
compression, with a substantially more compact representation than either vector or raster sketches (b, right).
Abstract
We propose SketchINR, to advance the representation
of vector sketches with implicit neural models. A vari-
able length vector sketch is compressed into a latent space
of fixed dimension that implicitly encodes the underlying
shape as a function of time and strokes. The learned func-
tion predicts the xypoint coordinates in a sketch at each
time and stroke. Despite its simplicity, SketchINR outper-
forms existing representations at multiple tasks: (i) Encod-
ing an entire sketch dataset into a fixed size latent vec-
tor, SketchINR gives 60×and10×data compression over
raster and vector sketches, respectively. (ii) SketchINR’s
auto-decoder provides a much higher-fidelity representa-
tion than other learned vector sketch representations, and
is uniquely able to scale to complex vector sketches such
as FS-COCO. (iii) SketchINR supports parallelisation that
can decode/render ∼100×faster than other learned vector
representations such as SketchRNN. (iv) SketchINR, for the
first time, emulates the human ability to reproduce a sketch
with varying abstraction in terms of number and complex-
ity of strokes. As a first look at implicit sketches, Sketch-
INR’s compact high-fidelity representation will support fu-
ture work in modelling long and complex sketches.
1. Introduction
The prevalence of touch-screen devices has triggered signif-
icant research progress on sketches [4, 6, 8, 51, 57]. Mod-elling digital sketches has become an important challenge
for learning systems that aim to stimulate human creativity
[17, 27]. Large-scale datasets [24, 28, 53] motivated sev-
eral downstream applications like image retrieval [15, 61],
image generation [37, 65], image editing [1, 41], and 3D
content creation [5, 8, 38], among others [58, 60].
These applications are underpinned by sketches captured
in raw form either as raster or vector images, and usu-
ally encoded into derived representations by ConvNets [15],
splines, and so on. For over a decade, discourse around
what is a “good” representation of human sketches has
persisted [31]. A now substantial body of work has fo-
cused on representations for sequential vector sketches that
model both explicit strokes, and their drawing over time
— most famously by the auto-regressive SketchRNN [28],
but also using representations such as parametric curves
[16], Gaussians [2], and Neural ODEs [17]. We introduce
a novel continuous-time representation of sequential vec-
tor sketches by taking an implicit neural representation per-
spective on sketches for the first time.
First, we review the limitations of current sketch repre-
sentations. The two most popular raw sketch representa-
tions are (i) raster sketch – a black and white image of a
line drawing, and (ii) vector sketch – a temporal sequence
of points and strokes. Raster sketch has a large but fixed
storage ( e.g.,256×256). Its compatibility with ConvNets
[64] made raster sketches popular but they lack the tempo-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
12565
ral information (i.e., stroke order) necessary for generative
modelling [17]. Vector sketches efficiently store the xy-
coordinates of points and strokes ( N×2). However, the
storage increases with the length of sketch N( Fig. 1b).
(iii) Raw vector sketches can be encoded by popular learned
sketch representations such as SketchRNN [28] and oth-
ers [16, 17, 39]. However, for longer ( N≥1000 ) scene
sketches, the quadratic computational cost for transform-
ers becomes intractable [39] and recurrent nets become un-
stable [28]. Nevertheless, most of these still suffer from
slow sequential autoregressive decoding, and they mostly
cannot accurately reconstruct complex sketches with many
strokes. To summarise, current sketch representations ei-
ther discard temporal information (raster), scale poorly in
storage size (vector), or become slow and inaccurate with
complex sketches (learned autoregressive vector).
To overcome the above issues, we propose an implicit
neural representation for sketches – SketchINR. We pig-
gyback on the well-established [8, 30, 34, 48] potential of
implicit functions [22] to encode an underlying shape as a
function of time/location. Different to standard implicits
[43, 48], the input of sketches as parametric functions is a
hierarchy of stroke sequence and point sequence. We de-
velop SketchINR to generate points ( xy-coordinates) given
both time and stroke inputs. To represent multiple sketches,
SketchINR learns a fixed size latent space and a condi-
tional implicit function that generates xy-coordinates of any
sketch instance given its corresponding latent code.
Despite its simplicity, SketchINR is competitive with ex-
isting sketch representations. (i) It can encode an entire
sketch dataset into a compact latent space ( e.g.,RN×512)
and decode using a fixed size function ( e.g., an 8layer
MLP). Compared to raster sketches ( 256×256), Sketch-
INR provides 128×storage compression. While simple
object-level vector sketches (length ≤300) have compa-
rable storage with SketchINR ( R512), for practical scene-
level [13] vector sketches (length ≥1000 ), SketchINR pro-
vides 10×better compression (Fig. 1b). (ii) For decod-
ing/generation, SketchINR can parallelly predict the xy-
coordinates for all time and strokes. In practice, this is
100×faster than an autoregressive vector sketch generator
with length ≥300. (iii) SketchINR can accurately represent
complex sketches (Fig. 1a,b) unlike competitors that suffer
from complexity limitations [16, 17] or the length limita-
tions of auto-regression [28]. The encoding fidelity is suffi-
ciently high that SketchINR provides a drop-in replacement
for raw sketch data, while being more compact. (iv) Sketch-
INR supports diverse applications spanning smooth latent
space interpolation, and sketch generation, sketch comple-
tion. Uniquely, it also supports sketch abstraction – the
human-like ability to replicate the essence of a sketch with
a variable number and complexity of strokes [31, 54].
In summary, our contributions are: (i) We present thefirst implicit neural representation approach to vector sketch
modelling, extending INRs as a function of time and
strokes, and defining a training objective for learning them.
(ii) SketchINR provides the first learned representation ca-
pable of representing complex sketches compactly and with
high-fidelity (Fig. 1a). Based on our compact high-fidelity
representation we introduce the task of sketch compression,
and demonstrate excellent compression results (Fig. 1b)).
(iii) We further demonstrate SketchINR’s applicability to a
variety of sketch related tasks including generation, inter-
polation, completion and abstraction.
2. Related Work
Sketch Representations: Digital sketches are predomi-
nantly captured as a function of ‘time’ [28], in a sequence
of coordinates traced by an artist on a canvas. Recent
works [24, 28] emphasise this temporal order of coordinates
as an indicator of their relative importance in depicting a
sketch-concept, as humans draw in a coarse-to-fine fashion.
As such, they [2, 7, 18, 28] exploit temporal information
in sketches for downstream tasks like sketch-assisted re-
trieval [7], generation [2, 18, 28] and modelling [38]. Vec-
tor sequences are further parameterised, for representation
with parametric curves like B ´eziers [16, 57] and B-Splines
[50, 66] for early-handwritten digit representation [50] to
recent representations of complex structures like real-world
objects [16, 57] and scenes [58]. Discarding temporal sig-
nificance of strokes, vector-sketches can be converted to
raster images by rendering coordinate strokes [9] on a 2D
canvas. While raster sketches are less informative than their
vector counterparts [28], they offer enhanced applicabil-
ity by expanding beyond digital sketches to paper -sketches
[18] where stroke order is already lost. Raster representa-
tions are encoded with translation-invariant networks like
CNNs [63] and Vision-Transformers [8], making them ex-
tremely useful to represent object-level information.
Non-Parametric Modelling: Both vector sequences and
raster renderings of these sequences are non-parametric rep-
resentations [16] of sketches. To preserve temporal stroke
order, vector sketches are handled as sequences [28] of
coordinates rather than unordered sets. Hence, they are
encoded with position-aware networks like RNNs [28],
LSTMs [15, 61], and Transformers [10, 51]. Without posi-
tional cues, raster sketches are processed by spatially-aware
CNNs [63] and Vision Transformers [52]. Semantic encod-
ings of non-parametric representations capture the expres-
sive nature of sketch, allowing its use as a creative input
for downstream segmentation [33], retrieval [7], and edit-
ing [41], as well as for interactive tasks like object-detection
[14] and image-inpainting [62]. These encodings are further
used in auto-regressive [28, 59] and V AE-like [10] mod-
elling of vector coordinate sequences for the generation of
textual-characters [1] and object sketches [28].
12566
Parametric Representation learning: Parametric Splines
[20], such as Beizer curves [42], approximate vector
sketches by per-sample fitting on individual sketch samples.
Amortised frameworks bypass this per-sample optimisation
by inferring curve control points [16] and degree [18] from
individual stroke features. Recent works exploit the rep-
resentative power of parametric curves for generation of
simple characters [26] to complex object [16, 18, 57] and
scene level sketches[58]. Both non-parametric and curve-
fitted B ´ezier parametric representations capture the explicit
form of a sketch as spatial and sequential forms.
Beyond explicit representations of sketches, we learn
visual implicits to capture spatial dynamics in vector-
sketches. Learned implicits can be used for sketch recon-
struction with particular control over abstraction through
number of reconstructed strokes. Somewhat similar to im-
plicit representations for vector sketches, CoSE [2] param-
eterises Gaussian Mixtures on individual strokes, learning
one implicit for one stroke only. New implicits are gener-
ated auto-regressively from previous predictions for auto-
regressive generation and completion of sketches. Unlike
CoSE, we learn a single implicit for the entire sketch, thus
having one implicit parameter to sample the sketch in one
pass. While SketchODE [17] similarly parameterises Neu-
ral Ordinary Differential Equations [12] to capture vector-
sketch dynamics, it’s optimisation is extremely slow (as
much as 120×slower) because of high time complexity in
solving higher order differentials. Three dimensional im-
plicits like Signed-Distance [48] and Occupancy [43] func-
tions learned from 3D point-clouds are particularly analo-
gous to our implicit representation.
Sketch Abstraction: The continuity of human cognition in
visual perception [36] is directly reflected in how we sketch.
This enables versatility [64] in sketch, allowing us to ex-
press a huge range of thoughts and visions directly on paper
in the form of coarse-grained ‘ideas’ [32] and fine-grained
‘objects’ [64]. Sketch abstraction, as a direct inverse of
this granularity, has been studied in depth as a function of
(i) drawing time: humans draw most representative strokes
first [28], and (ii) compactness: in a constrained stroke set-
ting, salient strokes are prioritised [46]. As abstraction gives
sketches a human touch, generative modelling of sketches
is focused on abstraction towards making generated draw-
ings “more humane” [19] and augmenting limited sketch
datasets with simulated sketches from photos [57, 58]. Im-
portantly, recent works [57] model abstraction as a function
ofstrokes to explicitly control abstraction levels by restrict-
ing the number of generated strokes. Here, we represent
sketches as visual implicits which allows us to reconstruct
them at varying levels of abstraction and detail by control-
ling the number of reconstructed strokes. We demonstrate
several downstream applications of this representation, par-
ticularly sketch compression, generation and completion.3. Methodology
Overview: Vector sketches are captured as pen movements
on a digital canvas, represented by coordinates (x, y)∈
[0,1]with binary pen-states δ∈ {0 :pen-down ,1 :pen-up }
at those coordinates. A sketch p={(xi, yi, δi)}N
i=1com-
prises Nvector way-points (coordinates), and is portrayed
(rendered) by retracing these points on a 2D canvas. This
process, termed as rasterisation produces a pixelated raster
sketch pson the canvas. In this work, we leverage scalabil-
ity [48] in implicit functions to learn a flexible representa-
tion for vector sketches with raster guidance.
3.1. Learning Visual Implicits for Sketches
Vector sketches can be modelled as implicits, where
a sketch is represented as function of time fθ(tj) :
R→R2+1parameterised by θ. Each time-stamp tj∈
[0,1)is mapped to a pen position in the form of xy-
coordinates (x, y)∈R2and binary pen-states δ∈
R1. The entire sketch can be reconstructed as a vec-
tor sequence by sampling tfrom 0to1with a learned
θasp={fθ(0), . . . , f θ(tj), . . . f θ(J−1
J)}for a to-
tal of Jtimestamps. This in itself is a flexible repre-
sentation for a vector sketch, as it allows for sampling
with arbitrary timestamps, thus potentially increasing or
decreasing resolution of the resulting sketch by modu-
lating number of strokes with sampling resolution ( t:
{0,1
3,2
3}v/st:{0,1
10, . . . ,9
10}). For increased con-
trol over reconstruction granularity in the form of ab-
straction, we model fθon an additional variable in the
form of strokestamps {sk}K
k=1, representing the sketch as
ˆp={fθ(0,0), . . . , f θ(tj, sk), . . . , f θ(J−1
J,K−1
K)}forK
strokes (Fig. 2). These strokestamps provide explicit con-
trol over pen-up and pen-down states, allowing us to recon-
struct the sketch with re-defined granularity during infer-
ence ( e.g.7or9strokes). Specifically, strokestamps rep-
resent the current stroke number as a fraction of the total
number of strokes Kwhich allows us to change strokes by
changing sk. Pen-states δare thus explicitly determined
from strokestamps only by toggling when skchanges value.
Our practical implementation of sketch implicits follows:
fθ(tj,∆t, sk,∆s) = p j (1)
where, time instances are computed as tj=tj−1+ ∆t,
t0= 0, and ∆t=1
J. Similarly, K-stroke instances are
computed as sk=sk−1+ ∆s,s0= 0, and ∆s=1
K.
To exploit bias in neural networks towards learning low fre-
quency functions [44, 49], we smoothen out high frequency
variations (0to1
K,0to1
S)for timestamps and strokestamps
by mapping them from Rto a higher dimension R2Lusing
positional embeddings PE (·)as
PE(x) =[sin(10−4/Lx),cos(10−4/Lx),
. . . ,sin(10−4L/Lx),cos(10−4L/Lx)](2)
12567
forx∈ {tn,∆t, sk,∆s}. We uniformly distribute Jtimes-
tamps across Kstrokes, where each stroke has J/K points.
For data augmentation, we train fθwith varying JandK
(±50% of ground truth value). This helps fθto scale to
different levels of abstraction, and number of points and
strokes during inference.
Figure 2. We model sketch as a function fθ(tj, sk)ofJtimes-
tamps and Kstrokes. Timestamps {ta, . . . , t b}where tj∈[0,1)
correspond to way-points for a specific stroke sk. Finally, fθ
with learned weights θcan be sampled with arbitrary number of
strokes as tj∈ {0,1
J, . . . ,J−1
J}andsk∈ {0,1
5, . . . ,4
5}or
sk∈ {0,1
10, . . . ,9
10}, leading to increasing or decreasing abstrac-
tion for K= 5orK= 10 , respectively.
3.2. Loss functions
To fit our implicit function fθto a particular sketch instance
p={(x0, y0, δ0), . . . , (xM−1, yM−1, δM−1)}withMvec-
tor way-points {pm}M−1
m=0, a naive approach would be to op-
timise θwith mean squared error against the ground truth
vectors as:
LMSE=MX
m=0fθm
M,k
K
−pm
2(3)
where each predicted coordinate and pen-state is optimised
directly against corresponding vector way-points from the
ground truth. However, this rigid point-to-point matching
enforces memorisation of particular points and strokes with-
out any spatial understanding. As such, sampling with a dif-
ferent resolution at this stage yields noisy output that does
not correspond to the raster sketch. To explicitly optimise
for sampling flexibility, we introduce a visual loss, penalis-
ing sketch reconstructions that do not match visually. For
this, we sample a template grid Gof 2D coordinates and
compute the region of influence of each vector-waypoint pm
of stroke skon coordinate gas an intensity map [3, 59]:
Ik(g, sk) = max
pm∈sk,r∈[0,1]exp(γ||g−rpm−(1−r)pm+1||2)(4)
where intensity Ikis smoothed with γfor points pmin
stroke skwhere m∈ {0, . . . ,K−1
K}. The intensity mapfor the entire sketch is formulated as a summation of maps
from all strokes for k∈(0,1]for both the ground truth
sketch vector pwithKstrokes and the arbitrarily sampled
sketch ˆpwith ˆKstrokes as ˆIk, composing the loss as:
LV=K−1
KX
k=0Ik−ˆK−1
ˆKX
k=0ˆIk
2(5)
Intensity Smoothing with γ:The sketch intensity map
Ikhelps us to ground vector sketches to raster definites
(Eq. (4)) by a visual rendering of individual strokes where
intensity in coordinates g∈Gdrops exponentially with
distance from pm∈sk. This exponential drop is controlled
by a smoothing factor γas a hyperparameter, where (i) a
lowγyields thicker lines on Ik(low-quality sketch recon-
structions) but offers better optimisation, while (ii) a high
γyields more refined lines (higher quality reconstructions),
making optimisation significantly slower (Fig. 3).
For faster optimisation, we assist the visual penalty with
a weighted MSE loss from Eq. (3) and a lower gamma, em-
pirically set to γ= 150 . The loss function weighted with
λMSE, looks can be written as:
Limplicit =Lγ=150
V +λMSE· LMSE (6)
where we set λMSEto0.7. This, however, inherently intro-
duces bias in our optimisation to match the order of strokes
with the ground truth order to some extent. Future works
could ideally set λMSE= 0 if they obtain faster conver-
gence from a better engineered implicit function fθ. Fig. 4a
summarises our implicit sketch learning framework.
10 20 50 100 200
StepsLoss
Increasing granularityReducing
Figure 3. Effect of smoothing factor γon training: Reducing γ
leads to higher intensity in the surrounding region near stroke sk.
This is similar to stroke dilation of intensity map Ik. A lower γ
leads to stable training (plot on right) but lacks fine-grained details.
A higher γgives a fine-grained sketch but is harder to train.
3.3. Generalising to Multiple Sketches
To efficiently represent multiple sketch implicits with a sin-
gle global and general function, we modify Eq. (1) to in-
clude a sketch descriptor νi∈Rdfor each sketch i. The
function fθ(νi, t,∆t, sk,∆s)is approximated with a fully
connected auto-decoder network, similar to DeepSDF [48].
Thus, feature descriptors νiare hidden vectors, learned
12568
...
GT Vector
Intensity
Map
:
(a): Single sketch Implicit Learning (b): Multi-sketch auto-decoder (c): Mapping to for generative modellingFigure 4. SketchINR model diagram. (a)Embedding a single vector sketch as an implicit model Fmapping stroke and time (s,t)to ink
coordinate p.(b)Embedding a vector sketch dataset as a shared decoder and set of latent vectors V.(c)Training a generative model for
implicit sketches by generating latent codes νusing an encoder Ethat inputs raster sketches.
jointly with the function parameters θon a training dataset
(i∈D). Similar to single sketch optimisation, we train
on multiple samples with a combination of MSE and visual
loss (Eq. (6)). Fig. 4b summarises our multi-sketch auto-
decoder through block diagrams.
Generative Modelling: The generalised auto-decoder, en-
ables simple generative modelling of sketches by generating
their feature descriptors νi∈ V, which allows us to create
novel sketch implicits. Towards this, we train a Variational
Auto-Encoder [35] as our generator, where we replace the
decoder with our pre-trained implicit decoder. For vari-
ational inference, our encoder E(·)represents each raster
sketch instance ias a set of dGaussians with mean µi∈Rd
and variance σi∈Rdwhich are re-parameterised [35] to a
latent encoding as zi=µi+σi·ϵwhere ϵ∈ N(0,I). This
latent is then projected to the d-dimensional implicit latent
space νi∈ V. We train the encoder as a ResNet-18 network
with (i) reconstruction loss from both our visual and vector
spaces (Eq. (6)) and (ii) KL-divergence loss to bring the dis-
tribution closer to a unit normal N(0,I). The loss function
for the encoder can be written as:
Lenc=Limplicit−β·DKL(q(zi|i),N(0,I)) (7)
where βis a weighing factor, set to 0.7andq(zi|i)repre-
sents the underlying probability distribution modelled with
encoder Ethat infers zifor a given sketch i. Fig. 4c sum-
marises our implicit sketch generative model. The decoder
can be used for sketch generation, while the encoder can
encode raster sketches for vectorisation.
4. Applications
Datasets: We learn implicits for sketches of various com-
plexities and abstraction levels, ranging from highly com-
plex scene sketches that correspond to a photo scenery
[13] to abstract doodles [28] drawn from memory. Based
off complexity in downstream applications, we use scene
sketches to demonstrate self-reconstruction and sketchcompression, while focusing on simpler object level
sketches for more complex tasks like sketch mixing (inter-
polation) and generation for faster optimisation. Specifi-
cally, we use the FS-COCO [13], Sketchy [53], and Quick-
Draw! [28] datasets. (i) FS-COCO [13] contains 10,000
hand-drawn sketches of complex MS-COCO [40] scenes,
drawn by amateurs without time constraints. Sketches in
FS-COCO have on a median of 64 strokes, resulting in the
sketch having ∼3000 waypoints. (ii) Comparatively sim-
pler than scene sketches, Sketchy [53] consists of ∼75K
sketches of 12.5K object photographs from 125 categories.
These sketches similar to FS-COCO are drawn from refer-
ence photographs and have photo-sketch pairs. (iii) Quick-
Draw! [28] sketches are drawn from memory and in a con-
strained time settings ( <20s). Hence, these sketches (doo-
dles) are abstract in nature, lacking any pre-defined config-
uration or orientation from reference photos. In addition to
these three datasets, we use a vectorised form of the MNIST
dataset, Vector-MNIST [17], consisting of ∼10K samples as
a simpler dataset to demonstrate complex downstream ap-
plications like generation and interpolation.
4.1. SketchINR: A Compact High-Fidelity Codec
We begin by demonstrating that SketchINR provides a high
fidelity neural representation for sketches. Specifically, we
train SketchINR on complex variable length vector sketch
datasets such as FS-COCO andSketchy . We represent each
sketch in these datasets as a fixed size vector with a dataset-
specific decoder fθ. We then render each implicit sketch
νi∈ V with the same number of strokes as the ground-
truth sketch pito evaluate reconstruction quality. Follow-
ing [17, 28], we measure reconstruction quality using (i)
Chamfer Distance (CD), and (ii) retrieval accuracy (top-10)
with a naive sketch retrieval network (ResNet-18) trained
on ground-truth sketch vectors pand evaluated on rendered
ˆ p. The quantitative results in Tab. 1, show that SketchINR
provides substantially higher fidelity encoding than alter-
native representations like RNN [28], B ´ezier strokes [16],
12569
FS-COCO [13] Sketchy [53]
CD R@10 Time (s) CD R@10 Time(s)
SketchRNN [28] – – – 0.16 41.65 0.11
CoSE [2] – – – 0.14 45.82 0.10
SketchODE [17] 0.41 5.17 1.68 0.17 37.20 0.13
B´ezierSketch [16] 0.57 2.96 1.22 0.23 25.72 0.08
Ours 0.011 15.61 0.3 0.008 57.29 0.0007
Table 1. Evaluating neural sketch representations’ reconstruction
quality (Chamfer distance ↓; recall@10 ↑) and decoding speed.
SketchINR
SketchINR
Quick-Draw! Sketchy
FS-COCO
Ground TruthGround Truth
SketchINR Ground Truth
Sketch-RNN
 SketchINR Ground Truth Sketch-RNN
Figure 5. Qualitative reconstruction results for neural sketch rep-
resentations. SketchINR is uniquely able to scale to sketches of
Sketchy andFS-COCO complexity.
ODE dynamics [17], and stroke embeddings [2] particu-
larly for extremely complex sketches in FS-COCO , where
SketchRNN and CoSE fail entirely. Qualitative results in
Fig. 5 show SketchINR to reconstruct simple and com-
plex sketches with high fidelity while SketchRNN performs
poorly for Sketchy and fails completely on FS-COCO .
Sketch Compression: We next show that SketchINR’s
high-fidelity encoding is extremely compact, thus providing
an efficient sketch codec. INRs store information implicitly
[55] in the network weights ( θ) and latent codes ( νi). To
store or transmit a sketch dataset, we only stream weights θ
once (fixed cost) and a compressed latent vector νi∈RD
for each sketch (variable cost). Hence, the space complexity
forNDsketches with SketchINR is |θ|+ND× |νi|bytes,
where νiis64dimensional latent. For streaming a large
number of sketches, we discount θas negligible streaming
overhead. Comparing to alternative representations, stream-
ingND(i) binary raster (BR) sketches of size (Nx×Ny)
takeND×Nx×Nybits , and (ii) vector scene sketches with
Ncoordinates and pen states ( R2+1) take ND×N×2×3
bytes in 16-bit floating precision ( N≈3000 inFS-COCO[13]). Raster representations can be further stored by only
storing the coordinates ( R2) of pixels containing the sketch
(i.e., inked pixels) as sparse binary rasters (SBR). Fig. 6
shows rate-distortion for storing 10,000scene sketches [13]
withνivarying from 128to8dimensional latent, and qual-
ity for raster and vector forms dropped with downsampling
and RDP simplification [23] respectively. Quality metrics
like PSNR and SSIM are more suited to photos. We use
Chamfer Distance as a quality metric for vector sketch and
observe ∼10×and∼60×more efficiency than vector and
raster sketches, respectively. Future works can further opti-
mise our models and thus reduce storage requirements via
model compression [21, 29, 56] and quantisation [11].
Chamfer
Distance
log( Bytes Per Point (BPP))
Figure 6. Rate Distortion: SketchINR can encode complex scene
sketches from FS-COCO [13] in highly compact ( R64) latent
codes. Specifically, despite nearly identical sketch quality (low
CD represents higher fidelity), SketchINR has ∼60×lower BPP
than PNG raster sketches and ∼10×lower than vectors sketches.
Intra-Sketch Variation: Unlike prior representations
(RNN [28], ODE dynamics [17]), humans do not draw exact
stroke-level replica of a raster sketch. Instead, we choose
the level of detail, the number of points, and the strokes
used to convey the underlying shape. Given a latent code
νi,SketchINR allows users to explicitly choose NandK,
where the number of points p={p1, . . . , p N}and strokes
s={s1, . . . , s K}. Choosing a low NandKleads to
abstract self-reconstructions with fewer points and strokes.
Fig. 7(top to bottom) shows that we can control Kto de-
crease or increase the level detail in the sketch.
4.2. Latent Space Interpolation (Creative Mixing)
Interpolation of learned latents allows us to explore latent
space continuity. Auto-regressive modelling of sketches
builds poor latents as explored by Das et. al. [19] due
to the lack of a full visibility of the sketch at any de-
coding timestep. Specifically, auto-regressive models use
the partially predicted sketch along with the sketch’s la-
tent representation to predict new way-points with consis-
tency. This, however, introduces noise in the latent →
sketch decoding so that small changes in these latents lead
to big changes in the final sketch . Deterministic latent to
sketch decoding solves this problem as noise is not added at
any point, leading to meaningful sketch interpolation from
corresponding latent space interpolations. We visualise in
12570
Recons.Less
StrokesGT
More
StrokesFigure 7. Given a latent code νi, we can reconstruct a sketch using
learned fθat multiple resolutions by controlling the number of
strokes Kduring sampling.
Cross-Sketch Interpolation with auto-regressive networks - SketchRNN
Cross-Sketch interpolation with implicits - SketchINR
Figure 8. Latent space interpolation shows continuity and transla-
tion from one sketch concept to another.
Fig. 8 how one sketch morphs into another by reconstruct-
ing from fθ(ν′, tn,∆t, sk,∆s), where ν′is sampled along
alatent walk :ν′= (1−δ)ν1+δν2, forδ∈[0,1]. Interpo-
lating between two sketches in the continuous latent space
gives creative mixing – modifying the sketch of airplane-
1 (facing right) into airplane-2 (facing left) by varying δ.
Fig. 8 shows that small changes in the latent (columns) leads
to smoother changes in sketch space for SketchINR. Mean-
while SketchRNN can exhibit big jumps that can zig-zag
over successive latent increments, as indicated by an exam-
ple tracked point in the figure.
4.3. INR inversion for sketch completion
Neural networks used to approximate implicit representa-
tions have been inverted in parallel literature [30] to appro-
priate samples not seen during training. This inversion, to
arrive at a latent νthat describes the new sample, is usu-
ally made possible by learned priors in fθ. Specifically,
an implicit decoder fθis frozen with latent νoptimised
forfθ(ν, . . . )against the ground truth. This behaviour is
easily replicated in SketchINR, where we optimise the la-
tentνto describe sketch samples (Fig. 5). To implement
(b) Input (V -MNIST)
 SketchRNN SketchINRSketch Completion using SketchRNN(a) Input Partial Stroke Vector -MNIST  (Left) and Quick-Draw! (Right)
Sketch Completion using SketchINRFigure 9. We perform sketch completion by inverting learned im-
plicit representations and decoding for occluded strokes or way-
points. (a) Temporal completion (b) Unlike auto-regressive meth-
ods like SketchRNN [28], we can perform a-temporal (or inverse-
temporal) completion, where given the second half of a partial
sketch, we can successfully reconstruct the first part.
sketch completion, we obtain a sketch descriptor νfrom
a partial sketch by just optimising for specific time-steps
(e.g. 1/4th of a sketch; t∈[0,1
4)) available in a partial
doodle. Then, by sampling for rest of the timesteps we
can complete the incomplete sketch. We summarise some
qualitative sketch completion results in Fig. 9(a). Com-
pleted sketches are varied because of randomness in op-
timisation, as partial sketches are often ambiguous. INR
inversion thus provides a novel approach to sketch com-
pletion, previously dominated by auto-regressive and mod-
els. Unlike auto-regressive models SketchINR can also per-
form a-temporal completion, inferring the first half of a
sketch given the second half of the sketch (Fig. 9b). We re-
mark that popular INR applications such as super-resolution
[47] and point-cloud densification [48] perform interpola-
tion after inversion on sparse inputs ( e.g.:t∈[0,1)given
t∈ {0,1
4,1
2,3
4}), while sketch completion requires extrap-
olation ( e.g.:t∈[0,1)given t∈[0,1
4)), making it more
analogous to visual outpainting.
4.4. Sketch Generation
We can perform conditional and unconditional sequential
vector sketch generation after learning a generative model
for sketch latents as described in Sec. 3.3.
Unconditional Generation: We unconditionally generate
sketches by random sampling ϵ∼ N (0,I)with the learnt
V AE latent space, obtaining novel sketch descriptors νi∈
V. We further sample fθ(νi, . . .)with varying timestamps
T∈[100,300] and strokestamps K∈[10,30]obtaining
sketches at varying levels of abstraction. Qualitative results
in Fig. 10 show diverse and plausible samples from Vector-
MNIST [17] and Quick-Draw! [28] datasets.
12571
SketchINR on V ector -MNIST SketchINR on Quick-Draw!Sketch-RNN on Quick-Draw! Sketch-RNN on V ector -MNIST
Figure 10. Unconditional sketch generation. Sampling sketch la-
tentsνas well as stroke and point complexity (K, T).
Conditional Generation: To condition sketch generation,
we use our pre-trained V AE encoder Eto embed raster
sketches, which can then be vectorised by the decoder fθ.
Fig. 11 shows qualitative results for sequential vector sketch
generation from raster images, where we obtain compara-
ble results with auto-regressive generative modelling [28].
We observe that treating vectorisation as a generative prob-
lem rather than a deterministic raster-to-vector mapping [7]
leads us to have variations in generated vectors particu-
larly in the form of stroke order resembling real copies of
sketches made by humans.
SketchINRSketch-RNN Raster Sketch
Less str okes More strokes
Figure 11. Conditional sketch generation (vectorisation) on
Vector-MNIST . Raster sketches (top left) can have multiple vector
forms, e.g., sketched with different numbers of strokes (bottom).
4.5. Ablations and Discussion
Global vs Local Time Modelling: We model Jtimes-
tamps across Kstrokes, by distributing timestamps uni-
formly over each stroke such that each stroke hasJ
Ktimes-
tamps. In other words, time ( t,∆t) is modelled globally for
the entire sketch p={p1, . . . , pN}. An alternative is to
model ( t,∆t) as0to1for each stroke separately (local time-
modelling) instead of the entire sketch. Particularly, each of
theKstrokes are modelled using Npoints, representing the
entire sketch using N·Kpoints as p={p1, . . . , pN·K}.
Visually inspecting global vs local time embedding shows
that local modelling leads to jittery sketches (see Fig. 12),
whereas global modelling gives visually smooth sketches.
We hypothesise that time is a global property of a sketch
and not an independent property localised to each stroke.
Hence, modelling time locally puts an additional optimisa-
tion burden on our implicit decoder to ensure smoothness
and continuity, resulting in poorly reconstructed sketches.
Global Time Embedding Local Time EmbeddingFigure 12. Global vs Local time embedding: Global embedding
gives smooth sketches compared to local, which gives jittering.
Fixed vs Variable Smoothing Factor ( γ):The intensity
mapIkin Eq. (4) guides our raster-based visual loss LV.
A key component in Ikis the smoothing parameter γ–
lowγ(thicker Ik) gives less accurate but smoother values
on the entire visual region (easier training); high γ(thin-
nerIk) gives more accurate but sharper (steeper) intensity-
gradients near ground-truth strokes (harder training). While
we use a fixed γwith MSE Loss in Eq. (6) to balance out
the optimisation complexity, we note that an alternative is
to vary γevery fixed number of steps with a scheduler to al-
low faster optimisation. Specifically, we vary γas20to200
on a linear scale by incremental ∆γevery few iterations.
Despite its theoretical advantage [25, 45], our initial exper-
iments suggest only minor improvement that reduces train-
ing time by 6.1%. A detailed study of bundle-adjustment
[45] for visual loss is an interesting future work.
Limitations and Future Work Despite it’s efficiency as a
representation for hand-drawn sketches, SketchINR suffers
from a number of limitations. We naturally share limitations
with other implicit representations including optimisation-
based encoding and poor cross category generalisation.
While unlike Sketch-RNN our generation network can be
trained on 2-3 categories at once , it fails to generalise on
more. In addition, a core limitation lies in slow convergence
due to pixel space optimisation. While vector-space loss in
the form of mean squared error mitigates this to some extent
(Fig. 3), convergence is still slow and can possibly be im-
proved by better engineering the implicit function. Strokes
could also be smoothed by explicit inclusion of stroke gra-
dient (slope) based regularisation in future work. Finally,
vectorisation for conditional generation performs poorly on
high complexity sketches.
5. Conclusion
We introduce visual implicits to represent vector sketches
with compressed latent descriptors. This neural represen-
tation provides a high-fidelity and compact representation
that raises the possibility of a sketch-specific codec for com-
pactly representing large sketch datasets. Our SketchINR
can decode sketches at varying levels of detail with control-
lable number of strokes, and provides superior cross-sketch
interpolation between implicits, demonstrating a smoother
latent space than auto-regressive models. Applications
such as sketch-completion are also supported, including
a-temporal completion not available with auto-regressive
models. Decoding is inherently parallel and can be over
100×faster than autoregressive models in practice.
12572
References
[1] Emre Aksan, Fabrizio Pece, and Otmar Hilliges. Deepwrit-
ing: Making digital ink editable via deep generative model-
ing. In CHI, 2018. 1, 2
[2] Emre Aksan, Thomas Deselaers, Andrea Tagliasacchi, and
Otmar Hilliges. Cose: Compositional stroke embeddings. In
NeurIPS , 2020. 1, 2, 3, 6
[3] Stephan Alaniz, Massimiliano Mancini, Anjan Dutta, Diego
Marcos, and Zeynep Akata. Abstracting sketches through
simple primitives. In ECCV , 2022. 4
[4] Hmrishav Bandyopadhyay, Pinaki Nath Chowdhury,
Ayan Kumar Bhunia, Aneeshan Sain, Tao Xiang, and
Yi-Zhe Song. What sketch explainability really means for
downstream tasks. In CVPR , 2024. 1
[5] Hmrishav Bandyopadhyay, Subhadeep Koley, Ayan Das,
Ayan Kumar Bhunia, Aneeshan Sain, Pinaki Nath Chowd-
hury, Tao Xiang, and Yi-Zhe Song. Doodle your 3d: From
abstract freehand sketches to precise 3d shapes. In CVPR ,
2024. 1
[6] Ayan Kumar Bhunia, Ayan Das, Umar Riaz Muhammad,
Yongxin Yang, Timothy M. Hospedales, Tao Xiang, Yulia
Gryaditskaya, and Yi-Zhe Song. Pixelor: A competitive
sketching ai agent. so you think you can beat me? In SIG-
GRAPH Asia , 2020. 1
[7] Ayan Kumar Bhunia, Pinaki Nath Chowdhury, Yongxin
Yang, Timothy M Hospedales, Tao Xiang, and Yi-Zhe Song.
Vectorization and rasterization: Self-supervised learning for
sketch and handwriting. In CVPR , 2021. 2, 8
[8] Alexandre Binninger, Amir Hertz, Olga Sorkine-Hornung,
Daniel Cohen-Or, and Raja Giryes. Sens: Part-aware
sketch-based implicit neural shape modeling. arXiv preprint
arXiv:2306.06088 , 2024. 1, 2
[9] Jack E Bresenham. Algorithm for computer control of a dig-
ital plotter. IBM Systems journal , 1965. 2
[10] Alexandre Carlier, Martin Danelljan, Alexandre Alahi, and
Radu Timofte. Deepsvg: A hierarchical generative network
for vector graphics animation. In NeurIPS , 2020. 2
[11] Qualcomm Innovation Center. Ai model efficiency toolkit
(aimet). https://github.com/quic/aimet , 2023.
6
[12] Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and
David K Duvenaud. Neural ordinary differential equations.
InNeurIPS , 2018. 3
[13] Pinaki Nath Chowdhury, Aneeshan Sain, Ayan Kumar Bhu-
nia, Tao Xiang, Yulia Gryaditskaya, and Yi-Zhe Song. Fs-
coco: Towards understanding of freehand sketches of com-
mon objects in context. In ECCV , 2022. 2, 5, 6
[14] Pinaki Nath Chowdhury, Ayan Kumar Bhunia, Aneeshan
Sain, Subhadeep Koley, Tao Xiang, and Yi-Zhe Song. What
can human sketches do for object detection? In CVPR , 2023.
2
[15] John Collomosse, Tu Bui, and Hailin Jin. Livesketch: Query
perturbations for guided sketch-based visual search. In
CVPR , 2019. 1, 2
[16] Ayan Das, Yongxin Yang, Timothy Hospedales, Tao Xiang,
and Yi-Zhe Song. B ´eziersketch: A generative model for scal-
able vector sketches. In ECCV , 2020. 1, 2, 3, 5, 6[17] Ayan Das, Yongxin Yang, Timothy Hospedales, Tao Xiang,
and Yi-Zhe Song. Sketchode: Learning neural sketch repre-
sentation in continuous time. In ICLR , 2021. 1, 2, 3, 5, 6,
7
[18] Ayan Das, Yongxin Yang, Timothy M Hospedales, Tao Xi-
ang, and Yi-Zhe Song. Cloud2curve: Generation and vector-
ization of parametric sketches. In CVPR , 2021. 2, 3
[19] Ayan Das, Yongxin Yang, Timothy Hospedales, Tao Xiang,
and Yi-Zhe Song. Chirodiff: Modelling chirographic data
with diffusion models. In ICLR , 2023. 3, 6
[20] Carl De Boor and Carl De Boor. A practical guide to splines .
Applied Mathematical Sciences, 1978. 3
[21] Zhen Dong, Zhewei Yao, Amir Gholami, Michael Mahoney,
and Kurt Keutzer. Hawq: Hessian aware quantization of neu-
ral networks with mixed-precision. In ICCV , 2019. 6
[22] Asen L. Dontchev and R. Tyrrell Rockafellar. Implicit Func-
tions and Solution Mappings . Springer-Verlag, 2014. 2
[23] David H Douglas and Thomas K Peucker. Algorithms for
the reduction of the number of points required to represent a
digitized line or its caricature. Cartographica: the interna-
tional journal for geographic information and geovisualiza-
tion, 1973. 6
[24] Mathias Eitz, James Hays, and Marc Alexa. How do humans
sketch objects? ACM TOG , 2012. 1, 2
[25] F. Dan Foresee and Martin T. Hagan. Gauss-newton approx-
imation to bayesian learning. In ICNN , 1997. 8
[26] Yaroslav Ganin, Tejas Kulkarni, Igor Babuschkin, SM Ali
Eslami, and Oriol Vinyals. Synthesizing programs for im-
ages using reinforced adversarial learning. In ICML , 2018.
3
[27] Songwei Ge, Vedanuj Goswami, C. Lawrence Zitnick, and
Devi Parikh. Creative sketch generation. In ICLR , 2021. 1
[28] David Ha and Douglas Eck. A neural representation of
sketch drawings. In ICLR , 2018. 1, 2, 3, 5, 6, 7, 8
[29] Hai Victor Habi, Roy H. Jennings, and Arnon Netzer. Hmq:
Hardware friendly mixed precision quantization block for
cnns. In ECCV , 2020. 6
[30] Amir Hertz, Or Perel, Raja Giryes, Olga Sorkine-Hornung,
and Daniel Cohen-Or. Spaghetti: Editing implicit shapes
through part aware generation. ACM TOG , 2022. 2, 7
[31] Aaron Hertzmann. Why do line drawings work? a realism
hypothesis. Perception , 2020. 1, 2
[32] Conghui Hu, Da Li, Yi-Zhe Song, Tao Xiang, and Timo-
thy M Hospedales. Sketch-a-classifier: Sketch-based photo
classifier generation. In CVPR , 2018. 3
[33] Conghui Hu, Da Li, Yongxin Yang, Timothy M Hospedales,
and Yi-Zhe Song. Sketch-a-segmenter: Sketch-based photo
segmenter generation. IEEE TIP , 2020. 2
[34] Heewoo Jun and Alex Nichol. Shap-e: Generat-
ing conditional 3d implicit functions. arXiv preprint
arXiv:2305.02463 , 2023. 2
[35] Diederik P Kingma and Max Welling. Auto-encoding varia-
tional bayes. arXiv preprint arXiv:1312.6114 , 2013. 5
[36] Kurt Koffka. Principles of Gestalt psychology . Routledge,
2013. 3
12573
[37] Subhadeep Koley, Ayan Kumar Bhunia, Aneeshan Sain,
Pinaki Nath Chowdhury, Tao Xiang, and Yi-Zhe Song. Pic-
ture that sketch: Photorealistic image generation from ab-
stract sketches. In CVPR , 2023. 1
[38] Changjian Li, Hao Pan, Adrien Bousseau, and Niloy J Mitra.
Free2cad: Parsing freehand drawings into cad commands.
ACM TOG , 2022. 1, 2
[39] Hangyu Lin, Yanwei Fu, Yu-Gang Jiang, and Xiangyang
Xue. Sketch-bert: Learning sketch bidirectional encoder rep-
resentation from transformers by self-supervised learning of
sketch gestalt. In CVPR , 2020. 2
[40] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context. In
ECCV , 2014. 5
[41] Hongyu Liu, Ziyu Wan, Wei Huang, Yibing Song, Xintong
Han, Jing Liao, Bin Jiang, and Wei Liu. Deflocnet: Deep
image editing via flexible low-level controls. In CVPR , 2021.
1, 2
[42] Asif Masood and Sidra Ejaz. An efficient algorithm for ro-
bust curve fitting using cubic bezier curves. In ICIC , 2010.
3
[43] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Se-
bastian Nowozin, and Andreas Geiger. Occupancy networks:
Learning 3d reconstruction in function space. In CVPR ,
2019. 2, 3
[44] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,
Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:
Representing scenes as neural radiance fields for view syn-
thesis. Communications of the ACM , 2021. 3
[45] Jorge J. Mo ´re. The levenberg-marquardt algorithm. In Nu-
merical Analysis , 1976. 8
[46] Umar Riaz Muhammad, Yongxin Yang, Yi-Zhe Song, Tao
Xiang, and Timothy M Hospedales. Learning deep sketch
abstraction. In CVPR , 2018. 3
[47] Quan H Nguyen and William J Beksi. Single image super-
resolution via a dual interactive implicit neural network. In
WACV , 2023. 7
[48] Jeong Joon Park, Peter Florence, Julian Straub, Richard
Newcombe, and Steven Lovegrove. Deepsdf: Learning con-
tinuous signed distance functions for shape representation.
InCVPR , 2019. 2, 3, 4, 7
[49] Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix
Draxler, Min Lin, Fred Hamprecht, Yoshua Bengio, and
Aaron Courville. On the spectral bias of neural networks.
InICML , 2019. 3
[50] Michael Revow, Christopher KI Williams, and Geoffrey E
Hinton. Using generative models for handwritten digit
recognition. IEEE TPAMI , 1996. 2
[51] Leo Sampaio Ferraz Ribeiro, Tu Bui, John Collomosse, and
Moacir Ponti. Sketchformer: Transformer-based representa-
tion for sketched structure. In CVPR , 2020. 1, 2
[52] Aneeshan Sain, Ayan Kumar Bhunia, Pinaki Nath Chowd-
hury, Subhadeep Koley, Tao Xiang, and Yi-Zhe Song. Clip
for all things zero-shot sketch-based image retrieval, fine-
grained or not. In CVPR , 2023. 2[53] Patsorn Sangkloy, Nathan Burnell, Cusuh Ham, and James
Hays. The sketchy database: learning to retrieve badly drawn
bunnies. ACM TOG , 2016. 1, 5, 6
[54] Heping Sheng, John Wilder, and Dirk B. Walther. Where to
draw the line? PLOS One , 2021. 2
[55] Yannick Str umpler, Janis Postels, Ren Yang, Luc van Gool,
and Federico Tombari. Implicit neural representations for
image compression. In ECCV , 2022. 6
[56] Stefan Uhlich, Lukas Mauch, Fabien Cardinaux, Kazuki
Yoshiyama, Javier Alonso Garcia, Stephen Tiedemann,
Thomas Kemp, and Akira Nakamura. Mixed precision dnns:
All you need is a good parametrization. In ICLR , 2020. 6
[57] Yael Vinker, Ehsan Pajouheshgar, Jessica Y Bo, Ro-
man Christian Bachmann, Amit Haim Bermano, Daniel
Cohen-Or, Amir Zamir, and Ariel Shamir. Clipasso:
Semantically-aware object sketching. ACM TOG , 2022. 1,
2, 3
[58] Yael Vinker, Yuval Alaluf, Daniel Cohen-Or, and Ariel
Shamir. Clipascene: Scene sketching with different types
and levels of abstraction. In ICCV , 2023. 1, 2, 3
[59] Alexander Wang, Mengye Ren, and Richard Zemel.
Sketchembednet: Learning novel concepts by imitating
drawings. In ICML , 2021. 2, 4
[60] Ximing Xing, Chuang Wang, Haitao Zhou, Jing Zhang, Qian
Yu, and Dong Xu. Diffsketcher: Text guided vector sketch
synthesis through latent diffusion models. In NeurIPS , 2023.
1
[61] Peng Xu, Yongye Huang, Tongtong Yuan, Kaiyue Pang, Yi-
Zhe Song, Tao Xiang, Timothy M Hospedales, Zhanyu Ma,
and Jun Guo. Sketchmate: Deep hashing for million-scale
human sketch retrieval. In CVPR , 2018. 1, 2
[62] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and
Thomas S Huang. Free-form image inpainting with gated
convolution. In ICCV , 2019. 2
[63] Qian Yu, Yongxin Yang, Yi-Zhe Song, Tao Xiang, and Tim-
othy Hospedales. Sketch-a-net that beats humans. In BMVC ,
2015. 2
[64] Qian Yu, Feng Liu, Yi-Zhe Song, Tao Xiang, Timothy M
Hospedales, and Chen-Change Loy. Sketch me that shoe. In
CVPR , 2016. 1, 3
[65] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding
conditional control to text-to-image diffusion models. In
ICCV , 2021. 1
[66] Wenni Zheng, Pengbo Bo, Yang Liu, and Wenping Wang.
Fast b-spline curve fitting by l-bfgs. Computer Aided Geo-
metric Design , 2012. 2
12574
