MoMask: Generative Masked Modeling of 3D Human Motions
Chuan Guo⇤Yuxuan Mu⇤Muhammad Gohar Javed⇤Sen Wang
Li Cheng
University of Alberta
{cguo2, ymu3, javed4, lcheng5 }@ualberta.ca
https://ericguo5513.github.io/momask/
Walking forward and steps over an object, and then continue walking.Taking two strides forward, pivot swiftly on left foot, and then walk the other way.
A person performs a standing back kick.Figure 1. Our MoMask, when provided with a text input, generates high-quality 3D human motion with diversity and precise control over
subtleties such as ” two strides forward ”, ”pivot on left foot ”, and ” pivot swiftly ”.
Abstract
We introduce MoMask, a novel masked modeling frame-
work for text-driven 3D human motion generation. In Mo-
Mask, a hierarchical quantization scheme is employed to
represent human motion as multi-layer discrete motion to-
kens with high-ﬁdelity details. Starting at the base layer,
with a sequence of motion tokens obtained by vector quan-
tization, the residual tokens of increasing orders are de-
rived and stored at the subsequent layers of the hierar-
chy. This is consequently followed by two distinct bidi-
rectional transformers. For the base-layer motion tokens,
a Masked Transformer is designated to predict randomly
masked motion tokens conditioned on text input at train-
ing stage. During generation (i.e. inference) stage, start-
ing from an empty sequence, our Masked Transformer iter-
atively ﬁlls up the missing tokens; Subsequently, a Residual
Transformer learns to progressively predict the next-layer
tokens based on the results from current layer. Extensive ex-
periments demonstrate that MoMask outperforms the state-
of-art methods on the text-to-motion generation task, with
1These authors contributed equally to this work.an FID of 0.045 (vs e.g. 0.141 of T2M-GPT) on the Hu-
manML3D dataset, and 0.228 (vs 0.514) on KIT-ML, re-
spectively. MoMask can also be seamlessly applied in re-
lated tasks without further model ﬁne-tuning, such as text-
guided temporal inpainting.
1. Introduction
Generating 3D human motions from textual descriptions,
aka text-to-motion generation, is a relatively new task that
may play an important role in a broad range of applications
such as video games, metaverse, and virtual reality & aug-
mented reality. In the past few years, it has generated in-
tensive research interests [ 9,15,16,21,23,36,42,49,50].
Among them, it has become popular to engage generative
transformers in modeling human motions [ 13,16,21,49].
In this pipeline, motions are transformed into discrete to-
kens through vector quantization (VQ), then fed into e.g.
an autoregressive model to generate the sequence of motion
tokens in an unidirectional order. Though achieving impres-
sive results, these methods shares two innate drawbacks. To
begin with, the VQ process inevitably introduces approxi-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
1900
mation errors, which imposes undesired limit to the motion
generation quality. Moreover, the unidirectional decoding
may unnecessarily hinder the expressiveness of the genera-
tive models. For instance, consider the following scenario:
at each time step, the motion content is generated by only
considering the preceding (rather than global) context; fur-
thermore, errors will often accumulate over the generation
process. Though several recent efforts using discrete diffu-
sion models [ 23,30] have considered to decode the motion
tokens bidirectionally, by relying on a cumbersome discrete
diffusion process, they typically require hundreds of itera-
tions to produce a motion sequence.
Motivated by these observations, we propose a novel
framework, MoMask, for high-quality and efﬁcient text-to-
motion generation by leveraging the residual vector quanti-
zation (RVQ) techniques [ 4,34,48] and the recent genera-
tive masked transformers [ 7,8,24,47]. Our approach builds
on the following three components. First, an RVQ-V AE is
learned to establish precise mappings between 3D motions
and the corresponding sequences of discrete motion tokens.
Unlike previous motion VQ tokenizers [ 13,16,49] that typ-
ically quantize latent codes in a single pass, our hierarchi-
cal RVQ employs iterative rounds of residual quantization
to progressively reduce quantization errors. This results in
multi-layer motion tokens, with the base layer serving to
perform standard motion quantization, and the rest layers
in the hierarchy capturing the residual coding errors of their
respective orders, layer by layer. Our quantization-based hi-
erarchical design is further facilitated by two distinct trans-
formers, the Masked Transformer (i.e. M-Transformer) and
Residual Transformer (R-Transformer), that are dedicated
to generating motion tokens for the base VQ layer and the
rest residual layers, respectively.
The M-Transformer, based on BERT [ 10], is trained to
predict the randomly masked tokens at the base layer, con-
ditioned on textual input. The ratio of masking, instead of
being ﬁxed [ 10,18], is a scheduled variable that ranges from
0 to 1. During generation, starting from all tokens being
masked out, M-Transformer produces a complete sequence
of motion tokens within a small number of iterations. At
each iteration, all masked tokens are predicted simultane-
ously . Predicted tokens with the highest conﬁdence will re-
main unchanged, while the others are masked again and re-
predicted in the next iteration. Once the base-layer tokens
are generated, the R-Transformer ensues to progressively
predict the residual tokens of the subsequent layer given the
token sequence at current layer. Overall, the entire set of
layered motion tokens can be efﬁciently generated within
merely 15 iterations, regardless of the motion’s length.
Our main contributions can be summarized as follows:
First, our MoMask is the ﬁrst generative masked modeling
framework for the problem of text-to-motion generation. It
comprises of a hierarchical quantization generative modeland the dedicated mechanism for precise residual quantiza-
tion, base token generation and residual token prediction.
Second, our MoMask pipeline produces precise and efﬁ-
cient text-to-motion generation. Empirically, it achieves
new state-of-the-art performance on text-to-motion gener-
ation task with an FID of 0.045 ( vs.0.141 in [ 49]) on Hu-
manML3D and 0.204 ( vs.0.514 in [ 49]) on KIT-ML. Third,
our MoMask also works well for related tasks, such as text-
guided motion inpainting.
2. Related Work
Human Motion Generation . Recently, we have witnessed
the surge of works for neural motion generation, with con-
ditioning on various domains such as motion preﬁx [ 29,33],
action class [ 6,14,31,35], audio [ 13,40,43,53], texts [ 9,
15,16,36,42]. Early works [ 1,12,20,27,38] commonly
model motion generation deterministically, resulting in av-
eraged and blurry motion results. This is properly addressed
by stochastic models. GAN modeling is adopted in [ 5,46]
for action-conditioned motion generation. Meanwhile, tem-
poral V AE framework and transformer architecture are ex-
ploited in the works of [ 17,35]. In terms of text-to-motion
generation, T2M [ 15] extended the temporal V AE to learn
the probabilistic mapping between texts and motions. Simi-
larly, TEMOS [ 36] takes advantage of Transformer V AE to
optimize a joint variational space between natural language
and motions, which is extended by TEACH [ 3] for long
motion compositions. MotionCLIP [ 41] and ohMG [ 28]
model text-to-motion in an unsupervised manner using the
large pretrained CLIP [ 39] model. The emerging diffu-
sion models and autoregressive models have signiﬁcantly
changed the ﬁeld of motion generation. In diffusion meth-
ods, a network is learned to gradually denoise the mo-
tion sequence, supervised by a scheduled diffusion pro-
cess [ 9,22,23,30,42,43,50]. Regarding autoregressive
models [ 13,16,21,49,52], motions are ﬁrstly discretized
as tokens via vector quantization [ 44], which are then mod-
eled by the expressive transformers as in language model.
Generative Masked Modeling. BERT [ 10] introduces
masked modeling for language tasks that word tokens are
randomly masked out with a ﬁxed ratio, and then the bi-
directional transformer learns to predict the masked tokens.
Despite being a decent pre-trained text encoder, BERT can-
not synthesize novel samples. In this regard, [ 7] proposes
to mask the tokens with a variable and traceable rate that
is controlled by a scheduling function. Therefore, new
samples can be synthesized iteratively following the sched-
uled masking. MAGE [ 24] uniﬁes representation learning
and image synthesis using the masked generative encoder.
Muse [ 8] extends this paradigm for text-to-image genera-
tion and editing. Magvit [ 47] suggests a versatile masking
strategy for multi-task video generation. Inspired by these
successes, we ﬁrst introduce generative masked modeling
1901
&0
&5
&2MaskCLIPMasked Transformer
(a) Motion Residual VQ-VAE(b) Masked TransformerPredicted Tokens *)5	EVQ	0VQ	1
VQ	V
Residual
D…Motion Tokens )5Residual TransformerMotion Tokens )5:4/0VQ ID%Predicted Tokens *)4
(c) Residual Transformer“A person walks in a circle.”Codebook IndexingMotion Tokens )5
“A person walks in a circle.”CLIPMasked tokenBase-layer tokenResidual–layer tokenResidual-layer VQ codeBase-layerVQ code
Figure 2. Approach overview. (a) Motion sequence is tokenized through vector quantization (VQ), also referred to as the base quantization
layer, as well as a hierarchy of multiple layers for residual quantization. (b) Parallel prediction by the Masked Transformer: the tokens in
the base layer t0are randomly masked out with a variable rate, and then a text-conditioned masked transformer is trained to predict the
masked tokens in the sequence simultaneously. (c) Layer-by-layer progressive prediction by the Residual Transformer. A text-conditioned
residual transformer learns to progressively predict the residual tokens tj>0from the tokens in previous layers, t0:j 1.
for human motion synthesis in this paper.
Deep Motion Quantization and RVQ. [2] learns seman-
tically meaningful discrete motif words leveraging triplet
contrastive learning. TM2T [ 16] starts applying vector
quantized-V AE [ 44] to learn the mutual mapping between
human motions and discrete tokens, where the autoencod-
ing latent codes are replaced with the selected entries from
a codebook. T2M-GPT [ 49] further enhances the perfor-
mance using EMA and code reset techniques. This is
adopted in several other works such as PoseGPT [ 31] and
MotionGPT [ 21,52]. Nevertheless, the quantization pro-
cess inevitably introduces errors, leading to suboptimal mo-
tion reconstruction. In this work, we adapt residual quanti-
zation [ 4,34,48], a technique used in neural network com-
pression [ 11,25,26] and audio quantization [ 4,45] which
iteratively quantizes a vector and its residuals. This ap-
proach represents the vector as a stack of codes, enabling
high-precision motion discretization.
3. Approach
Our goal is to generate a 3D human pose sequence m1:Nof
length Nguided by a textual description c, where mi2RD
with Ddenoting the dimension of pose features. As illus-
trated in Fig. 2, our approach consists of three principle
components: a residual-based quantizer that tokenizes mo-
tion sequence into multi-layer discrete tokens (Sec. 3.1),
a masked transformer that generates motion tokens in the
base layer (Sec. 3.2), and a residual transformer (Sec. 3.3)
that predicts the tokens in the subsequent residual layers.
The inference process of generation is detailed in Sec. 3.4.3.1. Training: Motion Residual VQ-VAE
Conventional motion VQ-V AEs [ 16,21,49,52] transform
a motion sequence into one tuple of discrete motion tokens.
Speciﬁcally, the motion sequence m1:N2RN⇥Dis ﬁrstly
encoded into a latent vector sequence ˜b1:n2Rn⇥dwith
downsampling ratio of n/N and latent dimension d, using
1D convolutional encoder E; each vector is subsequently
replaced with its nearest code entry in a preset codebook
C={ck}K
k=1⇢Rd, known as quantization Q(·). Then the
quantized code sequence b1:n=Q ( ˜b1:n)2Rn⇥dis pro-
jected back to motion space for reconstructing the motion
ˆm=D ( b). After all, the indices of the selected codebook
entries (namely motion tokens ) are used as the alternative
discrete representation of input motion. Though effective,
the quantization operation Q(·)inevitably leads to informa-
tion loss, which further limits the quality of reconstruction.
To address this issue, we introduce residual quantization
(RQ) as described in Fig. 2(a). In particular, RQ repre-
sents a motion latent sequence ˜basV+1ordered code
sequences, using V+1quantization layers. Formally, this
is deﬁned as RQ(˜b1:n)=[ bv
1:n]V
v=0, with bv
1:n2Rn⇥d
denoting the code sequence at the v-th quantization layer.
Concretely, starting from 0-th residual r0=˜b, RQ recur-
sively calculates bvas the approximation of residual rv, and
then the next residual rv+1as
bv=Q ( rv),rv+1=rv bv, (1)
forv=0,. . . ,V . After RQ, the ﬁnal approximation of latent
sequence ˜bis the sum of all quantized sequencesPV
v=0bv,
which is then fed into decoder Dfor motion reconstruction.
1902
M-TransformerRemask…M-TransformerR-Transformer…R-TransformerD6*(8)6*(:)6*+−- steps6+.−- steps6*:-Text embedding of ”A person walks in a circle”
Generating the base-layer tokens.Predicting the residual-layer tokens.Decoding
Masked tokenBase-layer tokenResidual–layer tokenFigure 3. Inference process. Starting from an empty sequence t0(0), the M-Transformer generates the base-layer token sequence t0inL
iterations. Following this, the R-Transformer progressively predicts the rest-layer token sequences t2:Vwithin V 1steps.
Overall, the residual VQ-V AE is trained via a motion re-
construction loss combined with a latent embedding loss at
each quantization layer:
Lrvq=km ˆmk1+ VX
v=1krv sg[bv]k2
2, (2)
where sg[·]denotes the stop-gradient operation, and  a
weighting factor for embedding constraint. This framework
is optimized with straight-though gradient estimator [ 44],
and our codebooks are updated via exponential moving av-
erage and codebook reset following T2M-GPT [ 49].
Quantization Dropout. Ideally, the early quantization lay-
ers are expected to restore the input motion as much as pos-
sible; then the later layers add up the missing ﬁner details.
To exploit the capacity of each quantizer, we adopt a quan-
tization dropout strategy, which randomly disables the last
0 toVlayers with probability q2[0,1]during training.
After training, each motion sequence mcan be repre-
sented as V+1 discrete motion token sequences T=
[tv
1:n]V
v=0where each token sequence tv
1:n2{1,. . . ,|Cv|}nis
the ordered codebook-indices of quantized embedding vec-
torsbv
1:n, such that bv
i=Cv
tv
ifori2[1,n]. Among these
V+1sequences, the ﬁrst (i.e. base) sequence possesses
the most prominent information, while the impact of subse-
quent layers gradually diminishes.
3.2. Training: Masked Transformer
Our bidirectional masked transformer is designed to model
the base-layer motion tokens t0
1:n2Rn, as illustrated
in Figure 2(b). We ﬁrst randomly masked out a varying
fraction of sequence elements, by replacing the tokens with
a special [MASK] token. With ˜t0denoting the sequence af-
ter masking, the goal is to predict the masked tokens given
textcand˜t0. We use CLIP [ 39] for extracting text features.
Mathematically, our masked transformer p✓is optimized to
minimize the negative log-likelihood of target predictions:
Lmask=X
˜t0
k=[MASK] logp✓(t0
k|˜t0,c). (3)Mask Ratio Schedule. We adopt a cosine function  (·)for
scheduling the masking ratio following [ 7,8]. Practically,
the mask ratio is obtained by  (⌧) = cos(⇡⌧
2)2[0,1],
where ⌧2[0,1]that⌧=0means the sequence is com-
pletely corrupted. During training, the ⌧⇠U (0,1)is ran-
domly sampled, and then m=d (⌧)·nesequence entries
are uniformly selected to be masked with ndenoting the
length of sequence.
Replacing and Remasking. To enhance the contextual rea-
soning of the masked transformer, we adopt the remasking
strategy used in BERT pretraining [ 10]. If a token is se-
lected for masking, we replace this token with (1) [MASK]
token 80% of the time; (2) a random token 10% of the time;
and (3) an unchanged token 10% of the time.
3.3. Training: Residual Transformer
We learn a single residual transformer to model the tokens
from the other Vresidual quantization layers. The residual
transformer has a similar architecture to the masked trans-
former (Sec. 3.2), except that it contains Vseparate embed-
ding layers. During training, we randomly select a quan-
tizer layer j2[1,V]to learn. All the tokens in the pre-
ceding layers t0:j 1are embedded and summed up as the
token embedding input. Taking the token embedding, text
embedding, and RQ layer indicator jas input, the residual
transformer p is trained to predict the j-th layer tokens in
parallel. Overall, the training objective is:
Lres=VX
j=1nX
i=1 logp (tj
i|t0:j 1
i,c ,j). (4)
We also share the parameters of the j-th prediction layer
and the (j+ 1)-th motion token embedding layer for more
efﬁcient learning.
3.4. Inference
As presented in Figure 3, there are three stages in in-
ference. Firstly, starting from an empty sequence t0(0)
that all tokens are masked out, we expect to generate the
base-layer token sequence t0of length ninLiterations.
1903
Datasets MethodsR Precision"FID#MultiModal Dist#MultiModality"Top 1 Top 2 Top 3HumanML3DTM2T [16]0.424±.0030.618±.0030.729±.0021.501±.0173.467±.0112.424±.093T2M [15]0.455±.0030.636±.0030.736±.0021.087±.0213.347±.0082.219±.074MDM [42]- -0.611±.0070.544±.0445.566±.0272.799±.072MLD [9]0.481±.0030.673±.0030.772±.0020.473±.0133.196±.0102.413±.079MotionDiffuse [50]0.491±.0010.681±.0010.782±.0010.630±.0013.113±.0011.553±.042T2M-GPT [49]0.492±.0030.679±.0020.775±.0020.141±.0053.121±.0091.831±.048ReMoDiffuse [51]0.510±.0050.698±.0060.795±.0040.103±.0042.974±.0161.795±.043MotionGPT [21]0.492±.0030.681±.0030.778±.0020.232±.0083.096±.0082.008±.084MoMask (base)0.504±.0040.699±.0060.797±.0040.082±.0083.050±.0131.050±.061MoMask0.521±.0020.713±.0020.807±.0020.045±.0022.958±.0081.241±.040KIT-MLTM2T [16]0.280±.0050.463±.0060.587±.0053.599±.1534.591±.0263.292±.081T2M [15]0.361±.0050.559±.0070.681±.0073.022±.1073.488±0282.052±.107MDM [42]- -0.396±.0040.497±.0219.191±.0221.907±.214MLD [9]0.390±.0080.609±.0080.734±.0070.404±.0273.204±.0272.192±.071MotionDiffuse [50]0.417±.0040.621±.0040.739±.0041.954±.0622.958±.0050.730±.013T2M-GPT [49]0.416±.0060.627±.0060.745±.0060.514±.0293.007±.0231.570±.039ReMoDiffuse [51]0.427±.0140.641±.0040.765±.0550.155±.0062.814±.0121.239±.028MoMask (base)0.415±.0100.634±.0110.760±.0050.372±.0202.931±.0411.097±.054MoMask0.433±.0070.656±.0050.781±.0050.204±.0112.779±.0221.131±.043Table 1. Quantitative evaluation on the HumanML3D and KIT-ML test set. ±indicates a 95% conﬁdence interval. MoMask (base)
means that MoMask only uses base-layer tokens. Bold face indicates the best result, while underscore refers to the second best.
Given the masked token sequence at l-th iteration t0(l), M-
Transformer ﬁrst predicts the probability distribution of to-
kens at the masked locations, and samples motion tokens
with the probability. Then the sampled tokens with the low-
estd (l
L)·neconﬁdences are masked again, and the other
tokens will remain unchanged for the rest iterations. This
new token sequence t0(l+1)is used to predict the token se-
quence at the next iteration until lreaches L. Once the base-
layer tokens are completely generated, the R-Transformer
progressively predicts the token sequence in the rest quanti-
zation layers. Finally, all tokens are decoded and projected
back to motion sequences through the RVQ-V AE decoder.
Classiﬁer Free Guidance. We adopt classiﬁer-free
guidance (CFG) [ 8,19] for the prediction of both M-
Transformer and R-Transformer. During training, we train
the transformers unconditionally c=;with probability of
10%. During inference, CFG takes place at the ﬁnal lin-
ear projection layer before softmax, where the ﬁnal logits
!gare computed by moving the conditional logits !caway
from the unconditional logits !uwith guidance scale s:
!g=( 1+ s)·!c s·!u. (5)
4. Experiments
Empirical evaluations are conducted on two widely used
motion-language benchmarks, HumanML3D [ 15] and KIT-
ML [ 37].HumanML3D dataset collects 14,616 motions
from AMASS [ 32] and HumanAct12 [ 14] datasets, with
each motion described by 3 textual scripts, totaling 44,970descriptions. This diverse motion-language dataset contains
a variety of actions, including exercising, dancing, and ac-
robatics. KIT-ML dataset consists of 3,911 motions and
6,278 text descriptions, offering an small-scale evaluation
benchmark. For both motion datasets, we adopt the pose
representation from the work of T2M [ 15]. The datasets are
augmented by mirroring, and divided into training, testing,
and validation sets with the ratio of 0.8:0.15:0.05 .
Evaluation metrics from T2M [ 15] are also adopted
throughout our experiments including: (1) Frechet Incep-
tion Distance (FID), which evaluates the overall motion
quality by measuring the distributional difference between
the high-level features of the generated motions and those
of real motions; (2) R-Precision andmultimodal distance ,
which gauge the semantic alignment between input text and
generated motions; and (3) Multimodality for assessing the
diversity of motions generated from the same text.
Though multimodality is indeed important, we stress its
role as a secondary metric that should be assessed in the
conjunction with primary performance metrics such as FID
and RPrecision. Emphasizing multimodality without con-
sidering the overall quality of generated results could lead
to optimization of models that produce random outputs for
any given input.
Implementation Details. Our models are implemented us-
ing PyTorch. For the motion residual VQ-V AE, we employ
resblocks for both the encoder and decoder, with a down-
scale factor of 4. The RVQ consists of 6 quantization layers,
where each layer’s codebook contains 512 512-dimensional
1904
MoMask(Ours)MLD
T2M-GPTMDM
Time axesA person sneaks away while walking sideways.This person stumbles left and right while moving forward.A person walks forwards, sits.
Figure 4. Visual comparisons between the different methods given three distinct text descriptions from HumanML3D testset. Only key
frames are displayed. Compared to previous methods, MoMask generates motions with higher quality and better understanding of the
subtle language concepts such as ” stumble ”, ”sneak ”, ”walk sideways ”. Please refer to the demo video for complete motion clips.
codes. The quantization dropout ratio qis set to 0.2. Both
the masked transformer and residual transformer are com-
posed of 6 transformer layers, with 6 heads and a latent di-
mension of 384, applied to the HumanML3D and KIT-ML
datasets. The learning rate reaches 2e-4 after 2000 itera-
tions with a linear warm-up schedule for the training of all
models. The mini-batch size is uniformly set to 512 for
training RVQ-V AE and 64, 32 for training transformers on
HumanML3D and KIT-ML, respectively. During inference,
we use the CFG scale of 4 and 5 for M-Transformer and
R-Transformer on HumanML3D, and (2, 5) on KIT-ML.
Meanwhile, Lis set to 10 on both datasets.
4.1. Comparison to state-of-the-art approaches
We compare our approach to a set of existing state-of-the-
art works ranging from V AE [ 15], diffusion-based mod-
els [9,42,51], to autoregressive models [ 16,49].
Quantitative Comparisons. Following previous prac-tices [ 15,42], each experiment is repeated 20 times, and
the reported metric values represent the mean with a 95%
statistical conﬁdence interval. Additionally, we conduct ex-
periments with MoMask exclusively generating the base-
layer motion tokens, denoted as MoMask (base). Quantita-
tive results for the HumanML3D and KIT-ML datasets are
presented in Table 1.
Overall, MoMask attains state-of-the-art performance on
both datasets, demonstrating substantial improvements in
metrics such as FID, R-Precision, and multimodal distance.
For the suboptimal performance on KIT-ML dataset, we
would like to point out that the leading model, ReMoDif-
fuse [ 51], involves more intricate data retrieval from a large
database to achieve high-quality motion generation. Ad-
ditionally, we observe that MoMask, even with the base-
layer tokens alone, already achieves competitive perfor-
mance compared to baselines, and the inclusion of residual
tokens further elevates the results to a higher level.
1905
(a) Comparisons on FID and Inference Cost(b) HumanML3D User StudyFigure 5. (a) Comparison of inference time costs. All tests are conducted on the same Nvidia2080Ti. The closer the model is to the
origin, the better. (b) User study results on the HumanML3D dataset. Each bar represents the preference rate of MoMask over the
compared model. Overall, MoMask is preferred over the other models most of the time. The dashed line marks 50%.
In Figure 5(a), we evaluate the efﬁciency and quality of
motion generation using various methods. The inference
cost is calculated as the average inference time over 100
samples on one Nvidia2080Ti device. Comparing to base-
line methods, MoMask positions itself more favorably be-
tween generation quality and efﬁciency.
User Study. We further conduct a user study on Amazon
Mechanical Turk to validate our previous conclusions. This
user study involves 42 AMT users with master recognition,
with the side-by-side comparisons between MoMask and
each of the state-of-the-art methods including MDM [ 42],
MLD [ 9] and T2M-GPT [ 49]. We generate the 50 mo-
tions for each method using the same text pool from Hu-
manML3D test set, and collect feedback from 3 distinct
users for each comparison. As shown in Fig. 5(b), MoMask
is preferred by users in most of the time, and even earns
42% of preference on par with ground truth motions.
Qualitative Comparisons. Figure 4displays qualitative
comparisons of our approach and MDM[ 42], MLD [ 9],
and T2M-GPT [ 49]. MDM [ 42] usually generates overall
semantically correct motions but fails to capture nuanced
concepts such as ” sneak ” and ” sideways ”. Though T2M-
GPT [ 49] and MLD [ 9] have improved performance in this
aspect, they still ﬁnd it difﬁcult to generate motions accu-
rately aligned with the textual description. For example, in
the bottom row, the motions from these two methods either
forget to walk sideways (T2M-GPT [ 49]) or to sneak away
(MLD [ 9]). Moreover, MLD [ 9] sometimes produces life-
less motions where the character slides around, as shown in
the top row. In comparison, our method is able to generate
high-quality motions faithful to the input texts. Please refer
to supplementary videos for dynamic visualizations.
4.2. Component Analysis
In Table 2, we comprehensively evaluate the impact of dif-
ferent design components in MoMask through various com-
parisons, showcasing the performance in both motion re-
construction and generation. Initially, we compare our ap-MethodsReconstruction Generation
FID# MPJPE # FID# MM-Dist #
Evaluation on KIT-ML dataset
M2DM [ 23] 0.413±.009- 0.515±.0293.015±.017
T2M-GPT [ 49] 0.472±.011- 0.514±.0293.007±.023
MoMask 0.112±.00237.2 0.228±.0112.774±.022
Evaluation on HumanML3D dataset
TM2T [ 16] 0.307±.002230.1 1.501±.0173.467±.011
M2DM [ 23] 0.063±.001- 0.352±.0053.116±.008
T2M-GPT [ 49] 0.070±.00158.0 0.141±.0053.121±.009
MoMask 0.019±.00129.5 0.051±.0022.957±.008
w/oRQ 0.091±.00158.7 0.093±.0043.031±.009
w/oQDropout 0.077±.00039.3 0.091±.0032.959±.008
w/oRRemask - - 0.063±.0033.049±.006
MoMask ( V, 0) 0.091±.00158.7 0.093±.0043.031±.009
MoMask ( V, 1) 0.069±.00154.6 0.073±.0033.031±.008
MoMask ( V, 2) 0.049±.00246.0 0.072±.0032.978±.006
MoMask ( V, 3) 0.037±.00142.5 0.064±.0032.970±.007
MoMask ( V, 4) 0.027±.00135.3 0.069±.0032.987±.007
MoMask ( V, 5) 0.019±.00129.5 0.051±.0022.962±.008
MoMask ( V, 6) 0.014±.00126.7 0.076±.0032.994±.007
MoMask ( V, 7) 0.014±.00025.3 0.084±.0042.968±.007
MoMask ( q, 0) 0.077±.00039.3 0.091±.0032.959±.008
MoMask ( q, 0.2) 0.019±.00129.5 0.051±.0022.957±.008
MoMask ( q, 0.4) 0.021±.00030.2 0.082±.0033.006±.007
MoMask ( q, 0.6) 0.024±.00033.2 0.053±.0032.946±.006
MoMask ( q, 0.8) 0.023±.00033.4 0.083±.0043.002±.008
Table 2. Comparison of our RVQ design vs. motion VQs from pre-
vious works [ 16,23,49], and further analysis on residual quantiza-
tion (RQ), quantization dropout (QDropout), and replacing & re-
masking (RRmask). Vandqare the number of RQ and QDropout
ratio, respectively. MPJPE is measured in millimeters. Generation
results are based on 18 inference steps.
proach with previous VQ-based motion generation meth-
ods [16,23,49] on the HumanML3D and KIT-ML datasets.
Notably, M2DM [ 23] incorporates orthogonality constraints
among all codebook entries to enhance VQ performance.
Our residual design shows clearly superior performance
when comparing with these single VQ-based approaches.
Ablation. In the ablation experiments, we observe that
both residual quantization (RQ) and quantization dropout
(QDropout) effectively contribute to the enhancement of
motion quality in terms of both reconstruction and gener-
1906
Input regionGenerated region
Effect range: 3s - 6sA person falls down and gets back up quickly.Effect range: last 2sA person bows.Effect range: 3s - 5sA person kicks something.
Effect range: first 2sA person is doing warm up.Figure 6. Examples of temporal inpainting. Dark dash line indicates the range(s) where the motion content(s) is given by the reference
sequence. Orange dash line indicates the range of motion content generated by MoMask, conditioned on the text prompt below.
Figure 7. Evaluation sweep over guidance scale s(top) and itera-
tion numbers L(bottom) in inference. We ﬁnd a accuracy-ﬁdelity
sweep spot around s=4, meanwhile 10 iterations ( L=1 0 ) for
masked decoding yield sufﬁciently good results.
ation. Additionally, replacing-and-remasking strategy, as
well as RQ, facilitates more faithful motion generation.
Number of Residual Layers ( V).In Tab. 2, we inves-
tigate RVQ with different numbers of quantization layers.
Generally, more residual VQ layers result in more precise
reconstruction, but they also increase the burden on the R-
Transformer for residual token generation. We particularly
observe that the generation performance starts to degrade
with more than 5 residual layers. This ﬁnding emphasizes
the importance of striking a balance in the number of resid-
ual layers for optimal performance.
Quantization Dropout ( q).We also analyze the impact
of quantization dropout ratio qin Tab. 2. As we increase
dropout probability from 0.2, the performance gains be-
come marginal, or even converse. We speculate that fre-
quent disabling quantization layers may disturb the learning
of quantization models.
Inference Hyper-parameters. The CFG scale sand the
number of iterations Lare two crucial hyperparameters dur-
ing the inference of masked modeling. In Fig. 7, we present
the performance curves of FID and multimodality distance
by sweeping over different values of sandL. Several keyobservations emerge. Firstly, an optimal guidance scale s
for M-Transformer inference is identiﬁed around s=4.
Over-guided decoding may even inversely deteriorate the
performance. Secondly, more iterations are not necessarily
better. As Lincreases, the FID and multimodality distance
converge to the minima quickly, typically within around 10
iterations. Beyond 10 iterations, there are no further perfor-
mance gains in both FID and multimodal distance. In this
regard, our MoMask requires fewer inference steps com-
pared to most autoregressive and diffusion models.
4.3. Application: Temporal Inpainting
In Fig. 6, we showcase the capability of MoMask in tem-
porally inpainting a speciﬁc region in a motion sequence.
The region can be freely located in the middle, sufﬁx, or
preﬁx. Speciﬁcally, we mask out all the tokens in the re-
gion of interest and then follow the same inference proce-
dure described in Sec. 3.4. For both tasks, our approach
generates smooth motions in coherence with the given text
descriptions. Additionally, we conduct a user study to
quantitatively compare our inpainting results with those of
MDM [ 42]. In this study, 40 samples are generated from
both methods using the same motion and text input, and
presented to users side-by-side. With 6 users involved, 68%
of the results from MoMask are preferred over MDM.
5. Conclusion
In conclusion, we introduce MoMask, a novel generative
masked modeling framework for text-driven 3D human
motion generation. MoMask features three advanced
techniques: residual quantization for precise motion
quantization, masked transformer and residual trans-
former for high-quality and faithful motion generation.
MoMask is efﬁcient and ﬂexible, achieving superior per-
formance without extra inference burden, and effortlessly
supporting temporal motion inpainting in multiple contexts.
1907
References
[1]Chaitanya Ahuja and Louis-Philippe Morency. Lan-
guage2pose: Natural language grounded pose forecasting.
In2019 International Conference on 3D Vision (3DV) , pages
719–728. IEEE, 2019. 2
[2]Andreas Aristidou, Daniel Cohen-Or, Jessica K Hodgins,
Yiorgos Chrysanthou, and Ariel Shamir. Deep motifs and
motion signatures. ACM Transactions on Graphics (TOG) ,
37(6):1–13, 2018. 3
[3]Nikos Athanasiou, Mathis Petrovich, Michael J Black, and
G¨ul Varol. Teach: Temporal action composition for 3d hu-
mans. In 2022 International Conference on 3D Vision (3DV) ,
pages 414–423. IEEE, 2022. 2
[4]Zal´an Borsos, Rapha ¨el Marinier, Damien Vincent, Eugene
Kharitonov, Olivier Pietquin, Matt Shariﬁ, Dominik Roblek,
Olivier Teboul, David Grangier, Marco Tagliasacchi, et al.
Audiolm: a language modeling approach to audio genera-
tion. IEEE/ACM Transactions on Audio, Speech, and Lan-
guage Processing , 2023. 2,3
[5]Haoye Cai, Chunyan Bai, Yu-Wing Tai, and Chi-Keung
Tang. Deep video generation, prediction and completion of
human action sequences. In Proceedings of the European
Conference on Computer Vision (ECCV) , pages 366–382,
2018. 2
[6]Pablo Cervantes, Yusuke Sekikawa, Ikuro Sato, and Koichi
Shinoda. Implicit neural representations for variable length
human motion generation. In European Conference on Com-
puter Vision , pages 356–372. Springer, 2022. 2
[7]Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T
Freeman. Maskgit: Masked generative image transformer.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 11315–11325, 2022.
2,4
[8]Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot,
Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Mur-
phy, William T Freeman, Michael Rubinstein, et al. Muse:
Text-to-image generation via masked generative transform-
ers.arXiv preprint arXiv:2301.00704 , 2023. 2,4,5
[9]Xin Chen, Biao Jiang, Wen Liu, Zilong Huang, Bin Fu, Tao
Chen, and Gang Yu. Executing your commands via motion
diffusion in latent space. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 18000–18010, 2023. 1,2,5,6,7
[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. Bert: Pre-training of deep bidirectional
transformers for language understanding. arXiv preprint
arXiv:1810.04805 , 2018. 2,4,1
[11] Sohrab Ferdowsi, Slava Voloshynovskiy, and Dimche
Kostadinov. Regularized residual quantization: a multi-
layer sparse dictionary learning approach. arXiv preprint
arXiv:1705.00522 , 2017. 3
[12] Anindita Ghosh, Noshaba Cheema, Cennet Oguz, Christian
Theobalt, and Philipp Slusallek. Synthesis of compositional
animations from textual descriptions. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 1396–1406, 2021. 2[13] Kehong Gong, Dongze Lian, Heng Chang, Chuan Guo,
Xinxin Zuo, Zihang Jiang, and Xinchao Wang. Tm2d: Bi-
modality driven 3d dance generation via music-text integra-
tion. arXiv preprint arXiv:2304.02419 , 2023. 1,2
[14] Chuan Guo, Xinxin Zuo, Sen Wang, Shihao Zou, Qingyao
Sun, Annan Deng, Minglun Gong, and Li Cheng. Ac-
tion2motion: Conditioned generation of 3d human motions.
InProceedings of the 28th ACM International Conference on
Multimedia , pages 2021–2029, 2020. 2,5
[15] Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji,
Xingyu Li, and Li Cheng. Generating diverse and natural 3d
human motions from text. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 5152–5161, 2022. 1,2,5,6
[16] Chuan Guo, Xinxin Zuo, Sen Wang, and Li Cheng. Tm2t:
Stochastic and tokenized modeling for the reciprocal genera-
tion of 3d human motions and texts. In European Conference
on Computer Vision , pages 580–597. Springer, 2022. 1,2,3,
5,6,7
[17] Chuan Guo, Xinxin Zuo, Sen Wang, Xinshuang Liu, Shihao
Zou, Minglun Gong, and Li Cheng. Action2video: Gener-
ating videos of human 3d actions. International Journal of
Computer Vision , 130(2):285–315, 2022. 2
[18] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr
Doll´ar, and Ross Girshick. Masked autoencoders are scalable
vision learners. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 16000–
16009, 2022. 2
[19] Jonathan Ho and Tim Salimans. Classiﬁer-free diffusion
guidance. arXiv preprint arXiv:2207.12598 , 2022. 5
[20] Ruozi Huang, Huang Hu, Wei Wu, Kei Sawada, Mi Zhang,
and Daxin Jiang. Dance revolution: Long-term dance gen-
eration with music via curriculum learning. arXiv preprint
arXiv:2006.06119 , 2020. 2
[21] Biao Jiang, Xin Chen, Wen Liu, Jingyi Yu, Gang Yu, and
Tao Chen. Motiongpt: Human motion as a foreign language.
arXiv preprint arXiv:2306.14795 , 2023. 1,2,3,5
[22] Jihoon Kim, Jiseob Kim, and Sungjoon Choi. Flame: Free-
form language-based motion synthesis & editing. In Pro-
ceedings of the AAAI Conference on Artiﬁcial Intelligence ,
pages 8255–8263, 2023. 2
[23] Hanyang Kong, Kehong Gong, Dongze Lian, Michael Bi Mi,
and Xinchao Wang. Priority-centric human motion genera-
tion in discrete latent space. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages 14806–
14816, 2023. 1,2,7
[24] Tianhong Li, Huiwen Chang, Shlok Mishra, Han Zhang,
Dina Katabi, and Dilip Krishnan. Mage: Masked generative
encoder to unify representation learning and image synthe-
sis. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , pages 2142–2152,
2023. 2
[25] Yue Li, Wenrui Ding, Chunlei Liu, Baochang Zhang, and
Guodong Guo. Trq: Ternary neural networks with residual
quantization. In Proceedings of the AAAI Conference on Ar-
tiﬁcial Intelligence , pages 8538–8546, 2021. 3
[26] Zefan Li, Bingbing Ni, Wenjun Zhang, Xiaokang Yang, and
Wen Gao. Performance guaranteed network acceleration via
1908
high-order residual quantization. In Proceedings of the IEEE
International Conference on Computer Vision , pages 2584–
2592, 2017. 3
[27] Angela S Lin, Lemeng Wu, Rodolfo Corona, Kevin Tai, Qix-
ing Huang, and Raymond J Mooney. Generating animated
videos of human activities from natural language descrip-
tions. Learning , 2018(1), 2018. 2
[28] Junfan Lin, Jianlong Chang, Lingbo Liu, Guanbin Li, Liang
Lin, Qi Tian, and Chang-wen Chen. Being comes from
not-being: Open-vocabulary text-to-motion generation with
wordless training. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
23222–23231, 2023. 2
[29] Zhenguang Liu, Shuang Wu, Shuyuan Jin, Shouling Ji, Qi
Liu, Shijian Lu, and Li Cheng. Investigating pose represen-
tations and motion contexts modeling for 3d motion predic-
tion. IEEE Transactions on Pattern Analysis and Machine
Intelligence , 45(1):681–697, 2022. 2
[30] Yunhong Lou, Linchao Zhu, Yaxiong Wang, Xiaohan Wang,
and Yi Yang. Diversemotion: Towards diverse human
motion generation via discrete diffusion. arXiv preprint
arXiv:2309.01372 , 2023. 2
[31] Thomas Lucas, Fabien Baradel, Philippe Weinzaepfel, and
Gr´egory Rogez. Posegpt: Quantization-based 3d human mo-
tion generation and forecasting. In European Conference on
Computer Vision , pages 417–435. Springer, 2022. 2,3
[32] Naureen Mahmood, Nima Ghorbani, Nikolaus F Troje, Ger-
ard Pons-Moll, and Michael J Black. Amass: Archive of
motion capture as surface shapes. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 5442–5451, 2019. 5
[33] Wei Mao, Miaomiao Liu, Mathieu Salzmann, and Hongdong
Li. Learning trajectory dependencies for human motion pre-
diction. In Proceedings of the IEEE/CVF International Con-
ference on Computer Vision , pages 9489–9497, 2019. 2
[34] Julieta Martinez, Holger H Hoos, and James J Little. Stacked
quantizers for compositional vector compression. arXiv
preprint arXiv:1411.2173 , 2014. 2,3
[35] Mathis Petrovich, Michael J Black, and G ¨ul Varol. Action-
conditioned 3d human motion synthesis with transformer
vae. In Proceedings of the IEEE/CVF International Con-
ference on Computer Vision , pages 10985–10995, 2021. 2
[36] Mathis Petrovich, Michael J Black, and G ¨ul Varol. Temos:
Generating diverse human motions from textual descriptions.
InEuropean Conference on Computer Vision , pages 480–
497. Springer, 2022. 1,2
[37] Matthias Plappert, Christian Mandery, and Tamim Asfour.
The kit motion-language dataset. Big data , 4(4):236–252,
2016. 5
[38] Matthias Plappert, Christian Mandery, and Tamim Asfour.
Learning a bidirectional mapping between human whole-
body motion and natural language using deep recurrent neu-
ral networks. Robotics and Autonomous Systems , 109:13–26,
2018. 2
[39] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-ing transferable visual models from natural language super-
vision. In International Conference on Machine Learning ,
pages 8748–8763. PMLR, 2021. 2,4
[40] Li Siyao, Weijiang Yu, Tianpei Gu, Chunze Lin, Quan Wang,
Chen Qian, Chen Change Loy, and Ziwei Liu. Bailando:
3d dance generation by actor-critic gpt with choreographic
memory. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pages 11050–
11059, 2022. 2
[41] Guy Tevet, Brian Gordon, Amir Hertz, Amit H Bermano,
and Daniel Cohen-Or. Motionclip: Exposing human motion
generation to clip space. In European Conference on Com-
puter Vision , pages 358–374. Springer, 2022. 2
[42] Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shaﬁr,
Daniel Cohen-Or, and Amit H Bermano. Human motion dif-
fusion model. arXiv preprint arXiv:2209.14916 , 2022. 1,2,
5,6,7,8
[43] Jonathan Tseng, Rodrigo Castellon, and Karen Liu. Edge:
Editable dance generation from music. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 448–458, 2023. 2
[44] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete
representation learning. Advances in Neural Information
Processing Systems , 30, 2017. 2,3,4
[45] Chengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang,
Long Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu, Huam-
ing Wang, Jinyu Li, et al. Neural codec language models
are zero-shot text to speech synthesizers. arXiv preprint
arXiv:2301.02111 , 2023. 3,1
[46] Zhenyi Wang, Ping Yu, Yang Zhao, Ruiyi Zhang, Yufan
Zhou, Junsong Yuan, and Changyou Chen. Learning diverse
stochastic human-action generators by learning smooth la-
tent transitions. In Proceedings of the AAAI Conference on
Artiﬁcial Intelligence , pages 12281–12288, 2020. 2
[47] Lijun Yu, Yong Cheng, Kihyuk Sohn, Jos ´e Lezama, Han
Zhang, Huiwen Chang, Alexander G Hauptmann, Ming-
Hsuan Yang, Yuan Hao, Irfan Essa, et al. Magvit:
Masked generative video transformer. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 10459–10469, 2023. 2
[48] Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan
Skoglund, and Marco Tagliasacchi. Soundstream: An end-
to-end neural audio codec. IEEE/ACM Transactions on Au-
dio, Speech, and Language Processing , 30:495–507, 2021.
2,3
[49] Jianrong Zhang, Yangsong Zhang, Xiaodong Cun, Shaoli
Huang, Yong Zhang, Hongwei Zhao, Hongtao Lu, and Xi
Shen. T2m-gpt: Generating human motion from textual
descriptions with discrete representations. arXiv preprint
arXiv:2301.06052 , 2023. 1,2,3,4,5,6,7
[50] Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou
Hong, Xinying Guo, Lei Yang, and Ziwei Liu. Motiondif-
fuse: Text-driven human motion generation with diffusion
model. arXiv preprint arXiv:2208.15001 , 2022. 1,2,5
[51] Mingyuan Zhang, Xinying Guo, Liang Pan, Zhongang Cai,
Fangzhou Hong, Huirong Li, Lei Yang, and Ziwei Liu.
Remodiffuse: Retrieval-augmented motion diffusion model.
arXiv preprint arXiv:2304.01116 , 2023. 5,6
1909
[52] Yaqi Zhang, Di Huang, Bin Liu, Shixiang Tang, Yan Lu, Lu
Chen, Lei Bai, Qi Chu, Nenghai Yu, and Wanli Ouyang. Mo-
tiongpt: Finetuned llms are general-purpose motion genera-
tors. arXiv preprint arXiv:2306.10900 , 2023. 2,3
[53] Zixiang Zhou and Baoyuan Wang. Ude: A uniﬁed driv-
ing engine for human motion generation. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 5632–5641, 2023. 2
1910
