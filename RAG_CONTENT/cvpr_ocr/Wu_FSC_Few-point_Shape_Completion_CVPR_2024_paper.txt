FSC: Few-point Shape Completion
Xianzu Wu1,3∗, Xianfeng Wu1*, Tianyu Luan2, Yajing Bai1, Zhongyuan Lai1†, Junsong Yuan2
1State Key Laboratory of Precision Blasting, Jianghan University, Wuhan, Hubei 430056, China
2Department of Computer Science and Engineering, University at Buffalo, Buffalo, New York 14260
3School of Geophysics and Petroleum Resources, Yangtze University, Wuhan, Hubei 430113, China
{xianfengwu, yajingbai }@stu.jhun.edu.cn laizhy@jhun.edu.cn {tianyulu, jsyuan }@buffalo.edu 202007847@yangtzeu.edu.cn
Abstract
While previous studies have demonstrated successful 3D
object shape completion with a sufﬁcient number of points,they often fail in scenarios when a few points, e.g. tensof points, are observed. Surprisingly, via entropy analy-sis, we ﬁnd that even a few points, e.g. 64 points, couldretain substantial information to help recover the 3D shapeof the object. To address the challenge of shape comple-tion with very sparse point clouds, we then propose Few-point Shape Completion (FSC) model, which contains anovel dual-branch feature extractor for handling extremelysparse inputs, coupled with an extensive branch for max-imal point utilization with a saliency branch for dynamicimportance assignment. This model is further bolstered bya two-stage revision network that reﬁnes both the extractedfeatures and the decoder output, enhancing the detail andauthenticity of the completed point cloud. Our experimentsdemonstrate the feasibility of recovering 3D shapes froma few points. The proposed Few-point Shape Completion(FSC) model outperforms previous methods on both few-point inputs and many-point inputs, and shows good gener-
alizability to different object categories. Code is available
athttps://github.com/xianzuwu/FSC .
1. Introduction
Point-cloud-based shape completion is showing exciting
prospects in applications such as autonomous driving,
*Equal contribution.
†Corresponding author. This work was supported by the National Key
Research and Development Program of China (No. 2021YFC3100804),
National Natural Science Foundation of China (No. 62106086), China
Scholarship Council (No. 201908420105), Key Research and Develop-
ment Program of Hubei Province (No. 2021BAD004), State Key Labo-
ratory of Precision Blasting, Jianghan University (No. PBSKL2022201),
Research Fund of Jianghan University (No. 2022SXZX16), National Un-
dergraduate Training Programs for Innovation and Entrepreneurship (Nos.
202311072004 and 202311072010), and Undergraduate Research Pro-
grams of Jianghan University (Nos. 2023zd111 and 2023zd117).Input GRNet PCN Ours Ground Truth Input GRNet PCN Ours Ground Truth
Figure 1. We use 64 points as input for point cloud completion.
The PCN [ 57] result does not result in good general shapes. GR-
Net [ 51] result has unexpected holes due to the lack of dense
representations. In comparison, our few-point shape completionmethod can provide much more reasonable results.
robotics, augmented reality, etc. Limited by the sensors’
range and resolution, the point cloud the sensors generate
can often be sparse, providing only a few points as input.This application scenario makes few-point shape comple-tion critically important. However, compared to existingworks that utilize a few thousand points as input, complet-ing the point cloud with no more than a few dozen points not
only faces the absence of local details in the input but moreimportantly, the highly compromised global shape informa-tion. This disparity makes the task of shape completion with
such limited input even more challenging.
Despite the progress in point cloud-based shape comple-
tion highlighted in works like [ 1,20,31,32,35,52,57,67],
few have tackled the challenge of completing a point cloudwith few input points. Previous works like [ 51,57] typi-
cally utilize a few thousand points as input and demonstratecompetent completion capabilities with this volume of in-put. However, these works do not investigate the poten-tial for point cloud completion when the number of inputpoints is signiﬁcantly reduced, which scenario might arisein many practical applications. They do not explore the ex-tent of the information contained in a few-point input orattempt to leverage this information to enhance point cloudcompletion. Previous studies like PCN [ 57] use feature en-
coders, often inspired by PointNet [ 24] or PointNet++[ 26],
for capturing input shape features. However, with a lim-ited number of points, where each point is more crucial, thefeature extraction must effectively use all points and prior-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
26077
itize their importance for diverse shape information. This
ensures the feature quality from few-point inputs is on parwith many-point inputs. Conversely, voxel-based methodslike GRNet [ 51] may succeed in global shape recovery, but
they face challenges in creating dense representations froma few points. Fig. 1shows 64 points input results in inaccu-
rate global shapes with PCN, while voxel-based approachesGRNet yield sparse outcomes.
To explore how much information that contained in the
few-point input, we use Shannon Entropy [ 29] to eval-
uate the information when we randomly drop the points
from input. We found that even when the input has only64 randomly chosen points, it still contains nearly 50%of the ground truth shape information. To effectively re-cover global shape information and generate a comprehen-sive point cloud from sparse inputs, we designed a dual-
branch feature extractor. The extensive branch ensures full
utilization of each input point, while the saliency branch dy-namically assigns importance to points based on their rele-vance. This design balances equitable point considerationwith the ﬂexibility to adjust point signiﬁcance. Addition-ally, our two-stage revision network reﬁnes the extracted
features and the output from the decoder, improving the out-
put’s authenticity and detail for a more accurate point cloud.
Speciﬁcally, our Few-point Shape Completion (FSC)
model contains two primary modules: recovery and revi-sion. The recovery module employs an MLP-based branchfor comprehensive feature extraction, utilizing a simple yeteffective MLP-based design to fulﬁll our objective of exten-sive feature extraction. In the meantime, the salient featureextraction branch incorporates a transformer-based design,facilitating dynamic exploration of the importance of indi-vidual points. For the revision module, we leverage Wasser-stein GAN (WGAN) to act upon both the latent feature andthe point cloud results, with both WGAN generators beingfashioned as simple MLPs. Our approach goes beyond uti-lizing WGAN solely for output point cloud enhancement; italso ensures that the global shape feature is closely alignedwith the ground truth input feature generated from our re-covery module, thereby improving the detailed shape ofthe point cloud. Additionally, we employ a PointNet-basedmodule to decode the extracted feature into ﬁnely detailedpoint clouds.
Our contributions are as follows:
• We investigate the potential of completing point clouds
with a few input points. This research gives a minimum ofhow many points would be enough to complete the entirepoint cloud.
• We present a solution named Few-point Shape Comple-
tion (FSC) model capable of completing a point cloudfrom a few sparse points. To our knowledge, our workis the ﬁrst one that focuses on the few-point completiontask.• We design a dual-branch feature recovery architecture to
separately capture the extensive and salient information.This dual-branch design equitably utilized the informa-tion of all points while adaptively adjusting the impor-
tance of each point.
• We design a two-stage revision module that uses WGAN
directly on the feature space and on point cloud results.This two-stage revision ensures the global shape featurealigns more closely with that of the authentic point cloudwhile enhancing the shape.
Our experiments demonstrate that our method surpasses
previous approaches with both many and few input points.We also verify that our method has good robustness to dif-ferent input point numbers, unseen point cloud categories,and different randomized input points.
2. Related Work
2.1. Point-cloud-based Shape Completion
Point cloud completion [ 3,6,21,22,33,34,36,38,43,
46,50,58,59,61,65] is a vital task in 3D computer vi-
sion with a multitude of applications, such as robot percep-tion, autonomous driving, and augmented reality. V oxel-based and point-based methods are the two primary cate-gories of point cloud completion approaches. The voxel-based method converts the input point cloud into voxels andsubsequently generates complementary results in both voxeland point cloud representations. GRNet [ 51] exempliﬁes
a voxel-based method that leverages 3D CNNs to extract
voxel features and multilayer perceptrons (MLPs) to pro-duce complementary results in point cloud representation.Nonetheless, voxel-based methods exhibit a lack of robust-ness when confronted with a limited number of input points,leading to missing voxel neighborhoods and, consequently,
the generation of results containing holes.
In contrast, point-based methods directly perform point
cloud completion on the input points. PCN [ 57], a trail-
blazing point-based method, segregates the completion pro-cess into an encoder and a decoder. The encoder utilizesPointNet to extract global features, while the decoder em-ploys fully connected and FoldingNet networks to generatecoarse and ﬁne point clouds, respectively. Given that thecompletion process negates the need for neighborhood cal-culation, point-based methods are more resilient to varia-tions in the number of input points. However, their com-
pletion outcomes are often suboptimal, prompting the ad-
vent of various enhancement methods such as PF-Net [ 14],
TopNet [ 25], and SA-Net [ 62]. These methods concentrate
on procuring more representative point clouds with distinctgeometric features and progressively amplifying the resolu-tion of the generated point cloud.
26078
2.2. Point Cloud Analysis
Point cloud analysis encompasses a range of tasks including
segmentation, object detection, registration, completion,and surface reconstruction, with deep learning-based meth-ods, such as [ 13,15,17–19,24,40,42,54], being widely
implemented. In these works, point clouds are typically en-
coded either by leveraging their connections with neigh-
boring points, as seen in methods like PointNet++[ 26],
DGCNN[ 40], and Point Transformer [ 63], or by employ-
ing simple networks such as MLP, as in the encoder ofPCN [ 57]. While these encoding designs have proven to
be effective in previous point cloud completion tasks, theytend to fall short when dealing with inputs comprising a lim-ited number of points. Given this constraint of limited inputinformation, our point cloud encoder must be signiﬁcantlymore effective than those employed in previous point cloudanalysis approaches.
3. Few-point Information Analysis
We ﬁrst analyze how many input points can provide enoughinformation for completing the point cloud. To measure theinformation, we use Fast Point Feature Histograms (FPFH)Entropy in [ 12] to encode the shape and calculate its Shan-
non Entropy. FPFH of a point cloud is deﬁned in [ 28].
FPFH is a widely used shape descriptor that encodes theshape of a point cloud into a histogram. Having this quanti-zation, we can calculate the FPFH Entropy as
S=−/summationdisplay
iFPFH ilog( FPFH i), (1)
where FPFH iis theith bin of normalized FPFH.
Having FPFH entropy, we can calculate how much per-
centage of information has been lost when we reduce thenumber of points. We show in Fig. 3the information reduc-
tion when the input number of points changes. We found
that only 64 points input contains almost 50% of the shapeinformation under a reasonable quantization level. In thenext section, we try to design a solution that can completethe point cloud of 64 or more points.
4. Few-point Shape Completion
Our task is to design a point cloud completion network ar-chitecture that can take very few partial points as input.Speciﬁcally, given the vertex locations of the input point
cloudX∈R
n×3(nis the input point number), our net-
work can be abstracted as:
Y=F(X), (2)
whereY∈Rm×3is the output point cloud and mis the out-
put point number. Different from the previous point cloudcompletion task that takes thousands of points as input, few-
point completion would require a much smaller number ofpoints. Typically, we would constrain the point number tono less than 64, i.e.m≥64.
4.1. Pipeline
Our Few-point Shape Completion (FSC) model uses an
encoder-decoder architecture illustrated in Fig. 2a. This ar-
chitecture consists of a dual-branch encoder that transforms
the input point cloud into a latent feature space. The dual
branches are speciﬁcally designed to extract both extensiveand salient information from the point cloud. After extract-ing these features, a two-stage revision network is applied toenhance the feature quality. This revision process is set upadversarially, aiming to bring the point cloud result closerto an actual point cloud and align the features more closelywith those derived from a real point cloud. This method im-proves both the features and the quality of the point cloudresults. Subsequently, the revised features are fed into adecoder network to generate a detailed point cloud. The fu-sion of local and global representations of the point features
is achieved using PointNet++[ 26] and multi-head external
attention[ 11]. Finally, the FoldingNet approach [ 53] is em-
ployed to create the ﬁnal detailed point cloud.
4.2. Dual-branch Feature Extraction
We focus on developing a feature extraction network thatcan effectively extract the coarse features f
coarse of point
cloudsXas
fcoarse=Fext(X), (3)
Our approach addresses the challenge of sparse feature ex-
traction from point clouds with limited points. We employa dual-branch feature extraction network, combining point-speciﬁc and global information. This includes a double-layer stacking technique for a richer representation and atransformer-based network to enhance global feature cor-relation. We balance detailed shape features with globallyrelevant ones, leveraging the strengths of PointNet[ 24] and
PCN [ 57] methodologies. This results in a robust, effective
solution for feature extraction in sparse point clouds.
Extensive branch. The extensive branch of the network
has a structure that is similar to the PCN feature extractionnetwork, which also utilizes a two-layer stacking architec-
ture. In the ﬁrst layer, the multilayer perceptron networktakes the n×3-dimensional sparse point cloud Xas input
to generate the feature matrix F
1
1. Then, the feature vec-
torf1
1is obtained through maximum pooling. In the second
layer, the global features f1
1are merged into each row of the
feature matrix F1
1, resulting in an expanded point feature
matrix˜F1
1. Subsequently, the feature vector f1
2is gener-
ated by applying two multilayer perceptrons and maximumpooling sequentially. Finally, f
1
1andf1
2are combined to
form the feature vector f1of the ﬁrst branch.
26079
Encoder
Input Point 
featureGlobal 
featureShared 
MLPShared MLPGlobal 
feature
Decoder
Ground truth Detailed output Coarse output
2D gridCoarse ground truthShared MLP Shared 
MLP
Add
DeformTile
CD
EMD
Shared 
MLPShared MLPXShared 
MLP Point-wise
Maxpool
ExpandShared 
MLPShared MLPGlobal 
feature
Shared 
MLPOffset 
AttentionPoint 
featureShared 
MLP Cascade 
AttentionShared 
MLPGenerator of WGAN
Shared 
MLPPointNet++
Shared 
MLP External
AttentionTile
Tile
TileShared 
MLP Point-wise
MaxpoolShared 
MLPMLP
    MLP + reshape
Shared 
MLPShared MLP
ExpandShared 
MLPShared 
MLP Point-wise
Maxpool
Shared 
MLP Point-wise
Maxpool
2
1F1
1F
1
1f1
1F
1
1
1
2
1F
2
1
1 2
1fExtensive Branch
Salient BranchFeature 
Revision
Point Cloud 
RevisionPoint 
featurePoint 
feature
2
2F1
2F
1f
Global 
feature2fcoarseffinef
coarse fine/ YY detailYgtY
gtY
gt
Y
Y
g
 
Figure 2. FSC pipeline. The dual-branch feature extraction network abstracts the input point cloud Xas a rough feature vector fcoarse .
The two-stage revision network corrects fcoarse and the coarse point cloud Ycoarse asfﬁneandYﬁne, respectively. The detail point cloud
Ydetail is generated by using fﬁne,Yﬁneand its point features.
Salient branch. The structure of the salient branch is
similar to that of the ﬁrst branch. However, it differs inthat the second branch includes an offset attention mod-ule [10] inserted after the multilayer perception network in
the ﬁrst layer of the two-layer stacked structure. Addition-
ally, a two-layer cascaded attention module [ 47] is inserted
between the two multilayer perception networks in the sec-ond layer to enhance the global representation of the pointfeatures in the second layer. The goal of this modiﬁcationis to improve the effectiveness of the feature extraction pro-cess for the salient branch. Finally, the coarse feature vectorf
coarse is obtained by concatenating the feature vectors f1
andf2of the two branches together.
4.3. Two-stage Revision
The two-stage revision network is used to correct the coarse
feature vector fcoarse and the rough point cloud Ycoarse .A s
shown in Fig. 2,this process can be abstracted as:
fﬁne=Frf(fcoarse)),
Ycoarse=M L P ( f ﬁne),
Yﬁne=Frp(Ycoarse),(4)
wherefﬁneis the revised feature using the ﬁrst stage feature
revision. Ycoarse is the resulting coarse point cloud getting
from the revised feature. Yﬁneis the shape revised point
cloud with the same number of points as Ycoarse .Frfand
Frpare the feature and point cloud revision network. In
this process, we aim to generate a point cloud that is closerto the real point cloud and revise the feature to be moresimilar to the features generated from the real point cloud.This adversarial approach leads to a better quality of both
the feature and the point cloud results.
Speciﬁcally, our two-stage revision network reﬁnes both
the coarse feature vector and the rough point cloud. In theﬁrst stage, a seven-layer fully connected generator adjuststhe coarse feature vector, using feature fusion through layerstitching. An attention-enhanced four-layer discriminatornetwork analyzes feature channel correlations. The secondstage focuses on aligning the rough point cloud with thedownsampled true point cloud, using a three-layer genera-tor and a four-layer discriminator. This method effectivelyenhances the ﬁdelity of both features and point clouds, en-suring they closely resemble those from real point clouds.
Finally, the decoder network is used to generate the de-
tail point cloud output Y
detail from the shape revised point
cloudYﬁneas
Ydetail=Fgd(Yﬁne), (5)
whereFgdis the detailed point cloud generation network.
4.4. Training Losses
The loss function is used to measure the difference between
the output point cloud and the true point cloud. Similar to[57], this loss function Lis deﬁned as follows:
L(Y
coarse,Ydetail,Ygt)=
d1/parenleftBig
Ycoarse,˜Ygt/parenrightBig
+αd2(Ydetail,Ygt)(6)
The above loss function consists of d1,d2, and weights α.
d1andd2represent the distance between the output point
26080
Figure 3. We analyze the FPFH Shannon Entropy of the input
point cloud and completion results on ShapeNet. We observe thatwhen the number of input points drops to 64, the input point cloudstill contains 45.51% of the amount of information compared to
the original 16,384 points ( blue line). When the number of points
further decreases, the amount of information drops sharply. In theexperiment of this paper, we use 64 as the minimum number ofpoints. The orange line shows the information of the point cloud
after completion.
cloud and the true value at the coarse level and the detail
level, respectively.
To ensure that the ﬁnal output point cloud matches the
true value in terms of both density distribution and overallstructure, while keeping the computational effort low, weutilize two methods in our implementation. We use EarthMover’s Distance (EMD) [ 27] ford
1calculation, which in-
volves fewer points but provides high-density consistency
with high computational complexity. For d2calculation, we
use Chamfer Distance (CD) [ 7], which involves more points
but provides high structural consistency with low computa-tional complexity. Speciﬁcally,
d1/parenleftBig
Ycoarse,˜Ygt/parenrightBig
=E M D/parenleftBig
Ycoarse,˜Ygt/parenrightBig
= min
φ:Ycoarse→˜Ygt1
|Ycoarse|/summationdisplay
p∈Ycoarse/bardblp−φ(p)/bardbl2.(7)
The above equation ﬁnds the bijection φfromYcoarse to˜Ygt
such that the average distance between the output point and
the corresponding real point is minimized.
d2(Ydetail ,Ygt)=C D( Ydetail ,Ygt)
=1
|Ydetail|/summationdisplay
p∈Ydetailmin
q∈Ygt/bardblp−q/bardbl2+1
|Ygt|/summationdisplay
q∈Ygtmin
p∈Ydetail/bardblq−p/bardbl2.
(8)
The above symmetric form of the chamfer distance is used
to calculate the average closest distance between the output
point cloud and the real point cloud to ensure that they are
covered by each other. More training details can be foundin Supplementary Material Sec. 1.5. Experiments
5.1. Data Generation and Datasets
ShapeNet data generation. We train and evaluate our
method on ShapeNet [ 2]. It is a synthetic CAD dataset
comprising 30,974 complete and detailed 3D point clouds.There are in total 16 categories of point clouds in ShapeNet.We use 8 of them for training and testing (“seen cate-gories”), and use the other 8 categories (“unseen cate-gories”) for the generalizability test. To simulate realis-tic input point clouds, we sample input point clouds fromthe mesh surface. Speciﬁcally, we ﬁrst uniformly sam-ple 16,384 points as Ground truth. Then, we generatedpartial point clouds with 2048 points by back-projecting2.5D depth images into 3D. Subsequently, we downsampledthese to create low-resolution partial point clouds with vary-ing densities: 1024, 512, 256, 128, and 64 points. The entiredataset of 30,974 3D point clouds was divided into train-ing (28,974 point clouds), validation (800 point clouds), andtesting sets (1,200 point clouds). We use the same ( CD-/lscript
1)
as in PCN [ 57] to evaluate our completion accuracy on both
datasets. CD-/lscript1is the Chamfer Distance using L1-norm,
which is deﬁned as
CD-/lscript1(ˆP,P)=1
|ˆP|/summationdisplay
x∈ˆPmin
y∈P/bardblx−y/bardbl1+1
|P|/summationdisplay
y∈Pmin
x∈ˆP/bardblx−y/bardbl1,
(9)
whereˆPandPare the completion and ground truth point
clouds.xandyare points in ˆPandP, respectively. |·|
is the size of a set, and /bardbl·/bardbl1indicates the L1-norm. To be
consistent with PCN, the number reported was multipliedby 1000 in experiments.KITTI dataset. The KITTI [ 8] dataset is an in-the-wild
benchmark dataset extensively used in autonomous driv-
ing and computer vision research, with a strong focus onLiDAR-based point clouds. It features a comprehensivecollection of point cloud data captured from LiDAR sen-sors, providing detailed 3D representations of urban envi-ronments for vehicle and pedestrian detection, and sceneunderstanding tasks. We evaluate our method on this datasetusing Minimum Matching Distance (MMD) in [ 55]a st h e
metric. MMD is deﬁned as
MMD/parenleftBig
ˆP,C/parenrightBig
=m i n
P∈CCD-/lscript2/parenleftBig
ˆP,P/parenrightBig
, (10)
CD-/lscript2is the Chamfer Distance the output ˆPand the point
cloudPfrom car category Cin ShapeNet that is closest to
ˆPin terms of CD-/lscript2, where
CD-/lscript2(ˆP,P)=1
|ˆP|/summationdisplay
x∈ˆPmin
y∈P/bardblx−y/bardbl2
2+1
|P|/summationdisplay
y∈Pmin
x∈ˆP/bardblx−y/bardbl2
2,
(11)
To be consistent with [ 55], the number reported was multi-
plied by 1000 in experiments.
26081
Avg CD-/lscript1↓Airplane Cabinet Car Chair Lamp Sofa Table Vassel
GRNet [ 51] 17.61 19.40 23.32 14.62 10.54 15.64 16.39 16.71 24.26
PoinTr [ 55] 13.68 10.53 15.72 12.59 13.84 14.59 15.45 14.70 11.98
PCN [ 57] 12.11 14.53 11.50 13.32 6.90 12.01 11.37 14.44 12.80
SeedFormer [ 64] 12.38 8.56 16.14 11.38 14.25 10.11 17.50 11.50 9.62
SVDFormer [ 66] 11.22 7.21 13.86 11.00 13.15 10.02 13.90 10.64 10.02
Ours 7.89 3.47 8.93 6.70 10.12 10.31 10.83 7.61 7.63
Table 1. 64 points completion accuarry on the 8 “seen categories” of ShapeNet. We can see that our method outperforms previous methods
on average and in most categories. Avg CD- /lscript1the lower the better. Bold number means the best.
Avg CD-/lscript1↓Airplane Cabinet Car Chair Lamp Sofa Table Vessel
3D-EPN [ 5] 20.15 13.16 21.80 20.31 18.81 25.75 21.09 21.712 18.54
FoldingNet [ 53] 14.31 9.49 15.80 12.61 15.55 16.41 15.97 13.65 14.99
AtlasNet [ 9] 10.85 6.37 11.94 10.10 12.06 12.37 12.99 10.33 10.61
CRN [ 37] 11.98 6.44 15.09 13.75 12.37 11.38 14.70 11.78 10.33
MSN [ 16] 10.00 5.60 11.90 10.30 10.20 10.70 11.60 9.60 9.90
PCN [ 57] 9.64 5.50 10.63 8.70 11.00 11.34 11.68 8.59 9.67
TopNet [ 25] 9.89 6.24 11.63 9.83 11.50 9.37 12.35 9.36 8.85
GRNet [ 51] 8.83 6.45 10.37 9.45 9.41 7.96 10.51 8.44 8.04
PMP-Net [ 44] 8.66 5.50 11.10 9.62 9.47 6.89 10.74 8.77 7.19
PoinTr [ 55] 8.38 4.75 10.47 8.68 9.39 7.75 10.93 7.78 7.29
SCRN [ 39] 8.29 4.80 9.94 9.31 8.78 8.66 9.74 7.20 7.91
NSFA [ 60] 8.06 4.76 10.18 8.63 8.53 7.03 10.53 7.35 7.48
SnowFlake [ 49] 7.21 4.29 9.16 8.08 7.89 6.07 9.23 6.55 6.40
Ours 7.02 4.07 9.12 8.10 7.21 5.88 9.30 6.26 6.25
Table 2. 2048 points completion accuarry on the 8 “seen categories” of ShapeNet. We can see that our
method outperforms previous methods in most categories and the average results.Methods Avg CD-/lscript2↓
PCN [ 57] 18.22
AtlasNet [ 9] 17.77
TopNet [ 25] 14.25
SoftPoolNet [ 41] 11.07
GRNet [ 51] 10.64
PMP-Net [ 44] 9.23
PoinTr [ 55] 9.22
CRN [ 37] 9.21
SCRN [ 39] 9.13
VRCNet [ 23] 8.12
PMP-Net++ [ 45] 7.97
Snowﬂake [ 49] 7.60
ASFM-Net [ 48] 6.68
Ours 6.66
Table 3. 2048 points comletion
accuracy on Completion3D.
Completion3D dataset. We also compare our results
on Completion3D [ 30], which is another 3D object point
cloud completion benchmark designed based on the data in
ShapeNet. Here, we use the same CD-/lscript2metric as deﬁned
in Eq. ( 11).
5.2. Shannon Information Analysis
We analyze the Shannon information for few-point inputs
and completion results on ShapeNet. We use Eq. ( 1)i n
Sec. 3to evaluate the FPFH Entropy of input point clouds.
We set the voxel size of FPFH to approximately 2% of the
point cloud’s overall size, meaning we do not take into con-sideration the errors that are smaller than that. The radiusand bin count are set to 1cm and 36, respectively. The re-sults are shown in Fig. 3. The blue line shows the entropy
of different inputs and the orange line shows the results of
the corresponding completed point cloud. Compared withthe ground truth point cloud, our 64-point input has 45.51%information of the ground truth We can also see from the or-
ange line that even with 64-point input, we can also recover
60% of the shape information.
5.3. Results
Few-point results on ShapeNet. We train and evaluate our
method on 8 categories of point clouds on ShapeNet. Tab. 1
presents the per-class completion results on ShapeNet forscenarios where the input point cloud contains only 6416 32 64 128 256 512 1024 2048
Number of Points510152025303540Our
PCN
GRNet
PoinTr
Figure 4. In our comparative analysis on ShapeNet, we varied the
input point number from 2048 to 16. We observe that our model(blue line) consistently outperformed previous methods across dif-
ferent input point counts. Notably, when the number of inputpoints was reduced to below 64, there was a signiﬁcant declinein performance for all evaluated methods.
points. In all eight “seen categories”, our method outper-
forms others, achieving the highest accuracy. We observe
a notable improvement over the previous method. This
demonstrates that our approach has a distinct advantage incompletion accuracy, especially in cases of few-point point
26082
Methods PCN [ 57] NSFA [ 60] CRN [ 37] PFNet [ 14] GRNet [ 51] PoinTr [ 55] SeedFormer [ 64] AnchorFormer [ 4] AdaPoinTr [ 56]Ours
MMD ↓ 1.366 0.891 0.872 0.792 0.568 0.526 0.516 0.458 0.392 0.239
Table 4. Completion results on KITTI dataset with various number of input points.
Figure 5. We report the completion result grouped by the number
of input points on LiDAR scans of the KITTI dataset. As shown inthe ﬁgure, our method ( green line) outperforms previous methods
in every number of input point groups. We also observe a sharp
error increase when the number of input points is lower than 64.
Input GRNet PCN Ours Ground Truth PoinTr
 SVDFormer
 SeedFormer
Figure 6. Visualized results for 64 input points. The top four
rows are the results of categories used during training (“seen” cat-
egories). The bottom four rows are the results of categories notseen during training (“unseen” categories). As we can see, ourmethod has better visualization results than previous methods.
cloud input. We also compare the performance on ShapeNet
when the input point number varies from 2048 to 16 inFig. 4. We can observe that our FSC outperforms previ-
ous approaches when the number of input points changes.
Besides, when the number input point is less than 64, theperformance of all methods drops sharply.
Figure 7. Visualization results compared on KITTI, Red and Blue
are before after aligning the centroids of the point clouds.
Many-point results. In Tab. 2, we present the per-class
completion results for scenarios with 2048 input pointson ShapeNet, and in Tab. 3on Completion3D. These re-
sults indicate that our method also outperforms previous ap-proaches in cases of regular input point counts (2048).
Results on KITTI. We directly evaluate KITTI using the
model trained on ShapeNet. As a real-world dataset, the in-put point number of KITTI varies from 1 to 2048. We ﬁrstreport the overall result on KITTI regardless of the inputpoint number in Tab. 4. Our completion error is smaller than
previous approaches. We also group the input point cloudin KITTI by the number of points to measure the perfor-mance when the input number of points varies. Our resultis shown in Fig. 5. We can observe that our method out-
performs previous approaches when the number of pointsdrops. The experiments on KITTI show the generalizability
of our method to different datasets.
Results on unseen categories of ShapeNet. Tab. 5gives
the per-class completion results for the shape from unseencategories when the input point cloud only contains 64points. Our method still achieves the best performance in all8 unknown categories. This experiment shows our methodhas reasonable generalizability to unseen objects.
Ablation studies. Tab. 6gives the contribution of each
module to the performance improvement under differentnumbers of input points, respectively. No matter how manyinput points there are and what we choose, using the salient
branch instead of the extensive branch can always im-prove the completion accuracy, and by combining the two
branches together, the accuracy can be further improved.This justiﬁes our design of dual-branch feature extraction
26083
Avg CD-/lscript1↓ Bus Bed Bookshelf Bench Guitar Motorbike Skateboard Pistol
GRNet [ 51] 17.03 17.59 25.61 21.93 15.26 10.85 14.83 14.96 15.17
PCN [ 57] 15.52 11.30 25.86 16.22 12.55 11.47 15.30 13.48 17.95
PoinTr [ 55] 14.92 14.62 20.44 16.84 14.22 11.56 13.40 14.45 13.80
SVDFormer [ 66] 13.60 13.30 21.47 13.73 11.69 8.00 13.25 9.91 17.44
Ours 12.90 7.94 22.60 12.98 9.59 9.77 13.56 11.55 14.92
Table 5. Results for 64 points on shapes from unseen categories in ShapeNet. We observe that even on unseen categories, we can still have
reasonable results, and our method still outperforms previous methods on average and most unseen categories.
Module Number of input points
Extensive branch
Baseline EncoderSalience
branchFeature
revisionPoint cloud
revisionPointNet++ TransformerDecoder
Baseline Decoder2048 1024 512 256 128 64
a /check /check 9.64 9.68 9.73 10.00 10.71 12.11
b /check/check 8.66 8.71 8.76 8.95 9.42 10.53
c /check/check /check 8.31 8.35 8.42 8.49 8.59 9.78
d /check/check /check 8.33 8.39 8.43 8.60 9 10
e /check/check /check 9.01 9.11 9.18 9.27 10.07 11.35
f /check/check /check /check 8.01 8.07 8.15 8.29 8.55 9.14
g /check/check /check 9.31 9.36 9.41 9.65 10.28 11.58
h /check/check /check 8.91 8.99 9.08 9.17 9.72 11.02
i /check/check /check /check 8.5 8.58 8.66 9.02 9.43 11.24
j /check/check /check /check /check /check 7.97 8.03 8.07 8.22 8.51 9.46
k /check/check /check /check /check /check 7.35 7.41 7.46 7.54 7.7 8.42
Ours /check/check /check /check /check /check /check 7.02 7.09 7.13 7.19 7.27 7.89
Table 6. Ablation study under different numbers of input points. Our performance decreases when we remove some necessary modules.
w/o saliency branch w/o extensive branch Ground Truth Dual branch
Figure 8. Visualized comparison of removing each branch in fea-
ture extractor. From left to right: removing saliency branch, re-moving extensive branch, keeping both branch (proposed method),and Ground Truth.
and the two-stage revision network. We also show an visu-
alized example of removing one of feature extraction dualbranches in Fig. 8. Our proposed method (third one from
the left) achieves the best results.
Visualizations. Fig. 6shows visualized results on
ShapeNet. The input point cloud only contains 64 points.
As we can see, GRNet [ 51] may only generate some non-
connected clusters around isolated input points, whereas thecategory information may be completely lost. AlthoughPCN [ 57] can generate connectivity, the results often con-
tain large deformation and noise, leading to either difﬁculty
or error in recognition. Although our method also containsdeformation and noise, most of them are within an accept-able range. This illustrates the effectiveness of our method
for the very low-resolution case. Besides, our visualizedresult on KITTI shows a similar result in Fig. 7.
Visualized result when input point number changes.
Fig.9gives the completion results of an airplane when the
number of input points gradually decreases from 2048 to16. As the number of input points decreases, our methodalways recovers a better point cloud, which further illus-64 128 256 1024 2048
GRNet512 Ground Truth
Ours
Figure 9. Qualitative completion results of a lamp when the num-
ber of points increases from 64 to 2048. Our visualization resultsoutperform GRNet for different numbers of points.
trates our stability against the variations in the number of
points of the input.
6. Conclusion
We have proposed a point cloud complementation networkspeciﬁcally for the case of very few points. To overcome theproblem of missing neighborhood information due to veryfew points, we select a PCN that does not rely on neigh-borhood information in the encoding phase as the backbone
network and combine a series of methods applicable to thevery few points cases to enhance the overall performance ofthe complementation network. These specialized designs
include 1) a dual-branch feature extraction network, 2) atwo-stage revision network, and 3) a detailed point gener-ation network with fused point features. Experimental re-
sults show that our method has a signiﬁcant performanceimprovement over existing methods in very few point cases.
Also, our method has good robustness for the shapes fromunseen categories and the number of different points.
26084
References
[1] Panos Achlioptas, Olga Diamanti, Ioannis Mitliagkas, and
Leonidas Guibas. Learning shape completion from pointclouds using 3d generative adversarial networks. In Proceed-
ings of the IEEE Conference on Computer Vision and PatternRecognition , pages 2772–2776, 2017. 1
[2] Angel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat
Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Mano-lis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, andFisher Yu. Shapenet: An information-rich 3d model reposi-tory, 2015. 5
[3] Chuanchuan Chen, Dongrui Liu, Changqing Xu, and Trieu-
Kien Truong. Genecgan: A conditional generative adversar-
ial network based on genetic tree for point cloud reconstruc-tion. Neurocomputing , 462:46–58, 2021. 2
[4] Zhikai Chen, Fuchen Long, Zhaofan Qiu, Ting Yao, Wen-
gang Zhou, Jiebo Luo, and Tao Mei. Anchorformer:Point cloud completion from discriminative nodes. In 2023
IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR) , pages 13581–13590, 2023. 7
[5] Angela Dai, Charles Qi, and Matthias Nießner. Shape com-
pletion using 3d-encoder-predictor cnns and shape synthesis.InProceedings of the IEEE conference on computer vision
and pattern recognition , pages 5868–5877, 2017. 6
[6] Vage Egiazarian, Savva Ignatyev, Al exey Artemov, Oleg
V oynov, Andrey Kravchenko, Youyi Zheng, Luiz Velho, and
Evgeny Burnaev. Latent-space laplacian pyramids for adver-sarial representation learning with 3d point clouds. ArXiv ,
abs/1912.06466, 2019. 2
[7] Haoqiang Fan, Hao Su, and Leonidas J Guibas. A point set
generation network for 3d object reconstruction from a singleimage. In Proceedings of the IEEE conference on computer
vision and pattern recognition , pages 605–613, 2017. 5
[8] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we
ready for autonomous driving? the kitti vision benchmarksuite. In 2012 IEEE Conference on Computer Vision and
Pattern Recognition , pages 3354–3361, 2012. 5
[9] Thibault Groueix, Matthew Fisher, Vladimir G. Kim,
Bryan C. Russell, and Mathieu Aubry. A papier-m ˆach´e ap-
proach to learning 3d surface generation. In Proceedings of
the IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR) , pages 216–224, 2018. 6
[10] Meng-Hao Guo, Jun-Xiong Cai, Zheng-Ning Liu, Tai-Jiang
Mu, Ralph R Martin, and Shi-Min Hu. Pct: Point cloudtransformer. Computational Visual Media , 7:187–199, 2021.
4
[11] Meng-Hao Guo, Zheng-Ning Liu, Tai-Jiang Mu, and Shi-
Min Hu. Beyond self-attention: External attention using twolinear layers for visual tasks. IEEE Transactions on Pattern
Analysis and Machine Intelligence , pages 1–13, 2022. 3
[12] Chenming Hu, Yu Ru, Shuping Fang, Hongping Zhou,
Jiangkun Xue, Yuheng Zhang, Jianping Li, Guopeng Xu,and Gaoming Fan. A tree point cloud simpliﬁcation methodbased on fpfh information entropy. F orests , 14(7):1507,
2023. 3
[13] Hengshuang Hu, Yangyan Li, and Xiaolin Li. Part attention
pointnet for ﬁne-grained point cloud classiﬁcation. In Pro-ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR) , pages 14972–14981, 2021.
3
[14] Zitian Huang, Yikuan Yu, Jiawen Xu, Feng Ni, and Xinyi Le.
Pf-net: Point fractal network for 3d point cloud completion.InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR) , pages 7662–7670,
2020. 2,7
[15] Chenyang Jiang, Bin Zhang, Hao Li, Jian Wang, and Yong
Liu. Point-wise convolutional neural networks for high-
resolution 3d point cloud analysis. IEEE Transactions on
Pattern Analysis and Machine Intelligence (TPAMI) , 42(5):
1272–1284, 2019. 3
[16] Minghua Liu, Lu Sheng, Sheng Yang, Jing Shao, and Shi-
Min Hu. Morphing and sampling network for dense point
cloud completion. Proceedings of the AAAI Conference on
Artiﬁcial Intelligence , 34(07):11596–11603, 2020. 6
[17] Tianyu Luan, Yali Wang, Junhao Zhang, Zhe Wang, Zhipeng
Zhou, and Yu Qiao. Pc-hmr: Pose calibration for 3d humanmesh recovery from 2d images/videos. In Proceedings of
the AAAI Conference on Artiﬁcial Intelligence , pages 2269–
2276, 2021. 3
[18] Tianyu Luan, Yuanhao Zhai, Jingjing Meng, Zhong Li,
Zhang Chen, Yi Xu, and Junsong Yuan. High ﬁdelity 3dhand shape reconstruction via scalable graph frequency de-composition. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 16795–
16804, 2023.
[19] Tianyu Luan, Zhong Li, Lele Chen, Xuan Gong, Lichang
Chen, Yi Xu, and Junsong Yuan. Spectrum auc difference(saucd): Human-aligned 3d shape evaluation. arXiv preprint
arXiv:2403.01619 , 2024. 3
[20] Yiwen Luo and Qixing Huang. Weakly supervised 3d
point cloud completion in real time. arXiv preprint
arXiv:2004.08751 , 2020. 1
[21] Yinyu Nie, Yiqun Lin, Xiaoguang Han, Shihui Guo, Jian
Chang, Shuguang Cui, and Jian Jun Zhang. Skeleton-bridgedpoint completion: From global inference to local adjustment.arXiv preprint arXiv:2010.07428 , 2020. 2
[22] Liang Pan. Ecg: Edge-aware point cloud completion with
graph convolution. IEEE Robotics and Automation Letters ,
5:4392–4398, 2020. 2
[23] Liang Pan, Xinyi Chen, Zhongang Cai, Junzhe Zhang, Haiyu
Zhao, Shuai Yi, and Ziwei Liu. Variational relational pointcompletion network. CoRR , abs/2104.10154, 2021. 6
[24] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.
Pointnet: Deep learning on point sets for 3d classiﬁcationand segmentation. In Proceedings of the IEEE conference
on computer vision and pattern recognition , pages 652–660,
2017. 1,3
[25] Charles Ruizhongtai Qi, Hao Su, Kaichun Mo, and
Leonidas J Guibas. Topnet: Topology based neural networkarchitecture search for 3d point cloud classiﬁcation. In Pro-
ceedings of the IEEE Conference on Computer Vision andPattern Recognition , pages 7224–7233, 2017. 2,6
[26] Charles R Qi, Li Yi, Hao Su, and Leonidas J Guibas. Point-
net++: Deep hierarchical feature learning on point sets in a
26085
metric space. In Advances in Neural Information Processing
Systems , pages 5099–5108, 2017. 1,3
[27] Yossi Rubner, Carlo Tomasi, and Leonidas J Guibas. The
earth mover’s distance as a metric for image retrieval. Inter-
national journal of computer vision , 40(2):99, 2000. 5
[28] Radu Bogdan Rusu, Nico Blodow, and Michael Beetz. Fast
point feature histograms (fpfh) for 3d registration. In 2009
IEEE international conference on robotics and automation ,
pages 3212–3217. IEEE, 2009. 3
[29] Mark P Silverman. Cheating or coincidence? statistical
method employing the principle of maximum entropy forjudging whether a student has committed plagiarism. 2015.2
[30] Lyne P Tchapmi, Vineet Kosaraju, Hamid Rezatoﬁghi, Ian
Reid, and Silvio Savarese. Topnet: Structural point cloud de-coder. In Proceedings of the IEEE/CVF conference on com-
puter vision and pattern recognition , pages 383–392, 2019.
6
[31] Hao Wang, Srinath Sridharan, Weixin Huang, Li Li, Rongjie
Li, Zhichao Li, Tianfu Li, and Shuang Li. 3d point cloudcompletion with multimodal deep autoencoders. In Proceed-
ings of the European Conference on Computer Vision , pages
641–656, 2018. 1
[32] Haowen Wang, Zihao Zhu, Jianzhuang Lin, and Guoping
Zhang. Deep global registration for point cloud completion.InProceedings of the IEEE Conference on Computer Vision
and Pattern Recognition , pages 2423–2432, 2019. 1
[33] Jun Wang, Yinghan Cui, Dongyan Guo, Junxia Li, Qingshan
Liu, and Chunhua Shen. Pointattn: You only need attentionfor point cloud completion. ArXiv , abs/2203.08485, 2022. 2
[34] Kaiqi Wang, Ke Chen, and Kui Jia. Deep cascade generation
on point sets. In International Joint Conference on Artiﬁcial
Intelligence , 2019. 2
[35] Xinxin Wang, Zizheng Zheng, Shu Liu, Jian Cao, and
Zhaopeng Zhu. 3d point cloud completion with graph con-volutional autoencoders. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition , pages
5247–5256, 2019. 1
[36] Xiaogang Wang, Marcelo H. Ang, and Gim Hee Lee. Point
cloud completion by learning shape priors. 2020 IEEE/RSJ
International Conference on Intelligent Robots and Systems(IROS) , pages 10719–10726, 2020. 2
[37] Xiaogang Wang, Marcelo H Ang Jr, and Gim Hee Lee. Cas-
caded reﬁnement network for point cloud completion. InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition , pages 790–799, 2020. 6,7
[38] Xiaogang Wang, Marcelo H Ang, and Gim Hee Lee. V oxel-
based network for shape completion by leveraging edge gen-eration. In 2021 IEEE/CVF International Conference on
Computer Vision (ICCV) , pages 13169–13178, 2021. 2
[39] Xiaogang Wang, Marcelo H Ang, and Gim Hee Lee. Cas-
caded reﬁnement network for point cloud completion withself-supervision. IEEE Transactions on Pattern Analysis and
Machine Intelligence , 44(11):8139–8150, 2021. 6
[40] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma,
Michael M Bronstein, and Justin M Solomon. Dynamicgraph cnn for learning on point clouds. ACM Transactions
on Graphics (TOG) , 38(5):1–12, 2019. 3[41] Yida Wang, David Joseph Tan, Nassir Navab, and Federico
Tombari. Softpoolnet: Shape descriptor for point cloud com-
pletion and classiﬁcation. In Computer Vision–ECCV 2020:
16th European Conference, Glasgow, UK, August 23–28,2020, Proceedings, Part IX , pages 70–85. Springer, 2020.
6
[42] Ying Wei and Qixing Huang. Adaptive point cloud de-
noising with deep feature regularization. In Proceedings of
the IEEE/CVF Conference on Computer Vision and PatternRecognition , pages 12304–12313, 2020.
3
[43] Xin Wen, Zhizhong Han, Yan-Pei Cao, Pengfei Wan, Wen
Zheng, and Yu-Shen Liu. Cycle4completion: Unpaired pointcloud completion using cycle transformation with missingregion coding. In 2021 IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition (CVPR) , pages 13075–
13084, 2021. 2
[44] Xin Wen, Peng Xiang, Zhizhong Han, Yan-Pei Cao, Pengfei
Wan, Wen Zheng, and Yu-Shen Liu. Pmp-net: Point cloudcompletion by learning multi-step point moving paths. InProceedings of the IEEE/CVF conference on computer vi-sion and pattern recognition , pages 7443–7452, 2021. 6
[45] Xin Wen, Peng Xiang, Zhizhong Han, Yan-Pei Cao, Pengfei
Wan, Wen Zheng, and Yu-Shen Liu. Pmp-net++: Point cloudcompletion by transformer-enhanced multi-step point mov-ing paths. pages 852–867, 2023. 6
[46] Hang Wu and Yubin Miao. Cross-regional attention network
for point cloud completion. In 2020 25th International Con-
ference on Pattern Recognition (ICPR) , pages 10274–10280,
2021. 2
[47] Xianfeng Wu, Xinyi Liu, Junfei Wang, Zhongyuan Lai, Jing
Zhou, and Xia Liu. Point cloud classiﬁcation based on trans-former. Computers and Electrical Engineering , 104:108413,
2022. 4
[48] Yaqi Xia, Yan Xia, Wei Li, Rui Song, Kailang Cao, and Uwe
Stilla. Asfm-net: Asymmetrical siamese feature matchingnetwork for point completion. pages 1938–1947, 2021. 6
[49] Peng Xiang, Xin Wen, Yu-Shen Liu, Yan-Pei Cao, Pengfei
Wan, Wen Zheng, and Zhizhong Han. Snowﬂakenet: Pointcloud completion by snowﬂake point deconvolution withskip-transformer. In Proceedings of the IEEE/CVF inter-
national conference on computer vision , pages 5499–5509,
2021. 6
[50] Chulin Xie, Chuxin Wang, Bo Zhang, Hao Yang, Dong
Chen, and Fang Wen. Style-based point generator with ad-versarial rendering for point cloud completion. In 2021
IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR) , pages 4617–4626, 2021. 2
[51] Haozhe Xie, Hongxun Yao, Shangchen Zhou, Jiageng Mao,
Shengping Zhang, and Wenxiu Sun. Grnet: Gridding resid-
ual network for dense point cloud completion. In Computer
Vision–ECCV 2020: 16th European Conference, Glasgow,UK, August 23–28, 2020, Proceedings, Part IX , pages 365–
381. Springer, 2020. 1,2,6,7,8
[52] Jie Yang, Zhe Zhao, Tao Wang, and Yang Liu. Context-
aware completion of 3d part assemblies with shape-guidedtree parsing. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition (CVPR) , pages
7121–7129, 2018. 1
26086
[53] Yaoqing Yang, Chen Feng, Yiru Shen, and Dong Tian. Fold-
ingnet: Point cloud auto-encoder via deep grid deformation.
In2018 IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition , pages 206–215, 2018. 3,6
[54] Yibo Yang, Chen Feng, Yi Shen, and Dong Tian. Point-
ﬂownet: Learning representations for rigid motion estima-tion from point clouds. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition(CVPR) , pages 8264–8273, 2020. 3
[55] Xumin Yu, Yongming Rao, Ziyi Wang, Zuyan Liu, Jiwen
Lu, and Jie Zhou. Pointr: Diverse point cloud comple-tion with geometry-aware transformers. In Proceedings of
the IEEE/CVF international conference on computer vision ,
pages 12498–12507, 2021. 5,6,7,8
[56] Xumin Yu, Yongming Rao, Ziyi Wang, Jiwen Lu, and Jie
Zhou. Adapointr: Diverse point cloud completion with adap-tive geometry-aware transformers. IEEE Transactions on
Pattern Analysis and Machine Intelligence , PP:1–17, 2023.
7
[57] Wentao Yuan, Tejas Khot, David Held, Christoph Mertz, and
Martial Hebert. Pcn: Point completion network. In 2018 in-
ternational conference on 3D vision (3DV) , pages 728–737.
IEEE, 2018. 1,2,3,4,5,6,7,8
[58] Junzhe Zhang, Xinyi Chen, Zhongang Cai, Liang Pan, Haiyu
Zhao, Shuai Yi, Chai Kiat Yeo, Bo Dai, and Chen ChangeLoy. Unsupervised 3d shape completion through gan inver-sion. In 2021 IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR) , pages 1768–1777, 2021. 2
[59] Wenxiao Zhang, Qingan Yan, and Chunxia Xiao. Detail pre-
served point cloud completion via separated feature aggre-gation. In Computer Vision – ECCV 2020 , pages 512–528.
Springer International Publishing, 2020. 2
[60] Wenxiao Zhang, Qingan Yan, and Chunxia Xiao. Detail pre-
served point cloud completion via separated feature aggre-
gation. In Computer Vision–ECCV 2020: 16th European
Conference, Glasgow, UK, August 23–28, 2020, Proceed-ings, Part XXV 16 , pages 512–528. Springer, 2020. 6,7
[61] Xuancheng Zhang, Yutong Feng, Siqi Li, Changqing Zou,
Hai Wan, Xibin Zhao, Yandong Guo, and Yue Gao. View-guided point cloud completion. In 2021 IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition (CVPR) ,
pages 15885–15894, 2021. 2
[62] Hengshuang Zhao, Li Zhang, Ji Liu, Tianwei Shi, Yingy-
ing Xia, and Xiaogang He. 3d point cloud segmentationwith self-attention and group convolutional neural network.InProceedings of the IEEE Conference on Computer Vision
and Pattern Recognition , pages 9361–9370, 2019. 2
[63] Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip Torr, and
Vladlen Koltun. Point transformer. In Proceedings of the
IEEE/CVF International Conference on Computer Vision
(ICCV) , pages 16259–16268, 2021. 3
[64] Haoran Zhou, Yun Cao, Wenqing Chu, Junwei Zhu, Tong
Lu, Ying Tai, and Chengjie Wang. Seedformer: Patch seeds
based point cloud completion with upsample transformer. In
Computer Vision – ECCV 2022: 17th European Conference,
Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part III
,
page 416–432, Berlin, Heidelberg, 2022. Springer-Verlag. 6,
7[65] Liping Zhu, Bingyao Wang, Gangyi Tian, Wenjie Wang, and
Chengyang Li. Towards point cloud completion: Point rank
sampling and cross-cascade graph cnn. Neurocomputing ,
461:1–16, 2021. 2
[66] Zhe Zhu, Honghua Chen, Xing He, Weiming Wang, Jing
Qin, and Mingqiang Wei. Svdformer: Complementing
point cloud via self-view augmentation and self-structure
dual-generator. In Proceedings of the IEEE/CVF Inter-
national Conference on Computer Vision (ICCV) , pages
14508–14518, 2023. 6,8
[67] Daoming Zong, Shiliang Sun, and Jing Zhao. Ashf-net:
Adaptive sampling and hierarchical folding network for ro-bust point cloud completion. In AAAI Conference on Artiﬁ-
cial Intelligence , 2021. 1
26087
