ERMVP: Communication-Efﬁcient and Collaboration-Robust Multi-Vehicle
Perception in Challenging Environments
Jingyu Zhang1Kun Yang1Yilei Wang1Hanqi Wang1Peng Sun2,∗Liang Song1,∗
1Academy for Engineering and Technology, Fudan University2Duke Kunshan University
{jingyuzhang22, yileiwang23 }@m.fudan.edu.cn ,songl@m.fudan.edu.cn
Abstract
Collaborative perception enhances perception perfor-
mance by enabling autonomous vehicles to exchange com-
plementary information. Despite its potential to revolu-
tionize the mobile industry, challenges in various environ-ments, such as communication bandwidth limitations, local-ization errors and information aggregation inefﬁciencies,
hinder its implementation in practical applications. In this
work, we propose ERMVP , a communication- Efﬁcient and
collaboration- Robust Multi- Vehicle Perception method in
challenging environments. Speciﬁcally, ERMVP has three
distinct strengths: i) It utilizes the hierarchical feature sam-pling strategy to abstract a representative set of feature vec-
tors, using less communication overhead for efﬁcient com-munication; ii) It employs the sparse consensus features to
execute precise spatial location calibrations, effectively mit-igating the implications of vehicle localization errors; iii) Apioneering feature fusion and interaction paradigm is intro-duced to integrate holistic spatial semantics among differ-ent vehicles and data sources. To thoroughly validate ourmethod, we conduct extensive experiments on real-worldand simulated datasets. The results demonstrate that theproposed ERMVP is signiﬁcantly superior to the state-of-the-art collaborative perception methods.
1. Introduction
Autonomous vehicles are widely recognized as a valu-
able means to enhance road safety and trafﬁc efﬁciency.
Equipped with lidar, cameras, and other sensors, these ve-hicles are capable of accurately sensing their surroundingsto ensure safe and reliable operation. However, the single-vehicle perception system has inevitable drawbacks [ 26,
39], such as a limited sensor ﬁeld of view that can be eas-
ily obstructed and the challenge of detecting distant ob-jects due to sparse and low-resolution data. Recently, the
*Corresponding authors. Our code is available at http s://
github.com/Terry9a/ERMVP .advances in vehicle-to-vehicle (V2V) communication tech-
nologies [ 12,17,36] and deep learning [ 4–6,10,16,24,45]
have spurred innovation and progress in the collaborativeperception technology. This technology allows connected
autonomous vehicles (CA Vs) to share sensory data, leadingto more comprehensive environmental perception.
Although collaborative perception technology shows
great potential in transforming the mobility industry, its
practical application faces several challenges, including
communication bandwidth limitations [ 15,35], localiza-
tion errors [ 11,25] and information aggregation inefﬁcien-
cies [ 38,47]. In practical situations, wireless communi-
cation resource and reliability constraints severely ham-per the efﬁcacy of delay-sensitive collaborative perception.While recent works [ 15,42] have achieved a balance be-
tween perception performance and communication band-width through well-designed mechanisms, these methods
have their limitations because they primarily considering
information compression over spatial redundancy. This
narrow focus exacerbates performance degradation at highcompression ratios.
Further, complex dynamic environments lead to localiza-
tion errors, which result in inaccurate relative transform es-timates and spatial feature misalignment. This relative pose
noise produces misleading features that adversely affect the
effectiveness of collaborative perception. Existing meth-ods [ 25,34] attempt to optimize the overall pose through
intensive computation, but the high latency makes them un-suitable for real-time dynamic perception. Meanwhile, col-laborative methods [ 3,38,41,42,46] only focus on ag-
gregated information, but overlook the inherent perceptualstrengths of ego vehicle. This paradigm is vulnerable to theperturbations introduced by collaborative noise, includingasynchronous motion blur and inaccurate projections. Suchdrawback becomes a bottleneck for achieving optimal per-ceptual performance. In contrast, ego-centric features maycontain locally accurate spatial location information that isnot affected by collaborative noise. Therefore, a priority forestablishing a pragmatic collaborative perception system isto effectively overcome the above challenges.
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
12575
Attention-based 
Feature Fusion
Accuracy Enhanced 
Feature Interaction
CAV1
Ego
CAV
CAV2
Detection 
Output
Feature Spatial 
Calibration
CA
C
CA
CA
CAV
CA
Filter and Merge
Feature Sampling
Detection 
Decoders 
Filter and Merge
Feature SamplingPoint Clouds        Feature Encoder
Shared
Shared
Metadata Sharing and Feature ExtractionLocal Observations
Ego-agent FeatureSampled Feature
Calibrated Feature
Fused Feature
Interacted Feature
Detection Output
Figure 1. The overall architecture of the proposed framework. The framework consists of six phases: metadata sharing and feature ex-
traction, ﬁlter and merge feature sampling, feature spatial calibration, attention-based feature fusion, accuracy enhanced feature interactionand detection decoders. The details of each individual component are illustrated in Section 3.
Based on these observations, we propose ERMVP ,
a communication-efﬁcient and collaboration-robust multi-vehicle perception method in challenging environments.
From Figure 1, ERMVP uses four proposed core compo-
nents to tackle existing challenges jointly. Speciﬁcally, ( i)
we ﬁrst design an advanced ﬁlter and merge feature sam-pling strategy to tackle the limitations of wireless commu-nication resources. This strategy considers both inter-class
and intra-class redundancy relationships to abstract a re-ﬁned set of feature vectors from the redundant features, us-
ing less communication overhead for efﬁcient communica-
tion. ( ii) Second, we introduce a plug-and-play feature spa-
tial calibration module to mitigate the implications of ve-hicle localization errors. This module ingeniously utilizes
consensus sparse foreground features to align the relative
pose relationships between ego-vehicle and collaborators
without any precise pose supervision. ( iii) Furthermore, we
present a pioneering feature fusion and interaction paradigm
to integrate holistic spatial semantics. This paradigm com-prises two key components: The ﬁrst is an attention-based
feature fusion module that alternates between local and
global attention to fuse heterogeneous information from dif-
ferent vehicles. The second is an accuracy enhanced fea-
ture interaction strategy that leverages the accurate posi-tional information inherent in ego-centric features to en-hance the rich semantic information provided by fused fea-tures. Through these tailored components, ERMVP repre-
sents a signiﬁcant advancement towards pragmatic collabo-rative perception. To validate the effectiveness of the ER-
MVP , we conduct extensive experiments on two collabora-
tive 3D object detection datasets including V2V4Real [ 44]and OPV2V [ 43]. Comprehensive experimental results
demonstrate that our method outperforms previous state-of-the-art methods under the bandwidth-limited noisy setting.The main contributions can be summarized as follows:
• We present ERMVP , a communication-efﬁcient and
collaboration-robust multi-vehicle perception method,
which addresses the communication bandwidth limita-tions, localization errors and information aggregation ef-
ﬁciency challenges.
• We develop a ﬁlter and merge feature sampling strategy to
enhance communication efﬁciency, a feature spatial cali-bration module for accurate spatial feature alignment, and
two information aggregation components to optimize thefusion process.
• We conduct extensive experiments on both real-world and
simulated datasets. The results show the superiority ofour method and the necessity of the proposed compo-nents.
2. Related Work
2.1. Multi-Agent Communication
Communication has played a pivotal role in the develop-ment of robust multi-agent systems. Early multi-agentcommunication [ 20,29,32] relied on predeﬁned protocols
and heuristics to regulate communication between agents.
However, these ﬁxed methods are unsuitable in complex
and dynamic settings. Advances in deep learning have in-spired the development of advanced strategies for informa-tion exchange and collaboration among agents. For exam-
ple, MAGIC [ 27] used a graph attention encoder to solve the
12576
problem of when and to whom to send messages. TMC [ 49]
utilized temporal smoothing to ensure efﬁcient communi-cation. EC-MARL [ 2] tackled high-dimensional continu-
ous control and partially observable states by introducing arapid communication protocol to resolve task dilemmas. In
comparison, our research focuses on LiDAR-based collabo-
rative 3D object detection tasks in complex driving scenar-ios. We propose a hierarchical feature sampling strategy toachieve efﬁcient communication across agents.
2.2. Collaborative Perception
Factors including limited ﬁelds of view of sensors and
physical obstructions in the environment can adversely im-
pact the perception capabilities of individual agents. To
address these challenges, collaborative perception within
multi-agent systems has become a key technology. Utilizingnewly available datasets [ 22,43,44], several novel collab-
orative perception methods have been proposed. For exam-ple, V2VNet [ 38] incorporated a graph neural network to
fuse data from different agents. V2X-ViT [ 42] introduced
a transformer architecture that combined information fromvehicles and infrastructures. Where2comm [ 15] used fea-
ture spatial heterogeneity to reduce bandwidth utilizationby transmitting sparse feature maps. CoBEVT [ 41] pro-
posed the ﬁrst multi-camera-based collaborative perception
framework and designed the fused axis attention module toenable multi-view interactions. In this work, we propose an
innovative feature calibration and interaction method thatensures robust vehicle collaboration in high-noise dynamicenvironments.
3. Method
This section introduces our efﬁcient and robust collabora-tive perception method. Figure 1shows the general proce-
dure of our method, which is divided into six phases. We
will detail each phase in the following introduction.
3.1. Metadata Sharing and Feature Extraction
In multi-vehicle collaboration scenarios, one of the CA Vs,known as the ego vehicle, constructs a communication
graph. In this graph, the ego vehicle serves as the requester,while other vehicles within its communication range act as
supporters. The ego vehicle broadcasts its metadata in-
formation, including position, heading, speed, and more.Supporters, upon receiving this metadata, project their lo-cal point cloud observations into the ego vehicle’s coordi-
nate system. Each vehicle then encodes these transformedpoint clouds into bird’s eye view (BEV) features, yield-
ing a visual representation. Given the i-th vehicle local
observations X
i, the extracted features are represented as
Fi=Φ enc(Xi)∈RH×W×C, whereΦenc(·)denotes the
PointPillar [ 19] encoder shared by all vehicles and H,W,andCstand for the height, width, and channel of the fea-
ture map, respectively. Then, these extracted features arefed into the ﬁlter and merge feature sampling module.
3.2. Filter and Merge Feature Sampling
Previous works have utilized well-designed mechanisms
such as information entropy communication selection [ 37]
and spatial heterogeneity map [ 15,35] to reduce the re-
quired transmission bandwidth. However, these methods
primarily focus on the inter-class redundancy between fore-
ground and background features, ignoring the intra-classredundancy among features, which results in sub-optimalcompression. To address this gap, we introduce an ad-
vanced Filtering and Merging Feature Sampling strategy
(FMS). This strategy considers both inter-class and intra-class redundancy relationships, efﬁciently extracting a con-
cise and distinctive set of feature vectors from the origi-
nal feature maps, thus reducing communication overheadsmore effectively. FMS is composed of two core components
as follows.
Filter Sampler. In object detection, foreground areas
containing objects are more signiﬁcant than the backgroundareas. Therefore, we implement the idea of reducing spa-
tial redundancy into a feature ﬁlter sampler module, aimingto preserve perceptually important yet sparse sets of featurevectors. Since explicitly learning a binary sampler is in-
feasible, we develop a conﬁdence ﬁlter strategy. Initially, a
detection conﬁdence map is generated for the feature map.It reﬂects the perceptual importance of different spatial ar-
eas, with higher levels indicating potential object areas andlower levels typically denoting redundant background areas.For the feature map F
iof thei-th vehicle, its conﬁdence
mapCiis deﬁned as:
Ci=Φ con gen(Fi)∈[0,1]H×W, (1)
whereΦcon gen(·)represents the conﬁdence generation net-
work with detection decoder structure. Then the conﬁdence
map is thresholded, followed by non-maximum suppres-sion, resulting in a binary mask B. Utilizing this binary
mask, we proceed to preserve the sparse foreground fea-tures/tildewideF
i=B⊙Fi. In order to cope with the dynamically
changing environmental conditions and fully ensure the ro-bustness of the system, the ﬁltering threshold can be variedwith the sensor data and the network state. We set αas the
ﬁlter rate and the number of remaining feature vectors is
L=α×HW .
Merge Sampler. Upon extracting a detailed fore-
ground feature vector set with the ﬁlter sampler, we em-
ploy the merge sampler for additional optimization, reﬁn-
ing similar or repetitive foreground feature vectors throughweighted merging. The process is divided into three stages:information-driven feature grouping, attention-inspired fea-
ture merging, and index-based feature reconstruction.
12577
(a) Information-Driven Feature Grouping. Initially, a
variant of the nearest neighbor clustering algorithm is ap-plied to group the foreground feature vector set. Given a set
of feature vector /tildewideF
i=[x1,x2,...,x L]/latticetopand cluster cen-
terXc, we compute the indicator δifor each feature vector.
δiis calculated as the minimum feature distance minus the
average pixel distance to any other cluster center vector, ex-
pressed as:
δi=m i n
j:xj∈Xc/parenleftBig
/bardblxi−xj/bardbl2
2−γ/bardblp(xi),p(xj)/bardbl2
2/parenrightBig
, (2)
whereδidenotes to which cluster the feature vector xi
should belong. p(·)means getting the position of the vec-
tor andγis a hyperparameter. Subsequently, we can divide
all the feature vectors in /tildewideFiintoKclusters represented by
G={G1,G2,...,G K}. The total number of clusters is
determined by dynamic clustering ratio β, and is calculated
asK=β×L.
(b) Attention-Inspired Feature Merging. A straight-
forward strategy for merging feature vectors is to average of
each feature vector within a cluster. However, this schemecan be severely affected by outlier feature vectors. Draw-
ing inspiration from attention mechanisms, we utilize theconﬁdence score as a guide to quantify the signiﬁcance of
each feature. Thus, the merged feature vector /tildewidex
ifor theith
clusterGiis computed as:
/tildewidexi=/summationtext
j∈Gicjxj/summationtext
j∈Gicj, (3)
wherecjandxjrepresent the conﬁdence score and the orig-
inal feature vector, respectively. In the end, we obtain a ﬁnal
set of feature vectors Zi={/tildewidex1,/tildewidex2,...,/tildewidexK}that is neces-
sary for transmission.
(c) Index-Based Feature Reconstruction. During the
feature grouping and merging process, each feature vector
is allocated to a cluster, and every cluster is represented by
a merged vector. We maintain a record of the index cor-respondence between original and merged feature vectors.
Utilizing this index record, the ego-vehicle ensures that themerged feature vectors are mapped to their correspondingpositions, leading to the reconstruction of feature maps.
3.3. Feature Spatial Calibration
Localization errors [ 30,31] may lead to the misalignment
of feature maps among vehicles. Such misalignment causesthe ego-vehicle to misinterpret the object’s location, result-
ing in sub-optimal perception, as depicted in the upper right
corner of Figure 2. To address this challenge, we introduce
the Feature Spatial Calibration module (FSC) to facilitate
precise feature alignment, as shown in Figure 2. The core
idea of FSC is that overlapping viewpoints allow multiple
vehicles to detect partially the same objects. The process
RANSAC
Cost matrix Match pairsȍȍȍ
ȍ




ȍ
ȍ
ȍȍ


ȍ
ȍȍ
ȍP
Q
Refined transformation matrix
Calibrate
Figure 2. Illustration of the proposed FSC. By employing FSC,
misaligned matching regions are corrected at the feature level,leading to more accurate predictions.
involves three phases: consensus matching, geometric veri-
ﬁcation, and error modulation.
Consensus Matching. After reconstructing the received
features, the ego vehicle generates sparse feature maps.Within these maps, areas rich in information signify poten-tial target regions, which correspond to the proposed match-ing areas. We denote the proposed matching regions of theego vehicle as Pand those identiﬁed by the collaborative
vehicle under noisy pose conditions as Q. Utilizing Pand
Q, a weighted bipartite graph is constructed, wherein the
weight of each edge is determined by the distance betweenthe nodes, encapsulated in a cost matrix. The matching pro-
cess is then converted into a linear assignment task, with the
objective of identifying a matching result with the lowest
cumulative edge weight. This procedure yields the match-ing pairsM.
Geometric Veriﬁcation. Invalid matches may occur due
to objects located in exclusive zones and detection noise.To tackle this issue, we utilize RANSAC to ﬁlter and sift
a consistent set of matches aligned with expected geomet-ric transformations. Initially, a random matching subset
M
s⊆Mis selected, and then the transformation ma-
trixΓsis computed using singular value decomposition.
WhenΓsis applied to all pairs within Ms, and if the post-
transformation distance between these pairs remains below
the threshold η, the set is considered correctly aligned. The
threshold ηreﬂects the allowable localization error within
the original collaborative framework. This process is it-
eratively conducted to identify the optimal transformationmatrix that correlates with the maximum number of correct
matches. Ultimately, a optimal reﬁned transformation ma-trixΓ
ris obtained and applied in subsequent spatial calibra-
tion operations, yielding the aligned feature /tildewideZj=Γ rZj.
Error Modulation. To enhance the adaptability of the
calibration method in various environments, we incorporate
an error modulation strategy. This strategy aims to achieve
a balance between localization errors and the estimation er-rors that emerge from the calibration process. It measures
the overlap ratios between ego and collaborative features in
both their adjusted and original states. Subsequently, fea-
12578
Global
Query
Generator
Global Query Token
CA VEgo
FusedEgo
CA VGlobal Local
Global
Query
Generator
Figure 3. A example of local and global Attention in AFF. This
shows how AFF processes both 3D local windows and the globalquery token for position-level and context-aware aggregation.
tures demonstrating the most optimal overlap are selected
as the ﬁnal input.
3.4. Attention-based Feature Fusion
In multi-vehicle collaboration scenarios, vehicles are able
to capture heterogeneous information from different spatialregions. In order to efﬁciently fuse perceptual features from
multiple vehicles, we propose an Attention-based FeatureFusion method (AFF). As shown in Figure 3, AFF utilizes
alternating local and global attention to achieve position-
level accurate matching in occlusion-variable trafﬁc scenesand to capture the global semantics attention of road topol-
ogy and trafﬁc states. Speciﬁcally, we stack all the vehicles’
features to S∈R
N×H×W×C, whereNis the number of
vehicles. The features are divided into 3D non-overlappingwindows, each of size N×P×P. The shape of the par-
tition tensor is (
H
P×W
P,N×P2,C). We perform 3D lo-
cal self attention on the local window and implement re-
gion interaction within the window. In contrast, global at-
tention goes beyond the limitations of the local view. Ituses the extracted global query token and is shared betweenall windows to interact with the local key and value repre-
sentations, thus helping to capture long-range dependencies
of the features. Inspired by the efﬁciency of [ 13,33], the
global query token gis generated as follows:
g=P(C
1×1(SE(G(D(x)))+ x)), (4)
whereD,G,SE,C1×1andPdenote the depth-wise sepa-
rable convolution, GELU activation function, Squeeze-and-Excitation operation, 1×1convolution and max pooling,
respectively. These designs enable the global query token
to provide advantages such as inductive bias and model-ing of inter-channel dependencies. We then construct our
proposed AFF module by combining this local and global
attention with typical designs of Transformers, including
LayerNorm [ 1], MLPs [ 9], and skip-connections [ 14]. This
module allows the system to analyze spatial correlations
from a local view while also capturing global feature re-
sponses, ensuring efﬁcient and precise perception in dy-
Split-Attention
Multiplication SoftmaxKeyContext
QueryValue
Figure 4. The architecture of the proposed AFI component.
namic, complex, and occlusion-variable trafﬁc scenarios.
Finally, we obtain the fused features Hi.
3.5. Accuracy Enhanced Feature Interaction
Previous works [ 15,21,38,42,43] have demonstrated
that fused features can provide richer semantic informa-tion, thereby enhancing perceptual performance. However,
they may be affected by collaborative noise, such as asyn-chronous motion blur and inaccurate projections, which can
compromise accurate position information and become a
bottleneck to the optimal realization of perceptual perfor-mance. Ego-centric features may contain locally criticalspatial location information without being affected by col-laborative noise. To this end, we propose a novel AccuracyEnhanced Feature Interaction (AEI) strategy that leveragesthe accurate positional information inherent in ego-centric
features to enhance the rich semantic information provided
by collaborative fused features. Firstly, we design a contextcross attention module that is tailored for feature interac-tion, as shown in Figure 4. It considers the features from
the ego-vehicle as queries, and obtains the context of thequery location from the fused features as the key and valueto achieve cross-attention. The computed output of the spa-tial location (i,j)is as follows:
y
ij=/summationdisplay
a,b∈N k(i,j)softmax/parenleftbig
q/latticetop
ijkab/parenrightbig
vab, (5)
whereNk(i,j)is context position of (i,j)andqij,kab,vab
represent the ego query, context key and value, respectively.
As the relevant features of the same object are often locatedin similar locations across different feature maps, we priori-tize the local context details of the query location. Addition-ally, considering that only a few positions within the sparsefeature map hold signiﬁcant information, focusing on thesecontext-rich areas is computationally efﬁcient. Followingthis, we employ the split attention [ 48] to adaptively fuse in-
formation from multiple branches, generating the enhanced
output feature /tildewiderH
i.
12579
Dataset V2V4Real OPV2V
Noise Level σt/σr(m/°) 0.0/0.0 0.4/0.4 0.8/0.8 0.0/0.0 0.4/0.4 0.8/0.8
No Collaboration 41.64/23.43 41.64/23.43 41.64/23.43 73.25/58.22 73.25/58.22 73.25/58.22
Late Fusion 56.80/27.62 40.81/13.31 26.65/10.66 86.46/79.21 76.56/40.01 45.57/21.00
F-Cooper [ 3] 61.12/32.36 51.71/24.48 41.90/19.91 87.39/79.39 79.33/40.12 52.91/16.90
AttFuse [ 7] 64.41/34.32 58.42/30.02 49.37/25.44 90.50/81.80 83.54/52.43 68.37/40.96
Where2comm [ 15] 63.69/34.79 58.48/28.09 49.77/24.25 90.46/84.22 82.17/56.47 73.95/45.86
DiscoNet [ 21] 64.19/35.25 57.71/28.41 49.19/24.94 89.58/81.45 83.25/53.22 61.37/30.53
V2VNet [ 38] 65.70/35.38 62.11/30.63 54.73/25.57 91.35/82.43 85.43/54.16 70.43/31.28
CoAlign [ 25] 64.06/36.60 59.58/31.52 53.02/26.60 90.93/84.28 84.48/57.45 73.95/48.86
V2X-ViT [ 42] 66.42/37.34 63.33/32.38 57.15/28.22 91.74/83.31 82.91/54.73 61.70/26.07
CoBEVT [ 41] 66.01/37.36 58.63/29.09 49.22/23.66 91.71/ 85.98 85.49/60.64 64.63/29.59
ERMVP (Ours) 67.66/42.97 65.01/40.45 60.88/35.01 92.18 /85.59 85.67/66.32 77.13/59.15
Table 1. Overall performance on V2V4Real and OPV2V datasets with pose noises. The results are reported in AP@0.5/0.7.
3.6. Decoder and Loss
Based on the ﬁnal fused feature /tildewiderHi, we use the detec-
tion decoder to generate the ﬁnal prediction output Yi=
Φdec(/tildewiderHi). Each position of Yirepresents a rotated box
with classes (c, x, y, h, w, cos α, sinα), corresponding to
class conﬁdence, position, size, and angle. These objectsare the ﬁnal output of the proposed collaborative perception
system. Following existing work [ 19], we adopt the smooth
L
1loss for regression and focal loss [ 23] for classiﬁcation.
4. Experiments
4.1. Datasets and Evaluation Metrics
Datasets. We validate the proposed ERMVP in the
task of LiDAR-based 3D object detection on two bench-mark datasets including V2V4real [ 44] and OPV2V [ 43].
V2V4real [44] is the ﬁrst large-scale real-world dataset
on V2V perception, collected by two vehicles equipped
with multi-modal sensors driving together in a varietyof scenarios. It covers a driving area of 410 kilome-ters, including 20K frames of point clouds, with train-
ing/validation/test sets containing 14210/2000/3986 frames,respectively. OPV2V [43] is a large-scale vehicle-to-
vehicle collaborative perception dataset, simulated byOpenCDA [ 40] and Carla [ 8]. It contains 73 different
scenarios, in which different numbers (2 to 7) of collab-
orative vehicles appear together, each equipped with a li-
dar sensor and 4 cameras. The dataset contains a totalof 11,464 frames of point clouds and RGB images. The
training/validation/test sets contain 6374, 1980, and 2170frames, respectively.
Evaluation Metrics. We adopt the Average Precision (AP)
at Intersection-over-Union (IoU) thresholds of 0.5 and 0.7
to evaluate the detection performance. The calculation for-
mat of communication volume in [ 15] is used to count the
message size by byte in the log scale with base 2.4.2. Implementation Details
We implement the proposed ERMVP and comparison
model on the Pytorch toolbox [ 28], and train them on a
NVIDIA GeForceRTX 3090 GPU using the Adam opti-mizer [ 18]. The initial learning rate is 2e-3, and it decays
every 15 epochs with a factor of 0.1. All models are trainedwith 60 epochs and the batch size is set to 2. Early stop-ping is used to ﬁnd the best epoch. We also add normalpoint cloud data augmentation for all experiments, includ-
ing scaling, rotation, and ﬂipping. All detection models are
based on PointPillar [ 19] backbone to extract 2D features
from point clouds and a 0.4 m width/length is used for eachvoxel. AFF component has 3 encoded layers and a window
size of 8 for both local and global attention. The balancehyperparameter γis 0.1 and the error tolerance threshold
ηis 0.25. Each vehicle has a communication range of 70
m based on [ 42], while vehicles outside of this broadcast-
ing radius will be ignored. To simulate the localization andheading errors, we add Gaussian noise with a standard devi-ation ofσ
tfor localization errors and σrfor heading errors.
4.3. Quantitative Evaluation
Comparison of Detection Performance . Table 1shows
the performance comparison results of 3D detection ontwo datasets. We use the No Collaboration method as abaseline, which only uses ego vehicle’s point clouds data
without collaboration. Late Fusion allows vehicles to ex-
change detected outputs and utilizes non-maximum sup-pression to produce the ﬁnal result. For intermediate fusion
strategies, we evaluate the existing state-of-the-art (SOTA)
methods: AttFuse [ 43], F-Cooper [ 3], V2VNet [ 38], Dis-
coNet [ 21], V2X-ViT [ 42], CoAlign (only fusion) [ 25],
CoBEVT [ 41] and Where2comm [ 15]. The proposed ER-
MVP achieves average improvements of 12.48%/18.47%and 6.18%/10.02% on two datasets in AP@0.5/0.7 com-
pared with No Collaboration and Late Fusion methods,
demonstrating the superiority of our collaborative method.
12580
Figure 5. Collaborative perception performance comparison on the V2V4Real and OPV2V with varying communication volume.
Meanwhile, it is superior to the SOTA collaborative per-
ception method in both real-world and simulated scenarios:improves the SOTA collaborative performance by 1.24%
and 5.61% on V2V4Real datasets in AP@0.5/0.7, and by
0.44% on OPV2V datasets in AP@0.5. These results fullydemonstrate the superiority of the ERMVP collaborationparadigm.
Dataset V2V4Real OPV2V
Noise Level σt(m) 0.5 1.0 0.5 1.0
Late Fusion 11.02 10.13 53.88 38.80
F-Cooper [ 3] 24.60 18.41 32.24 12.71
AttFuse [ 43] 28.10 24.36 42.95 35.53
DiscoNet [ 21] 28.36 24.05 45.36 24.96
V2VNet [ 38] 29.57 23.67 47.48 25.82
Where2comm [ 15] 28.02 23.53 46.59 45.59
CoAlign [ 21] 29.84 25.82 49.09 46.66
V2X-ViT [ 42] 30.99 23.67 44.83 21.77
CoBEVT [ 41] 24.67 22.01 41.91 26.05
ERMVP (Ours) 39.26 33.94 77.25 76.09
Table 2. Detection AP@0.7 on V2V4Real and OPV2V datasets
with localization error.
Robustness to Localization and Heading Errors. To eval-
uate the sensitivity of existing methods to localization andheading errors, we use the same noise setting as [ 42] and
conduct extensive experiments on two datasets. As shownin Tables 1and 2, under ideal settings, Late Fusion and
some advanced intermediate fusion methods can detect ob-ject vehicles with high accuracy, but their detection accu-racy rapidly decreases as the standard deviation of errorsincreases. For example, when the localization and headingerrors of OPV2V dataset are over 0.4 m and 0.4°, the Dis-conet [ 21] and F-Cooper [ 3] methods fail and perform even
worse than No Collaboration. Our method exceeds previous
SOTA models at all noise levels and consistently outper-forms the No Collaboration baseline, clearly demonstratingthe robustness of ERMVP to pose errors. It is noteworthy
that for localization errors, ERMVP can maintain consistentaccuracy levels in high noise environments, showing supe-
rior robustness. When the localization error is 1.0 m, it out-performs the second-best method by 29.43% and 8.08% inAP@0.7 on OPV2V and V2V4Real datasets. The reason-able explanations are: (i) feature spatial calibration aligns
mismatched collaborative features; (ii) accuracy enhancedfeature interaction reduces performance degradation caused
by collaborative noise.
Comparison of Communication Volume . To achieve
pragmatic collaborative perception, it is crucial to evalu-
ate the perceptual performance under different communi-cation volumes. Figure 5shows the results of the per-
formance comparison under different bandwidth consump-
tion conditions. Note that we do not consider any addi-tional model/data/feature compression for a fair compari-
son. The results indicate that ERMVP achieves excellentperceptual performance and bandwidth consumption trade-
off under all communication bandwidth conditions, consis-tently outperforming Where2comm [ 15] . At the same time,
it achieves the same detection performance as the previousmodel [ 3,38,41,42] with less communication volume on
both real-world and simulated datasets.
4.4. Ablation Studies
AFF FSC AEI V2V4Real OPV2V
51.71/24.48 79.33/40.12
 62.24/37.70 84.30/61.43
 64.36/39.59 84.98/62.67
 65.01/40.45 85.67/66.32
Table 3. Ablation study results of the proposed core components
on the both datasets with noise level of 0.4/0.4. AFF: Attention-
based Feature Fusion; FSC: Feature Spatial Calibration; AEI: Ac-
curacy Enhanced Feature Interaction.
Effect of Core Components. Table 3details the contribu-
tion of each core components in our ERMVP framework.
The base model is the naive position-wise maximum fu-
sion method. We then assess the impact of each compo-nent by sequentially introducing i) attention-based feature
12581
漑a漒V2VNet                                  漑b漒CoBEVT 漑c漒V2X-ViT                                漑d漒ERMVP
Figure 6. Qualitative comparison results in real-world scenarios from the V2V4Real dataset with noise level of 0.4/0.4. Green and red
boxes denote the ground truths and detection results, respectively.
fusion (AFF), ii) feature spatial calibration (FSC), iii) ac-
curacy enhanced feature interaction (AEF). The consistentrise in detection results over both datasets demonstrates the
effectiveness of each introduced component. Notably, inte-grating all three components boosts detection performance
by 13.3% and 15.97% on the V2V4Real dataset for AP@0.5and AP@0.7, respectively.
Figure 7. Hyperparameter analysis of αandβ
Hyperparameters αandβ.We explore the impact
of the hyperparameters αandβ(as described in Sec-
tion 3.2), which jointly determine the communication vol-
ume. Speciﬁcally, the feature compression ratio is repre-sented as α×β. The communication volume, in turn, is
expressed as the product of the original feature size andthe compression rate. By modulating the values of αand
βwithin the interval of 10% to 80%, we chart the resul-
tant object detection performance (measured by AP@0.7)in Figure 7. As expected, a drop in either αorβwill impair
ERMVP’s efﬁcacy. Yet, it’s worth noting that ERMVP’s
performance generally exhibits robustness to these hyper-parameters. The performance decline remains fairly mini-
mal even under signiﬁcant compression, such as a factor of
100. The results demonstrate the effectiveness of our pro-
posed ﬁlter and merge feature sampling module. Therefore,
ERMVP establishes an appropriate and effective communi-
cation paradigm among vehicles, ensuring an optimal trade-
off between perception and communication.4.5. Qualitative Evaluation
Figure 6displays the detection results of V2VNet [ 38],
CoBEVT [ 41], V2X-ViT [ 42], and ERMVP in two scenar-
ios. Clearly, ERMVP offers more precise and comprehen-
sive detection than the prior SOTA methods. To begin with,
ERMVP produces a greater number of predicted bounding
boxes that are well aligned with the ground truths, while
other methods show signiﬁcant discrepancies. This demon-
strates ERMVP’s robustness, particularly in high-noise en-vironments. Moreover, ERMVP detects more dynamic ob-
jects (more ground truth bounding boxes ﬁnd matches).This suggests that ERMVP can effectively combine the in-puts from nearby vehicles, leading to a thorough scene rep-resentation. Overall, these qualitative assessments conﬁrmERMVP’s strengths in delivering accurate and comprehen-
sive perception, especially in challenging conditions.
5. Conclusion
In this paper, we present ERMVP , a communication-
efﬁcient and collaboration-robust multi-vehicle perception
method in challenging environments. We introduce a ﬁlterand merge feature sampling strategy to efﬁcient communi-
cation, a feature spatial calibration module to align featuresin ﬁne-grid and two spatial information aggregation compo-
nents to solve the bottleneck of the fusion method. Exten-
sive experiments prove the superiority of ERMVP and theeffectiveness of our components. The feasibility of our ap-proach for other modalities will be explored in future work.
Acknowledgments
This work is supported by the Shanghai Key Research Lab-
oratory of NSAI, the Innovation Platform for Academi-
cians of Hainan Province, Haikou, 570228, the Speciﬁc Re-search Fund of the Innovation Platform for Academicians of
Hainan Province under Grant YSPTZX202314, and NFSC
under grant 62250410368.
12582
References
[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-
ton. Layer normalization. arXiv preprint arXiv:1607.06450 ,
2016. 5
[2] Marwa Chaﬁi, Salmane Naoumi, Reda Alami, Ebtesam Al-
mazrouei, Mehdi Bennis, and Merouane Debbah. Emergent
communication in multi-agent reinforcement learning for fu-ture wireless networks. arXiv preprint arXiv:2309.06021 ,
2023. 3
[3] Qi Chen, Xu Ma, Sihai Tang, Jingda Guo, Qing Yang, and
Song Fu. F-cooper: Feature based cooperative perception forautonomous vehicle edge computing system using 3d pointclouds. In Proceedings of the 4th ACM/IEEE Symposium on
Edge Computing , pages 88–100, 2019. 1,6,7
[4] Zhaoyu Chen, Bo Li, Shuang Wu, Jianghe Xu, Shouhong
Ding, and Wenqiang Zhang. Shape matters: deformablepatch attack. In European conference on computer vision ,
pages 529–548. Springer, 2022. 1
[5] Zhaoyu Chen, Bo Li, Shuang Wu, Shouhong Ding, and
Wenqiang Zhang. Query-efﬁcient decision-based black-boxpatch attack. IEEE Transactions on Information F orensics
and Security , 2023.
[6] Zhaoyu Chen, Bo Li, Shuang Wu, Kaixun Jiang, Shouhong
Ding, and Wenqiang Zhang. Content-based unrestricted ad-
versarial attack. Advances in Neural Information Processing
Systems , 36, 2024. 1
[7] Yimian Dai, Fabian Gieseke, Stefan Oehmcke, Yiquan Wu,
and Kobus Barnard. Attentional feature fusion. In Proceed-
ings of the IEEE/CVF Winter Conference on Applications of
Computer Vision , pages 3560–3569, 2021. 6
[8] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Anto-
nio Lopez, and Vladlen Koltun. Carla: An open urban driv-ing simulator. In Conference on robot learning , pages 1–16.
PMLR, 2017. 6
[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale. arXiv preprint
arXiv:2010.11929 , 2020. 5
[10] Ian Goodfellow, Y oshua Bengio, and Aaron Courville. Deep
learning . MIT press, 2016. 1
[11] Jiaming Gu, Jingyu Zhang, Muyang Zhang, Weiliang Meng,
Shibiao Xu, Jiguang Zhang, and Xiaopeng Zhang. Feaco:Reaching robust feature-level consensus in noisy pose condi-
tions. In Proceedings of the 31st ACM International Confer-
ence on Multimedia , page 3628–3636, New Y ork, NY , USA,
2023. Association for Computing Machinery. 1
[12] Monowar Hasan, Sibin Mohan, Takayuki Shimizu, and
Hongsheng Lu. Securing vehicle-to-everything (v2x) com-munication platforms. IEEE Transactions on Intelligent V e-
hicles , 5(4):693–713, 2020. 1
[13] Ali Hatamizadeh, Hongxu Yin, Greg Heinrich, Jan Kautz,
and Pavlo Molchanov. Global context vision transformers.InInternational Conference on Machine Learning , pages
12633–12646. PMLR, 2023. 5[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition , pages 770–778, 2016. 5
[15] Y ue Hu, Shaoheng Fang snd Zixing Lei, Yiqi Zhong, and
Siheng Chen. Where2comm: Communication-efﬁcient col-laborative perception via spatial conﬁdence maps. In Thirty-
sixth Conference on Neural Information Processing Systems(NeurIPS) , 2022. 1
,3,5,6,7
[16] Kaixun Jiang, Zhaoyu Chen, Hao Huang, Jiafeng Wang,
Dingkang Yang, Bo Li, Yan Wang, and Wenqiang Zhang.Efﬁcient decision-based black-box patch attacks on videorecognition. In Proceedings of the IEEE/CVF International
Conference on Computer Vision (ICCV) , pages 4379–4389,
2023. 1
[17] Hamza Khan, Sumudu Samarakoon, and Mehdi Bennis. En-
hancing video streaming in vehicular networks via resourceslicing. IEEE Transactions on V ehicular Technology , 69(4):
3513–3522, 2020. 1
[18] Diederik P Kingma and Jimmy Ba. Adam: A method
for stochastic optimization. In International Conference on
Learning Representations (ICLR) , 2015. 6
[19] Alex H Lang, Sourabh V ora, Holger Caesar, Lubing Zhou,
Jiong Yang, and Oscar Beijbom. Pointpillars: Fast encoders
for object detection from point clouds. In Proceedings of
the IEEE/CVF conference on computer vision and patternrecognition , pages 12697–12705, 2019. 3,6
[20] Yiming Li, Bir Bhanu, and Wei Lin. Auction protocol for
camera active control. In 2010 IEEE International Confer-
ence on Image Processing , pages 4325–4328. IEEE, 2010.
2
[21] Yiming Li, Shunli Ren, Pengxiang Wu, Siheng Chen, Chen
Feng, and Wenjun Zhang. Learning distilled collaborationgraph for multi-agent perception. Advances in Neural Infor-
mation Processing Systems , 34:29541–29552, 2021. 5,6,7
[22] Yiming Li, Ziyan An, Zixun Wang, Yiqi Zhong, Siheng
Chen, and Chen Feng. V2x-sim: A virtual collaborative
perception dataset for autonomous driving. arXiv preprint
arXiv:2202.08449 , 2022. 3
[23] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and
Piotr Dollar. Focal loss for dense object detection. In Pro-
ceedings of the IEEE International Conference on ComputerVision (ICCV) , 2017. 6
[24] Yang Liu, Jing Liu, Kun Yang, Bobo Ju, Siao Liu, Y uzheng
Wang, Dingkang Yang, Peng Sun, and Liang Song. Amp-net: Appearance-motion prototype network assisted auto-
matic video anomaly detection system. IEEE Transactions
on Industrial Informatics , 20(2):2843–2855, 2024. 1
[25] Yifan Lu, Quanhao Li, Baoan Liu, Mehrdad Dianati, Chen
Feng, Siheng Chen, and Yanfeng Wang. Robust collabora-tive 3d object detection in presence of pose errors. In 2023
IEEE International Conference on Robotics and Automation(ICRA) , pages 4812–4818. IEEE, 2023. 1,6
[26] Zonglin Meng, Xin Xia, Runsheng Xu, Wei Liu, and Jiaqi
Ma. Hydro-3d: Hybrid object detection and tracking for co-
operative perception using 3d lidar. IEEE Transactions on
Intelligent V ehicles , 2023. 1
12583
[27] Yaru Niu, Rohan R Paleja, and Matthew C Gombolay. Multi-
agent graph-attention communication and teaming. In AA-
MAS , pages 964–973, 2021. 2
[28] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,
James Bradbury, Gregory Chanan, Trevor Killeen, ZemingLin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An im-perative style, high-performance deep learning library. Ad-
vances in neural information processing systems , 32, 2019.
6
[29] Faisal Qureshi and Demetri Terzopoulos. Smart camera net-
works in virtual reality. Proceedings of the IEEE , 96(10):
1640–1656, 2008. 2
[30] Qingyu Song, Changan Wang, Zhengkai Jiang, Yabiao
Wang, Ying Tai, Chengjie Wang, Jilin Li, Feiyue Huang, andYang Wu. Rethinking counting and localization in crowds:A purely point-based framework. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 3365–3374, 2021. 4
[31] Zhiying Song, Fuxi Wen, Hailiang Zhang, and Jun Li. An ef-
ﬁcient and robust object-level cooperative perception frame-
work for connected and automated driving. arXiv preprint
arXiv:2210.06289 , 2022. 4
[32] Ming Tan. Multi-agent reinforcement learning: Independent
vs. cooperative agents. In Proceedings of the tenth inter-
national conference on machine learning , pages 330–337,
1993. 2
[33] Mingxing Tan and Quoc Le. Efﬁcientnetv2: Smaller models
and faster training. In International conference on machine
learning , pages 10096–10106. PMLR, 2021. 5
[34] Nicholas V adivelu, Mengye Ren, James Tu, Jingkang Wang,
and Raquel Urtasun. Learning to communicate and correct
pose errors. In Conference on Robot Learning , pages 1195–
1210. PMLR, 2021. 1
[35] Binglu Wang, Lei Zhang, Zhaozhong Wang, Y ongqiang
Zhao, and Tianfei Zhou. Core: Cooperative reconstructionfor multi-agent perception. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages 8710–
8720, 2023. 1,3
[36] Jian Wang, Yameng Shao, Y uming Ge, and Rundong Y u. A
survey of vehicle to everything (v2x) testing. Sensors , 19(2):
334, 2019. 1
[37] Tianhang Wang, Guang Chen, Kai Chen, Zhengfa Liu, Bo
Zhang, Alois Knoll, and Changjun Jiang. Umc: A uniﬁedbandwidth-efﬁcient and multi-resolution based collaborativeperception framework. arXiv preprint arXiv:2303.12400 ,
2023. 3
[38] Tsun-Hsuan Wang, Sivabalan Manivasagam, Ming Liang,
Bin Yang, Wenyuan Zeng, and Raquel Urtasun. V2vnet:V ehicle-to-vehicle communication for joint perception andprediction. In European Conference on Computer Vision ,
pages 605–621. Springer, 2020. 1,3,5,6,7,8
[39] Qianyi Wu, Kaisiyuan Wang, Kejie Li, Jianmin Zheng, and
Jianfei Cai. Objectsdf++: Improved object-compositionalneural implicit surfaces. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages 21764–
21774, 2023. 1[40] Runsheng Xu, Yi Guo, Xu Han, Xin Xia, Hao Xiang, and
Jiaqi Ma. Opencda: an open cooperative driving automa-
tion framework integrated with co-simulation. In 2021 IEEE
International Intelligent Transportation Systems Conference
(ITSC) , pages 1155–1162. IEEE, 2021. 6
[41] Runsheng Xu, Zhengzhong Tu, Hao Xiang, Wei Shao, Bolei
Zhou, and Jiaqi Ma. Cobevt: Cooperative bird’s eye viewsemantic segmentation with sparse transformers, 2022. 1,3,
6,7,8
[42] Runsheng Xu, Hao Xiang, Zhengzhong Tu, Xin Xia, Ming-
Hsuan Yang, and Jiaqi Ma. V2x-vit: V ehicle-to-everythingcooperative perception with vision transformer. In European
Conference on Computer Vision , page 107–124. Springer,
2022. 1,3,5,6,7,8
[43] Runsheng Xu, Hao Xiang, Xin Xia, Xu Han, Jinlong Li, and
Jiaqi Ma. Opv2v: An open benchmark dataset and fusionpipeline for perception with vehicle-to-vehicle communica-tion. In 2022 International Conference on Robotics and Au-
tomation (ICRA) , pages 2583–2589. IEEE, 2022. 2,3,5,6,
7
[44] Runsheng Xu, Xin Xia, Jinlong Li, Hanzhao Li, Shuo Zhang,
Zhengzhong Tu, Zonglin Meng, Hao Xiang, Xiaoyu Dong,
Rui Song, et al. V2v4real: A real-world large-scale dataset
for vehicle-to-vehicle cooperative perception. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision andPattern Recognition , pages 13712–13722, 2023. 2,3,6
[45] Kun Yang, Peng Sun, Jieyu Lin, Azzedine Boukerche, and
Liang Song. A novel distributed task scheduling frame-work for supporting vehicular edge intelligence. In 2022
IEEE 42nd International Conference on Distributed Com-puting Systems (ICDCS) , pages 972–982. IEEE, 2022. 1
[46] Kun Yang, Dingkang Yang, Jingyu Zhang, Mingcheng Li,
Yang Liu, Jing Liu, Hanqi Wang, Peng Sun, and Liang Song.Spatio-temporal domain awareness for multi-agent collabo-rative perception, 2023. 1
[47] Kun Yang, Dingkang Yang, Jingyu Zhang, Hanqi Wang,
Peng Sun, and Liang Song. What2comm: Towardscommunication-efﬁcient collaborative perception via featuredecoupling. In Proceedings of the 31st ACM International
Conference on Multimedia , page 7686–7695, New Y ork, NY ,
USA, 2023. Association for Computing Machinery. 1
[48] Hang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu,
Haibin Lin, Zhi Zhang, Y ue Sun, Tong He, Jonas Mueller,R. Manmatha, Mu Li, and Alexander Smola. Resnest: Split-
attention networks. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition (CVPR)Workshops , pages 2736–2746, 2022. 5
[49] Sai Qian Zhang, Qi Zhang, and Jieyu Lin. Succinct and ro-
bust multi-agent communication with temporal message con-trol. Advances in Neural Information Processing Systems ,
33:17271–17282, 2020. 3
12584
