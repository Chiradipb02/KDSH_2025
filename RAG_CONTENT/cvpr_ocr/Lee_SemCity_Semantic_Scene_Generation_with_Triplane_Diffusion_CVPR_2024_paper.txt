SemCity: Semantic Scene Generation with Triplane Diffusion
Jumin Lee1*Sebin Lee1*Changho Jo2Woobin Im1Juhyeong Seon1Sung-Eui Yoon1
1KAIST2Neosapience, Inc.
SSC
(d) Scene inpainting(c) Scene outpaintingTopview
SSC refinement Sensor 
observation
(a) Scene generation (b) Semantic scene completion refinement
Topview
GT
Figure 1. We introduce a diffusion framework, SemCity , designed for generating semantic scenes in real-world outdoor environments as
shown in (a). We extend our diffusion model to various practical tasks: semantic scene completion refinement, scene outpainting, and scene
inpainting. For instance, the comprehensive scenario is displayed in (b) →(c)→(d): the refined scene (SSC refinement) (b) is outpainted
to a broader scene (c); then, an object (in this case, a car) is seamlessly integrated into the scene via our inpainting process (d).
Abstract
We present “SemCity, ” a 3D diffusion model for seman-
tic scene generation in real-world outdoor environments.
Most 3D diffusion models focus on generating a single ob-
ject, synthetic indoor scenes, or synthetic outdoor scenes,
while the generation of real-world outdoor scenes is rarely
addressed. In this paper, we concentrate on generating a
real-outdoor scene through learning a diffusion model on a
real-world outdoor dataset. In contrast to synthetic data,
real-outdoor datasets often contain more empty spaces due
to sensor limitations, causing challenges in learning real-
outdoor distributions. To address this issue, we exploit a
triplane representation as a proxy form of scene distribu-
tions to be learned by our diffusion model. Furthermore,
we propose a triplane manipulation that integrates seam-
lessly with our triplane diffusion model. The manipulation
improves our diffusion model’s applicability in a variety
of downstream tasks related to outdoor scene generation
such as scene inpainting, scene outpainting, and semantic
*Both authors contributed equally to this work as co-first authors.scene completion refinements. In experimental results, we
demonstrate that our triplane diffusion model shows mean-
ingful generation results compared with existing work in
a real-outdoor dataset, SemanticKITTI. We also show our
triplane manipulation facilitates seamlessly adding, remov-
ing, or modifying objects within a scene. Further, it also
enables the expansion of scenes toward a city-level scale.
Finally, we evaluate our method on semantic scene comple-
tion refinements where our diffusion model enhances pre-
dictions of semantic scene completion networks by learn-
ing scene distribution. Our code is available at https:
//github.com/zoomin-lee/SemCity .
1. Introduction
Diffusion models [16] have emerged as a promising gen-
eration tool owing to its state-of-the-art generation results
in image domain [35, 36]. This advance has led to ac-
tive exploration in extending diffusion models to 3D data
generation across both academic and industrial groups. In
the 3D domain, diffusion models have shown remarkable
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
28337
capabilities in generating diverse 3D forms ( e.g., voxels,
meshes) [24, 26]. While those 3D diffusion models primar-
ily aim to craft a single object, generating scenes consisting
of multiple objects remains a relatively unexplored area in
the 3D diffusion domain.
Scene generative diffusion models focus on crafting
both geometrically and semantically coherent environ-
ments. Compared with a single object generation, generat-
ing a scene with multiple objects requires an understanding
of more complex geometric and semantic structure due to a
larger spatial extent [41]. There are primarily two streams
of scene generative diffusion models, each tailored to ei-
ther indoor or outdoor settings. In particular, the outdoor
environments have inherent challenges caused by a broader
landscape than indoor ones.
We propose to leverage a triplane representation [10] for
broader outdoor scenes, a method of factorizing 3D data
onto three orthogonal 2D planes, as utilized in 3D object re-
construction and NeRF models [2, 31, 46]. We excavate its
advantages in addressing the data sparsity problem typically
found in outdoor datasets due to the sensor limitations ( e.g.,
occlusions, range constraints) in capturing outdoor scenes.
Triplane representation helps to reduce the inclusion of un-
necessary empty information through the factorization of
3D data to 2D planes [10]. This efficiency in capturing rele-
vant spatial detail makes it an effective tool for representing
the many objects typically found in outdoor environments.
In this paper, we design our diffusion framework based
on triplane representations. Our triplane autoencoder learns
to compress a voxelized scene into a triplane representa-
tion by reconstructing semantic labels of the scene. Fol-
lowing this, the triplane diffusion model is trained and used
to generate new scenes, as shown in Fig. 1(a), by creating
novel triplanes based on the efficient representation. Fur-
ther, we propose a triplane manipulation method, which ex-
tends our triplane diffusion model toward several practical
tasks ( i.e., scene inpainting, scene outpainting, and seman-
tic scene completion refinements) as shown in Fig. 1(b-d).
Our method can seamlessly add, remove, and modify ob-
jects in real-outdoor scenes while maintaining the semantic
coherence of environments.
Our contributions are summarized as follows:
• We disclose the applicability of the triplane representation
through generating semantic scenes for real-outdoor en-
vironments and extend its views in practical downstream
tasks: scene inpainting, scene outpainting, and semantic
scene completion refinement.
• We propose to manipulate triplane features during our
diffusion process, facilitating seamlessly extending our
method toward the downstream tasks.
• We demonstrate that the proposed method significantly
enhances the quality of generated scenes in real-world
outdoor environments.2. Related Work
Diffusion Models. Diffusion models [16] learn data dis-
tributions via iterative denoising processes based on score
functions [40]. Its generated results have shown remark-
ably realistic appearances with high fidelity and diversity in
a variety of 2D image synthesis such as outpainting [35, 57],
inpainting [27, 35] and text-to-image generation [32, 36].
Built upon these achievements, diffusion models have also
been extended into the 3D domain, generating impressive
results in various 3D shapes, including voxel grids [24, 58],
point clouds [28, 53, 54], meshes [26], and implicit func-
tions [20, 37, 38, 48]. While these models can craft a sin-
gle 3D object, our model focuses on generating a 3D scene
composed of multiple objects using a categorical voxel data
structure, which is a relatively under-explored area in the
3D diffusion domain.
Diffusion Models for Scene Generation. In contrast to
a single object generation, scene generation involves an
understanding of the larger 3D space, causing more se-
mantic and geometric complexities [41]. Diffusion mod-
els for scene generation have been studied in both indoor
and outdoor environments. In indoor settings, diffusion
models aim to learn distributions of relations among ob-
jects by representing them as scene graphs [19]. The scene
graphs contain object attributes ( e.g., location, orientation,
and size), capturing the intricate inter-object relationships
within bounded spaces [41, 55]. For outdoor scenes, the
challenges are distinct, frequently including a lot of empty
areas ( e.g., sky, open areas) resulting from the broader land-
scapes. Traditional approach [22] has relied on discrete dif-
fusion methods [17] on voxel space, necessitating a detailed
representation of every air volume.
In this paper, we demonstrate that triplane diffusion is
highly effective for generating real-outdoor scenes. By ab-
stracting 3D spaces into three orthogonal 2D planes, tri-
plane representation [10] effectively captures the vastness
of outdoor environments, predominantly composed of air.
Beyond its data efficiency, the triplane excels in focusing on
other significant objects ( e.g., vehicles, buildings) by allo-
cating lesser attention to less informative elements like air.
Our approach stands in stark contrast to prior work [22],
which were constrained to synthetic datasets with consid-
erably less empty space. In real datasets, inherent sensor
limitations, such as a limited field-of-view, a limited ranges,
and the inability to capture occluded areas like the rears of
buildings, lead to a prevalence of empty space. Further-
more, we emphasize the versatility of our framework by
demonstrating its extension to various downstream tasks,
including scene inpainting, outpainting, and semantic scene
completion refinement.
28338
axis-wise 
avg pool
Triplane
axis-wise 
avg pool
axis-wise 
avg pool…
Triplane Triplane
(b) Triplane diffusion for outdoor scene generation(a) Triplane learning for efficient outdoor scene compressionEncoder Scene Decoder Class
probabilities
axis-wise 
avg pool
Scene decode decode
Denoising (Eq. 3)Diffusion…
Scene 
(c)TriplanediffusionextensionInpainting
Outpainting
SSC Refinement
Original OverlappingInpaint
OutpaintFigure 2. Overview of ours. (a) A 3D semantic map xis encoded by a triplane encoder fθand factorized to a triplane h. The triplane
coupled with a positional encoding PE(p)is decoded by an implicit decoder gθ, resulting in class probabilities for each coordinate p. (b)
Our triplane diffusion model Dϕlearns to generate a novel triplane for semantic scene generation via denoising diffusion process. (c) We
further extend our triplane diffusion beyond a simple scene generation toward various practical scenarios by manipulating triplanes in (b).
3D Inpainting and Outpainting. In 3D inpainting, the
primary objective is to fill in missing portions or modify
existing elements of 3D data while maintaining geometric
consistency. Most existing works concentrate on single-
object inpainting [3, 23, 44]; for instance, they seamlessly
transit a 3D chair’s leg count from three to four. Contrary
to inpainting, 3D outpainting is to extrapolate a given scene
over an unobserved space. The existing work [1] focuses
on scene outpainting within bounded indoor environments
such as rooms. While in 2D images, inpainting and out-
painting are not restricted to a single object [12, 18, 52].
Likewise, in 3D space, we focus on scene-level inpaint-
ing, which seamlessly adds, removes, or modifies objects
in a scene. Further, our scene-level outpainting is not con-
strained to the bounded scenes; we extrapolate the outdoor
scene from sensor range ( e.g., LiDAR) to city-scale.
Semantic Scene Completion. Semantic scene comple-
tion (SSC) [39] is pivotal for 3D scene understanding,
where it jointly infers completion and semantic segmenta-
tion of the 3D scene from sensor observations such as RGB
images [9, 25, 29] or point clouds [49–51]. In addition,
SSC plays a crucial role in supporting comprehensive au-
tonomous navigation systems, notably in essential down-
stream tasks like path planning [5, 11, 43] and map con-
struction [13, 42]. Despite significant progress in the field,
a persistent challenge is the semantic and geometric dis-
crepancies between the SSC-estimated scenes and their real
counterparts, as illustrated in Fig. 1(c). These discrepan-
cies can undermine the performance of downstream tasks.
Our triplane diffusion model can help to bridge this gap by
exploiting 3D scene priors. This approach enhances the re-
liability and effectiveness of SSC, which is expected to im-
prove its application in autonomous navigation systems.3. Method
In this section, we elucidate our triplane diffusion model
and its extensions. Our triplane diffusion model aims to
synthesize novel real-outdoor scenes by generating a tri-
plane, a proxy representation, which effectively addresses
the inherent challenges of real-outdoor scene synthesis.
This triplane representation is learned by our triplane au-
toencoder, which abstracts the geometric and semantic in-
tricacies of a scene into three orthogonal 2D feature planes,
namely, the xy,xz, and yzplanes (Sec. 3.1). Then, our
diffusion model learns triplane distributions of scenes, gen-
erating novel triplanes (Sec. 3.2). We extend our triplane
diffusion model toward various practical scenarios: scene
inpainting, scene outpainting, and semantic scene comple-
tion refinements (Sec. 3.3).
3.1. Representing a Semantic Scene with Triplane
To represent a 3D scene as a triplane, our triplane autoen-
coder learns to compress a 3D scene into a triplane repre-
sentation as shown in Fig. 2(a). The autoencoder consists
of two modules: (1) an encoder fθyielding a triplane, and
(2) an implicit multi-layer perceptron (MLP) decoder gθfor
reconstruction from the triplane.
The encoder fθtakes a voxelized scene x∈RX×Y×Z
containing Nclasses within a spatial grid of resolution
X×Y×Z. It then yields an axis-aligned triplane rep-
resentation h= [hxy,hxz,hyz]. The triplane consists of
three planes, each characterized by distinct dimensional
properties: hxy∈RCh×Xh×Yh,hxz∈RCh×Xh×Zh, and
hyz∈RCh×Yh×Zh, where Chstands for a feature dimen-
sion, and Xh,Yh, and Zhdenote spatial dimension of the
triplane. During the encoding phase, a 3D feature volume is
extracted by 3D convolutional layers from the scene x, re-
28339
sulting in the triplane via axis-wise average pooling. Given
a 3D coordinate p= (x, y, z ), the triplane is interpreted as
a summation of vectors bilinearly interpolated from each
plane: h(p)=hxy(x, y)+hxz(x, z)+hyz(y, z).
To reconstruct the 3D scene x, we decode the encoded
triplane hwith an implicit MLP decoder gθthat predicts
semantic class probabilities. The decoder takes the tri-
plane vector h(p)with its sinusoidal positional embed-
dingPE(p)[30], resulting in class probabilities c(p) =
gθ(h(p),PE(p))∈[0,1]N. The positional embedding pro-
duces high-frequency features according to the coordinates
p, which helps the implicit decoder gθrepresent high-
frequency scene contents [45].
The encoder fθand the MLP decoder gθare trained with
the autoencoder loss LAEand scene label x(p)as:
LAE=Ep∼P[ℓCE(c(p),x(p)) +λℓLZ(c(p),x(p))],(1)
where λis a loss weight, and Pis the set of grid coordi-
nates of the scene. We use the weighted cross-entropy loss
ℓCE[33] and the Lov ´asz-softmax loss ℓLZ[7] to learn imbal-
anced semantic distributions of the scene.
3.2. Triplane Diffusion
Based on the triplane representation of the 3D seman-
tic scene, our triplane diffusion model Dϕlearns to gen-
erate a novel triplane through denoising diffusion proba-
bilistic models [16] as shown in Fig. 2(b). This triplane
generation leads to generation of 3D scene through de-
coding the generated triplane with the implicit MLP de-
coder gθ. Through the x0-parameterization [4], the diffu-
sion model Dϕis trained to reconstruct the triplane hgiven
its corrupted triplane htsampled from a diffusion process
q(ht|h)=N(√¯αth,(1−¯αt)I), where Nis the Gaussian
distribution, ¯αt=Qt
i=1αi, and αt= 1−βtwith a vari-
ance schedule βt. The diffusion process q(ht|h)is derived
from the Markovian chain rule with a single step’s diffusion
process q(ht|ht−1)=N(√1−βtht−1, βtI). Thus, the tri-
plane diffusion loss is defined as:
LD=Et∼U(1,T)||h−Dϕ(ht, t)||p, (2)
where Tis the number of denoising steps, and prepresents
the order of the norm. The timestep tis sampled from the
discrete uniform distribution U.
After training, the diffusion model Dϕgenerates a novel
triplane h0via the iterative DDPM generation process [16]
starting from hT∼ N(0,I):
ht−1∼ N 
γtht+δtDϕ(ht, t), β2
tI
, (3)
withγt:=√αt(1−¯αt−1)/(1−¯αt)andδt:=√¯αt−1βt/(1−
¯αt). From the generated triplane h0, we generate a novel
3D semantic scene x0by querying coordinates pto the im-
plicit decoder, i.e.,gθ(h0(p),PE(p)).3.3. Applications with Triplane Manipulation
Building on the triplane diffusion process (Sec. 3.2), we
propose a triplane manipulation that allows our model to
facilitate a variety of practical downstream tasks with few
modifications, as illustrated in Fig. 2(c).
Scene Inpainting. Our scene inpainting randomly edits a
3D scene, seamlessly adding, modifying, or removing ob-
jects while maintaining the consistency and realism of the
scene. For instance, the inpainting includes scenarios where
cars or sidewalks appear and then disappear, or vice versa,
as shown in Fig. 1(d). Inspired by the RePaint sampling
strategy [27], we propose a 3D-aware inpainting approach
with semantic coherence. RePaint focuses on the image do-
main without explicitly considering the fidelity of the un-
derlying 3D scene. To facilitate 3D-aware inpainting, we
inpaint the triplane, serving as a compact proxy represen-
tation for the scene. We define a binary spatial trimask
m= [mxy,mxz,myz]covering inpainting regions on tri-
plane space, allowing us to control the generation process
on the masked region. The trimask mis set to have ones
for inpainting regions and zeros for others. We override the
t-th triplane ht= [hxy
t,hxz
t,hyz
t]of the generation process
(Eq. 3) as follows:
ht←m⊗ht+ (1−m)⊗hknown
t, (4)
where ⊗is the element-wise product, and the known tri-
plane hknown
t for intact regions is sampled from the diffu-
sion process, i.e.,q(hknown
t|h):=q(ht|h), which adheres to
a known Gaussian distribution.
Scene Outpainting. Our scene outpainting extends the
boundaries of the 3D scene without additional training as
with inpainting. To seamlessly outpaint a scene, the regions
to be extended should be conditioned on the original scene.
We propose to inject this intuition into triplane represen-
tation as shown in Fig. 2(c). For covering regions to be
outpainted, our diffusion model generates a novel triplane
that is partially overlapped with the original triplane. We
implement this with the concept of the trimask mand the
known triplane hknown
t as in our scene inpainting. The tri-
maskmcovers regions to be outpainted, and the hknown
t is
obtained from the intersections of triplanes between origi-
nal and outpainted regions. Based on the outpainting strat-
egy, we extend a given scene toward cardinal and intercar-
dinal directions, facilitating the creation of an unbounded
scene. While our triplane diffusion model is trained using
triplanes of a fixed size, it demonstrates the capability to
outpaint scenes to be several times bigger than the original
scene, as illustrated in Fig. 1(c).
28340
SemanticKITTI [ 6] CarlaSC [ 46]SSD [ 22] Ours Data
distribution pole
vegetationbuilding
road
vehiclefence
groundother
sidewalkpedestrian
empty (air)motorcycle
bicyclistparking
bicycle
personsidewalk
truck
terrainroad
vehicle
trunkground
vegetation
fencebuilding
motorcyclisttraffic -sign
empty (air)car
pole
Figure 3. Scene generation results using both real and synthetic outdoor datasets – SemanticKITTI [6] and CarlaSC [47]. Our results
showcase the effective generation of overall structures, including roads and buildings, along with detailed objects such as cars.
Semantic Scene Completion Refinement. SSC models
complete and segment a 3D scene from sensor observations
such as images or point clouds. We observe that the SSC re-
sults show a geometric and semantic discrepancy compared
with data distributions, as shown in Fig. 1(b). We extend our
triplane diffusion model to refine predictions of SSC models
toward reducing the discrepancy. To effectively condition
our triplane diffusion scheme to SSC model’s prediction
xssc, we utilize its triplane representation hssc=fθ(xssc)
derived by our triplane encoder fθ. We extend our triplane
diffusion scheme with a simple modification of the triplane
htin the diffusion loss (Eq. 2) and the generation process
(Eq. 3) as follows:
ht=hssc
t⊕hssc, (5)
where ⊕is concatenation, and hssc
tis at-th diffused triplane
sampled by the DDPS diffusion process [21] with the SSC
prediction’s triplane hssc.
4. Experiments
4.1. Experimental Details
Training Dataset. We validate our method on the
SemanticKITTI [6] and CarlaSC [47] datasets. Se-
manticKITTI provides 3D semantic scenes of real-outdoor
environments with labels for 20 semantic classes. Eachscene is represented by a voxel grid of 256×256×32,
covering an area of 51.2 m in front of the car, extending
51.2 mon each side, and reaching up to a height of 6.4 m.
The dataset retains object motion traces as a result of sensor
frame integration, which is employed to establish a dense
ground truth. In contrast, CarlaSC is a synthetic dataset that
provides 3D semantic outdoor scenes without the trace of
moving objects. The dataset contains annotated 11 seman-
tic classes with a voxel grid of 128×128×8and covers a
distance of 25.6 min front and behind the car, 25.6 mlater-
ally on each side, and 3 min height.
Implementation Details. Our experiments are deployed
on a single NVIDIA RTX 3090 GPU with a batch size of 4
for the triplane autoencoder and 18 for the triplane diffusion
model. For the triplane autoencoder, the input scene is en-
coded to triplane with a spatial resolution (Xh, Yh, Zh) =
(128,128,32), and the feature dimension Chis 16. The loss
weight λin Eq. 1 is set to 1.0. The order of the norm pin
Eq. 2 is set to 1 for SSC refinements and 2 for other cases.
For the diffusion model, the learning rate is initialized to 1e-
4 and then decreases linearly. During the diffusion process,
we use the default settings [34] with 100 time steps ( T). For
our triplane inpainting and outpainting, we employ the Re-
Paint sampling strategy [27] as a reference and perform a
repaint with 5 resampling and a jump size of 20.
28341
Model FID ↓KID↓IS↑Prec↑Rec↑
SemanticKITTI [6]
SSD [22] 112.82 0.12 2.23 0.01 0.08
SemCity (Ours) 56.55 0.04 3.25 0.39 0.32
CarlaSC [47]
SSD [22] 87.39 0.09 2.44 0.14 0.07
SemCity (Ours) 40.63 0.02 3.51 0.31 0.09
Table 1. Results of semantic scene generation. Each metric is
computed between the rendered image of the generated scene and
the ones of the actual scene at a resolution of 1440×2048 .
Evaluation Metrics. Following the scene generation
works [41, 55], we evaluate the performance of semantic
scene generation by examining both the diversity and fi-
delity of 3D semantic scenes within the rendered images.
We use recall to evaluate diversity, while precision and in-
ception score (IS) are used to evaluate fidelity. The Fr ´echet
Inception Distance (FID) [15] and Kernel Inception Dis-
tance (KID) [8] metrics are also utilized, as they reflect the
combined effect of both diversity and fidelity on scene qual-
ity [14]. In terms of semantic scene completion (SSC) re-
finement performance, we follow the protocols defined in
SSC works [9, 29, 49, 50]. The Intersection-over-Union
(IoU) metric is used to quantify scene completeness, while
the mean IoU (mIoU) provides a measure for the quality
of semantic segmentation. These metrics together enable a
comprehensive evaluation of how well the semantic scene
completion methods perform in terms of accurately filling
in and labeling the scene components.
4.2. Semantic Scene Generation
Fig. 3 illustrates a qualitative comparison on the Se-
manticKITTI [6] and CarlaSC [47] datasets. SSD [22]
shows impressive results on the CarlaSC dataset. How-
ever, its performance on the SemanticKITTI dataset, which
is a real-world dataset, is notably constrained. This limi-
tation primarily arises from the SSD’s voxel-based repre-
sentation, which struggles with the more prevalent empty
spaces in real-outdoor datasets compared to synthetic ones.
This issue is especially evident in the generation of build-
ings and roads, where SSD often fails to define boundaries
accurately. Furthermore, the model’s limitations extend to
representing finer structures, such as trunks and leaves, as
well as traffic light poles and signals. It also struggles with
generating uniform shapes of vehicles, often resulting in ir-
regular shapes. In contrast, our method demonstrates the
ability to effectively synthesize detailed scenes even on the
real dataset, as illustrated in Fig. 3. It performs better than
SSD [22] in accurately capturing complex building shapes
on the CarlaSC dataset. In addition, our method exhibits
remarkable proficiency in generating the overall contours
of roads and buildings, along with intricate details on the
SemanticKITTI dataset. Tab. 1 provides a detailed compar-
Figure 4. Higher-resolution scene generation. Building upon
our implicit decoder, higher-resolution scene ( 1024×1024×128)
can be generated compared with a resolution of training dataset
(256×256×32).
Reference Our inpainting results
(a) (b)
Figure 5. Scene inpainting of our method. The red boxes denote
inpainting regions. (a) and (b) show our inpainting examples from
reference images.
ative evaluation using various metrics. Our model shows
significant improvements in both the fidelity and diversity
of the generated scenes. Moreover, our generated result is
not tied to fixed resolution by means of the implicit neural
representation, as depicted in Fig. 4. For additional results,
please refer to the Supplementary Material.
4.3. Applications of Triplane Diffusion
Scene Inpainting. Fig. 5 presents the qualitative results
of our inpainting, demonstrating its effectiveness in inpaint-
ing both small and large regions within a scene, while main-
taining the coherence of 3D contexts. In detail, the second
row of (a)and(b)illustrates the model’s seamless removal
28342
Figure 6. Our scene outpainting results. The red box at the center of the figure in the first row represents the given scene for outpainting.
The zoomed views of the outpainted scene are depicted in the second row. The outpainted scene is expanded from the given size of
256×256×32to1792×3328×32.
SSC Input Method IoU ↑ mIoU↑
RGBMonoScene [9] 37.12 11.50
MonoScene + Ours 50.44 17.08
OccDepth [29] 41.60 12.84
OccDepth + Ours 50.20 16.79
Point CloudSSA-SC [51] 58.25 24.54
SSA-SC + Ours 60.71 25.58
SCPNet [49] 50.24 37.55
SCPNet + Ours 59.25 38.19
Table 2. Quantitative results of refining SSC on Se-
manticKITTI validation set [6]. The results are based on the
weights released by the authors on GitHub.
of vehicles, which harmonizes with the adjacent road. The
third row demonstrates the insertion of new entities — a ve-
hicle in (a)and a person in (b)— that are contextually con-
gruent with the reference scene. The fourth row in (a)ex-
emplifies the model’s dual functionality in both modifying
and adding vehicles within the scene. The fifth row in both
columns underscores the model’s proficiency in modifying
scenes. Here, the model alters existing scene components,
showcasing its ability to transform the overall ambiance of
the scene. These results show our model’s adeptness not just
in object-level inpainting but also in scene-level inpainting.
Scene Outpainting. Fig. 6 illustrates a generated out-
painting city-level scene, extending a 256×256×32scene
to a substantial 1792×3328×32landscape. Although our
model was not designed to generate cityscapes, it demon-
strates the capability of maintaining coherence over a large
area. The roads are connected in a meaningful and varied
way, and various objects such as buildings, cars, and people
have been created around them. Please check the details of
SSC Our SSC refinement Ground truthMotionSC
[34]MonoScene OccDepth SSA-SC SCPNet(40.35%, 13.72%) (53.15%, 23.11%)
(37.86%, 24.01%) (43.25%, 31.43%)
(65.83%, 38.52%) (67.11%, 43.63%)
(49.81%, 36.32%) (65.16%, 39.41%)SSD
[34]Figure 7. Semantic scene completion refinement. The SSC met-
rics are reported in the parentheses as (IoU, mIoU).
the scene in Fig. 6.
Semantic Scene Completion Refinement. In Fig. 7,
there is a notable semantic and geometric discrepancy be-
tween scenes predicted by existing semantic scene comple-
tion (SSC) methods and their real scene counterparts. Our
model helps to bridge this gap by employing a 3D scene
prior, which is effectively modeled through our diffusion
model. SCPNet [49] also attempts to learn geometric pri-
ors from a teacher model trained with merged sequential
frames. Yet, its completion network is inclined toward con-
servative estimations, which frequently result in partially
filled spaces. In the case of SSA-SC [51], which utilizes
bird’s-eye view features, it is sometimes inappropriate due
to the label’s inherent limitations of the bird’s-eye perspec-
tive in capturing certain semantic details. This discrepancy
28343
(a) (b)Figure 8. Semantic scene to RGB. (a) Semantic maps are ren-
dered into a driving view from our generated scenes. (b) We gen-
erate images through ControlNet [56], an easily accessible image-
to-image model, with generated semantic maps without shadows.
Method FID ↓KID↓IS↑Prec↑Rec↑
Ours 56.55 0.04 3.25 0.39 0.32
w/oPE 59.14 0.04 3.21 0.30 0.31
xy-plane (w/o triplane) 75.56 0.06 3.08 0.28 0.27
3D-volume (w/o triplane) 136.50 0.15 1.96 0.14 0.12
Table 3. Ablation studies on scene generation. We ablate our
model variants on SemanticKITTI [6].
issue is even more evident in RGB-based methods, such as
MonoScene [9] and OccDepth [29], which often exhibit di-
minished sharpness when image features are projected into
3D space. While SSC models show variations from real-
world data distributions, our model shows potential in align-
ing these more closely with reality, as shown in Fig. 7.
As indicated in Tab. 2, our SSC refinement process ap-
pears to offer improvements to all state-of-the-art SSC mod-
els. These preliminary results suggest our model’s effec-
tiveness in providing not just more accurate semantic seg-
mentation but also a more complete scene.
Semantic Scene to RGB Image. As illustrated in Fig. 8,
we conducted an image-to-image generation experiment.
The semantic maps are rendered into a driving view with-
out shadows; then, they are utilized as inputs for Control-
Net [56]. The generated RGB images are geometrically
and semantically plausible but display a synthetic quality
since the pretrained ControlNet was not trained on actual
autonomous driving datasets.
4.4. Ablation
As shown in Tab. 3, we conducted ablation studies on our
triplane diffusion model for scene generation, focusing on
two key design elements:
Positional Embedding. Our variant excluded positional
embedding PE, which generates high-frequency features
critical for detailed scene reconstruction. Its absence re-
sulted in lower performance across all metrics.
Triplane Representation. We evaluated the effectiveness
of the triplane representation for real-outdoor scene gen-eration. The triplane and xy-plane enhance the genera-
tion quality compared with 3D features, while the xy-plane
shows lower performance than triplanes. We suspect that
the excessive factorization limits the representation capa-
bility of the xy-plane compared to the triplane.
4.5. Limitation
While our model demonstrates significant progress in the
generation of 3D real-outdoor scenes, it inherently reflects
the characteristics of its training data, as detailed in Sec. 4.1.
This reliance introduces several limitations. One notable
challenge is the model’s difficulty in accurately depicting
areas occluded from the sensor’s viewpoint, such as the rear
sides of buildings, often leading to their incomplete rep-
resentation in our generated scenes. Moreover, since the
dataset is captured from a driving view, there is an inher-
ent shortfall in capturing the full height of buildings. This
brings a partial representation of the vertical structure of
buildings and other tall elements in the scenes. Another is-
sue is the model tends to produce traces of moving objects
stemming from dataset pre-processing that merges sequen-
tial frames. For future work, incorporating prior knowledge
of the city into the model could yield better results, such as
addressing occluded areas or building heights.
5. Conclusion
We have proposed a diffusion framework called SemCity for
real-world outdoor scene generation. The seminal idea is
to generate a scene by factorizing real-outdoor scenes into
triplane representations. Our triplane representation outper-
forms traditional voxel-based approaches, producing scenes
that are not only visually more appealing but also rich in
semantic detail, effectively capturing the complexity of var-
ious objects within the scene. Ours is not constrained by
fixed resolutions thanks to the incorporation of an implicit
neural representation. We have further expanded the capa-
bilities of our triplane diffusion model to several practical
applications, including scene inpainting, scene outpainting,
and semantic scene completion refinement. Specifically,
by manipulating triplanes during the diffusion process, we
achieve seamless inpainting and outpainting at both the ob-
ject and scene levels. Ours is used to more closely align
the scenes predicted by existing semantic scene completion
methods with the actual data distribution using our learned
3D prior. We believe that our work provides a road map of
real-outdoor scene generation to research communities.
Acknowledgement. This work was supported by IITP
(Institute of Information & Communications Technology
Planning & Evaluation) and ITRC (Information Technology
Research Center), funded by Korea government (MSIT)
(RS-2023-00237965(2024) and IITP-2024-2020-0-01460).
Prof. Sung-Eui Yoon is a corresponding author.
28344
References
[1] Ali Abbasi, Sinan Kalkan, and Yusuf Sahillio ˘glu. Deep 3d
semantic scene extrapolation. The Visual Computer , 35:271–
279, 2019. 3
[2] Titas Anciukevi ˇcius, Zexiang Xu, Matthew Fisher, Paul Hen-
derson, Hakan Bilen, Niloy J Mitra, and Paul Guerrero. Ren-
derdiffusion: Image diffusion for 3d reconstruction, inpaint-
ing and generation. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
12608–12618, 2023. 2
[3] Titas Anciukevi ˇcius, Zexiang Xu, Matthew Fisher, Paul Hen-
derson, Hakan Bilen, Niloy J. Mitra, and Paul Guerrero. Ren-
derdiffusion: Image diffusion for 3d reconstruction, inpaint-
ing and generation. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition (CVPR) ,
pages 12608–12618, 2023. 3
[4] Jacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tar-
low, and Rianne Van Den Berg. Structured denoising dif-
fusion models in discrete state-spaces. Advances in Neural
Information Processing Systems , 34:17981–17993, 2021. 4
[5] Luca Bartolomei, Lucas Teixeira, and Margarita Chli.
Perception-aware path planning for uavs using semantic seg-
mentation. In 2020 IEEE/RSJ International Conference on
Intelligent Robots and Systems (IROS) , pages 5808–5815,
2020. 3
[6] Jens Behley, Martin Garbade, Andres Milioto, Jan Quen-
zel, Sven Behnke, Cyrill Stachniss, and Jurgen Gall. Se-
mantickitti: A dataset for semantic scene understanding of
lidar sequences. In Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision , pages 9297–9307,
2019. 5, 6, 7, 8
[7] Maxim Berman, Amal Rannen Triki, and Matthew B
Blaschko. The lov ´asz-softmax loss: A tractable surrogate for
the optimization of the intersection-over-union measure in
neural networks. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 4413–4421,
2018. 4
[8] Mikołaj Bi ´nkowski, Danica J Sutherland, Michael Arbel, and
Arthur Gretton. Demystifying mmd gans. In International
Conference on Learning Representations , 2018. 6
[9] Anh-Quan Cao and Raoul de Charette. Monoscene: Monoc-
ular 3d semantic scene completion. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 3991–4001, 2022. 3, 6, 7, 8
[10] Eric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano,
Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J
Guibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient
geometry-aware 3d generative adversarial networks. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 16123–16133, 2022. 2
[11] Devendra Singh Chaplot, Dhiraj Prakashchand Gandhi, Ab-
hinav Gupta, and Russ R Salakhutdinov. Object goal navi-
gation using goal-oriented semantic exploration. Advances
in Neural Information Processing Systems , 33:4247–4258,
2020. 3
[12] Yen-Chi Cheng, Chieh Hubert Lin, Hsin-Ying Lee, Jian Ren,
Sergey Tulyakov, and Ming-Hsuan Yang. Inout: Diverseimage outpainting via gan inversion. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 11431–11440, 2022. 3
[13] Xu Cui, Chenggang Lu, and Jinxiang Wang. 3d semantic
map construction using improved orb-slam2 for mobile robot
in edge computing environment. IEEE Access , 8:67179–
67191, 2020. 3
[14] Prafulla Dhariwal and Alexander Nichol. Diffusion models
beat gans on image synthesis. Advances in neural informa-
tion processing systems , 34:8780–8794, 2021. 6
[15] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
Bernhard Nessler, and Sepp Hochreiter. Gans trained by a
two time-scale update rule converge to a local nash equilib-
rium. Advances in neural information processing systems ,
30, 2017. 6
[16] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-
fusion probabilistic models. Advances in neural information
processing systems , 33:6840–6851, 2020. 1, 2, 4
[17] Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick
Forr´e, and Max Welling. Argmax flows and multinomial dif-
fusion: Learning categorical distributions. Advances in Neu-
ral Information Processing Systems , 34:12454–12465, 2021.
2
[18] Changho Jo, Woobin Im, and Sung-Eui Yoon. In-n-out:
Towards good initialization for inpainting and outpainting.
arXiv preprint arXiv:2106.13953 , 2021. 3
[19] Justin Johnson, Agrim Gupta, and Li Fei-Fei. Image gener-
ation from scene graphs. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition , pages
1219–1228, 2018. 2
[20] Heewoo Jun and Alex Nichol. Shap-e: Generat-
ing conditional 3d implicit functions. arXiv preprint
arXiv:2305.02463 , 2023. 2
[21] Zeqiang Lai, Yuchen Duan, Jifeng Dai, Ziheng Li, Ying Fu,
Hongsheng Li, Yu Qiao, and Wenhai Wang. Denoising diffu-
sion semantic segmentation with mask prior modeling. arXiv
preprint arXiv:2306.01721 , 2023. 5
[22] Jumin Lee, Woobin Im, Sebin Lee, and Sung-Eui Yoon. Dif-
fusion probabilistic models for scene-scale 3d categorical
data. arXiv preprint arXiv:2301.00527 , 2023. 2, 6
[23] Jiabao Lei, Jiapeng Tang, and Kui Jia. Generative scene syn-
thesis via incremental view inpainting using rgbd diffusion
models. arXiv preprint arXiv:2212.05993 , 2022. 3
[24] Muheng Li, Yueqi Duan, Jie Zhou, and Jiwen Lu. Diffusion-
sdf: Text-to-shape via voxelized diffusion. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 12642–12651, 2023. 2
[25] Yiming Li, Zhiding Yu, Christopher Choy, Chaowei Xiao,
Jose M Alvarez, Sanja Fidler, Chen Feng, and Anima Anand-
kumar. V oxformer: Sparse voxel transformer for camera-
based 3d semantic scene completion. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 9087–9098, 2023. 3
[26] Zhen Liu, Yao Feng, Michael J Black, Derek
Nowrouzezahrai, Liam Paull, and Weiyang Liu. Meshdif-
fusion: Score-based generative 3d mesh modeling. arXiv
preprint arXiv:2303.08133 , 2023. 2
28345
[27] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher
Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting
using denoising diffusion probabilistic models. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 11461–11471, 2022. 2, 4, 5
[28] Shitong Luo and Wei Hu. Diffusion probabilistic models for
3d point cloud generation. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR) , pages 2837–2845, 2021. 2
[29] Ruihang Miao, Weizhou Liu, Mingrui Chen, Zheng Gong,
Weixin Xu, Chen Hu, and Shuchang Zhou. Occdepth:
A depth-aware method for 3d semantic scene completion.
arXiv preprint arXiv:2302.13540 , 2023. 3, 6, 7, 8
[30] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,
Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:
Representing scenes as neural radiance fields for view syn-
thesis. In ECCV , 2020. 4
[31] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,
Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:
Representing scenes as neural radiance fields for view syn-
thesis. Communications of the ACM , 65(1):99–106, 2021.
2
[32] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
and Mark Chen. Hierarchical text-conditional image gener-
ation with clip latents. arXiv preprint arXiv:2204.06125 , 1
(2):3, 2022. 2
[33] Luis Roldao, Raoul de Charette, and Anne Verroust-Blondet.
Lmscnet: Lightweight multiscale 3d semantic completion.
In2020 International Conference on 3D Vision (3DV) , pages
111–119. IEEE, 2020. 4
[34] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¨orn Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 10684–10695, 2022. 5
[35] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee,
Jonathan Ho, Tim Salimans, David Fleet, and Mohammad
Norouzi. Palette: Image-to-image diffusion models. In
ACM SIGGRAPH 2022 Conference Proceedings , pages 1–
10, 2022. 1, 2
[36] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour,
Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,
et al. Photorealistic text-to-image diffusion models with deep
language understanding. Advances in Neural Information
Processing Systems , 35:36479–36494, 2022. 1, 2
[37] Jaehyeok Shim, Changwoo Kang, and Kyungdon Joo.
Diffusion-based signed distance fields for 3d shape gener-
ation. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition (CVPR) , pages 20887–
20897, 2023. 2
[38] J Ryan Shue, Eric Ryan Chan, Ryan Po, Zachary Ankner,
Jiajun Wu, and Gordon Wetzstein. 3d neural field generation
using triplane diffusion. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 20875–20886, 2023. 2
[39] Shuran Song, Fisher Yu, Andy Zeng, Angel X Chang, Mano-
lis Savva, and Thomas Funkhouser. Semantic scene com-pletion from a single depth image. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 1746–1754, 2017. 3
[40] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-
hishek Kumar, Stefano Ermon, and Ben Poole. Score-based
generative modeling through stochastic differential equa-
tions. In International Conference on Learning Represen-
tations , 2020. 2
[41] Jiapeng Tang, Yinyu Nie, Lev Markhasin, Angela Dai, Jus-
tus Thies, and Matthias Nießner. Diffuscene: Scene graph
denoising diffusion probabilistic model for generative indoor
scene synthesis. arXiv preprint arXiv:2303.14207 , 2023. 2,
6
[42] Chaoqun Wang, Wenzheng Chi, Yuxiang Sun, and Max Q-
H Meng. Autonomous robotic exploration by incremental
road map construction. IEEE Transactions on Automation
Science and Engineering , 16(4):1720–1731, 2019. 3
[43] Chaoqun Wang, Delong Zhu, Teng Li, Max Q-H Meng, and
Clarence W De Silva. Efficient autonomous robotic explo-
ration with semantic road map in indoor environments. IEEE
Robotics and Automation Letters , 4(3):2989–2996, 2019. 3
[44] Xinying Wang, Dikai Xu, and Fangming Gu. 3d model in-
painting based on 3d deep convolutional generative adversar-
ial network. IEEE Access , 8:170355–170363, 2020. 3
[45] Yiqun Wang, Ivan Skorokhodov, and Peter Wonka. Pet-neus:
Positional encoding tri-planes for neural surfaces. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 12598–12607, 2023. 4
[46] Zhen Wang, Shijie Zhou, Jeong Joon Park, Despoina
Paschalidou, Suya You, Gordon Wetzstein, Leonidas Guibas,
and Achuta Kadambi. Alto: Alternating latent topologies for
implicit 3d reconstruction. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR) , pages 259–270, 2023. 2
[47] Joey Wilson, Jingyu Song, Yuewei Fu, Arthur Zhang, An-
drew Capodieci, Paramsothy Jayakumar, Kira Barton, and
Maani Ghaffari. Motionsc: Data set and network for real-
time semantic mapping in dynamic environments. arXiv
preprint arXiv:2203.07060 , 2022. 5, 6
[48] Rundi Wu, Ruoshi Liu, Carl V ondrick, and Changxi Zheng.
Sin3dm: Learning a diffusion model from a single 3d tex-
tured shape. arXiv preprint arXiv:2305.15399 , 2023. 2
[49] Zhaoyang Xia, Youquan Liu, Xin Li, Xinge Zhu, Yuexin
Ma, Yikang Li, Yuenan Hou, and Yu Qiao. Scpnet: Se-
mantic scene completion on point cloud. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 17642–17651, 2023. 3, 6, 7
[50] Xu Yan, Jiantao Gao, Jie Li, Ruimao Zhang, Zhen Li, Rui
Huang, and Shuguang Cui. Sparse single sweep lidar point
cloud segmentation via learning contextual shape priors from
scene completion. In Proceedings of the AAAI Conference on
Artificial Intelligence , pages 3101–3109, 2021. 6
[51] Xuemeng Yang, Hao Zou, Xin Kong, Tianxin Huang, Yong
Liu, Wanlong Li, Feng Wen, and Hongbo Zhang. Seman-
tic segmentation-assisted scene completion for lidar point
clouds. In 2021 IEEE/RSJ International Conference on Intel-
ligent Robots and Systems (IROS) , pages 3555–3562. IEEE,
2021. 3, 7
28346
[52] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and
Thomas S Huang. Generative image inpainting with con-
textual attention. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 5505–5514,
2018. 3
[53] Xin Yu, Peng Dai, Wenbo Li, Lan Ma, Zhengzhe Liu, and
Xiaojuan Qi. Texture generation on 3d meshes with point-
uv diffusion. In Proceedings of the IEEE/CVF International
Conference on Computer Vision (ICCV) , pages 4206–4216,
2023. 2
[54] Xiaohui Zeng, Arash Vahdat, Francis Williams, Zan Gojcic,
Or Litany, Sanja Fidler, and Karsten Kreis. Lion: Latent
point diffusion models for 3d shape generation. In Advances
in Neural Information Processing Systems (NeurIPS) , 2022.
2
[55] Guangyao Zhai, Evin Pınar ¨Ornek, Shun-Cheng Wu, Yan
Di, Federico Tombari, Nassir Navab, and Benjamin Busam.
Commonscenes: Generating commonsense 3d indoor scenes
with scene graphs. Advances in Neural Information Process-
ing Systems , 36, 2024. 2, 6
[56] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding
conditional control to text-to-image diffusion models. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 3836–3847, 2023. 8
[57] Qinsheng Zhang, Jiaming Song, Xun Huang, Yongxin Chen,
and Ming-Yu Liu. Diffcollage: Parallel generation of
large content with diffusion models. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 10188–10198, 2023. 2
[58] Linqi Zhou, Yilun Du, and Jiajun Wu. 3d shape generation
and completion through point-voxel diffusion. In Proceed-
ings of the IEEE/CVF International Conference on Com-
puter Vision , pages 5826–5835, 2021. 2
28347
