CoDi-2: In-Context, Interleaved, and Interactive Any-to-Any Generation
Zineng Tang1,4*Ziyi Yang2†Mahmoud Khademi2Yang Liu2Chenguang Zhu3‡Mohit Bansal4†
1UC Berkeley2Microsoft Azure AI3Zoom4UNC Chapel Hill
https://codi-2.github.io
Abstract
We present CoDi-2, a Multimodal Large Language
Model (MLLM) for learning in-context interleaved multi-
modal representations. By aligning modalities with lan-
guage for both encoding and generation, CoDi-2 empowers
Large Language Models (LLMs) to understand modality-
interleaved instructions and in-context examples and au-
toregressively generate grounded and coherent multimodal
outputs in an any-to-any input-output modality paradigm.
To train CoDi-2, we build a large-scale generation dataset
encompassing in-context multimodal instructions across
text, vision, and audio. CoDi-2 demonstrates a wide range
of zero-shot and few-shot capabilities for tasks like edit-
ing, exemplar learning, composition, reasoning, etc. CoDi-
2 surpasses previous domain-speciﬁc models on tasks such
as subject-driven image generation, vision transformation,
and audio editing and showcases a signiﬁcant advance-
ment for integrating diverse multimodal tasks with sequen-
tial generation.
1. Introduction
Multimodal generation has achieved remarkable advance-
ments in recent years, e.g., generating high-ﬁdelity im-
age, video, audio and music samples from prompt provided
by users. Recent advancements in AI-Generated Content
(AIGC) highlight in-context generation [ 24,38], concept
learning [ 28], editing [ 2], and ﬁne-grained control [ 46]. Re-
cently, Tang et al. [ 32] proposed CoDi, the ﬁrst model ever
that can generate any combinations of modalities from any
combinations of input ones. Building upon this founda-
tional work, the subsequent study by [ 40] further advances
CoDi by proposing a model that can facilitates conversa-
tional abilities and expansion to additional modalities.
*Work done while at Microsoft internship and UNC.
†Correspondence: ziyiyang@microsoft.com, mbansal@cs.unc.edu
‡Work done while at Microsoft.Although remarkable advances have been made in mul-
timodal generation, several critical challenges remain: (1)
Zero-shot ﬁne-grained and complex user-control of multi-
modal generation is difﬁcult: current multimodal genera-
tive models (MGM) cannot follow in-context generation ex-
amples without ﬁnetuning on subtasks, such as replicating
or transferring an editing effect via an ‘analogy’ setting or
subject driven generation, as demonstrated in the prompt
(as in the row “Exemplar Learning” and “’Subject Driven’
of Table 3). Moreover, the reasoning ability of MGM is
rather limited, e.g., the input prompts are usually descrip-
tive where the generation do not require capabilities such as
logical, compositional, and analytical intelligence. (2) The
inputs in previous MGMs mostly only contain one or two
modalities. The ability to understand modality-interleaved
inputs, such as language instruction mixing with contextual
visual and auditory inputs is critical to building a fundamen-
tal multimodality model. Hence, overall, a versatile any-
to-any MGM, that can follow interleaved in-context mul-
timodal instructions and interactive multi-round chatting is
strongly needed. (3) The user-and-model interaction is usu-
ally constrained to single-round, or it is challenging for cur-
rent models to follow multi-round instructions while ensur-
ing the consistency and faithfulness of responses across the
rounds, as shown in Figure 1.
To this end, we propose CoDi-2, a versatile Multimodal
Large Language Model (MLLM) that understand in-context
examples, follow modality-interleaved instructions, per-
form multi-round chatting and editing. Enabling in-context
learning and following interleaved multimodal instructions
in multimodal generation is challenging. In previous mul-
timodal generative models, the backbone is mostly diffu-
sion models (DMs) which are good at generation but in-
trinsically lack the capability to perform in-context under-
standing [ 39]. We therefore propose to harness a Large Lan-
guage Model (LLM) as the “brain” to understand modality-
interleaved human instructions and in-context examples,
and output multimodal signals. LLMs have strong language
reasoning capabilities for complex instructions in the lan-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
27425
Learn thesubject in. Generate with it on theconcept represented by
Edit this image in the vibe of(Raining Sound)
What's the editbetween? Apply it to theimage and tell uswhat the effect is.andI changed the seasonof the scene.Composition and Concept Learning
Multimodal EditingExemplar Learning
HumanCoDi-2CoDi-2
CoDi-2HumanHuman
Figure 1. Multi-round conversation between humans and CoDi-2 offering in-context multimodal instructions for image editing.
guage domain. By mapping all modalities to the space of
language (as proposed in CoDi [ 32]) and connecting these
modalities to LLM through encoder and synchronized de-
coders, CoDi-2 can process multimodal inputs within their
context by aligning vision or audio features to the language
model input and output space, and understand the delicate
modality-interleaved instructions for zero-shot or few-shot
generation.
For generation, we propose to train the MLLM to auto-
gressively predict the features of the output modality. The
predicted features are then input to (synchronized) diffusion
models. This end-to-end any-to-any generative framework
enables CoDi-2 to conduct elaborate reasoning for under-
standing and generate multiple modalities, and therefore
allows diverse tasks such imitation, editing, compositional
creation, etc. In training CoDi-2, the gradient obtained from
the diffusion models’ generation loss also directly back-
propagates to the LLM which can enhance the perceptual
faithfulness to the inputs including images or audio.
The development of the alignment data to train such
model is also challenging, hindered by the scarcity of spe-
cialized data such as multimodal reasoning or in-context
learning. To start with, we comprehensively collect lat-
est instructional generation datasets across vision, audio,
and language. We then propose to convert these instruc-
tional datasets to in-context generation ones, such that in
the prompt (e.g., row “Exemplar Learning” of Table 1) and
more can be referred in Tables 1to3. To further diversify
the in-context learning datasets, we propose a novel method
to build text-only datasets for multimodal in-context learn-
ing. Since language and other modalities (vision and audio)are mapped to the same space through the aligned encoders,
we can ﬂexibly build multimodal datasets with only lan-
guage, where the multimodal components are represented
by their respective textual descriptions (e.g., using image
caption instead of the pixels to represent the image).
Empirical assessments of our multimodal generation
tasks, which include a diverse array of complex and inter-
twined instructions, yield remarkable results. These tasks
encompass audio fusion and editing, image generation with
intricate composition, the use of in-context exemplars, and
sophisticated reasoning, as well as understanding and gen-
erating videos. This wide range of tasks show strong ca-
pability in both zero-shot and few-shot prompting settings,
showcasing our system’s adaptability and robust perfor-
mance across different scenarios.
2. Related Work
2.1. Multimodal Large Language Models
Recent years have witnessed the rapid evolution of LLMs,
setting a new precedent in natural language understand-
ing and generation [ 23,34,35]. Multimodal LLMs extend
LLMs to multimodal learning [ 42,45], enabling the pro-
cessing of diverse input forms, not just limited to text but
also incorporating visual and other sensory data [ 7,16,18,
20,43,44]. The innovation in this space has led to mod-
els that are not only capable of understanding multimodal
inputs but also adept at generating multifaceted outputs,
thereby pushing the boundaries of creative and contextual
AI-generated content [ 32,40]. Another notable line of work
is using LLM to ground image generation [ 13,31].
27426
2.2. Multimodal In-Context
Multimodal in-context requires sometimes interleaved in-
context understanding of multimodal inputs like images
and text like Wikipedia (with images), documents, videos
with narrations or QAs, etc. This domain has expanded
yet facing its set of challenges. While there is a plethora
of research focusing on the understanding aspect of mul-
timodal data [ 16,49], the generation of raw sensory per-
ceptions such as images or audio remains a complex hur-
dle. The concept of treating images as a foreign language
opened new avenues, particularly in in-context image gener-
ation [ 25]. However, these pioneering techniques are still in
nascent stages, often constrained by their training regimes
and lacking genuine in-context learning capabilities, which
limits their performance and adaptability.
2.3. Multimodal Generation
Recent years have witnessed a signiﬁcant growth in image
editing and manipulation research, which can be broken into
image editing [ 2,21], exemplar learning [ 38] for image gen-
eration, image composition [ 14,25,28], and concept learn-
ing [11] from images.
Image editing [ 21] uses guidance control and edit the at-
tributes of an image. To align the guidance with human in-
structions, InstructPix2Pix [ 2] takes in instructional image
editing prompts to directly transform an image. The realm
of image composition are tasks that compose one or more
images into a single image and demand high ﬁdelity to in-
put images, which poses unique challenges. Techniques in-
volved in subject-driven image generation [ 28] have shown
promise in transforming a subject into a new scene. How-
ever, they often necessitate task-speciﬁc or subject-speciﬁc
tuning. This specialization often conﬁnes the models within
the boundaries of their training data, impeding their ability
to generalize beyond learned tasks or subjects. Kosmos-
G[25] furthers the efforts for zero-shot image generation
with in-context interleaved image and text. But its efforts is
limited to image composition. Lastly, learning visual con-
cepts and apply them in image generation is also a growing
direction [ 11,14]. For example, multi-concept customiza-
tion to text-to-image generation [ 14] requires the model to
extract visual concept like a moon gate or a certain sub-
ject and apply them in image generation. The aspiration
to develop a model with in-context multimodal reasoning
abilities to transcend these limitations inspires our versatile
framework that takes in task instructions and perform in-
context zero-shot generation.
3. Model Architecture
CoDi-2 is designed to process in-context multimodal in-
puts, including text, images, and audio, utilizing speciﬁc in-
structions to facilitate in-context learning and generate cor-ImageDecoderbird, flowers, art
Given the art style, generate the subjectrepresented in
(Sound of Bird)
ImageEncoderAudioEncoderText EmbedText EmbedMultimodal Large Language ModelImageDecoderbird,flowers, art
Given theart style, generate thesubjectrepresented in
(Sound of Bird)
ImageEncoderAudioEncoderTextEmbedTextEmbedMultimodal Large Language Model
Figure 2. Model Architecture: CoDi-2 comprises a multimodal
large language model that encompasses encoder and decoder for
both audio and vision inputs, as well as a large language model.
This architecture facilitates the decoding of image or audio inputs
using diffusion models. In the training phase, our approach em-
ploys pixel loss obtained from the diffusion models alongside to-
ken loss, adhering to the standard causal generation loss.
responding text, images, or audio outputs. The model is
distinguished by the several key features as introduced in
following subsections.
3.1. Multimodal LLM as the Fundamental Engine
Building such an any-to-any foundation model that can di-
gest interleaved inputs of modalities, understand and rea-
son over complex instructions (e.g., multi-round conversa-
tion, in-context examples), and interact with multimodal
diffusers requires a powerful fundamental “engine”. We
propose to leverage MLLM for this engine, which is built
by empowering a text-only LLM with multimodal percep-
tions.
The motivation of harnessing LLM is intuitively in-
spired by the observation that LLMs exhibit exceptional
ability such as chatting, zero-shot learning, instruction-
following, etc, in language-only domain [ 48]. By lever-
aging projections from aligned multimodal encoders (e.g.,
[32]), we can seamlessly empower the LLM to perceive
modality-interleaved input sequence. Speciﬁcally, in pro-
cessing the multimodal input sequence, we ﬁrst use the
multimodal encoder to project the multimodal data into a
feature sequence. Special tokens are prepended and ap-
pended to the features sequence, e.g. “ haudio i[audio
feature sequence] h/audio i”. By such for in-
stance, a modality-interleaved input sequence “ A cat sit-
ting on [image0:an image of a couch ]is making
27427
the sound of [audio0:audio of cat purring ]”
is then transformed to “ A cat sitting on himage i
[image feature sequence] h/image iis making
the sound of haudio i[audio feature sequence]
h/audio i, before inputting to the MLLM to process and
generation.
3.2. Multimodal Generation with MLLM
To generate text, the MLLM can naturally generate text to-
kens autoregressively; for multimodal generation, one com-
mon way in previous works was to transform the multi-
modal target (e.g., the ground-truth image) into discrete to-
kens such that they can be generated autoregressively like
text. However, the generation quality of this methodol-
ogy is intrinsically constrained by the V AE-like genera-
tion decoder, while current SOTA multimodal generation
frameworks generally adopt Diffusion Models (DMs) [ 27].
Therefore, we propose to integrate DMs into MLLM to
generate multimodal outputs, following nuanced modality-
interleaved instructions and prompts. Recall the training ob-
jective of a diffusion model is given as:
LDM=Ez,✏,tk✏ ✏✓ 
zt,t ,C y(y) 
k2
2, (1)
where yis the conditional modality, Cyis the conditional
encoder for y,✏✓is the U-Net, and ztis the noisy latent
variable at time step t.
We propose to train the MLLM to generate the condi-
tional feature c=Cy(y)that will be fed into DM to syn-
thesize the target output x. By such, the generative loss
of DM can be used to train MLLM. To further provide a
stronger and directer supervision signal for MLLM, and to
retain the perceptual characteristics inherent in the original
input, we explicitly induce that c=Cx(x), i.e., MLLM is
trained to generate Cx(x)and DM is expected to function
as an autoencoder in this case1. The mean squared error be-
tween MLLM output feature cMLLM andCx(x), together
with LDM, and text token prediction loss Ltis the ﬁnal
training loss: L=↵MSE 
cMLLM ,Cx(x) 
+LDM+Lt
controlled by weighting ↵.
4. Building Diverse Multimodal In-Context
Generation Data
4.1. Dataset Construction
We construct and employ a variety of datasets to facilitate
interleaved and in-context multimodal generation, enrich-
ing the capabilities of CoDi-2.
Multimodal In-Context Learning Datasets. Our ap-
proach leverages the strength of multimodal in-context un-
1We leverage the condition encoder aligned across modalities from
CoDi (Tang et al. [ 32], named prompt encoder in the original paper)derstanding, and to bolster this aspect, we integrate MIMIC-
IT [16] into our tasks. MIMIC-IT offers an extensive and
diverse dataset comprising 2.8 million instruction-response
pairs, speciﬁcally designed to elevate the performance of
Vision-Language Models (VLMs) in real-world scenarios.
This augmentation equips VLMs with abilities in percep-
tion, reasoning, and planning. Despite its output is text-
only, it can help model’s in-context understanding of mul-
timodal inputs and overall instruction following. For ex-
ample in perceptual understanding, given two images with
only subtle differences, the instruction is to spot the differ-
ent. By another example for reasoning, given video frames
of football, the instruction is to predict what will happen
next.
Multimodal Paired Datasets. Paired datasets like image-
text are natural multimodal data for cross-modal genera-
tion. We use LAION-400M [ 29] that consists of 400 mil-
lion image-text pairs ﬁltered using CLIP. For audio paired
dataset, we use AudioSet [ 8]. AudioSet offers a com-
prehensive ontology of 632 audio event classes. It also
boasts a collection of 2,084,320 human-labeled 10-second
sound clips sourced from YouTube videos. For video paired
dataset, we use Webvid [ 1], featuring 10.7 million short
video-caption pairs, totaling 52,000 hours, gathered from
stock footage websites, showcasing diverse content. We
construct two tasks with these datasets, 1) instructing to
generate caption given an image or audio, and 2) instructing
to generate the image or audio from caption.
Instructional Editing Datasets. Instructional Editing is
a task structured as an input image, an editing instruction,
and the resulting edited image. We use Instructpix2pix [ 2]
for image instructional editing. For audio editing dataset,
our approach is built on top of AudioSet [ 8]. We develop
instructional editing versions of it, taking cues from AU-
DIT [ 37]. We have developed three versions of this dataset:
audio addition (overlay), removal, and replacement, result-
ing in a dataset three times the size of AudioSet. By over-
laying two distinct audio segments, a, b , we obtain a new
combined audio a+b. This combined audio can also serve
as an input for audio removal a+b!a, or removing audio
bfrom audio a+b. Audio replacement is constructed by in-
tegrating two different audio segments b, c into the same
base audio a, and then we get a+b!a+c, or replacing b
with cfor audio a+bto get a+c.
Constructed In-context Multimodal Generation
Datasets. To further stimulate the multimodal in-context
ability, we construct several in-context datasets for mul-
timodal generation. InstructPix2Pix can be extended to
interleaved in-context multimodal format, given its multiple
image pairs corresponding to the same editing prompt.
27428
Task Type Example Prompt Output
Zero-Shot Prompting
Instruction
EditingTurn this
 into Van Gogh style
Composition A
 on
 .
Reasoning Given
 and
 , what happens next?
One-Shot/Few-Shot Prompting
Exemplar
LearningWe apply a new concept to
 and got
 .
Apply the same concept to
 .
Concept
LearningGiven the artistic style represented in
 and
 ,
create a new artwork similar to it.
Subject
DrivenGiven a set of pictures portraying your neighbor’s cat
and
 , create a new image of this cat.
Table 1. Zero-shot, one-shot, and few-shot image generation examples by CoDi-2.
27429
Task Type Example Prompt Output
Zero-Shot Prompting
Instruction
EditingAdd echoing to this audio
 (person speaking) .
(person speaks in
echoes)
One-Shot/Few-Shot Prompting
Exemplar
LearningWe overlaid a new sound to
 (street noise) and got
(street noise, raining) .
Apply this same new sound to
 (train noise) .
(train noise,
raining)
Table 2. Audio generation examples of CoDi-2.
Consequently, we deﬁne the in-context learning template
as:Input: “Given the transformation between [image0]
and [image1] , apply the same editing to [image2] .”
Target: [image3] . This approach can also be adapted
for audio editing datasets using the same template structure.
In addition, we utilize the Kosmos-G dataset [ 25], con-
structed using Open Images V7 [15] with 9M images for
image composition. Here, entities from captions are ex-
tracted to produce image segmentation for each identity.
For instance, for descriptions like ‘A cat on a couch’,
we obtain: Input: [image0] on[image1] Target:
[image2] , where [image0] and [image1] repre-
sent the segmented cat and couch images derived from
[image2] , respectively.
Text-Only Datasets Repurposed for Interleaved Mul-
timodal In-Context. We propose to employ text-only
datasets for enhancing generation with multimodal rea-
soning. Since the encoder features for all modalities are
aligned, replacing text tokens with text encoder features can
enhance interleaved multimodal understanding. Concretely,
we randomly select phrases or words in the sentence and
encode them with text feature encoder to swap out the orig-
inal text embeddings. This innovative strategy bolsters the
model’s proﬁciency in understanding complex, interleaved
multimodal scenarios by aligning the encoder features and
the original language model text embeddings. We employ
instructional dataset alpaca [ 33]. For example, we convert “
A cat typically has a compact, ﬂexible body, covered in soft
fur that can come in a variety of colors and patterns. ” to
“[text0] typically has [text1] , covered in [text2]
that can come in a variety of colors and patterns. ” where[text0] [text1] [text2] are respectively text fea-
tures from “ a cat ”, “a compact, ﬂexible body ”, and “ soft
fur”.
4.2. In-Context Instruction Task Types
Tables 1to3offer a comprehensive overview of the task
types utilized in in-context multimodal generation. Each
task type presents a unique approach to prompting models
to generate or transform in-context multimodal content, in-
cluding images, audio, and combinations thereof.
Zero-Shot Prompting. Zero-shot prompting tasks re-
quire the model to reason and generate new content without
any prior examples. For instance, in Table 1, model trans-
forms an image to match Van Gogh’s style or compositing
two separate images to form a coherent scene exempliﬁes
the model’s capacity to understand and apply complex in-
structions directly. Model also can perform reasoning and
predict the next image in a sequence, which is one cube
in consistent style. Table 2shows adding echoes to an
audio. Table 3shows visual editing with sound vibe and
frame+sound prediction of a video sequence.
One-Shot/Few-Shot Prompting. One-shot or few-shot
prompting provides the model with one or a few examples
to learn from before performing a similar task. This method
is evident in tasks where the model adapts a learned concept
from one image to another or creates a new piece of artwork
by understanding the styles depicted in provided exemplars.
Exemplar learning is a subset of few-shot prompting
where the model is explicitly shown an example of the de-
sired output before being asked to apply this learning to a
27430
Task Type Example Prompt Output
Zero-Shot Prompting
Instruction
Editing
(raining) took place in
 .
ReasoningGiven video frames,
 ,
what will happen next? Generate the sound and image for it.
(stirring the batter)
One-Shot/Few-Shot Prompting
Exemplar
LearningFor image
 , music
 (soft jazz music)
captures its vibe. What’s the right music to
 ?
(dance music)
Subject
DrivenGiven a set of pictures portraying your neighbor’s cat
and
 , create a video and sound of this cat.
(cat meowing)
Table 3. Example generation with multimodal inputs and outputs. The instructions, image, and audio are interleaved demanding in-context
understanding of the inputs. The outputs are either unimodal and multimodal requiring model’s synchronized generation abilities.
new instance. This technique is particularly useful when
trying to generalize a concept from a speciﬁc instance to a
new, but related, context. In Table 1, model is shown a set
of images with season change and then asked to apply the
same to a similar image. In Table 2, model is shown a set
of audio where the latter one has raining sound on top of it,
and then asked to apply the same to a new audio. In Table 3,
model is shown the audio caption of an image and asked togenerate audio for an image with different vibe.
Concept learning involves the model learning from
shared concept/attributes of given examples, such as artis-
tic styles or patterns, and then creating new content that
exhibits similar concept/attributes. The model’s ability to
discern and replicate complex patterns indicates a sophis-
ticated understanding of visual styles. In Table 1, model
learns the intricate ﬂoral patterns and then draws a new im-
27431
Model DINO"CLIP-I"CLIP-T"Real Images (Oracle) 0.774 0.885 -Fine-TuningDreamBooth [28] 0.668 0.803 0.305Test Time Tuning FreeRe-Imagen [4] 0.600 0.740 0.270KoSMOS-G [25] 0.694 0.847 0.287Ours0.703 0.852 0.311Model CLIPSIM"Diffusion Model OnlySDEdit-1/2T [21] 0.134InstructPix2Pix [2] 0.151Diffusion Model + LLMOurs 0.147Table 4. Left: Comparisons on DreamBench. Right: Image Editing on MS-COCO.Adding Dropping ReplacementModel Text LSD(#) KL(#)F D (#)LSD(#) KL(#)F D (#)LSD(#) KL(#)F D (#)SDEdit-3/4T [21] caption 1.54 1.68 28.87 1.54 1.14 29.66 1.63 1.58 28.78SDEdit-1/2T [21] caption 1.43 1.38 28.75 1.43 1.05 28.19 1.52 1.27 27.71SDEdit-1/4T [21] caption 1.38 1.30 28.25 1.40 1.30 31.31 1.46 1.15 26.72AUDIT [37] instruction 1.35 0.92 21.80 1.37 0.95 22.40 1.37 0.84 21.65Ours instruction1.21 0.88 19.72 1.26 0.90 18.06 1.25 0.80 17.32Table 5. Evaluation results of the adding, dropping, and replacement tasks on auditory data.age that reﬂect the same intricate styles.Subject-driven learningfocus on generating new con-tent based on a set of provided images. This approach teststhe model’s ability to understand and recreate the subjectwith variations in pose, lighting, or context, while maintain-ing the subject’s distinct features. In Table1, given severalpictures of a speciﬁc cat, the model will create a new imageof the same cat in new poses. In Table3, given the cat im-ages, the model can create a video+sound of the same cat.5. Experiments5.1. Model SetupsOur implementation is based on Llama2 [35], speciﬁcallyLlama-2-7b-chat-hf. We use ImageBind [9] which hasaligned image, video, audio, text, depth, thermal, and IMUmodality encoders. We use ImageBind to encode the im-age and audio features and project it to the input dimensionof the LLM (Llama-2-7b-chat-hf) with a multilayer percep-tron (MLP) that consists of a linear mapping, activation,normalization, and one more linear mapping. When theLLM generates the image or audio features, we project themback to ImageBind feature dimension with another MLP.Our image diffusion model is based on StableDiffusion-2.1 [27] (stabilityai/stable-diffusion-2-1-unclip [26]), Audi-oLDM2 [19], and zeroscopev22.For images or audio that require higher ﬁdelity to theoriginal input, we additionally feed the original image or au-dio to the diffusion model alongside the generated featuresby concatenation of the diffusion noise [2,19,27]. This ap-proach is particularly effective in preserving the most per-ceptual features of the input including instruction editing2https://huggingface.co/cerspense/zeroscopev2576wlike adding new content or changing style.5.2. Image Editing EvaluationSection4shows the evaluation results of subject driven im-age generation on Dreambench [28] and CLIPSIM scoreson MSCOCO. Our method achieves very competitive zero-shot performance, showing our model’s generalization tonew unseen tasks.5.3. Audio Editing EvaluationTable5provides an overview of our evaluation results con-cerning audio manipulation tasks—namely, adding, drop-ping, and replacing elements within audio tracks. These re-sults are pivotal in understanding the effectiveness of theproposed methods. It is evident from this table that ourapproach demonstrates superior performance in compari-son to previous methodologies. Notably, it has achievedthe lowest scores across all metrics—Log Spectral Distance(LSD), Kullback-Leibler (KL) divergence, and Fr´echet Dis-tance (FD)—across all three editing tasks.6. ConclusionWe introduced CoDi-2, a model for multimodal generationwith groundbreaking abilities such as modality-interleavedinstruction following, in-context generation, user-modelinteraction through multi-round conversations. CoDi-2 isable to processes complex modality-interleaved input andinstructions by MLLM, and then autoregressively producethe latent features that is fed to diffusers for multimodalgeneration. The evaluations show that CoDi-2 has excep-tional zero-shot and few-shot ability on tasks including styleadaptation, subject-driven generation, and editing acrossmodalities. CoDi-2 represents a remarkable explorationto build the GPT-like fundamental multimodal system.
27432
References
[1]Max Bain, Arsha Nagrani, G ¨ul Varol, and Andrew Zisser-
man. Frozen in time: A joint video and image encoder for
end-to-end retrieval. In Proceedings of the IEEE/CVF Inter-
national Conference on Computer Vision , pages 1728–1738,
2021. 4
[2]Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-
structpix2pix: Learning to follow image editing instructions.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 18392–18402, 2023.
1,3,4,8
[3]Sihan Chen, Xingjian He, Longteng Guo, Xinxin Zhu, Wein-
ing Wang, Jinhui Tang, and Jing Liu. Valor: Vision-audio-
language omni-perception pretraining model and dataset.
arXiv preprint arXiv:2304.08345 , 2023. 1
[4]Wenhu Chen, Hexiang Hu, Chitwan Saharia, and William W
Cohen. Re-imagen: Retrieval-augmented text-to-image gen-
erator. arXiv preprint arXiv:2209.14491 , 2022. 8
[5]Prafulla Dhariwal and Alexander Nichol. Diffusion models
beat gans on image synthesis. Advances in neural informa-
tion processing systems , 34:8780–8794, 2021. 1
[6]Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng,
Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao,
Hongxia Yang, and Jie Tang. Cogview: Mastering text-
to-image generation via transformers. arXiv preprint
arXiv:2105.13290 , 2021. 1
[7]Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie
Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xi-
angyu Yue, et al. Llama-adapter v2: Parameter-efﬁcient vi-
sual instruction model. arXiv preprint arXiv:2304.15010 ,
2023. 2
[8]Jort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren
Jansen, Wade Lawrence, R Channing Moore, Manoj Plakal,
and Marvin Ritter. Audio set: An ontology and human-
labeled dataset for audio events. In 2017 IEEE interna-
tional conference on acoustics, speech and signal processing
(ICASSP) , pages 776–780. IEEE, 2017. 4
[9]Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat
Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan
Misra. Imagebind: One embedding space to bind them all.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 15180–15190, 2023.
8
[10] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-
Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.
Lora: Low-rank adaptation of large language models. arXiv
preprint arXiv:2106.09685 , 2021. 1
[11] Ziqi Huang, Tianxing Wu, Yuming Jiang, Kelvin CK Chan,
and Ziwei Liu. Reversion: Diffusion-based relation inversion
from images. arXiv preprint arXiv:2303.13495 , 2023. 3
[12] Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee, and
Gunhee Kim. Audiocaps: Generating captions for audios
in the wild. In Proceedings of the 2019 Conference of the
North American Chapter of the Association for Computa-tional Linguistics: Human Language Technologies, Volume
1 (Long and Short Papers) , pages 119–132, 2019. 1
[13] Jing Yu Koh, Daniel Fried, and Ruslan Salakhutdinov. Gen-
erating images with multimodal language models. arXiv
preprint arXiv:2305.17216 , 2023. 2
[14] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli
Shechtman, and Jun-Yan Zhu. Multi-concept customization
of text-to-image diffusion. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 1931–1941, 2023. 3
[15] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Ui-
jlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan
Popov, Matteo Malloci, Alexander Kolesnikov, et al. The
open images dataset v4: Uniﬁed image classiﬁcation, object
detection, and visual relationship detection at scale. Interna-
tional Journal of Computer Vision , 128(7):1956–1981, 2020.
6
[16] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,
Jingkang Yang, and Ziwei Liu. Otter: A multi-modal
model with in-context instruction tuning. arXiv preprint
arXiv:2305.03726 , 2023. 2,3,4
[17] Dongxu Li, Junnan Li, and Steven CH Hoi. Blip-
diffusion: Pre-trained subject representation for control-
lable text-to-image generation and editing. arXiv preprint
arXiv:2305.14720 , 2023. 1
[18] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.
Visual instruction tuning. arXiv preprint arXiv:2304.08485 ,
2023. 2
[19] Haohe Liu, Qiao Tian, Yi Yuan, Xubo Liu, Xinhao Mei, Qi-
uqiang Kong, Yuping Wang, Wenwu Wang, Yuxuan Wang,
and Mark D Plumbley. Audioldm 2: Learning holistic audio
generation with self-supervised pretraining. arXiv preprint
arXiv:2308.05734 , 2023. 8,1
[20] Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei
Chang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao.
Chameleon: Plug-and-play compositional reasoning with
large language models. arXiv preprint arXiv:2304.09842 ,
2023. 2
[21] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia-
jun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided
image synthesis and editing with stochastic differential equa-
tions. arXiv preprint arXiv:2108.01073 , 2021. 3,8
[22] Ron Mokady, Amir Hertz, and Amit H Bermano. Clip-
cap: Clip preﬁx for image captioning. arXiv preprint
arXiv:2111.09734 , 2021. 1
[23] OpenAI. Gpt-4 technical report, 2023. 2
[24] Xichen Pan, Li Dong, Shaohan Huang, Zhiliang Peng,
Wenhu Chen, and Furu Wei. Kosmos-g: Generating images
in context with multimodal large language models. arXiv
preprint arXiv:2310.02992 , 2023. 1
[25] Xichen Pan, Li Dong, Shaohan Huang, Zhiliang Peng,
Wenhu Chen, and Furu Wei. Kosmos-g: Generating images
in context with multimodal large language models, 2023. 3,
6,8
[26] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
and Mark Chen. Hierarchical text-conditional image gener-
ation with clip latents. arXiv preprint arXiv:2204.06125 ,1
(2):3, 2022. 8
27433
[27] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¨orn Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 10684–10695, 2022. 4,8,1
[28] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,
Michael Rubinstein, and Kﬁr Aberman. Dreambooth: Fine
tuning text-to-image diffusion models for subject-driven
generation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 22500–
22510, 2023. 1,3,8
[29] Christoph Schuhmann, Richard Vencu, Romain Beaumont,
Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo
Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m:
Open dataset of clip-ﬁltered 400 million image-text pairs,
2021. 4
[30] Paul Hongsuck Seo, Arsha Nagrani, Anurag Arnab, and
Cordelia Schmid. End-to-end generative pretraining for mul-
timodal video captioning. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 17959–17968, 2022. 1
[31] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong
Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun
Huang, and Xinlong Wang. Generative pretraining in multi-
modality. arXiv preprint arXiv:2307.05222 , 2023. 2
[32] Zineng Tang, Ziyi Yang, Chenguang Zhu, Michael Zeng, and
Mohit Bansal. Any-to-any generation via composable diffu-
sion. In Thirty-seventh Conference on Neural Information
Processing Systems , 2023. 1,2,3,4
[33] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B
Hashimoto. Stanford alpaca: An instruction-following llama
model, 2023. 6
[34] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timoth ´ee Lacroix, Baptiste
Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al.
Llama: Open and efﬁcient foundation language models.
arXiv preprint arXiv:2302.13971 , 2023. 2
[35] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,
Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.
Llama 2: Open foundation and ﬁne-tuned chat models. arXiv
preprint arXiv:2307.09288 , 2023. 2,8
[36] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li,
Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang.
Git: A generative image-to-text transformer for vision and
language. arXiv preprint arXiv:2205.14100 , 2022. 1
[37] Yuancheng Wang, Zeqian Ju, Xu Tan, Lei He, Zhizheng Wu,
Jiang Bian, and Sheng Zhao. Audit: Audio editing by follow-
ing instructions with latent diffusion models. arXiv preprint
arXiv:2304.00830 , 2023. 4,8
[38] Zhendong Wang, Yifan Jiang, Yadong Lu, Yelong Shen,
Pengcheng He, Weizhu Chen, Zhangyang Wang, and
Mingyuan Zhou. In-context learning unlocked for diffusion
models. arXiv preprint arXiv:2305.01115 , 2023. 1,3
[39] Peter West, Ximing Lu, Nouha Dziri, Faeze Brahman, Lin-
jie Li, Jena D Hwang, Liwei Jiang, Jillian Fisher, AbhilashaRavichander, Khyathi Chandu, et al. The generative ai para-
dox:” what it can create, it may not understand”. arXiv
preprint arXiv:2311.00059 , 2023. 1
[40] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng
Chua. Next-gpt: Any-to-any multimodal llm. arXiv preprint
arXiv:2309.05519 , 2023. 1,2
[41] D Yang, J Yu, H Wang, W Wang, C Weng, Y Zou, and
D Diffsound Yu. Discrete diffusion model for text-to-sound
generation. arxiv 2022. arXiv preprint arXiv:2207.09983 .1
[42] Ziyi Yang, Yuwei Fang, Chenguang Zhu, Reid Pryzant,
Dongdong Chen, Yu Shi, Yichong Xu, Yao Qian, Mei Gao,
Yi-Ling Chen, et al. i-code: An integrative and composable
multimodal learning framework. In Proceedings of the AAAI
Conference on Artiﬁcial Intelligence , pages 10880–10890,
2023. 2
[43] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan,
Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi,
Yaya Shi, et al. mplug-owl: Modularization empowers
large language models with multimodality. arXiv preprint
arXiv:2304.14178 , 2023. 2
[44] Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen
Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, and
Yinfei Yang. Ferret: Refer and ground anything anywhere
at any granularity, 2023. 2
[45] Rowan Zellers, Jiasen Lu, Ximing Lu, Youngjae Yu, Yan-
peng Zhao, Mohammadreza Salehi, Aditya Kusupati, Jack
Hessel, Ali Farhadi, and Yejin Choi. Merlot reserve: Neu-
ral script knowledge through vision and language and sound.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 16375–16387, 2022.
2
[46] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding
conditional control to text-to-image diffusion models. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 3836–3847, 2023. 1
[47] Ziqi Zhang, Yaya Shi, Chunfeng Yuan, Bing Li, Peijin Wang,
Weiming Hu, and Zheng-Jun Zha. Object relational graph
with teacher-recommended learning for video captioning. In
Proceedings of the IEEE/CVF conference on computer vi-
sion and pattern recognition , pages 13278–13288, 2020. 1
[48] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei
Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie
Zhang, Zican Dong, et al. A survey of large language mod-
els.arXiv preprint arXiv:2303.18223 , 2023. 3
[49] Kaizhi Zheng, Xuehai He, and Xin Eric Wang. Minigpt-
5: Interleaved vision-and-language generation via generative
vokens. arXiv preprint arXiv:2310.02239 , 2023. 3
27434
