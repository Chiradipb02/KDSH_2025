BA-SAM: Scalable Bias-Mode Attention Mask for Segment Anything Model
Yiran Song1*, Qianyu Zhou1*, Xiangtai Li2, Deng-Ping Fan3,4, Xuequan Lu5†, Lizhuang Ma1†
1Shanghai Jiao Tong University;2Nanyang Technological University;
3Nankai International Advanced Research Institute (SHENZHEN FUTIAN);
4VCIP, CS, Nankai University;5La Trobe University
1{songyiran,zhouqianyu,lzma }@sjtu.edu.cn ,
2xiangtai94@gmail.com ,3,4dengpfan@gmail.com ,5b.lu@latrobe.edu.au
Code: https://github.com/zongzi13545329/BA-SAM
Model
small
Large
…
Model
Resize
…
…
MSA
Patch sequence length changing
…
…
Our (BA-SAM)
Large
small
small
Large
Resize
·
COD10K
SAM
Our (BA-SAM)
SAM-pretrain
Downstream Datasets
ISIC
DIS
DUTS
FlexiViT
CASS 
VST
IS-Net
SINet
MobileSAM
0.42
0.09
0.05
0.30
0.04
0.03
0.37
0.07
0.05
0.22
0.09
0.05
ISIC
DUTS
DIS
COD10K
1024
1024
1024
1024
2048
2048
2048
2048
4096
4096
4096
4096
0.059
0.053
0.144
0.118
0.205
0.190
0.034
0.031
0.082
0.078
0.235
0.226
0.057
0.055
0.065
0.061
0.101
0.098
0.03
0.029
0.063
0.057
0.138
0.118
Figure 1. Top: contrast between prior methods [4, 82] and BA-SAM. For large-scale datasets, previous approaches often resize images or
change patch sizes to handle the issue of varying resolutions. In contrast, we propose a Scalable Bias-Mode Attention Mask (BA-SAM),
which enhances SAM’s adaptability to varying image resolutions while eliminating structure modifications. Bottom (left) : We introduce
a generalized model that outperforms state-of-the-art methods across all four datasets. Bottom (right) : With resolution variations, prior
models’ performance degrades drastically. In comparison, our BA-SAM consistently alleviates this issue. The evaluation metric is MAE.
Abstract
In this paper, we address the challenge of image resolu-
tion variation for the Segment Anything Model (SAM). SAM,
known for its zero-shot generalizability, exhibits a perfor-
mance degradation when faced with datasets with varying
image sizes. Previous approaches tend to resize the image
to a fixed size or adopt structure modifications, hindering
the preservation of SAM’s rich prior knowledge. Besides,
such task-specific tuning necessitates a complete retrain-
ing of the model, which is cost-expensive and unacceptable
*Equal contribution.
†Corresponding authors.for deployment in the downstream tasks. In this paper, we
reformulate this challenge as a length extrapolation prob-
lem, where token sequence length varies while maintaining
a consistent patch size for images with different sizes. To
this end, we propose a Scalable Bias-Mode Attention Mask
(BA-SAM) to enhance SAM’s adaptability to varying image
resolutions while eliminating the need for structure modi-
fications. Firstly, we introduce a new scaling factor to en-
sure consistent magnitude in the attention layer’s dot prod-
uct values when the token sequence length changes. Sec-
ondly, we present a bias-mode attention mask that allows
each token to prioritize neighboring information, mitigating
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
3162
the impact of untrained distant information. Our BA-SAM
demonstrates efficacy in two scenarios: zero-shot and fine-
tuning. Extensive evaluation of diverse datasets, including
DIS5K, DUTS, ISIC, COD10K, and COCO, reveals its abil-
ity to significantly mitigate performance degradation in the
zero-shot setting and achieve state-of-the-art performance
with minimal fine-tuning. Furthermore, we propose a gen-
eralized model and benchmark, showcasing BA-SAM’s gen-
eralizability across all four datasets simultaneously.
1. Introduction
Recently, the computer vision community [9, 22, 26, 28–
30, 47, 51, 52, 60, 68, 76, 86–94] has experienced a surge in
the development of various foundation models [21, 34, 56].
Notably, Meta has introduced the Segment Anything Model
(SAM [37]). SAM can segment any object in an image or
video by incorporating a single visual prompt, such as a
box or a point, without requiring additional training. SAM
is trained on an extensive SA-1B dataset [37], consisting of
over 11 million images and one billion masks. Its emer-
gence has undeniably showcased robust generalization ca-
pabilities across diverse images and objects, paving the way
for new possibilities and avenues in intelligent image analy-
sis and understanding [8, 33, 79, 82]. Based on SAM, some
variants have been proposed, such as MobileSAM [79] and
SAM-Adapter [8]. These efforts typically focus on improv-
ing SAM’s performance on specific datasets.
During the pre-training of SAM [37], the input image
size is fixed at 1024 . As a foundational model, SAM is ex-
pected to exhibit generalization capabilities across various
downstream tasks, each associated with datasets featuring
different image sizes. This is particularly crucial for high-
resolution (HQ) datasets with larger dimensions and more
details. SAM performs well when the resolutions align with
its training resolution of 1024. However, significant perfor-
mance degradation is observed when inference resolutions
are larger than 1024. Hence, we aim to study a practical and
realistic problem to enhance SAM’s adaptability to varying
image resolutions of different datasets.
Since SAM adopts the standard Vision Transformer [14]
architecture, two previous common approaches address the
inconsistency between training and inference sizes for the
ViT architecture. As depicted in Fig. 1, the first approach,
e.g., MSA [82] and SAM-Adapter [8], involves directly
resizing all datasets to match the predefined size. Con-
versely, the second approach, exemplified by FlexiViT [4],
entails adjusting the patch size to accommodate larger im-
age resolutions. Nevertheless, tuning the image or patch
size necessitates a complete retraining of the model, which
is cost-expensive and unacceptable for deployment in the
downstream tasks. Besides, it prevents leveraging the rich
prior knowledge reserved in the pre-trained model of SAM.
Thus, we aim to explore a solution that enhances SAM’sadaptability to datasets of varying resolutions while avoid-
ing structural modifications.
In this paper, we introduce a novel perspective that re-
frames the challenge of image resolution variation as a
length extrapolation problem. Specifically, as shown in
Fig. 1, we employ different token sequence lengths for im-
ages of varying sizes while keeping a consistent patch size.
It has been observed that the inconsistency in token length
between training and prediction is a key factor in perfor-
mance degradation. This inconsistency manifests in two as-
pects: Firstly, changes in token length lead to variations in
the magnitude of attention module values. When the dot
product result becomes significantly large in magnitude, it
can drive the subsequent Softmax layer into regions with
minimal gradients. Consequently, the attention distribution
after Softmax becomes highly concentrated, giving rise to
the issue of vanishing gradients. Secondly, longer predic-
tions rely on untrained information, such as additional po-
sition encodings. The introduction of untrained parameters
brings a considerable amount of noise to the model, which,
in turn, affects its performance.
To address these issues, we propose a Scalable Bias-
Mode Attention Mask (BA-SAM) to enhance the length
extrapolation capability of SAM. Our approach introduces
two novel designs. Firstly, we present an improved scaling
factor to ensure consistency in the attention layer’s dot prod-
uct value. This factor effectively regulates the magnitude of
values within the attention layer, mitigating disruptive ef-
fects resulting from substantial changes in dot product op-
erations and context length. Secondly, we introduce a novel
bias-mode attention mask to maintain consistency in atten-
tion focus areas. This attention mask penalizes attention
scores between distant query-key pairs, with the penalty in-
creasing as the distance between the key and query grows.
Consequently, when the context length varies, the influence
of untrained distant information on each token diminishes.
We achieve this mask by adding a bias after the query-key
dot product. This design is highly lightweight and could be
seamlessly integrated into SAM-based models with mini-
mal computational overhead.
Our approach demonstrates efficacy in two scenar-
ios: zero-shot and fine-tuning. Extensive evaluations on
datasets from five diverse tasks are conducted, including
DIS5K [55], DUTS [66], ISIC [12], COD10K [16], and
COCO [46]. These datasets vary in resolution, mostly
exceeding SAM’s default resolution (1024 ×1024). In
the zero-shot setting, our BA-SAM alleviates the model’s
performance degradation caused by expanding the infer-
ence resolution without requiring additional training. With
a few fine-tuning epochs on downstream tasks, our BA-
SAM consistently achieves state-of-the-art accuracy across
all datasets, as shown at the bottom of Fig. 1. Additionally,
to further demonstrate BA-SAM’s generalizability, we pro-
3163
pose a generalized model and a new benchmark, which uti-
lize one model to attain state-of-the-art performance across
all four datasets simultaneously.
2. Related Work
Visual Foundation Models. Large models that are trained
on broad datasets and can be adapted to numerous down-
stream tasks are called “Foundation Models” [5, 41, 42,
44, 69, 74, 78, 85]. Vision-Language Models (VLM)
(CLIP [56] and DALL-E [57]) combine computer vision
and natural language processing to understand and generate
descriptions or analyze visual content using textual and vi-
sual information. Masked Image Modeling [50, 75] (MIM)
approaches mask parts of an image during the training to en-
courage a model to learn contextual information and com-
plete missing regions. SAM [37] is a large vision founda-
tion model designed for segmenting objects or areas in im-
ages, offering precise segmentation capabilities. We use a
variant of SAM called MobileSAM [79] as the baseline.
Resolution Variation Processing. To enable models to be
more adaptable to variations in resolutions, previous works
have relied on adjustments to positional embeddings [39]
and patch sizes [4, 6, 24, 29, 38, 45, 77]. For example,
Patch n’ Pack [13] employed sequence packing during the
training to handle inputs with arbitrary resolutions and as-
pect ratios. They all necessitate training from scratch, in-
curring substantial computational and time costs. In con-
trast to these methods, we extend the concept of length ex-
trapolation from NLP into the context of addressing scale
variations in CV . Length extrapolation refers to a model’s
ability to generalize well to longer inputs than those it was
trained on. In NLP, it has been successfully used, such as
in ALIBI [54] and KERPLE [11], to enable models to adapt
to longer sequences without significant performance degra-
dation. Our approach seamlessly extends to two scenarios:
zero-shot and fine-tuning. Our proposed method allows us
to leverage prior knowledge embedded in the SAM and sig-
nificantly reduce training efforts.
Parameter Efficient Tuning. There have been some pio-
neering works for the Parameter Efficient Tuning (PEFT) of
visual models, such as AdaptFormer [7] and visual prompt
tuning (VPT) [35]. He et al. [27] analyzed the unified view
among PETL techniques such as prefix tuning [43], Prompt-
tuning [35], and adapter [7]. Our method belongs to the
category of Parameter Efficient Tuning.
Visual Attention Modeling. Various studies have incor-
porated attention mechanisms into neural network archi-
tectures designed for visual tasks [3, 32, 40, 65, 70, 96].
These mechanisms are employed in a channel-wise manner
to capture cross-feature information [10, 68, 83]. They are
also used for selecting paths in different branches of a net-
work [63], or a combination of both strategies [80]. The
advent of transformers has led to hybrid architectures thatintroduce other modules. Bello’s work [2] introduces ap-
proximate content attention with a positional attention com-
ponent. Child et al. [71] observe that many early layers in
the network learn locally connected patterns akin to con-
volutions, indicating that hybrid architectures inspired by
both transformers and convolutional networks are a com-
pelling design choice. Several recent studies explore this
line for various tasks [25, 61, 67, 72]. In contrast to prior
work, we do not introduce a new attention structure. In-
stead, we offer theoretical proof for optimizing existing at-
tention mechanisms. This resulting optimization approach
is applicable across various attention designs and demon-
strates strong performance across multiple datasets.
3. Preliminaries
SAM. Segment Anything Model (SAM) [37] consists of
three core modules: image encoder, prompt encoder, and
mask decoder. It has been trained on SA-1B dataset [37],
which comprises more than 1 billion automatically gener-
ated masks. Consequently, SAM exhibits valuable and ro-
bust zero-shot generalization to new data without necessi-
tating further training, and details can be referred to [37].
Our Scalable Bias-Mode Attention Mask (BA-SAM) opti-
mizes the image encoder while keeping the structures of the
mask decoder and prompt encoder unchanged.
Attention in Transformer. In this work, we define the
input sequence of image patches, x= (x1, . . . ,xn)with
length N, where xi∈Rdx.qi,kj,vjare calcu-
lated by xiWQ,xjWK,xjWV. Here, the projections
WQ,WK,WV∈Rdx×dkare parameter matrices.
(i) Scaling Factor. The two most commonly used atten-
tion functions are additive attention [1] and dot-product at-
tention [64]. The vanilla Transformer chooses dot-product
attention for its space efficiency in practice. However, for
larger values of dk, the dot products grow large in magni-
tude, pushing the Softmax function into regions with mini-
mal gradients. They use scaling factor λd=1√dkto scale
the dot products, where dkdenotes the dimension. To better
analyze the role of the scaling factor, we express the output
element Oiand the weight coefficient ai,jas follows:
Oi=NX
j=1ai,jvj, a i,j=eλdqi·kj
PN
j=1eλdqi·kj, (1)
where λdrepresents the scaling factor.
(ii) Absolute & Relative Position Encoding. The original
Transformer [64] incorporates absolute non-parametric po-
sitional encoding p= (p1, . . . , p n)withxasxi=xi+pi.
Other works replace them with parametric encoding [23], or
adopted Fourier-based kernelized versions [53]. Absolute
position encoding enforces a fixed size for inputs. Recent
work [58] considers the pairwise relationships between el-
ements, which encodes the relative position between input
3164
Prediction Context Length
Prediction Context Length
Bias Mode Attention Mask
+
=
(a)  Our New Scaling Factor 
(b)   Our Bias-Mode Attention Mask 
Train
Softmax
Test
SAM
BA-SAM
…
…
Softmax
Train
Test
…
…
Softmax
Training Token
LengthFigure 2. Illustration of the proposed BA-SAM method. (a) In the original SAM, when the length of the input token sequences varies
during testing, the magnitude of the Softmax outputs changes drastically. We propose a new scaling factor to address this issue. (b) We
introduce a bias-mode attention mask, which increases attention scores’ penalties as the distance between the query and key grows.
xiandxjinto vectors pv
i,j, pq
i,j, pk
i,j∈Rdk. Then, we can
reformulate Eq. (1) as follows:
Oi=NX
j=1ai,j 
vj+pv
i,j
, (2)
ai,j=eλ(qi+pq
i,j)·(kj+pk
i,j)
PN
j=1eλ(qi+pq
i,j)·(kj+pk
i,j), (3)
where pv
i,j, pq
i,j, pk
i,jis learned during training.
4. Methodology
Based on the preliminaries, we further analyze that the orig-
inal SAM sets the input to a fixed resolution of 1024, where
it uses absolute position encoding and the dot product. As
a result, there are significant limitations in the processing
of length extrapolation problems. To address this, as shown
in Fig. 2, we present a Scalable Bias-mode Attention Mask
(BA-SAM). In Sec. 4.1, we provide a theoretical explana-
tion for the scaling factor used in the original Transformer
and introduce a new scaling factor to regulate the magnitude
inconsistency caused by length extrapolation. In Sec. 4.2,
we design a bias-mode attention mask to place more focus
on neighboring tokens, mitigating the impact of untrained
distant information. Finally, we explain how we will embed
our BA-SAM into the SAM-based structure in Sec. 4.3.
4.1. New Scaling Factor
When the dot product becomes significantly large in mag-
nitude in the original attention module of SAM [64], it can
drive the Softmax layer into regions with minimal gradients.
This is because the attention distribution after Softmax be-
comes highly concentrated, giving rise to the issue of van-
ishing gradients. Upon examination of Eq. (1), it is obviousthat the computation of the q·kis intrinsically tied to both
the token sequences length Nand the dimension dk. When
the token length Nand the dimension dksignificantly in-
crease, the overall efficacy of the attention is affected, thus
leading to a noticeable performance degradation.
To address this issue, we attempt to design a new scaling
factor that allows the model to cope with variations in N
anddk. When Nordkgrows significantly, we expect to
regulate the magnitude of the values within the attention
layer, maintaining a similar magnitude. [64] introduced a
scaling factor λ=1√dkto counteract the effect of the large
growth in magnitude due to the dot products. Below, we
will provide a theoretical derivation of this scaling factor
and then elaborate on our proposed new one.
The dimension dk.Following the work [64], we assume
the components of qandkare independent random vari-
ables with mean 0 and variance 1. The mean of q·kis:
E[q·k] = E"dkX
i=1qiki#
=dkX
i=1E [qi] E [ki] = 0 (4)
Similarly, we formulate the variance of q·kas follows:
var[q·k] = var"dkX
i=1qiki#
=dkX
i=1var [qi] var [ ki] =dk
(5)
Given this, we can approximately consider the q·kval-
ues to be within the range of −3√dkto3√dk, according
to properties of Gaussian distribution. For larger models,
dkis generally a larger positive value, resulting in a sig-
nificant increase in the magnitude of numerical values of
q·k, compared to the additive attention option, which has
the range of [−3,3]. Consequently, the attention distribu-
tion after Softmax becomes highly concentrated. This leads
3165
to severe gradient vanishing, which hampers the effective-
ness of the training and induces less desired performance.
As the q·kvalues lie in the range of [−3√dk,3√dk], the
scaling factor can be simply defined as λd=1√dk, in order
to maintain a similar magnitude.
Our new scaling factor. We have provided the interpreta-
tion on how the original scaling factor was designed. Now,
we explain the design of our new scaling factor.
According to Eq. (4) and Eq. (5), the scale of dot-
product attention q·khas the similar magnitude with the
additive attention by λd, which can be seen as ai,jis inde-
pendent of dk. We simplify λdqi·kjwith using xi,jand
further discuss the effect of length Nonai,j.
In Eq. (1) , ai,jcan be seen as the conditional distri-
bution with ibeing the condition and jbeing the random
variable. Inspired by [62], we introduce information en-
tropy to constrain ai,j. Specifically, entropy is a measure
of uncertainty, and we expect the uncertainty of ai,jto be
insensitive to the length N,i.e.,the value of each ai,jwill
change when the token increases, but the entropy value of
the overall ai,jcan remain relatively stable. The entropy of
ai,jis :Hi=−PN
j=1ai,jlogai,jand we substitute Eq. (1):
Hi= logNX
j=1eλnxi,j−PN
j=1eλnxi,j(λnxi,j)
PN
j=1eλnxi,j(6)
Then, we substitute the approximate estimates into Eq. (6):
NX
j=1exi,j=N×1
NNX
j=1exi,j≈NEj[exi,j]
Ej[exi,j(xi,j)]≈0,Ej[exi,j] =O(1)(7)
Here, we use λnto offset the effect of NonHi. Then, we
have the following result:
Hi≈logN−kλn= 0⇒λn=logN
k, (8)
where kis a parameter value. We denote the token sequence
length during the training as Ntrain and the token sequence
length during the testing as Ntest, where Ntest>> N train .
When N=Ntrain ,λn= 1 (consistent with the train-
ing length). As such, k= log Ntrain and finally we have
λn=logNtrainNtest. Considering both λdandλn, we can
ultimately derive our new scaling factor as:
λ=λdλn=logNtrainNtest√dk(9)
Our new scaling factor in Eq. (9) ensures attention compu-
tation remains consistent, regardless of variations in dkand
N. It will enhance the extrapolative capacity of the model.4.2. Bias-Mode Attention Mask
Another challenge is that token sequence length changes
will lead to positional encoding variations. It is important to
ensure the insensitivity of the model when such positional
encoding variations occur during the testing.
One possible way is absolute encoding without trainable
parameters, such as Sinusoidal [64]. It requires the position
encoding to have strong local-to-global inference capabili-
ties. Nevertheless, this assumes that the given function has
high-order smoothness (higher-order derivatives exist and
are bounded). Commonly used positional encodings are of-
ten combined with trigonometric functions. These methods
fail to satisfy the requirement of bounded high-order deriva-
tives, making it less accurate to estimate the extrapolated
results. Another potential approach is using local attention
[49], which constrains the model’s field of view and remains
insensitive to variations in token sequence length. However,
local attention is typically implemented using a local win-
dow, necessitating modifications to the SAM structure and
further re-training from scratch.
To this end, we propose enabling the attention layer to
focus more on the current token’s neighboring tokens. In
this manner, even with an increase in the length of a token
sequence, each token is scarcely affected by the untrained
tokens from distant positions. In particular, we design a
simple yet effective bias-mode mask, which is achieved by
introducing a bias after the query-key dot product.
As shown in Fig. 3, this mask exhibits a bias specified on
the distance between the query-key pairs (i.e., q·k). We ex-
pect that this proposed mask imposes penalties on attention
scores between distant query-key pairs, and the penalty in-
creases as the distance between a key qand a query kgrows.
To achieve this, we simply define the bias as bi,j=β|i−j|.
ai,j=eλ(qi·kj+bi,j)
PN
j=1eλ(qi·kj+bi,j), (10)
where βis a hyperparameter.
We now discuss the setting of βbased on different cases.
We set βto a static, non-learned fixed value when con-
ducting zero-shot generalization without fine-tuning. The
experimental section will present the specific value setting
(Sec. 5). When fine-tuning is required, we make βtrain-
able. Since our Bias-Mode Attention Mask is lightweight,
it incurs negligible training overhead.
4.3. BA-SAM Model
As shown in Fig. 3, our BA-SAM is simple to implement,
and can be seamlessly integrated into SAM [37] and its
variants (such as MobileSAM). Specifically, our design in-
volves a new scaling factor (NSF) for the attention layer
and a bias-mode attention mask (BM-AM) in the encoder
part. Our method does not involve any alterations to the
3166
Prompt 
Encoder
 NSF
BM-AM
Image Encoder
Mask
Decoder
Prompt
Result
New Scaling Factor
 Bias-Mode Attention MaskFigure 3. Embedding of our BA-SAM into a SAM backbone.
NSF indicates our new scaling factor, and BM-AM denotes our
designed bias-mode attention mask.
model structure and is suitable for fine-tuning and zero-shot
modes. During fine-tuning, BA-SAM only introduces neg-
ligible computation costs, as shown in Tab. 5.
5. Experiments
5.1. Datasets and Implementation
Datasets. For a comprehensive evaluation of our proposed
BA-SAM, we conduct extensive experiments on a wide
range of segmentation tasks, i.e.,salient object segmenta-
tion [15, 17, 19], complex object segmentation [18], skin
lesion segmentation [96], camouflaged object detection [20,
31], which correspond to four datasets: DUTS [66], DIS-
TE4 [55], ISIC [12] and COD10K [16]. Besides, we also
verify BA-SAM on the challenging COCO [46] instance
segmentation benchmark.
Implementation Details. In the zero-shot setting, we use
the original SAM [37] backbone. For fine-tuning scenarios,
we employ MobileSAM [79] as a baseline. MobileSAM
is a lightweight version of SAM, where its encoder is dis-
tilled from the original SAM. For various object segmen-
tation tasks, a random point is extracted from the ground
truth as the prompt input during the fine-tuning. We use
the ViT-B [14] backbone for the instance segmentation on
COCO. In particular, we use the state-of-the-art detector
Deformable-DETR [95] trained on the COCO [46] dataset
with Swin-L [49] backbone as box prompt generator. More
details are provided in the supplementary material.
Evaluation Metrics. In the experiments, we use the widely
used Mean Absolute Error (MAE) and Average Precision
(AP) for evaluation. A lower MAE score and a higher AP
score indicate better model performance.
5.2. Main Results
Results of Various Object Segmentation Tasks. Tab. 1
shows the effectiveness of our approach across four diverse
segmentation datasets. ∆diffdenotes the value of the per-
formance degradation due to resolution changes during the
inference. The upper and lower parts of the table indicate
the results without and with fine-tuning. We have three ob-servations: Firstly, our proposed BA-SAM consistently out-
performs both SAM [37] and MobileSAM [79] baselines
on all four datasets. This is mainly because these baselines
do not consider the issue of varying image resolutions. In
contrast, our presented scaling factor and bias-mode atten-
tion mask explicitly handle this issue and further alleviate
the performance degradation. Secondly, when using higher-
resolution images than the training images, SAM [37] and
MobileSAM [79] baselines show less desirable results than
the original image size. In contrast, our BA-SAM incurs
significantly less performance drop in different datasets.
Thirdly, we observe negligible computational overhead,
whether fine-tuning is applied or not, which supports the
claim in the method section. See Sec. 5.3 for details.
Results of Instance Segmentation. In Tab. 3, we evalu-
ate the performance of our method on the COCO bench-
mark. For a fair comparison, all experiments are conducted
in a zero-shot manner, with the same initialized parame-
ters for the comparative methods and without the use of any
additional training data. Our proposed BA-SAM consis-
tently outperforms SAM [37] and MobileSAM [79] base-
lines, demonstrating better zero-shot generalization capa-
bility on instance segmentation.
Comparisons with State-of-the-Art Methods: To fur-
ther demonstrate the superiority and generalizability of our
method, we compare our method with the state-of-the-art
approaches in Tab. 2. From the table, we have two fol-
lowing observations: Firstly, all the state-of-the-art ap-
proaches [16, 36, 48, 55, 59, 81, 84, 96] show less-desirable
performance in each dataset. In comparison, our BA-SAM
(specialized models) consistently outperforms these meth-
ods when fine-tuned on each downstream dataset. Secondly,
almost all of these state-of-the-art techniques are specifi-
cally designed for one task and cannot be generalized well
to other tasks. Due to the strong zero-shot generalization
capability of SAM [37], our proposed BA-SAM can also be
employed as a generalized model, which fine-tunes with all
these downstream datasets in a unified and shared model.
Importantly, unlike [12, 48, 59, 81], we eliminate the need
for employing additional techniques to enhance the perfor-
mance further. As shown in Tab. 2, our generalized model
also consistently promotes the performance of SAM on all
datasets, demonstrating its remarkable generalizability.
5.3. Ablation Study and Analysis
In this section, we first conduct ablation study to study the
contribution of each component. Then, we investigate the
impact of the new scaling factor (NSF) and the bias-mode
attention mask (BM-AM) with a more detailed analysis.
Ablations Studies of Each Component. Tab. 4 summa-
rizes the effect of each component on the settings with and
without fine-tuning, respectively. The baseline means us-
ing the MobileSAM [79] as the base network that uses the
3167
MethodTrain
SizeTest
SizeISIC [12] ∆diff DUTS [66] ∆diff DIS-TE4 [55] ∆diff COD10K [16] ∆diff
Without fine-tuning
SAM [37]- 1024 0.421 - 0.298 - 0.362 - 0.217 -
- 2048 0.601 18.0% 0.360 6.2% 0.411 4.9% 0.391 17.4%
Ours (w [37])- 1024 0.417 - 0.294 - 0.356 - 0.208 -
- 2048 0.589 17.2% 0.348 5.4% 0.406 5.0% 0.387 17.9%
MobileSAM [79]- 1024 0.463 - 0.502 - 0.544 - 0.465 -
- 2048 0.641 17.8% 0.437 6.5% 0.427 11.7% 0.346 11.9%
4096 0.693 23.0% 0.328 17.4% 0.355 18.9% 0.300 16.5%
Ours (w [79])- 1024 0.452 - 0.486 - 0.515 - 0.440 -
- 2048 0.611 15.9% 0.413 7.3% 0.406 10.9% 0.321 11.9%
4096 0.657 20.5% 0.283 20.3% 0.361 15.4% 0.246 19.4%
With fine-tuning
MobileSAM [79]10241024 0.059 - 0.034 - 0.057 - 0.030 -
2048 0.144 8.5 % 0.082 4.8% 0.065 0.8% 0.063 3.3%
4096 0.205 14.6 % 0.235 20.1% 0.101 4.4% 0.138 10.8%
20482048 0.083 - 0.045 - 0.056 - 0.036 -
4096 0.227 14.4% 0.091 4.6% 0.066 1.0% 0.059 2.3%
Ours (w [79])10241024 0.053 - 0.031 - 0.055 - 0.029 -
2048 0.118 6.5 % 0.078 4.4% 0.061 0.6% 0.057 2.5%
4096 0.190 13.7% 0.226 19.2% 0.098 4.3% 0.118 8.6%
20482048 0.080 - 0.043 - 0.053 - 0.033 -
4096 0.214 13.4% 0.088 4.4% 0.061 0.8% 0.056 2.3%
Table 1. Performance comparisons in varying image resolutions. We employed the widely used MAE (Mean Absolute Error) score. Lower
MAE scores indicate better model performance. ∆diffdenotes the performance degradation due to resolution changes. Compared to the
SAM [37] and MobileSAM [79] baselines, our proposed BA-SAM achieves smaller degradation when encountering token sequence length
changes. The best MAE performance is highlighted in bold, and the smallest performance degradation is underlined.
ISIC
1024
Ours
GT
2048
Ours
GT
DUTS
DIS
COD10K
Figure 4. Visualization results of our BA-SAM on four object segmentation tasks, i.e.,skin lesion segmentation, salient object segmentation,
complex object segmentation, camouflaged object detection, which correspond to four datasets: ISIC [12], DUTS [66], DIS-TE4 [55], and
COD10K [16]. Our BA-SAM can accurately handle the issue of varying image resolutions and segments in different tasks.
vanilla scaling factor (VSF) in the attention layer [64]. New
Scaling Factor and Bias-Mode Attention Mask are abbre-
viated as NSF and BM-AM, respectively. The table shows
that NSF performs better compared to the VSF baseline.
This is because the vanilla attention in SAM [37] and Mo-bileSAM [79] does not consider maintaining the magnitude
consistency when Softmax outputs change drastically due
to varying input resolutions during the testing. In contrast,
our NSF explicitly maintains the magnitude consistency and
alleviates the performance degradation. Furthermore, by
3168
Methods ISIC [12] DUTS [66] DIS-TE4 [55] COD10K [16]
Specialized models
CASS [59] 0.086 - - -
DINO [81] 0.081 - - -
MSA [73] 0.049 - - -
VST [48] - 0.037 - -
ICONet [96] - 0.037 - -
Gate [84] - - 0.109 -
IS-Net [55] - - 0.072 -
SINet [16] - - - 0.092
SegMaR [36] - - - 0.034
The same framework, 4 Specialized models
Ours (BA-SAM) 0.053 0.031 0.055 0.029
Generalized model
SAM [37] 0.419 0.298 0.373 0.217
Ours (BA-SAM) 0.054 0.030 0.054 0.054
Table 2. Comparison results (MAE) with state-of-the-art special-
ized models on various segmentation tasks.
Model AP AP50AP75APSAPMAPL
SAM [37] 42.5 69.6 44.7 29.7 47.0 56.7
Ours (w [37]) 43.0 70.0 45.4 30.0 47.4 57.1
MobileSAM [79] 40.8 68.4 41.6 26.0 44.4 57.6
Ours (w [79]) 41.2 69.0 42.1 26.2 44.8 58.2
Table 3. Results (AP) on COCO [46] instance segmentation.
adding BA-SAM, the performance could be further boosted
when extrapolating to a larger test length. The improve-
ments confirm that these individual components are com-
plementary, which results in mutual benefits.
Impact of Slope in Bias-Mode Attention Mask. In the
Bias-Mode Attention Mask, the magnitude of the slope β
determines penalty rates in different heads. We found that
the best performance is achieved when β= 0.1. Besides,
our method is robust to different slope choices. We use a
fixed slope β= 1by default in the zero-shot setting.
Computational Efficiency. In Tab. 5, we analyze the
computational efficiency between the baselines and our
BA-SAM. All the experiments are conducted on the same
NVIDIA RTX 4090GPU to ensure fair comparisons. The
table shows that our BA-SAM is highly lightweight, incur-
ring negligible computational overhead to the models. The
reasons are two-fold: firstly, the NSF exhibits nearly iden-
tical computational complexity to the vanilla one. In addi-
tion, the BM-AM is seamlessly incorporated by adding a
mask matrix to the query-key dot product before applying
the Softmax operation. Although there is a slight increase in
memory usage, it remains negligible compared to the mem-
ory occupied by large models.
Visualization. In Fig. 4, we present several visual exam-
ples on various datasets. Our BA-SAM shows fine-grained
segmentation results on various high-resolution inputs.Methods ISIC [12] DUTS [66] DIS-TE4 [55] COD10K [16]
Without fine-tuning
Baseline [79] 17.8 6.5 11.7 11.9
+ NSF 16.4 7.5 11.3 12.0
+ BM-AM 16.8 7.2 11.7 11.9
+ Both 15.9 7.9 10.9 11.9
With fine-tuning (1024)
Baseline [79] 42.2 4.8 0.8 3.3
+ NSF 40.9 4.6 0.7 3.0
+ BM-AM 41.2 4.5 0.8 2.7
+ Both 40.4 4.4 0.6 2.5
With fine-tuning (2048)
Baseline [79] 14.4 4.6 1.0 2.3
+ NSF 13.1 4.6 0.8 2.4
+ BM-AM 13.7 4.5 0.9 2.3
+ Both 13.4 4.4 0.8 2.3
Table 4. Ablation study of each component on the settings with
and without fine-tuning. Numbers indicate the performance degra-
dation,∆diff. A lower ∆diffmeans a better performance.
Model Params (M) Speed (ms) Train Hours (h)
SAM [37] 81 113.9 -
Ours (w [37]) 81 114.0 -
MobileSAM [79] 9.66 16.2 0.64
Ours (w [79]) 9.67 16.5 0.65
Table 5. Comparisons of computational efficiency between the
baselines and our BA-SAM. Params: number of parameters.
Speed: inference speed. The top part uses the zero-shot setting,
and the bottom part uses fine-tuning.
6. Conclusion
In this paper, we address the important problem of vary-
ing image resolutions in SAM models by reformulating it
as a problem of length extrapolation. To enhance the length
extrapolation capability of SAM, we propose the Scalable
Bias-mode Attention Mask for SAM (BA-SAM). A new
scaling factor is introduced to maintain the consistent mag-
nitude of attention. In addition, a bias-mode attention mask
is designed to prioritize neighboring information, mitigat-
ing the impact of untrained distant information. Extensive
evaluation on diverse datasets reveals its ability to signif-
icantly alleviate performance degradation in the zero-shot
setting and achieve state-of-the-art performance with min-
imal fine-tuning. Furthermore, we propose a generalized
model and benchmark, showcasing BA-SAM’s generaliz-
ability across all four datasets.
Acknowledgement
The work is supported by Shanghai Municipal Science and
Technology Major Project (2021SHZDZX0102), Shang-
hai Science and Technology Commission (21511101200),
National Natural Science Foundation of China (No.
72192821), YuCaiKe [2023] (No.14105167-2023).
3169
References
[1] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.
Neural machine translation by jointly learning to align and
translate. ICLR , 2015. 3
[2] Irwan Bello. Lambdanetworks: Modeling long-range inter-
actions without attention. In CVPR , 2021. 3
[3] Irwan Bello, Barret Zoph, Quoc Le, Ashish Vaswani, and
Jonathon Shlens. Attention augmented convolutional net-
works. In ICCV , 2019. 3
[4] Lucas Beyer, Pavel Izmailov, Alexander Kolesnikov,
Mathilde Caron, Simon Kornblith, Xiaohua Zhai, Matthias
Minderer, Michael Tschannen, Ibrahim Alabdulmohsin, and
Filip Pavetic. Flexivit: One model for all patch sizes. In
CVPR , 2023. 1, 2, 3
[5] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Alt-
man, Simran Arora, Sydney von Arx, Michael S Bernstein,
Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al.
On the opportunities and risks of foundation models. arXiv
preprint arXiv:2108.07258 , 2021. 3
[6] Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and
Song Han. Once for all: Train one network and specialize it
for efficient deployment. ICLR , 2020. 3
[7] Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang,
Yibing Song, Jue Wang, and Ping Luo. Adaptformer:
Adapting vision transformers for scalable visual recognition.
NeurIPS , 2022. 3
[8] Tianrun Chen, Lanyun Zhu, Chaotao Ding, Runlong Cao,
Shangzhan Zhang, Yan Wang, Zejian Li, Lingyun Sun,
Papa Mao, and Ying Zang. Sam fails to segment
anything?–sam-adapter: Adapting sam in underperformed
scenes: Camouflage, shadow, and more. In arXiv preprint
arXiv:2304.09148 , 2023. 2
[9] Xinlei Chen and Kaiming He. Exploring simple siamese rep-
resentation learning. In CVPR , 2021. 2
[10] Yinpeng Chen, Xiyang Dai, Mengchen Liu, Dongdong
Chen, Lu Yuan, and Zicheng Liu. Dynamic convolution: At-
tention over convolution kernels. In CVPR , 2020. 3
[11] Ta-Chung Chi, Ting-Han Fan, Peter J Ramadge, and Alexan-
der Rudnicky. Kerple: Kernelized relative positional embed-
ding for length extrapolation. NeurIPS , 35, 2022. 3
[12] Noel CF Codella, David Gutman, M Emre Celebi, Brian
Helba, Michael A Marchetti, Stephen W Dusza, Aadi
Kalloo, Konstantinos Liopyris, Nabin Mishra, Harald Kit-
tler, et al. Skin lesion analysis toward melanoma detection:
A challenge at the 2017 international symposium on biomed-
ical imaging (isbi), hosted by the international skin imaging
collaboration (isic). In ISBI, 2018. 2, 6, 7, 8
[13] Mostafa Dehghani, Basil Mustafa, Josip Djolonga, Jonathan
Heek, Matthias Minderer, Mathilde Caron, Andreas Steiner,
Joan Puigcerver, Robert Geirhos, Ibrahim Alabdulmohsin,
et al. Patch n’pack: Navit, a vision transformer for any aspect
ratio and resolution. NeurIPS , 2023. 3
[14] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale. ICLR , 2020. 2, 6[15] Deng-Ping Fan, Ming-Ming Cheng, Jiang-Jiang Liu, Shang-
Hua Gao, Qibin Hou, and Ali Borji. Salient objects in clut-
ter: Bringing salient object detection to the foreground. In
ECCV , 2018. 6
[16] Deng-Ping Fan, Ge-Peng Ji, Guolei Sun, Ming-Ming Cheng,
Jianbing Shen, and Ling Shao. Camouflaged object detec-
tion. In CVPR , 2020. 2, 6, 7, 8
[17] Deng-Ping Fan, Zheng Lin, Zhao Zhang, Menglong Zhu, and
Ming-Ming Cheng. Rethinking rgb-d salient object detec-
tion: Models, data sets, and large-scale benchmarks. TNNLS ,
2021. 6
[18] Deng-Ping Fan, Tengpeng Li, Zheng Lin, Ge-Peng Ji, Ding-
wen Zhang, Ming-Ming Cheng, Huazhu Fu, and Jianbing
Shen. Re-thinking co-salient object detection. TPAMI , 2022.
6
[19] Deng-Ping Fan, Jing Zhang, Gang Xu, Ming-Ming Cheng,
and Ling Shao. Salient objects in clutter. TPAMI , 2022. 6
[20] Deng-Ping Fan, Ge-Peng Ji, Peng Xu, Ming-Ming Cheng,
Christos Sakaridis, and Luc Van Gool. Advances in deep
concealed scene understanding. VI, 2023. 6
[21] Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu,
Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue
Cao. Eva: Exploring the limits of masked visual representa-
tion learning at scale. In CVPR , 2023. 2
[22] Zhengyang Feng, Qianyu Zhou, Qiqi Gu, Xin Tan, Guan-
gliang Cheng, Xuequan Lu, Jianping Shi, and Lizhuang Ma.
Dmt: Dynamic mutual training for semi-supervised learning.
PR, 2022. 2
[23] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats,
and Yann N Dauphin. Convolutional sequence to sequence
learning. In ICML , 2017. 3
[24] Chengyue Gong, Dilin Wang, Meng Li, Xinlei Chen,
Zhicheng Yan, Yuandong Tian, Vikas Chandra, et al. Nasvit:
Neural architecture search for efficient vision transformers
with gradient conflict aware supernet training. In ICLR ,
2021. 3
[25] Benjamin Graham, Alaaeldin El-Nouby, Hugo Touvron,
Pierre Stock, Armand Joulin, Herv ´e J´egou, and Matthijs
Douze. Levit: a vision transformer in convnet’s clothing for
faster inference. In ICCV , 2021. 3
[26] Qiqi Gu, Qianyu Zhou, Minghao Xu, Zhengyang Feng,
Guangliang Cheng, Xuequan Lu, Jianping Shi, and Lizhuang
Ma. Pit: Position-invariant transform for cross-fov domain
adaptation. In ICCV , 2021. 2
[27] Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-
Kirkpatrick, and Graham Neubig. Towards a unified view
of parameter-efficient transfer learning. In ICLR , 2021. 3
[28] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In CVPR ,
2016. 2
[29] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr
Dollar, and Ross Girshick. Masked autoencoders are scalable
vision learners. In CVPR , 2022. 3
[30] Lu He, Qianyu Zhou, Xiangtai Li, Li Niu, Guangliang
Cheng, Xiao Li, Wenxuan Liu, Yunhai Tong, Lizhuang Ma,
and Liqing Zhang. End-to-end video object detection with
spatial-temporal transformers. In ACM MM , 2021. 2
3170
[31] Ge-Peng Ji, Deng-Ping Fan, Yu-Cheng Chou, Dengxin Dai,
Alexander Liniger, and Luc Van Gool. Deep gradient learn-
ing for efficient camouflaged object detection. MIR, 2023.
6
[32] Ge-Peng Ji, Deng-Ping Fan, Peng Xu, Ming-Ming Cheng,
Bowen Zhou, and Luc Van Gool. Sam struggles in concealed
scenes–empirical study on” segment anything”. SCIS , 2023.
3
[33] Wei Ji, Jingjing Li, Qi Bi, Wenbo Li, and Li Cheng. Segment
anything is not always perfect: An investigation of sam on
different real-world applications. CVPR Workshop , 2023. 2
[34] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,
Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom
Duerig. Scaling up visual and vision-language representation
learning with noisy text supervision. In ICML , 2021. 2
[35] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie,
Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Vi-
sual prompt tuning. In ECCV , 2022. 3
[36] Qi Jia, Shuilian Yao, Yu Liu, Xin Fan, Risheng Liu, and
Zhongxuan Luo. Segment, magnify and reiterate: Detect-
ing camouflaged objects the hard way. In CVPR , 2022. 6,
8
[37] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,
Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-
head, Alexander C Berg, Wan-Yen Lo, et al. Segment any-
thing. ICCV , 2023. 2, 3, 5, 6, 7, 8
[38] Aditya Kusupati, Gantavya Bhatt, Aniket Rege, Matthew
Wallingford, Aditya Sinha, Vivek Ramanujan, William
Howard-Snyder, Kaifeng Chen, Sham Kakade, Prateek Jain,
and Ali Farhadi. Matryoshka representations for adaptive
deployment. arXiv preprint arXiv:2205.13147 , 2022. 3
[39] Kenton Lee, Mandar Joshi, Iulia Raluca Turc, Hexiang Hu,
Fangyu Liu, Julian Martin Eisenschlos, Urvashi Khandel-
wal, Peter Shaw, Ming-Wei Chang, and Kristina Toutanova.
Pix2struct: Screenshot parsing as pretraining for visual lan-
guage understanding. In ICML , 2023. 3
[40] Xiang Li, Wenhai Wang, Xiaolin Hu, and Jian Yang. Selec-
tive kernel networks. In CVPR , 2019. 3
[41] Xiangtai Li, Henghui Ding, Wenwei Zhang, Haobo Yuan,
Guangliang Cheng, Pang Jiangmiao, Kai Chen, Ziwei Liu,
and Chen Change Loy. Transformer-based visual segmenta-
tion: A survey. arXiv pre-print , 2023. 3
[42] Xiangtai Li, Haobo Yuan, Wei Li, Henghui Ding, Size Wu,
Wenwei Zhang, Yining Li, Kai Chen, and Chen Change Loy.
Omg-seg: Is one model good enough for all segmentation?
CVPR , 2024. 3
[43] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing
continuous prompts for generation. In ACL, 2021. 3
[44] Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency.
Foundations and recent trends in multimodal machine learn-
ing: Principles, challenges, and open questions. arXiv
preprint arXiv:2209.03430 , 2022. 3
[45] Mingbao Lin, Mengzhao Chen, Yuxin Zhang, Ke Li, Yun-
hang Shen, Chunhua Shen, and Rongrong Ji. Super vision
transformer. IJCV , 2022. 3
[46] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and C LawrenceZitnick. Microsoft coco: Common objects in context. In
ECCV , 2014. 2, 6, 8
[47] Fengqi Liu, Jingyu Gong, Qianyu Zhou, Xuequan Lu, Ran
Yi, Yuan Xie, and Lizhuang Ma. Cloudmix: Dual mixup
consistency for unpaired point cloud completion. TVCG ,
2024. 2
[48] Nian Liu, Ni Zhang, Kaiyuan Wan, Ling Shao, and Junwei
Han. Visual saliency transformer. In ICCV , 2021. 6, 8
[49] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng
Zhang, Stephen Lin, and Baining Guo. Swin transformer:
Hierarchical vision transformer using shifted windows. In
ICCV , 2021. 5, 6
[50] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie,
Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al.
Swin transformer v2: Scaling up capacity and resolution. In
CVPR , 2022. 3
[51] Shaocong Long, Qianyu Zhou, Chenhao Ying, Lizhuang Ma,
and Yuan Luo. Diverse target and contribution scheduling
for domain generalization. arXiv preprint arXiv:2309.16460 ,
2023. 2
[52] Shaocong Long, Qianyu Zhou, Chenhao Ying, Lizhuang Ma,
and Yuan Luo. Rethinking domain generalization: Discrim-
inability and generalizability. arXiv preprint , 2023. 2
[53] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz
Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Im-
age transformer. In ICML , 2018. 3
[54] Ofir Press, Noah A Smith, Mike Lewis, et al. Train short,
test long: Attention with linear biases enables input length
extrapolation. ICLR , 2021. 3
[55] Xuebin Qin, Hang Dai, Xiaobin Hu, Deng-Ping Fan, Ling
Shao, and Luc Van Gool. Highly accurate dichotomous im-
age segmentation. In ECCV , 2022. 2, 6, 7, 8
[56] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-
ing transferable visual models from natural language super-
vision. In ICML , 2021. 2, 3
[57] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
and Mark Chen. Hierarchical text-conditional image gen-
eration with clip latents. arXiv preprint arXiv:2204.06125 ,
2022. 3
[58] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-
attention with relative position representations. In Proceed-
ings of NAACL-HLT , 2018. 3
[59] et al Singh, Pranav. Cass: cross architectural self-supervision
for medical image analysis. NeurIPS , 2022. 6, 8
[60] Yiran Song, Qianyu Zhou, and Lizhuang Ma. Rethinking im-
plicit neural representations for vision learners. In ICASSP ,
2023. 2
[61] Aravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon
Shlens, Pieter Abbeel, and Ashish Vaswani. Bottleneck
transformers for visual recognition. In CVPR , 2021. 3
[62] JianLin Su. Transformer upgrade road: 7, length extrapola-
tion and local attention, 2023. 5
[63] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,
Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent
Vanhoucke, and Andrew Rabinovich. Going deeper with
convolutions. In CVPR , 2015. 3
3171
[64] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia
Polosukhin. Attention is all you need. In NeurIPS , 2017. 3,
4, 5, 7
[65] Fei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, Cheng
Li, Honggang Zhang, Xiaogang Wang, and Xiaoou Tang.
Residual attention network for image classification. In
CVPR , 2017. 3
[66] Lijun Wang, Huchuan Lu, Yifan Wang, Mengyang Feng,
Dong Wang, Baocai Yin, and Xiang Ruan. Learning to de-
tect salient objects with image-level supervision. In CVPR ,
2017. 2, 6, 7, 8
[67] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao
Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyra-
mid vision transformer: A versatile backbone for dense pre-
diction without convolutions. In ICCV , 2021. 3
[68] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-
ing He. Non-local neural networks. In CVPR , 2018. 2, 3
[69] Xiao Wang, Guangyao Chen, Guangwu Qian, Pengcheng
Gao, Xiao-Yong Wei, Yaowei Wang, Yonghong Tian, and
Wen Gao. Large-scale multi-modal pre-trained models: A
comprehensive survey. MIR, 2023. 3
[70] Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So
Kweon. Cbam: Convolutional block attention module. In
ECCV , 2018. 3
[71] Bichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan,
Peizhao Zhang, Zhicheng Yan, Masayoshi Tomizuka, Joseph
Gonzalez, Kurt Keutzer, and Peter Vajda. Generating
long sequences with sparse transformers. arXiv preprint
arXiv:2006.03677 , 2020. 3
[72] Bichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan,
Peizhao Zhang, Zhicheng Yan, Masayoshi Tomizuka, Joseph
Gonzalez, Kurt Keutzer, and Peter Vajda. Visual transform-
ers: Token-based image representation and processing for
computer vision. arXiv preprint arXiv:2006.03677 , 2020.
3
[73] Junde Wu, Rao Fu, Huihui Fang, Yuanpei Liu, Zhaowei
Wang, Yanwu Xu, Yueming Jin, and Tal Arbel. Medical sam
adapter: Adapting segment anything model for medical im-
age segmentation. arXiv preprint arXiv:2304.12620 , 2023.
8
[74] Jianzong Wu, Xiangtai Li, Shilin Xu, Haobo Yuan, Henghui
Ding, Yibo Yang, Xia Li, Jiangning Zhang, Yunhai Tong,
Xudong Jiang, Bernard Ghanem, and Dacheng Tao. Towards
open vocabulary learning: A survey. TPAMI , 2024. 3
[75] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin
Bao, Zhuliang Yao, Qi Dai, and Han Hu. Simmim: A simple
framework for masked image modeling. In CVPR , 2022. 3
[76] Hongyi Xu, Fengqi Liu, Qianyu Zhou, Jinkun Hao, Zhijie
Cao, Zhengyang Feng, and Lizhuang Ma. Semi-supervised
3d object detection via adaptive pseudo-labeling. In ICIP ,
2021. 2
[77] Jiahui Yu, Pengchong Jin, Hanxiao Liu, Gabriel Bender,
Pieter-Jan Kindermans, Mingxing Tan, Thomas Huang, Xi-
aodan Song, Ruoming Pang, and Quoc Le. Bignas: Scaling
up neural architecture search with big single-stage models.
InECCV , 2020. 3[78] Haobo Yuan, Xiangtai Li, Chong Zhou, Yining Li, Kai Chen,
and Chen Change Loy. Open-vocabulary sam: Segment
and recognize twenty-thousand classes interactively. arXiv
preprint , 2024. 3
[79] Chaoning Zhang, Dongshen Han, Yu Qiao, Jung Uk Kim,
Sung-Ho Bae, Seungkyu Lee, and Choong Seon Hong.
Faster segment anything: Towards lightweight sam for mo-
bile applications. arXiv preprint arXiv:2306.14289 , 2023. 2,
3, 6, 7, 8
[80] Hang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu,
Haibin Lin, Zhi Zhang, Yue Sun, Tong He, Jonas Mueller,
R Manmatha, et al. Resnest: Split-attention networks. In
CVPR , 2022. 3
[81] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun
Zhu, Lionel M Ni, and Heung-Yeung Shum. Dino: Detr
with improved denoising anchor boxes for end-to-end object
detection. ICLR , 2023. 6, 8
[82] Kaidong Zhang and Dong Liu. Customized segment any-
thing model for medical image segmentation. In arXiv
preprint arXiv:2304.13785 , 2023. 1, 2
[83] Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring
self-attention for image recognition. In CVPR , 2020. 3
[84] Xiaoqi Zhao, Youwei Pang, Lihe Zhang, Huchuan Lu, and
Lei Zhang. Suppress and balance: A simple gated network
for salient object detection. In ECCV , 2020. 6, 8
[85] Chong Zhou, Xiangtai Li, Chen Change Loy, and Bo Dai.
Edgesam: Prompt-in-the-loop distillation for on-device de-
ployment of sam. arXiv preprint arXiv:2312.06660 , 2023.
3
[86] Qianyu Zhou, Zhengyang Feng, Qiqi Gu, Guangliang
Cheng, Xuequan Lu, Jianping Shi, and Lizhuang Ma.
Uncertainty-aware consistency regularization for cross-
domain semantic segmentation. CVIU , 2022. 2
[87] Qianyu Zhou, Zhengyang Feng, Qiqi Gu, Jiangmiao Pang,
Guangliang Cheng, Xuequan Lu, Jianping Shi, and Lizhuang
Ma. Context-aware mixup for domain adaptive semantic seg-
mentation. TCSVT , 2022.
[88] Qianyu Zhou, Ke-Yue Zhang, Taiping Yao, Ran Yi,
Shouhong Ding, and Lizhuang Ma. Adaptive mixture of ex-
perts learning for generalizable face anti-spoofing. In ACM
MM, 2022.
[89] Qianyu Zhou, Ke-Yue Zhang, Taiping Yao, Ran Yi, Kekai
Sheng, Shouhong Ding, and Lizhuang Ma. Generative do-
main adaptation for face anti-spoofing. In ECCV , 2022.
[90] Qianyu Zhou, Chuyun Zhuang, Ran Yi, Xuequan Lu, and
Lizhuang Ma. Domain adaptive semantic segmentation via
regional contrastive consistency regularization. In ICME ,
2022.
[91] Qianyu Zhou, Qiqi Gu, Jiangmiao Pang, Xuequan Lu, and
Lizhuang Ma. Self-adversarial disentangling for specific do-
main adaptation. TPAMI , 2023.
[92] Qianyu Zhou, Xiangtai Li, Lu He, Yibo Yang, Guangliang
Cheng, Yunhai Tong, Lizhuang Ma, and Dacheng Tao.
Transvod: end-to-end video object detection with spatial-
temporal transformers. TPAMI , 2023.
[93] Qianyu Zhou, Ke-Yue Zhang, Taiping Yao, Xuequan Lu, Ran
Yi, Shouhong Ding, and Lizhuang Ma. Instance-aware do-
main generalization for face anti-spoofing. In CVPR , 2023.
3172
[94] Qianyu Zhou, Ke-Yue Zhang, Taiping Yao, Xuequan Lu,
Shouhong Ding, and Lizhuang Ma. Test-time domain gener-
alization for face anti-spoofing. In CVPR , 2024. 2
[95] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang,
and Jifeng Dai. Deformable detr: Deformable transformers
for end-to-end object detection. In ICLR , 2020. 6
[96] Mingchen Zhuge, Deng-Ping Fan, Nian Liu, Dingwen
Zhang, Dong Xu, and Ling Shao. Salient object detection
via integrity learning. TPAMI , 2022. 3, 6, 8
3173
