Physics-aware Hand-object Interaction Denoising
Haowen Luo1, Yunze Liu1,3, Li Yi1,2,3
1Tsinghua University,2Shanghai Artificial Intelligence Laboratory,3Shanghai Qi Zhi Institute
Figure 1. Given a noisy hand-object interaction sequence, our method produces de-noised hand poses conditioning on the object trajectory,
mitigating physically-implausible artifacts such as erroneous contact and hand-object penetration. In this example of a human hand (yellow)
manipulating a hand model (grey), the de-noised result demonstrates higher physical plausibility. Please see our supplementary material
for more annimated results.
Abstract
The credibility and practicality of a reconstructed hand-
object interaction sequence depend largely on its physical
plausibility. However, due to high occlusions during hand-
object interaction, physical plausibility remains a challeng-
ing criterion for purely vision-based tracking methods. To
address this issue and enhance the results of existing hand
trackers, this paper proposes a novel physically-aware hand
motion de-noising method. Specifically, we introduce two
learned loss terms that explicitly capture two crucial as-
pects of physical plausibility: grasp credibility and manipu-
lation feasibility. These terms are used to train a physically-
aware de-noising network. Qualitative and quantitative
experiments demonstrate that our approach significantly
improves both fine-grained physical plausibility and over-
all pose accuracy, surpassing current state-of-the-art de-
noising methods.1. Introduction
Hand pose tracking during hand-object interaction is a cru-
cial task for various applications such as gaming, virtual
reality, and robotics. Vision-based hand tracking methods
have made significant progress in recent years by estimat-
ing hand poses from vision data sequences. However, heavy
occlusions often occur during hand-object interaction, lead-
ing to ambiguity for vision-based trackers. Consequently,
even state-of-the-art tracking methods still produce obvious
errors and generate physically implausible artifacts such as
inter-penetrating hand-objects and unrealistic manipulation.
It is critical to remove such artifacts, increase the physical
plausibility, and ensure the usefulness of tracking results.
Several previous works have proposed to post-process
the estimations generated by vision-based trackers using de-
noising techniques. Some works have attempted to opti-
mize hand poses by minimizing low-level penetration and
attraction energy [5]. However, such optimization methods
may struggle to handle severe noise, as they easily get stuck
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
2341
at local optima. Other works instead train neural networks
for de-noising purposes [9][31], leveraging data-driven pose
or contact priors to correct potentially significant pose er-
rors. Nevertheless, purely data-driven approaches rely on
the quality of the training dataset labels to achieve satis-
factory visual effects and physical plausibility, and may be
susceptible to overfitting and over-smoothing. Additionally,
there is no guarantee that the resulting neural network is
physically-aware.
In an effort to overcome the limitations of existing meth-
ods, we propose to combine data-driven de-noising with
explicit modeling of the physical plausibility. In partic-
ular, such modeling needs to cover two essential aspects:
i) grasp credibility, which demands that the hand pose in
each frame be realistic given the object’s geometry, avoid-
ing interpenetration in particular; ii) manipulation feasibil-
ity, which considers the object’s movement and requires
proper hand-object contact that can plausibly explain the
object’s trajectory through hand manipulation, complying
with physical laws.
Incorporating physics-based constraints into data-driven
de-noising necessitates reshaping the loss landscape, en-
abling the network to learn to de-noise hand motions while
adhering to physical constraints. While this idea appears
straightforward, achieving it is difficult due to the intricate
and non-differentiable process of verifying the plausibility
of hand motions. Furthermore, when the de-noising algo-
rithm violates physical constraints, it is necessary to provide
a suitable path to guide it back to feasible motions. Meeting
this requirement is even more challenging.
To address the aforementioned challenges, we intro-
duce neural physical losses for assessing grasp credibility
and manipulation feasibility, respectively. These losses are
differentiable and can approximate non-differentiable and
computationally intensive physical metrics effectively. Fur-
thermore, they not only differentiate physically invalid hand
motions from valid ones but also offer good projection di-
rections to correct physically implausible hand motions. We
integrate these neural physical losses into a novel hand mo-
tion denoising framework. Specifically, we design a de-
noising auto-encoder that operates on a dual hand-object-
interaction representation, along with a two-stage training
process that effectively balances physical constraints and
data priors.
To demonstrate the effectiveness of our designs, we con-
duct experiments on both data with synthetic errors and ac-
tual errors caused by trackers and achieve both qualitative
and quantitative improvements over the previous state of the
arts. To sum up, our main contributions are:
First , we propose a physically-aware hand-object inter-
action de-noising framework which nicely combines data
priors and physics priors.
Second , we introduce differentiable neural physicallosses to model both grasp credibility and manipulation fea-
sibility to support end-to-end physically-aware de-noising.
Third , we demonstrate the generalization of our neural
physical losses regarding object, motion, and noise pattern
variations, and show their effectiveness on different bench-
marks.
2. Related work
Hand reconstruction and tracking The problem of re-
constructing 3D hand surfaces from RGB or depth observa-
tions has garnered considerable attention in research. The
existing body of work can be broadly classified into two
distinct paradigms. Discriminative approaches focus on di-
rectly estimating hand shape and pose parameters from the
observation, employing techniques such as 3D CNNs and
volumetric representations [1, 6, 8, 17, 30]. In contrast, gen-
erative approaches adopt an iterative optimization process
to refine a parametric hand model, iteratively aligning its
projection with the observed data [23, 25, 26]. While recent
advancements have explored more challenging scenarios,
such as reconstructing two interacting hands [18, 22, 28],
these approaches often overlook the presence of objects,
leading to decreased reliability in interaction-intensive sce-
narios. The absence of object-awareness greatly limits their
ability to accurately reconstruct hand surfaces in complex
and dynamic environments.
Hand pose denoising The goal of hand pose denoising
is to improve the reliability and accuracy of the hand pose
estimation or tracking system, enabling more robust and
realistic hand motion analysis and interaction in applica-
tions such as virtual reality, augmented reality, robotics, and
human-computer interaction. TOCH [31]improves motion
refinement by establishing spatio-temporal correspondence
between objects and hands. GraspTTA [15] utilizes con-
tact consistency reasoning to generate realistic and stable
human-like grasps. D-Grasp [7] generates physically real-
istic and dynamic grasps for interactions between the hand
and objects. Grabnet [24] aims to create accurate and visu-
ally realistic hand mesh models while interacting with pre-
viously unseen objects.
Physical plausibility in hand-object interaction The
physical plausibility during hand-object interaction has
been studied by many previous works. [29] uses a neu-
ral network to learn from human motion and synthesize
physics-plausible manipulation. [7] and [27] generate a
physics-based hand control policy with deep reinforcement
learning to reach specific grasping or moving goals. While
these works focus on synthesizing hand motions, some
works, such as [2, 9, 14, 16, 19], explore reconstructing or
refining hand-object interaction leveraging physics priors,
2342
which is more relevant to our work. However, these works
have limitations such as being limited to static grasps, re-
lying on purely data-driven approaches and assuming only
finger tips as source of forces.
3. Method
In this section, we describe our method for physically-aware
hand pose de-noising. We begin by introducing the prob-
lem and outlining our approach. Given a potentially noisy
hand pose trajectory during human-object interaction, rep-
resented by a sequence of hand meshes over Tframes de-
noted by eH= (eHi)1≤i≤TwitheHi∈RK×3, our method
conditions on the object’s geometry and dynamic informa-
tion to refine the input and get more physically-plausible
results ˆH= (ˆHi)1≤i≤T. We consider hand-object interac-
tion sequences containing a single hand and a single rigid
object. Let O= (Oi)1≤i≤TwithOi∈RL×3denote object
vertices over Tframes.
We choose to only de-noise the hand poses in our setting
while assuming accurate object trajectory because tracking
rigid objects is much easier than tracking articulated rigid
body with many joints like hands, especially for marker-
based motion capture systems popularly used in laborato-
ries and industry. What’s more, beyond refining tracking
results for traditional hand object interaction reconstruction
tasks, our setting is also critical for broader applications
such as virtual object manipulation and interaction retarget-
ing in VR/AR and gaming, where clean object trajectory is
usually accessible.
To combine data priors and physics priors in the de-
noising process, we introduce a dual representation of hand-
object interaction F= (Fi)1≤i≤T= (F(Hi, Oi))1≤i≤T
that enables physical reasoning besides capturing data pri-
ors about hand-object interaction. We elaborate on this rep-
resentation in Section 3.1. Using Fas an intermediate rep-
resentation, we refine hand poses via mapping noisy repre-
sentation eFto its correct version ˆFwith a de-noising auto-
encoder. Using the refined representation ˆF, the corrected
hand pose sequence ˆHis fitted. Details regarding the archi-
tecture of our de-noising network and the de-noising frame-
work can be found in Section 3.1.
To produce more physically plausible results, during the
training of the de-noising network, besides traditional data
losses, we propose two neural physical loss terms for as-
sessing grasp credibility and manipulation feasibility. They
are capable of conducting physical reasoning given the HOI
representation Fand producing assessment scores differen-
tiable to F. What’s more, with carefully designed training
scheme and training targets, these loss terms form smooth
landscape, and therefore yield contributive signals that ef-
fectively guide the de-noising network to gain physical-
awareness and produce more physically-plausible results.
Section 3.2 and 3.3 describe the construction of these twoloss terms. And we describe their usage in the training pro-
cess in Section 3.1.
3.1. Training framework
Dual HOI representation Instead of directly using hand
vertices as the representation in the de-noising process,
we propose a dual hand-object interaction representation
that combines holistic hand pose information and fine-
grained hand-object relation. This intermediate representa-
tion bridges explicit hand mesh vertices and physics-related
knowledge learned by the proposed neural loss terms, al-
lowing knowledge to flow between them and eventually im-
prove the physical plausibility of the refined hand pose. On
the one hand, by emphasizing hand-object relation at two
different granularity, this dual representation enables the
physical loss terms to reason about contact and deep pen-
etration cases and better evaluate the physical plausibility
of a given hand pose. On the other hand, with its focus on
the physics-related characters of hand poses, when used to
fit the explicit hand pose representation, this proposed rep-
resentation can effectively improve vital aspects of the hand
pose regarding physical plausibility.
Given hand vertices H= (Hi)1≤i≤Tof hand MANO
[21] meshes and object vertices O= (Oi)1≤i≤ToverT
frames, the whole HOI representation (Fi)1≤i≤Twe use is
denoted as:
Fi= (Si,Ti)
We regress the 21 hand key points (Si)1≤i≤TwithSi∈
R21×3from hand vertices and model object-centric hand-
object correspondence with the implicit field proposed
by [31], that is, (Ti)1≤i≤Twhere Ti={Ci
j}N
j=1=
{(mi
j, di
j, pi
j)}N
j=1. With Npoints randomly sampled on
the object surface in the i-th frame, Ci
jrepresents the cor-
responding hand point of the j-th object point. To find
hand-object correspondence, we cast rays in the object sur-
face’s normal directions from the sampled points and con-
sider hand points the rays first hit, as the corresponding
points. mi
jis set to 1 if the j-th object point has correspond-
ing hand point and is 0 otherwise. di
jdenotes the distance
between the j-th object points and its corresponding hand
point, while pi
j∈R3encodes the semantic information
of the hand point encoded as its position in the canonical
space. After obtaining the refined representation, we can
fit hand mesh to the representation and obtain the denoised
hand poses. We show the inference process in Figure 2.
De-noising anto-encoder We use an auto-encoder as our
backbone, which consumes noisy data in the form of our
dual HOI representation eFand produces the corrected ver-
sion ˆF. Our de-noising network adopts the PointNet [20]
architecture to process the the TOCH field part Tiof the
input representation into a global object feature for the i-
2343
Figure 2. Overview of the training and inference frameworks.
th frame, which is then concatenated with the hand-centric
partSiin our representation to form the HOI frame feature
xi.(xi)1≤i≤Tis considered as time series data and further
processed by a RNN, whose output is used to decode the
refined representation ˆF.
Training process To obtain a physically aware de-noising
network, we train the auto-encoder in two stages as shown
in Figure 2. In the stage I, we train the model with supervi-
sion from ground truth data, learning a projection mapping
from noisy hand pose representation to the clean data man-
ifold. The training loss of stage Ican be expressed as:
LI=αTLT(ˆT,TGT) +αSLS(ˆS, S GT)
where LTandLSmeasure the difference between the re-
fined representation ˆFand the ground truth representation
FGT. To reinforce the physical awareness of the de-noising
network, we introduce the two physical loss terms in stage
II. Specifically, we freeze the auto-encoder trained in stage
I, but plug in a mapping layer ϕbetween the encoder and the
decoder, which is trained with the neural physical losses in
stage II.ϕis a MLP with residual connection that consumes
latent vectors produced by the encoder, together with the
linear accelerations of the object obtained with finite differ-
ence method. As the dual representation is in object-frame,
it is necessary to introduce of linear acceleration to enable
ϕto reason about manipulation feasibility. We enable ϕto
capture the physical reasoning enforced by the two losses,
therefore reshaping the clean hand pose manifold learned in
stage I. The training loss of stage II integrates three compo-
nents:
LII=αgraspLgrasp(ˆS, O) +αmanipLmanip(ˆT)
+αreg||ϕ(v, a)−v||2+αGTLI
whereLgraspandLmanip are the learned physical losses to be
explained in the following sections, while vdenotes the la-
tent vector produced by the encoder and a= (ai)1≤i≤Tde-
notes the linear accelerations of the object. The regulationterm ensures proximity between the mapped and the orig-
inal latent vector. Together with supervision from ground
truth hand pose, it helps to maintain the data priors learned
in the first stage.
3.2. Grasp credibility loss
Given a single frame from a HOI sequence, with no knowl-
edge about the movement of the object, the geometry re-
lation between hand and object is the most important clue
for humans to evaluate the plausibility of such a frame. To
be specific, the hand pose must conform to the object’s ge-
ometry, forming a plausible grasp and avoiding penetration
or collision. While many recent works, such as [13], [10]
and [4], focus on mitigating hand-object penetration during
hand pose estimation, they tend to be helpless when deal-
ing with deep penetration cases where the hand penetrates
through the object completely instead of just entering the
object surface, which are common when dealing with thin
and delicate object parts.
When deep penetration happens, the direction to move
the vertices to resolve penetration becomes ambiguous as
the hand mesh lies on both side of the object, and it is very
challenging to have a differentiable term producing correct
gradients. While our method understands how to handle
deep penetration through data prior learned from a novel
penetration depth metric PD.
PDquantifies the severity of penetration of a noisy hand
poseeHi, which is paired with the corresponding clean hand
poseHi, with respect to the object mesh vertices Oi. The
metric is computed by comparing eHwith the ground truth
hand mesh vertices.
To compute this penetration depth metric, we focus on
the hand vertices in eHwhose counterparts in Hare in con-
tact with the object. Specifically, we consider hand points
with distances less than a threshold ccontact = 2mm to
the object surface as contact points. For Ccontact points
{hi}C
i=1on the ground truth hand mesh, whose counterparts
on the evaluated mesh are denoted as {ehi}C
i=1, we compute
their shift between two meshes as {⃗hi=ehi−hi}C
i=1. At
2344
the object contact points {oi}C
i=1, we compute the normal
{⃗ ni}C
i=1of the object surface. Then the metric is computed
as:
PD(eHi, Hi|Oi) = ( max
j=1,2,...,C
||⃗hj−(⃗ nj·⃗hj)⃗ nj||<c tangent(−⃗ nj·⃗hj))+
where x+denotes max(0 , x)andctangent is set to 1cm em-
pirically so that only hands vertices with small shift in di-
rections orthogonal to the object surfaces are considered for
penetration evaluation, avoiding false positive cases where
the hand vertices shift so much that their positions with re-
spect to the object change completely. This definition can
be considered as an approximation of the penetration depth
at the most severe penetration position.
Compared with previous works that measure the inter-
section volume or count hand vertices inside the object
mesh to evaluate hand-object inter-penetration, our metric
better reflects the severity of deep penetration cases com-
monly encountered with thin and delicate objects, while ex-
isting loss terms only leverage local geometry information
and tend to get stuck at local minima when exploited to rem-
edy penetration.
This metric, however, isn’t differentiable to our interme-
diate hand-object interaction representation and requires the
ground truth hand pose to be computed, hence can’t be di-
rectly used for improving the geometry credibility of results
produced by our de-noising network. We train a physics-
aware neural network ψgraspwith a PointNet-like backbone
that consumes hand skeletons and object point clouds to
produce prediction results pbetween 0 and 1, where 0 in-
dicates no severe penetration and 1 indicates the opposite.
We introduce a threshold cPDuse the comparison result of
PD > c PDas a hard target, as well as the original PD as a
soft training target to encourage smooth prediction output.
The loss for training Lgraspcan be defined as:
Lgrasp train (p, PD ) =αgrasp hard BCE(p, b hard)
+αgrasp soft BCE(p, b soft)
bhard=1PD≥cPD,bsoft= 1−e−csoft∗PD
We set csoft=ln(2)
cPDsuch that bsoft= 0.5when PD=cPD
to make sure that bhardandbsoftare consistent. We set
cPD= 1.5cm empirically. BCE(·,·)denotes the binary
cross entropy function.
As shown in Figure 3, the combination of soft target and
hard target during the training allows Lgrasp to distinguish
deep penetration cases smoothly, improving its usability as
a loss term.
We also attempt to improve the generalization ability of
Lgrasp and the loss landscape smoothness by assuring data
variation in its training dataset. Using a HOI dataset con-
taining ground truth data {(Hi, Oi)}D
i=1, we first get its per-
turbed version {(eHi, Oi)}D
i=1by adding Gaussian noise to
Figure 3. The proposed grasp credibility loss and manipulation
feasibility loss can help quantify the physical plausibility of hand
pose estimation results in a smooth way. The darkness of hand
colors indicates the value of our proposed neural losses on frames
with different noise levels. In this example, as the noise level of
hand poses decreases, our proposed neural losses also decrease
gradually, despite the discrete nature of the hand-object interaction
(contact vs non-contact), providing smooth guidance for training
of physics-aware de-noising network.
the MANO parameters. To assure variation in noise mag-
nitude, we further conduct linear interpolation between the
MANO parameters of the ground truth data and the per-
turbed data to get mhand poses from each perturbed-clean
hand pose pair. The dataset we finally obtain can be ex-
pressedS
i=1,2,...,D{(˘Hi
j, Oi)}m
j=1where ˘Hi
jdenotes the
j-th interpolation result between ˘Hi
1=Hiand˘Hi
m=eHi.
3.3. Manipulation feasibility loss
For a HOI sequence to be realistic, besides grasp credibil-
ity that only focuses on the single frame, whether the ob-
ject’s movement seems feasible also matters. To this end,
we propose two manipulation feasibility metrics, force er-
ror (FE) and manipulation expense ( ME ), that evaluate
whether the given hand-object contact can feasibly move
the object along its trajectory. The two metrics are used
as hard target and soft target respectively to train the neu-
ral physical loss term Lmanip. The two metrics, as well as
the neural loss term Lmanip, take the hand-object correspon-
dence part Tof our representation and the object’s linear
acceleration aas input.
In the first force error metric, we measure to what extent
can the object’s movement be explained by forces applied
at the the contacts within the corresponding friction cones.
Given an implicit field Ti, we consider the Mcontact
points among the randomly sampled Npoints on the ob-
ject surface, and denote the object surface normal at these
contact points as {⃗ n}M
j=1. Let ⃗F=m0(−⃗ g+⃗ a)denote
the force needed for the object with mass m0to achieve
its acceleration ⃗ awhen subject to gravity m0⃗ g. We ob-
2345
tain⃗ awith finite difference method. And let {⃗fj}M
j=1with
fj∈R3denotes the set of contact forces applied to the ob-
ject that we solve for. We require the forces to lie in the
corresponding friction cones specified by the coefficient of
static friction µ. Then the force error metric can be ex-
pressed as:
FE({⃗ nj}M
j=1,⃗F) = min
⃗fj
||⃗fj||·(−⃗ nj)≥q
1
1+µ2||MX
j=1⃗fj−⃗F||
Notice that FEis always between 0 and 1, and ideally, for
a physically plausible frame, FEshould be 0. Since the ob-
ject mass is only a relative value for solving forces, there-
fore doesn’t affect resultant force distribution of the opti-
mization process, and we only care about the relative force
error instead of the absolute force value, m0can be set to
any non-zero constant. We set m0= 1kg .µis set to 0.8 em-
pirically following the common practice in previous works
such as [14, 29]. In practice, we find that as long as the
selected friction coefficient isn’t too off, the result of force
error and manipulation expense can align well with human
perception concerning the manipulation feasibility.
While this force error metric can be used as a binary re-
sult to verify whether the movement of the object is feasi-
ble given a certain hand pose, a major drawback is that its
value doesn’t correctly reflect how infeasible a hand pose
is. However, to obtain a loss term with smoother landscape,
a metric with continuous result indicating the degree of ma-
nipulation feasibility would be more favorable. Therefore,
we propose the manipulation expense metric that evaluates
the distance between the given hand pose and the closest
feasible hand pose.
Intuitively, this manipulation expense metric considers
all the plausible force distribution maps that yield the re-
quired total force, and find the one that best match the cur-
rent contact map, in that least forces are applied at object
points which are actually far from the hand. The difference
between the current contact map and its best match found
in the above manner can reflect the quality of the current
contact map regarding manipulation feasibility.
In this metric, we consider all Nsampled object points
inTi, and djdenotes the signed distance between the j-th
sampled object point and its corresponding hand point. Let
{⃗fj}N
j=1with⃗fj∈R3denote potential forces exerted at the
Nsampled points, the manipulation expense metric can be
expressed as:
ME({⃗ nj}N
j=1,{dj}N
j=1,⃗F)
= min
⃗fj
||⃗fj||·(−⃗ nj)≥q
1
1+µ2
PN
j=1⃗fj=⃗FNX
j=1||⃗fj||·(|dj|−ccontact)+We calculate FE andME by solving for {⃗fj}M
j=1
and{⃗fj}N
j=1through optimization processes respectively.
Please refer to our supplementary material for details. We
train a PointNet-like neural predictor ψmanip that produces a
output qbetween 0 and 1, to form the manipulation feasi-
bility loss. The training loss of ψmanip is formed as:
Lmanip train(q, FE, ME ) =αmanip hard BCE(q, s hard)
+αmanip soft BCE(q, s soft)
shard=1FE≥cFE,ssoft=1FE≥cFE·(0.5+arctan (ME)
π)
αmanip hard andαmanip soft are constant weights. We use the
same training dataset for Lmanip as the one used to train
Lgrasp .
4. Experiments
In this section we evaluate the proposed method on data
with synthetic noise and actual tracking noise. We first in-
troduce the datatsets (Section 4.1) and the evaluation met-
rics (Section 4.2). Results of our approach on correcting
synthetic tracking error and refining results from vision-
based trackers are in (Section 4.3) and (Section 4.4) respec-
tively. Finally, we conduct ablation studies to evaluate the
advantages of our physically-aware loss design in Section
4.5.
4.1. Datasets
GRAB . We train our de-noising network and the two neural
loss terms on GRAB [24], a MoCap dataset for whole-body
grasping of objects containing 51 objects from [3]. We fol-
low the recommended split and select 10 objects for evalu-
ation and testing.
HO-3D . HO-3D [11] is a dataset of hand-object inter-
action videos captured by RGB-D cameras, paired with
frame-wise annotations of 3D hand poses and object poses.
We use HO-3D to evaluate how well our pipeline trained
with synthetic pose errors can generalize to real tracking er-
rors produced by vision-based trackers. We use the second
official release of HO-3D.
4.2. Metrics
Mean Per-Joint Position Error (MPJPE) andMean Per-
Vertex Position Error (MPVPE) . We report the average
Euclidean distances between refined and ground truth 3D
hand joints and vertices. These metrics measure the accu-
racy of hand poses and shapes.
Intersection Volume (IV) . This metric measures vol-
ume of the intersection between the hand mesh and the
object mesh. It reflects the degree of hand-object inter-
penetration.
2346
Table 1. Quantitative results on refining GRAB dataset with sythetic noise. T-0.01 denotes the dataset with translation noise complying the
distribution N(0,0.01),θ-0.3 denotes the dataset with pose noise complying the distribution N(0,0.3), other noise patterns are denoted
similarly. We additionally add ∆r∼ N(0,0.05)to the global orientation in all the datasets. We train the network only on the ”T-0.01, θ-
0.3” dataset and test it on datasets with different noise patterns. Our method is robust to different noise patterns and magnitudes, especially
in its ability to produce physically plausible results.
T-0.01 T-0.02 θ-0.3 θ-0.5T-0.01
θ-0.3T-0.02
θ-0.5
MPJPE 16.01 →6.91 27.83→10.01 7.52→5.61 8.74→6.38 18.86→7.39 31.49→11.97
MPVPE 16.32 →6.24 28.43→10.46 6.23 →6.31 8.85 →6.97 19.13→6.78 33.25→11.24
contact IoU 3.31 →24.82 2.39→21.77 5.45→25.14 4.46→24.18 3.62→23.94 2.47→20.50
IV 0.91 →0.90 1.43→1.10 0.88 →0.92 1.77 →0.94 0.87 →1.13 2.35 →1.12
PD 0.77 →0.32 0.85→0.43 0.74→0.44 1.22→0.47 0.80→0.44 1.56→0.45
plausible rate 0.42 →0.95 0.33→0.92 0.45→0.93 0.43→0.93 0.42→0.91 0.31→0.90
Penetration Depth (PD) . To better measure cases of
deep penetration, we also report the penetration depth met-
ric proposed in Section 3.2. Since the ground truth hand
poses of HO-3D are withheld, we cannot calculate PD on
HO-3D and only report it on GRAB.
Contact IoU . This metric assesses the Intersection-over-
Union between the ground truth contact map and the pre-
dicted contact map. The contact maps are obtained by con-
sidering object vertices within 2mm from the hand as in
contact. This metric is also reported on GRAB only.
Plausible Rate . To consider both grasp credibility and
manipulation feasibility for a holistic assessment regarding
physical plausibility, we use this metric that reflects both
aspect. For a given frame of hand pose to be plausible, i)
the PD metric should be less than 1.5cm (this threshold is
chosen following [29]), ii) the force error metric proposed
in 3.3 should be less than 0.1. Since PD is not available for
HO-3D, we only consider ii) for evaluation on HO-3D.
4.3. Refining sythetic Error
To employ our de-noising method in more realistic appli-
cations and achieve best potential performance, it would
be ideal to train the model on the predictions of the tar-
get tracking method to be augmented. Yet this might lead
to overfitting to tracker-specific noise, which is undesirable
for generalization. Hence we trained the network on dataset
with Gaussian noise of different composition and magni-
tude, which synthesizes the hand errors that tracking meth-
ods induce. Quantitative results are shown in Table 1 and
qualitative results are presented in Figure 4.
4.4. Refining vision-based hand tracker
We also use our network to refine the results produced by
state-of-the-art vision-based models on HO-3D dataset. To
evaluate how well our method generalize to actual tracker
error, both the de-noising auto-encoder and the two neu-
ral loss terms are trained only on GRAB with synthetic er-
rors, and then used on HO-3D without further fine-tuning.
Figure 4. Qualitative results on GRAB dataset. We can see
that TOCH produces physically implausible results such as hand-
object penetration when dealing with thin and delicate parts of ob-
jects, while our results are more realistic.
Hasson et al. [12], a RGB-based hand pose tracker is used
to predict hand poses on the test split of HO-3D dataset.
TOCH [31], Physical Interaction [14] and our method are
used to refine the results it produces. Physical Interaction is
an optimization-based method that solves for contact forces
at finger tips and the refined hand pose simultaneously. The
results are shown in Table 2 and Figure 5. Physical plau-
sibility is significantly improved, which is indicated by IV
and plausibility rate.
Table 2. Quantitative results on HO-3D dataset. The hand joint
and mesh errors are obtained after Procrustes alignment following
the official evaluation protocol of HO-3D.
MPJPE MPVPE IV plausible rate
Hasson et al. 11.4 11.4 8.75 0.71
TOCH 10.9 11.3 7.24 0.73
Physical Intercation 11.4 11.6 8.33 0.83
Ours 10.7 11.2 5.95 0.85
2347
Table 3. Ablation on the physical neural losses. The best result is highlighted in red, while the second-best result is highlighted in blue.
MPJPE MPVPE contactIoU IV PD plausible rate
GT - - - 0.75 - 0.92
TOCH 13.17 12.24 21.83 2.24 0.55 0.78
Physical Interaction 13.5 14.6 7.15 1.94 1.27 0.89
Ours(w/o Lmanip) 7.41 6.78 23.96 0.96 0.41 0.82
Ours(w/o Lgrasp) 7.28 6.65 23.51 2.43 0.59 0.81
Ours 7.39 6.78 23.94 1.13 0.44 0.91
Figure 5. Qualitative results on HO-3D dataset. Our method ef-
fectively denoises tracking result, and produces more physically
plausible hand-object interaction than TOCH.
4.5. Ablation Studies
Physically-aware loss . To demonstrate the advantages of
our proposed physically-aware loss terms in modeling and
improving physical plausibility, in stage II, we remove sig-
nals from the two neural loss terms perspectively and train
two baseline de-noising networks. Comparison between
them and our complete method on the GRAB dataset is pre-
sented in Table 3.
It can be observed that the highest overall plausible rate
is achieved when combining the two neural losses druing
training. Without the grasp credibility loss, the network
tends to more actively adhere the hand to the object surface,
increasing the contact area so that FE and ME can likely
be reduced. However, the increased manipulation feasibil-
ity is achieved at the expense of more severe hand-object
inter-penetration, indicated by IV and PD. Therefore, the
overall plausible rate which considers both aspects is lim-
ited. Removing the manipulation feasibility loss induces
the opposite result. While lowest IV and PD are attained,
cases where the object is hovering in the air appear more
frequently. This result reflects the network’s tendency of
avoiding inter-penetration regardless of the manipulation
feasibility, when only signals from the grasp credibility ispresent during the training process.
Soft target for training neural losses . When training
the physically-aware neural loss terms, we use both soft tar-
get and hard target for smooth loss landscape. To demon-
strate the advantages of this design, we train baseline neural
losses with hard target only and compare them with neural
losses trained with both targets, and present comparisons
in Table 4 regarding their classification performance on test
set and ability of improving the de-noising network when
exploited.
Table 4. We use the losses to discern implausible frames on the
test split of Perturbed GRAB and report their F-scores. While the
classification performance of losses trained with two types of tar-
get choices are close, neural losses trained with both targets yield
better results when exploited.
(a) Target used to train grasp credibility loss.
F-score PD IV
hard 0.85 0.49 1.52
hard + soft 0.93 0.44 1.13
(b) Target used to train manipulation feasibility loss.
F-score plausible rate
hard 0.90 0.84
hard + soft 0.88 0.91
5. Conclusion
We propose a physically-aware hand-object interaction
de-noising framework which combines data priors and
physics priors to generate plausible results.In particular,
our differentiable neural physical losses effectively assess
grasp credibility and manipulation feasibility of given hand
poses and form smooth loss landscape for hand poses
with different noise level, enabling physically-aware de-
noising. Experiments demonstrate that our method gen-
eralizes well to novel objects, motions and noise pat-
terns.
2348
References
[1] Adnane Boukhayma, Rodrigo de Bem, and Philip HS Torr.
3d hand shape and pose from images in the wild. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 10843–10852, 2019. 2
[2] Samarth Brahmbhatt, Cusuh Ham, Charles C Kemp, and
James Hays. Contactdb: Analyzing and predicting grasp
contact via thermal imaging. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 8709–8719, 2019. 2
[3] Samarth Brahmbhatt, Chengcheng Tang, Christopher D
Twigg, Charles C Kemp, and James Hays. Contactpose:
A dataset of grasps with object contact and hand pose. In
Computer Vision–ECCV 2020: 16th European Conference,
Glasgow, UK, August 23–28, 2020, Proceedings, Part XIII
16, pages 361–378. Springer, 2020. 6
[4] Thomas Buffet, Damien Rohmer, Loic Barthe, Laurence
Boissieux, and Marie-Paule Cani. Implicit untangling: A
robust solution for modeling layered clothing. ACM Trans-
actions on Graphics (TOG) , 38(4):1–12, 2019. 4
[5] Jiayi Chen, Mi Yan, Jiazhao Zhang, Yinzhen Xu, Xiaolong
Li, Yijia Weng, Li Yi, Shuran Song, and He Wang. Tracking
and reconstructing hand object interactions from point cloud
sequences in the wild. arXiv preprint arXiv:2209.12009 ,
2022. 1
[6] Liangjian Chen, Shih-Yao Lin, Yusheng Xie, Yen-Yu Lin,
and Xiaohui Xie. Mvhm: A large-scale multi-view hand
mesh benchmark for accurate 3d hand pose estimation. In
Proceedings of the IEEE/CVF Winter Conference on Appli-
cations of Computer Vision , pages 836–845, 2021. 2
[7] Sammy Christen, Muhammed Kocabas, Emre Aksan, Jemin
Hwangbo, Jie Song, and Otmar Hilliges. D-grasp: Physically
plausible dynamic grasp synthesis for hand-object interac-
tions. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , pages 20577–20586,
2022. 2
[8] Liuhao Ge, Zhou Ren, Yuncheng Li, Zehao Xue, Yingying
Wang, Jianfei Cai, and Junsong Yuan. 3d hand shape and
pose estimation from a single rgb image. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 10833–10842, 2019. 2
[9] Patrick Grady, Chengcheng Tang, Christopher D Twigg,
Minh V o, Samarth Brahmbhatt, and Charles C Kemp. Con-
tactopt: Optimizing contact to improve grasps. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 1471–1481, 2021. 2
[10] Erhan Gundogdu, Victor Constantin, Amrollah Seifoddini,
Minh Dang, Mathieu Salzmann, and Pascal Fua. Garnet: A
two-stream network for fast and accurate 3d cloth draping.
InProceedings of the IEEE/CVF International Conference
on Computer Vision , pages 8739–8748, 2019. 4
[11] Shreyas Hampali, Mahdi Rad, Markus Oberweger, and Vin-
cent Lepetit. Honnotate: A method for 3d annotation of hand
and object poses. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
3196–3206, 2020. 6[12] Yana Hasson, Bugra Tekin, Federica Bogo, Ivan Laptev,
Marc Pollefeys, and Cordelia Schmid. Leveraging photomet-
ric consistency over time for sparsely supervised hand-object
reconstruction. In Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition , pages 571–580,
2020. 7
[13] Yana Hasson, G ¨ul Varol, Cordelia Schmid, and Ivan
Laptev. Towards unconstrained joint hand-object reconstruc-
tion from rgb videos. In 2021 International Conference on
3D Vision (3DV) , pages 659–668. IEEE, 2021. 4
[14] Haoyu Hu, Xinyu Yi, Hao Zhang, Jun-Hai Yong, and Feng
Xu. Physical interaction: Reconstructing hand-object inter-
actions with physics. In SIGGRAPH Asia 2022 Conference
Papers , pages 1–9, 2022. 2, 6, 7
[15] Hanwen Jiang, Shaowei Liu, Jiashun Wang, and Xiaolong
Wang. Hand-object contact consistency reasoning for human
grasps generation. arXiv preprint arXiv:2104.03304 , 2021.
2
[16] Nikolaos Kyriazis and Antonis Argyros. Physically plausible
3d scene tracking: The single actor hypothesis. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition , pages 9–16, 2013. 2
[17] Jameel Malik, Ibrahim Abdelaziz, Ahmed Elhayek, Soshi
Shimada, Sk Aziz Ali, Vladislav Golyanik, Christian
Theobalt, and Didier Stricker. Handvoxnet: Deep voxel-
based network for 3d hand shape and pose estimation from
a single depth map. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
7113–7122, 2020. 2
[18] Franziska Mueller, Micah Davis, Florian Bernard, Oleksandr
Sotnychenko, Mickeal Verschoor, Miguel A Otaduy, Dan
Casas, and Christian Theobalt. Real-time pose and shape
reconstruction of two interacting hands with a single depth
camera. ACM Transactions on Graphics (TOG) , 38(4):1–13,
2019. 2
[19] Iason Oikonomidis, Nikolaos Kyriazis, and Antonis A Argy-
ros. Full dof tracking of a hand interacting with an object by
modeling occlusions and physical constraints. In 2011 Inter-
national Conference on Computer Vision , pages 2088–2095.
IEEE, 2011. 2
[20] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.
Pointnet: Deep learning on point sets for 3d classification
and segmentation. In Proceedings of the IEEE conference
on computer vision and pattern recognition , pages 652–660,
2017. 3
[21] Javier Romero, Dimitrios Tzionas, and Michael J Black. Em-
bodied hands: Modeling and capturing hands and bodies to-
gether. arXiv preprint arXiv:2201.02610 , 2022. 3
[22] Breannan Smith, Chenglei Wu, He Wen, Patrick Peluse,
Yaser Sheikh, Jessica K Hodgins, and Takaaki Shiratori.
Constraining dense hand surface tracking with elasticity.
ACM Transactions on Graphics (TOG) , 39(6):1–14, 2020.
2
[23] Srinath Sridhar, Helge Rhodin, Hans-Peter Seidel, Antti
Oulasvirta, and Christian Theobalt. Real-time hand track-
ing using a sum of anisotropic gaussians model. In 2014
2nd International Conference on 3D Vision , pages 319–326.
IEEE, 2014. 2
2349
[24] Omid Taheri, Nima Ghorbani, Michael J Black, and Dim-
itrios Tzionas. Grab: A dataset of whole-body human grasp-
ing of objects. In Computer Vision–ECCV 2020: 16th Eu-
ropean Conference, Glasgow, UK, August 23–28, 2020, Pro-
ceedings, Part IV 16 , pages 581–600. Springer, 2020. 2, 6
[25] Jonathan Taylor, Lucas Bordeaux, Thomas Cashman, Bob
Corish, Cem Keskin, Toby Sharp, Eduardo Soto, David
Sweeney, Julien Valentin, Benjamin Luff, et al. Efficient and
precise interactive hand tracking through joint, continuous
optimization of pose and correspondences. ACM Transac-
tions on Graphics (TOG) , 35(4):1–12, 2016. 2
[26] Jonathan Taylor, Vladimir Tankovich, Danhang Tang, Cem
Keskin, David Kim, Philip Davidson, Adarsh Kowdle, and
Shahram Izadi. Articulated distance fields for ultra-fast
tracking of hands interacting. ACM Transactions on Graph-
ics (TOG) , 36(6):1–12, 2017. 2
[27] Zeshi Yang, Kangkang Yin, and Libin Liu. Learning to use
chopsticks in diverse gripping styles. ACM Transactions on
Graphics (TOG) , 41(4):1–17, 2022. 2
[28] Baowen Zhang, Yangang Wang, Xiaoming Deng, Yinda
Zhang, Ping Tan, Cuixia Ma, and Hongan Wang. Interact-
ing two-hand 3d pose and shape reconstruction from single
color image. In Proceedings of the IEEE/CVF International
Conference on Computer Vision , pages 11354–11363, 2021.
2
[29] He Zhang, Yuting Ye, Takaaki Shiratori, and Taku Ko-
mura. Manipnet: neural manipulation synthesis with a hand-
object spatial representation. ACM Transactions on Graphics
(ToG) , 40(4):1–14, 2021. 2, 6, 7
[30] Zhengyi Zhao, Tianyao Wang, Siyu Xia, and Yangang Wang.
Hand-3d-studio: A new multi-view system for 3d hand re-
construction. In ICASSP 2020-2020 IEEE International
Conference on Acoustics, Speech and Signal Processing
(ICASSP) , pages 2478–2482. IEEE, 2020. 2
[31] Keyang Zhou, Bharat Lal Bhatnagar, Jan Eric Lenssen, and
Gerard Pons-Moll. Toch: Spatio-temporal object-to-hand
correspondence for motion refinement. In Computer Vision–
ECCV 2022: 17th European Conference, Tel Aviv, Israel,
October 23–27, 2022, Proceedings, Part III , pages 1–19.
Springer, 2022. 2, 3, 7
2350
