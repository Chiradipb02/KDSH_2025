Towards 3D Vision with Low-Cost Single-Photon Cameras
Fangzhou Mu†, Carter Sifferman†, Sacha Jungerman, Yiquan Li‡, Mark Han‡,
Michael Gleicher, Mohit Gupta, Yin Li
†co-ﬁrst author,‡equal contribution
University of Wisconsin-Madison
Low-Cost Proximity Sensor Captured Scene Neural SDF Scene Re construction
Captured TransientsRendered Transients
Minimize
5mmBreakout BoardSensor
(TMF8820)
Figure 1. We demonstrate that measurements from spatially distributed low-cost single-photon proximity sensors (left) can be used to
reconstruct 3D shape of real world objects (right). Our method combines a differentiable image formation model and neural rendering
to recover 3D geometry based on measurements (transient histograms) from sensors with known poses. This is done by minimizing the
difference between the observed and rendered sensor measurements. For clarity, a subset of sensor poses and measurements are shown.
Abstract
We present a method for reconstructing 3D shape of
arbitrary Lambertian objects based on measurements by
miniature, energy-efﬁcient, low-cost single-photon cam-
eras. These cameras, operating as time resolved image
sensors, illuminate the scene with a very fast pulse of dif-
fuse light and record the shape of that pulse as it returns
back from the scene at a high temporal resolution. We pro-
pose to model this image formation process, account for its
non-idealities, and adapt neural rendering to reconstruct
3D geometry from a set of spatially distributed sensors with
known poses. We show that our approach can successfully
recover complex 3D shapes from simulated data. We fur-
ther demonstrate 3D object reconstruction from real-world
captures, utilizing measurements from a commodity prox-
imity sensor. Our work draws a connection between image-
based modeling and active range scanning, and offers a
step towards 3D vision with single-photon cameras. Our
project webpage is at https://cpsiff.github.io/
towards_3d_vision/ .
1. Introduction
Reconstructing 3D shape of real objects remains a central
problem in vision, solutions to which have evolved into
two parallel branches. Image-based modeling [45] lever-
ages a plethora of visual cues from multiple photographs(e.g., stereo, motion, shading), leading to problems includ-
ing multi-view stereo [ 42], photometric stereo [ 1] and the
more recent neural radiance ﬁelds (NeRF) [ 33]. Conversely,
active range scanning [22] combines an active light source
with an imaging sensor, giving rise to imaging techniques
such as structured light [ 13], and time-of-ﬂight [ 16]. Con-
ventional wisdom suggests that range scanning yields more
precise 3D geometry than image-based modeling at the cost
of using specialized, expensive hardware.
An emerging approach for range scanning is direct time-
of-ﬂight imaging with active single-photon cameras , a form
of time-resolved image sensor. This approach couples
a pico-to-nanosecond detector with a fast coherent light
source, illuminates the scene with a very short pulse of light,
and measures the intensity of the light over time as it reﬂects
back from the scene. The resulting incident wavefront is
recorded and quantized, forming a transient histogram . A
special case of this approach is single-photon LiDAR, in
which the light source (laser) is highly focused, the detec-
tor ﬁnds the peak in the histogram, and the sensor reports a
single distance value per detector pixel. When using a dif-
fuse light source, these time-resolved sensors capture visual
information beyond the distance measurements extracted in
LiDAR. Transient histograms in this case record distribu-
tions of times-of-ﬂight, encoding the product of scene ge-
ometry and reﬂectance over each imaged scene patch [ 23].
The entirety of information in the transient histogram has
been previously utilized in applications such as ﬂuorescence
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
5302
lifetime imaging [ 26] and non-line-of-sight imaging [ 48].
Recently, time-resolved measurements from such sensors
have been explored for line-of-sight 3D reconstruction and
novel view synthesis [ 30]. Unfortunately, these systems,
due to their prototype-like nature, exist in a bulky benchtop
form factor, require signiﬁcant power, and are costly (over
$10,000 USD). They are thus not suited for many appli-
cations including autonomous drones, wearable computing,
and augmented reality, in which low-cost, energy efﬁcient,
miniature sensors are required.
Low-cost single-photon cameras have recently become
available in the form of active single photon avalanche
diodes (SPADs). They include one or more SPADs paired
with an eye-safe diffuse light source ( e.g., an infrared VC-
SEL laser), are very small ( <20 mm3), inexpensive ( <$5
USD), and power efﬁcient ( <10 milliwatts per measure-
ment) [ 2,32]. These are sold as proximity sensors and some
can be conﬁgured to report transient histograms. Com-
pared to laboratory-grade systems, however, they lack pre-
cise optics, calibration, and timing characteristics and have
an order of magnitude lower spatial and temporal resolu-
tion. Still, these sensors have proven successful for material
classiﬁcation [ 4], human pose recognition [ 41], and simple
shape recovery ( i.e., a planar surface) [ 23,43].
In this work, we address the problem of reconstruct-
ing 3D shape of arbitrary Lambertian objects from a set
of spatially distributed low-cost single-photon cameras with
known poses. We present an approach that combines a neu-
ral signed distance ﬁeld surface representation, a differen-
tiable transient formation model for practical active single-
photon cameras, and an optimization scheme following the
analysis-by-synthesis pipeline. Fig. 1illustrates our sen-
sor, imaging setting, and approach. We show that our ap-
proach can successfully recover complex 3D shapes from
simulated data. We further demonstrate 3D object recon-
struction from real-world captures, utilizing measurements
from a low-cost, off-the-shelf proximity sensor.
Our approach draws a connection between image-based
modeling and active range scanning while avoiding some
of their pitfalls. By using an active light source and SPADs,
our approach illuminates and images an object from mul-
tiple angles, resembling the imaging setup of multiview
photometric stereo. Our model also echoes the design of
NeRF. The key difference is that our model considers an
input of low spatial resolution ( i.e., hundreds of pixels per
scene), and leverages rich temporal information encoded in
the transient histograms. Unlike image-based modeling, our
approach can operate under low light conditions, and recon-
struct objects without texture. Unlike most range scanning
methods, our approach performs favorably in the presence
of strong ambient ﬂux and is robust to mildly specular sur-
faces (see top of the spray bottle in Fig. 1).
Practical Applications. Our approach presents a compact,energy-efﬁcient, and low-cost solution for 3D sensing when
relative sensor poses are known. We envision applying our
method to settings where relative sensor poses are ﬁxed, and
3D geometry can be estimated on a per-frame basis by com-
bining measurements from a distributed set of sensors. This
includes applications like wearable hand tracking [ 10,20],
and collision avoidance and mapping for drones, mobile
robots, and robot manipulators. This work provides a ﬁrst
step towards enabling such applications.
Scope and limitation. Similarly to other methods for active
range scanning, the proposed method fails on highly specu-
lar objects and requires hundreds of views to reconstruct a
complex 3D shape. Our imaging system is tailored for low-
cost sensors with limited range (4-6 meters) and temporal
resolution (over 100 picoseconds), and thus it is unproven
at imaging larger space or ﬁner details. While methods exist
for fast NeRF [ 35], our approach is not optimized for speed
and takes a few hours to reconstruct a single object. Instead,
we focus on demonstrating the feasibility of reconstructing
real-world 3D objects using commodity sensors.
2. Related Work
Time-resolved imaging. Time-resolved sensors have long
been used in applications such as non-line-of-sight (NLOS)
imaging [ 11,50], where a scene is recovered from around
the corner, ﬂuorescence lifetime imaging [ 27], a mi-
croscopy technique for characterizing a biological sample,
and to measure distance using direct time-of-ﬂight [ 15,17].
Recent techniques [ 30] used high-end time-resolved sensors
to directly recover scene geometry. However, these methods
rely on hardware prototypes which are often prohibitively
expensive and not accessible to consumers. For example,
Transient NeRF [ 30] performs view synthesis and 3D re-
construction using a lab prototype with no ambient light and
2-5views of512×512transient histograms, each with 1500
bins. In contrast, in this work, we use commodity hardware
with ambient light and 128-240total transients, each com-
prised of 128bins.
3D Imaging with low-cost SPADs. With cheap time-
resolved proximity sensors becoming commonplace, recent
works have investigated their use for 3D reconstruction.
Callenberg et al . [6] demonstrate that, with some addi-
tional hardware, high-resolution depth imaging from a sin-
gle viewpoint is possible. A low-cost SPAD has been used
to augment an RGB SLAM system [ 51]. Other works uti-
lize supervised machine learning to recover geometric in-
formation from single low-cost SPAD measurements, such
as 3D human pose [ 41] or high-resolution depth images
[23,53]. Jungerman et al. [23] use differentiable rendering
to recover two degrees of freedom of a planar surface from
a single low-cost SPAD transient histogram. Sifferman et
al. [43] extend this method to fully recover a planar surface
5303
using a low-cost SPAD with multiple detector pixels.
Neural implicit representations. Neural representations,
as popularized by NeRF [ 33], enable novel view synthe-
sis and 3D reconstruction by representing the scene as a
neural network. While the original NeRF representation
encoded view-dependent volumetric effects, alternative en-
codings have been proposed to better model geometry and
reconstruct surfaces. NeuS [ 49] represents the scene as
a level set, allowing for better modeling of surfaces at
the expense of not being able to represent volumetric ef-
fects. Many works extend these ideas to work with dif-
ferent sensing modalities and external supervision, such as
depth queues from structure-from-motion [ 9], RGB images
plus continuous-wave time-of-ﬂight sensors [ 3], only depth
information [ 29,37], or more recently using only tran-
sients [ 21,30]. In this work, we perform 3D reconstruction
using only transient histograms as captured using commod-
ity hardware by adapting the implicit surface representation
introduced by NeuS to render transients.
Multiview photometric stereo. Our approach illuminates
and images an object from multiple views in order to recon-
struct its 3D shape, having conceptual similarity to image-
based modeling methods. This concept has previously been
explored as multiview photometric stereo (MVPS) [ 7,18]:
reconstructing 3D geometry given distributed views of the
scene and various lighting conditions. More recent ap-
proaches resort to deep learning models [ 55] and consider
NeRF-based representations [ 24,52].
3. 3D Vision with Single-Photon Cameras
We propose to reconstruct 3D scene geometry using a
sparse set of measurements from low-cost time-resolved
SPAD sensors. Our approach assumes a distributed set
ofNSPAD sensors with known pose, each comprising a
single-pixel detector co-located with a diffuse laser. The
time of ﬂight, t, of returning photons, received over the
detector FoV1is recorded and binned into transient his-
tograms [23,43],{hj}N
j=1: time-resolved measurements
that encode rich information about the scene geometry and
reﬂectance. Our goal is to recover scene geometry from
these measurements.
Traditional depth ranging. Our setup differs from the
conventional use of SPADs for depth ranging [ 15,17,30],
which assumes a collimated laser and focused detector.
There, the depths of individual scene points can be indepen-
dently estimated from separate transients, and a dense scan
of thousands of scene points is needed to obtain full scene
geometry. These systems require high-quality beam lasers
and detectors with high temporal resolution. In contrast,
our imaging conﬁguration is indicative of the real-world use
1We assume that the illumination and detector FoV coincide and refer
to both as the sensor FoV in the rest of the paper.cases of low-cost SPAD sensors with diffuse lasers , wide
FoVs, and low-resolution detectors. In our case, the tran-
sient encodes rich information beyond its peak location.
Low-cost SPAD sensing. Our setup extends recent sys-
tems [ 23,43] built around low-cost SPAD sensors for 3D
sensing in two ways. First, our system consists of multi-
ple posed sensors as opposed to a single sensor. Second,
it is capable of capturing and reconstructing complex, non-
parametric scenes as opposed to the parametric geometry of
a single plane. With these upgrades, our system is reminis-
cent of multi-view stereo and NeRF-like systems for con-
ventional RGB cameras, and is a substantial step towards
practical 3D vision using commodity proximity sensors.
3.1. Transient Formation Model
We model the transient formation process in two steps. We
ﬁrst derive the transient waveform of a scene by modeling
the interaction of light with scene geometry. The next step
uses this waveform as the input to a sensor model that ac-
counts for non-linearities that occur when capturing tran-
sient histograms with real hardware.
Transient waveform. As in [ 23,43], we ignore high-order
light paths and only model direct reﬂection from the object
surface. The transient waveform τs(t)of a sensor slocated
at the origin can be written in path integral form [ 47] as
τs(t) =/integraldisplay
Sρ(x)G(x,ω)
∥x∥4δ/parenleftbigg
∥x∥2−ct
2/parenrightbigg
dA(x),
G(x,ω) =fr(x,nx,ω)V(x)⟨ω,nx⟩2,(1)
wheredA(x)is an inﬁnitesimal area around point xon the
object surface S,ρis the albedo, δis the Dirac delta func-
tion,cthe speed of light, and ωis a ray direction from xto
s. Time-independent effects such as V, the visibility func-
tion governed by scene geometry and sensor FoV , frthe
bidirectional reﬂectance distribution function (BRDF), and
foreshortening effects where nxis the surface normal at x,
are modeled as G(x,ω).
Sensor model. The sensor model accounts for laser and de-
tector characteristics when converting a transient waveform
into a transient histogram [ 19]. Our sensor model consid-
ers the laser pulse, laser power, detector quantum efﬁciency,
ambient photon ﬂux, internal detector noise, pile-up effect,
and time jitter. In practice, the laser pulse is not a perfect
impulse and, despite bandpass ﬁlters, the measured tran-
sient also captures some constant ambient light. To model
this, we convolve τ(t)2with the laser’s impulse response
g(t), scaled by φscalewhich absorbs laser power and quan-
tum efﬁciency of the detector, and then offset its intensity by
φbkgdwhich encapsulates ambient photon ﬂux and internal
detector noise:
˜τ(t) =φscale(τ∗g)(t)+φbkgd. (2)
2We drop the subscript shereafter for clarity.
5304
Sample Points in FoV Optimization
Observed
RenderedPer-Ray Transient
Transient Rendering Sensor Modeling
Laser Impulse
 Transient Waveform over Figure 2. Method Overview: The scene is modeled as a neural implicit surface in the form of an SDF. To render a transient, we approximate
Eq.8by sampling rays within each pixel’s FoV , and subsequently points on those rays. This idealized transient waveform is then convolved
with the sensor’s laser impulse response to model the transient histogram formation. Finally, we optimize the scene representation by
minimizing a loss between the rendered transients and the observations.
˜τ(t)is subsequently discretized into a histogram of Poisson
ratesr= [r1,...,rB]withBbins. The probability qiof at
least one photon falling inside the ithbin is given by [ 8]
qi= 1−exp(−ri). (3)
In practice, SPADs aggregate photon counts over Claser
cycles, with only the ﬁrst incident photon being detected in
each cycle. This results in pile-up , a nonlinear distortion of
transients, leaving photons arriving at a later timestamp less
likely to be detected [ 39]. Speciﬁcally, the probability piof
detecting a photon in the ithbin in a cycle is given by [ 39]
pi=qiΠi−1
k=1(1−qk). (4)
The photon counts [h1,...,hB]in a transient histogram ˜h
follow a multinomial distribution:
[h1,...,hB+1]∼Multinomial( C,(p1,...,pB+1)),(5)
wherepB+1= 1−/summationtextB
i=1pi, andhB+1counts the number
of cycles without detected photons. ˜his subsequently con-
volved with a discretized time jitter kernelsto account for
the temporal uncertainty of photon detection events, yield-
ing the ﬁnal histogram hmeasured by a SPAD detector:
h[b] = Σkh[k]s[b−k]. (6)
We now present a reconstruction algorithm based on the
analysis-by-synthesis principle to recover scene geometry
from the transients. The input to this system is a distributed
set of posed transients {hj}N
j=1captured of the scene.
3.2. Neural Scene Reconstruction from SPADs
The reconstruction problem that we attempt to solve is ex-
tremely challenging. Unlike in conventional depth ranging
where the depth of a scene point can be directly determined
from the histogram via peak ﬁnding, a transient from our
system represents the superposition of light reﬂected from
numerous scene points, as illuminated by a diffuse laser,
and is further contaminated by non-idealities of the detector
(e.g., pile-up). The direct inversion of the signal is thus ahighly ill-posed problem. Further, we cannot adapt meth-
ods from the NLOS imaging literature [ 28,38,50] as they
only support dense 2D scans, whereas our system uses a
distributed, sparse and unstructured set of measurements.
To overcome these challenges, we resort to an analysis-
by-synthesis approach based on differentiable rendering.
Our approach allows ﬂexible positioning of sensors and ac-
curate modeling of histogram formation, thereby enabling
high-quality reconstruction of scene geometry. We now de-
scribe our reconstruction algorithm in detail.
Neural scene representation. Following NeuS [ 49], we
represent the scene geometry as a signed distance function
(SDF), parameterized as a multi-layer perceptron (MLP)
fθ:R3→R.fθmaps the position-encoded (PE) xyz-
coordinates of a point xto its signed distance d:
d=fθ(PE(x)). (7)
Compared to [ 23,43], this neural SDF allows our method to
represent scene geometry beyond simple parametric shapes
as the level set S={x∈R3|fθ(x) = 0}.
Transient volume rendering. The key idea behind our
analysis-by-synthesis approach is to render fθinto tran-
sients and compare them with those captured by our system.
To adapt Equation 1for the rendering of fθ, we ﬁrst rewrite
it in angular integral form as
τ(t) =/integraldisplay
Ωρ
πV(x)⟨−ω,nx⟩
∥x∥2δ/parenleftbigg
∥x∥2−ct
2/parenrightbigg
dω,(8)
whereωare ray directions in the sensor FoV Ω, andxthe
point where ωintersects with the object surface S(∞if no
intersection). For simplicity, we assume a learned spatially
uniform albedo ρand Lambertian BRDF fr= 1/π3.
Inspired by NeRF [ 33] and NeuS [ 49], we approximate
Equation 8via volume rendering to resolve surface disconti-
nuities, enabling the optimization of θvia gradient descent:
ˆτ(t) =/integraldisplay
Ωρ
πT2(t)σ(p(ω,t))⟨−ω,np⟩
∥p(ω,t)∥2dω. (9)
3This assumption may be relaxed to allow more expressive BRDF mod-
els such as the Phong reﬂection model [ 5].
5305
Here,p(ω,t) =ct/2ωare points along ω, the volume den-
sityσis a function of fθas in NeuS [ 49], and the transmit-
tanceTis given by
T(t) = exp/parenleftbigg
−/integraldisplayt
0σ(p(u))du/parenrightbigg
. (10)
In practice, we discretize ˆτ(t)over the transient bin in-
tervals{[ti,ti+1)}B
i=1and work with the histogram ˆτ=
[ˆτ1,...ˆτB], where
ˆτi=/integraldisplay
Ωρ
π/integraldisplayti+1
tiT2(t)σ(p(ω,t))⟨−ω,np⟩
∥p(ω,t)∥2dtdω.(11)
We estimate the intractable Equation 11via Monte Carlo
sampling of ωand subsequently of p(ω,t).
Bilevel importance sampling. Similar to [ 33,49], the sam-
pling ofp(ω,t)is weighted by a probability density func-
tion (PDF) over the equally sized bin intervals. This PDF is
proportional to the per-bin weights wigiven by
wi= exp/parenleftbig
−Σi−1
j=1σj∆/parenrightbig
(1−exp(−σi∆)), (12)
whereσiis evaluated at the mid-point of the ithbin, and∆
is the bin size in distance.
We extend this idea to the importance sampling of ω.
Speciﬁcally, the sampling PDF over a uniform partitioning
of FoVΩis proportional to the cumulative weights w(k)
over rays ω(k)drawn from each partition k:
w(k)= ΣB
i=1w(k)
i. (13)
Intuitively, this allows us to point more rays at high-density
regions occupied by the object surface.
Differentiable sensor modeling. Modeling sensor behav-
ior is particularly important for our analysis-by-synthesis
approach. This is because the synthesis targets τare not
determined by the scene geometry alone but reﬂect the com-
plex interplay of geometry with sensor non-idealities in-
cluding pulse shape, pile-up and time jitter. To this end,
we cascade ˆτto adifferentiable sensor model Γto simulate
the transformation applied by the sensor to raw waveforms.
Speciﬁcally, Γclosely follows the sensor model in Sec-
tion 3.1; Equations 2-4are differentiable and applied se-
quentially on ˆτ, yielding per-bin photon detection probabil-
itiesˆp= [ˆp1,...,ˆpB]. Instead of sampling photon counts
using Equation 5, we directly convolve ˆpwith the jitter ker-
nel as in Equation 6. This allows us to sidestep the non-
differentiable sampling step while producing an unbiased
estimate of the transient ˆh= Γ(ˆτ)for loss evaluation.
Loss functions. The optimization of θis driven by three
loss terms: a histogram reconstruction loss Lhistthat mini-
mizes the L1 distance between ˆhandh, an Eikonal loss [ 14]LEikonal that regularizes the SDF, and a total variation reg-
ularizer [ 34]LTVthat penalizes ﬂoaters in empty space.
The combined loss function Lis thus given by
L=Lhist+λEikonalLEikonal+λTVLTV, (14)
whereλEikonal andλTVare the respective loss weights.
Comparison to NLOS-NeuS [ 12].NLOS-NeuS is a con-
current work for scene reconstruction using SPAD sensors.
Despite similarities in the imaging model and reconstruc-
tion algorithm, the two works differ substantially in ap-
plication (direct line-of-sight vs. non-line-of-sight), laser
characteristics (eye-safe diffuse laser vs. high-energy beam
laser), sensor quality and cost (commodity, low-cost vs.
laboratory-grade, high-cost), transient modeling (with vs.
without sensor non-idealities) and scan pattern (distributed
and sparse vs. dense and structured 2D grid). We believe
the two works complement each other and together unveil
an exciting avenue toward single-photon 3D vision systems.
4. Experiments
We demonstrate the effectiveness of our method for 3D ge-
ometric reconstruction of various objects in simulation, and
in the real world with a low-cost SPAD. We provide quali-
tative and quantitative results for both settings. See the sup-
plement for implementation details ( e.g. learning rates).
Baselines. We compare our method to two baselines: re-
projection [ 15,17] and space carving [ 25,46].
Reprojection , also known as back-projection, recon-
structs a scene as a point cloud and is the de facto standard
for depth ranging. We compare to two forms of reprojec-
tion. The peak method ﬁnds the distance dcorresponding
to the histogram bin with the highest intensity. For a sen-
sor at position swith an outwards pointing optical axis u, a
point is placed in the scene at position s+du. The threshold
method works in the same way but ﬁnds dby locating the
lowest-index bin with intensity above a threshold tp. If no
bin passes the threshold, no point is projected. We do not
apply surface reconstruction to the generated point clouds,
as it is often unreliable for poorly imaged scenes. See the
supplement for further discussion.
Space carving reconstructs a scene as a voxel grid. Like
thresholded reprojection, it ﬁnds the distance dcorrespond-
ing to the lowest-index bin with intensity above a threshold
ts. All voxels in the sensor’s FoV and nearer than dare
marked empty, along with voxels outside the FoV . V oxels
in the FoV and further than dare marked as occupied. The
carved scene is the union of the occupied set for all sensors.
To ensure strong baselines for real-world experiments,
we perform a brute-force search over tpandtsand choose
values that minimize Chamfer distance over the entire real-
world dataset. Space carving voxel size was set to 1.0cm.
5306
Chamfer Distance (mm) ↓
Method Armadillo Bear Bunny Digit Einstein Skull Soap Sphere
Reprojection (Peak) 54.29 40.36 34.95 55.85 43.25 48.90 51.71 51.07
Reprojection (Threshold) 65.43 60.72 54.05 60.64 61.31 65.14 68.74 63.16
Space Carving 34.78 24.53 22.29 45.44 26.60 25.49 21.44 25.47
Ours 3.93 5.95 3.84 3.27 3.51 3.22 3.23 3.77
Table 1. Quantitative results on simulated data . Our method more accurately recovers 3D shapes than baselines across 8objects.
Reference ImageEinstein Digit BunnyOur Method Space Carving Reprojection (Threshold) Reprojection (Peak)
Figure 3. Qualitative results on simulated data . Our method reconstructs dense and detailed 3D shapes. Space carving provides only
hulls of a target shape, and is prone to carving away extra space when thin structures are present. Reprojection yields sparse points.
Evaluation protocol. Following NeuS [ 49], we evaluate
all methods using Chamfer distance. We report standard
(two-way) Chamfer on simulated data. For real-world cap-
tures, we report Chamfer in both directions to evaluate the
quality of reconstruction. Prior to Chamfer calculation, we
convert ground-truth meshes and reconstructions from our
method to point clouds by drawing 5million points uni-
formly at random on the mesh surface. For space carving,
occupied voxels are converted to points if they touch unoc-
cupied space, excluding the edge of the grid.
4.1. Simulated Experiments
Experiment setup. We simulate transients for eight scenes
of varying complexity using the image formation model in
Section 3.1. The objects are centered on the ground plane
(z= 0) with the largest dimension ≈0.3m. Sensors with
a conical FoV are uniformly distributed on a hemisphere at
the origin with a radius of 0.5m, and are all pointed at the
origin. In our simulation, N= 256 ,B= 256 ,∆ = 5mm ,
FoV = 30◦,φscale= 1 ,φbkgd= 0.001,C= 5000
andρ= 0.8. The laser pulse, g, has a full-width-at-half-
maximum (FWHM) of 50ps , andsis a tabulated PDF ob-
tained from experiments [ 19]. The sensor parameters are
deliberately chosen to reﬂect the characteristics of low-costsensors. See the supplement for a sensitivity study on how
these parameters affect the quality of reconstruction.
Results. Table 1summarizes the quantitative results of all
methods. Our method achieves an average Chamfer dis-
tance of<5mm , an order of magnitude lower than all base-
lines. A key reason is that the baselines only use depth in-
formation from a single histogram bin, whereas our method
makes effective use of the entire waveform, which contains
rich geometry cues about a large scene patch.
We provide visualizations of our results in Figure 3. Our
method recovers global scene structure as well as local ge-
ometry details. In contrast, reprojection yields sparse point
clouds without sufﬁcient coverage of the scene. While
space carving produces dense reconstructions, the occu-
pancy grid only represents an envelope of the scene, leaving
it difﬁcult to recognize the precise shape of an object.
4.2. Hardware Prototype
We use the SPAD-based AMS TMF8820 proximity sen-
sor [2], which retails for $10 USD. We connect the sensor to
a microcontroller via I2C and use the AMS-provided driver
to extract transient histograms.
The sensor contains a total of 216 SPADs, which are
pooled onboard the sensor into 3×3zones, each of which
5307
Chamfer Distance (mm) ↓Rec→GT / GT→Rec
Method Big Box∗Block∗Pyramid∗Toy Container†Cereal Box†
Reprojection (Peak) 77.4 24.9/52.5 51.8 12.8/39.0 94.7 17.5/77.1 71.0 24.9/46.0 49.3 17.3/31.9
Reprojection (Threshold) 67.5 14.8/52.7 52.3 8.5/43.8 75.4 5.9/69.5 52.4 8.9/43.5 51.6 19.2/32.4
Space Carving 67.9 35.1/32.8 69.2 33.4/35.8 80.1 39.5/40.6 98.9 52.8/46.0 44.1 24.4/19.6
Ours 12.5 6.1/6.4 9.8 5.6/4.2 18.4 9.0/ 9.3 11.5 5.8/5.6 16.3 8.3/8.0
Table 2. Quantitative results on real-world captures . Our method more accurately reconstructs real-world objects with homogeneous
(*) and rich (†) texture. Reprojection yields sparse and unevenly distributed points, harming one-way Chamfer from GT to reconstruction.
MicrocontrollerTMF8820
Sensor
3D Printed
MountTarget ObjectRobot Arm
Figure 4. To capture real-world data from a wide set of viewpoints,
we mount the TMF8820 proximity sensor to a robot arm. Forward
kinematics of the robot are used to gather sensor pose.
images a different FoV . The sensor captures one transient
histogram for each zone. We pool histograms from all
zones, which is equivalent to capturing one wide-FoV his-
togram per-measurement as SPADs do not suffer from read-
out noise [ 54]. In doing so, we avoid inter-histogram inter-
ference previously observed by [ 43] and avoid the need to
model individual ﬁelds-of-view of the sensor, which we em-
pirically observed to have soft and poorly speciﬁed bound-
aries. We slightly modify our method to accommodate the
AMS TMF8820 sensor used in real-world experiments.
Laser Impulse. The laser impulse response of the
TMF8820 is not Gaussian and varies slightly between mea-
surements. Fortunately, the sensor captures the shape of
its laser impulse for each measurement in a “reference
histogram”. We record this histogram for each measure-
ment and incorporate it into our forward model by cross-
correlating the idealized scene response with this recorded
reference histogram. We observe that the bin size ∆rof
the reference histogram is smaller than the bin size ∆of
the transient histograms captured by the sensor. To account
for this, we scale the reference histogram in the temporal
dimension by a factor ∆r/∆before cross-correlation. Fur-
ther, we ﬁnd that it is necessary to temporally shift the refer-
ence histogram by a ﬁxed amount φdelaybefore correlation.
To calibrate the parameters ∆,∆r, andφdelay, we per-
form the one-off intrinsic calibration procedure separately
introduced by [ 43]. The TMF8820 sensor is pointed at a
planar surface from a range of known distances and angles-
of-incidence. A differentiable render-and-compare method
is used to optimize for the unknown sensor intrinsic param-
eters given known planar geometry.Pile-up Correction. While our forward model assumes that
the target transients exhibit nonlinear distortion due to pile-
up, the TMF8820 sensor performs pile-up correction on-
sensor, and it cannot be disabled. To accommodate this,
we incorporate the differentiable Coates’ correction [ 8] as a
ﬁnal step in the forward model.
Other Sensors. Our design can be easily applied to any
SPAD with a co-located diffuse illumination source, includ-
ing other low-cost sensors [ 31], high-end setups [ 23,36],
and things in between. The only modiﬁcation necessary is
the calibration of sensor intrinsics ( e.g., using [ 43]).
4.3. Real­world Experiments
Experiment Setup. We capture a real-world tabletop
dataset of eight objects4of varying geometry and texture.
To capture many posed views of the target object, we attach
the sensor to a Universal Robots UR5 robot arm. We pro-
gram the arm to automatically move to a set of poses and
record sensor measurements at each pose. To obtain sensor
poses, we use the forward kinematics of the robot, which
are accurate to ±0.5mm [ 40]. Each object is captured from
between 128 and 240 viewpoints. Five of the objects are
simple geometric primitives, for which we manually gener-
ate ground-truth meshes based on the dimensions of the tar-
get object and measurements of its position from the robot’s
forward kinematics. Meshes are trimmed to an axis-aligned
bounding box 16cm larger than the target object in each di-
mension before the Chamfer distance calculation.
Results. As seen in Table 2, our method outperforms all
baselines by a wide margin as measured by two-way Cham-
fer distance. While reprojection is at times competitive in
one-way distance from reconstruction to ground truth, it
performs poorly in the opposite direction due to the sparse
and unevenly distributed point cloud generated, as visual-
ized in Figure 5. While space carving outperforms repro-
jection on simulated data under a highly structured sensor
pose distribution ( i.e. all sensors are facing the center of the
object), it yields poor results on real-world scenes, in which
we vary sensor orientation by ±10◦to emulate real-world
capture conditions and increase coverage. By contrast, our
method beneﬁts from the more varied sensor poses as is
shown in Figure 6.
4See supplement for full results over the entire dataset.
5308
Reference ImageSpray Bottle Toy Container Pyramid Big BoxOur Method Space Carving Reprojection (Threshold) Reprojection (Peak) Ground Truth + Poses
240 poses208 poses240 poses
240 poses
(no GT)
Figure 5. Qualitative results on real-world captures . Our method again attains the highest reconstruction quality. Poses in column two
are subsampled by a factor of two for clarity. See supplement for additional qualitative results.
Sensors Face Center
Space Carving Space Carving Ours Ours±10° Variation in Sensor Angle
Chamfer: 26.5 Chamfer: 18.4 Chamfer: 80.1 Chamfer: 40.6
Figure 6. Space carving performs more poorly when sensor poses
include some variation in target point, while our system takes ad-
vantage of the increased view diversity and coverage.
Bust
Reference Ours Space Carving Reference Ours Space CarvingKettle
Figure 7. Because our reconstruction method assumes a Lamber-
tian surface, it fails to reconstruct highly specular scenes, such as
a glossy white bust (left) or mirror-ﬁnish kettle (right).
Further, our method is surprisingly robust to violation of
assumptions made about surface reﬂectance; it successfully
reconstructs non-Lambertian objects with rich texture de-
spite assuming Lambertian BRDF with a spatially uniform
albedo. These include both simple shapes (Toy Container in
Figure 5and Cereal Box) and challenging objects with com-
plex geometry (Spray Bottle in Figure 5). We hypothesizethat the overlapping FoVs of distributed sensors help con-
strain the optimization of our model and encourage a plau-
sible reconstruction that best explains all transients. Our
strong results on simulated and real-world data validate our
modeling approach and demonstrate a single-photon 3D vi-
sion system for real-world scene reconstruction.
5. Discussion and Future Work
Despite assuming spatial uniform reﬂectance and albedo,
our method is robust to rich textures (Fig. 5) and compares
favorably to baseline methods for reconstructing challeng-
ing scenes with high specularities (Fig. 7). Future work will
investigate recovering spatially varying reﬂectance or incre-
mental learning of geometry as new measurements become
available [ 37,44], enabling applications like real-time map-
ping and SLAM. Our method may be particularly relevant
in applications such as robotics and wearable computing,
where the small size, low power requirements, and robust
hardware of proximity sensors are very valuable.
Acknowledgements: This work was supported by Los Alamos
National Lab and the Department of Energy, Wisconsin Alumni
Research Foundation, NSF CAREER Award 1943149, NSF grant
CNS-2107060, and a SONY Faculty Innovation Award. Gleicher
and Gupta hold concurrent appointments at Amazon and Cruise,
respectively. This work is not associated with either company.
5309
References
[1] Jens Ackermann and Michael Goesele. A survey
of photometric stereo techniques. Foundations and
Trends® in Computer Graphics and Vision , 2015.
[2] AMS OSRAM AG. TMF882X Datasheet . AMS OS-
RAM AG.
[3] Benjamin Attal, Eliot Laidlaw, Aaron Gokaslan,
Changil Kim, Christian Richardt, James Tompkin,
and Matthew O’Toole. T ¨oRF: Time-of-ﬂight radiance
ﬁelds for dynamic scene view synthesis. NeurIPS ,
2021.
[4] Cienna N Becker and Lucas J Koerner. Plastic classiﬁ-
cation using optical parameter features measured with
the tmf8801 direct time-of-ﬂight depth sensor. Sen-
sors, 2023.
[5] Phong Bui-Tuong. Illumination for computer gener-
ated pictures. CACM , 1975.
[6] Clara Callenberg, Zheng Shi, Felix Heide, and
Matthias B Hullin. Low-cost spad sensing for
non-line-of-sight tracking, material classiﬁcation and
depth imaging. ACM Transactions on Graphics
(TOG) , 2021.
[7] Ziang Cheng, Hongdong Li, Yuta Asano, Yinqiang
Zheng, and Imari Sato. Multi-view 3D reconstruction
of a texture-less smooth surface of unknown generic
reﬂectance. In CVPR , 2021.
[8] PB Coates. The correction for photonpile-up’in the
measurement of radiative lifetimes. Journal of Physics
E: Scientiﬁc Instruments , 1968.
[9] Kangle Deng, Andrew Liu, Jun-Yan Zhu, and Deva
Ramanan. Depth-supervised nerf: Fewer views and
faster training for free. In CVPR , 2022.
[10] Nathan Devrio and Chris Harrison. DiscoBand: Mul-
tiview depth-sensing smartwatch strap for hand, body
and environment tracking. In UIST , pages 1–13, 2022.
[11] Daniele Faccio, Andreas Velten, and Gordon Wet-
zstein. Non-line-of-sight imaging. Nature Reviews
Physics , 2020.
[12] Yuki Fujimura, Takahiro Kushida, Takuya Funatomi,
and Yasuhiro Mukaigawa. NLOS-NeuS: Non-line-of-
sight neural implicit surface. In ICCV , 2023.
[13] Jason Geng. Structured-light 3D surface imaging: a
tutorial. Advances in optics and photonics , 2011.
[14] Amos Gropp, Lior Yariv, Niv Haim, Matan Atzmon,
and Yaron Lipman. Implicit geometric regularization
for learning shapes. In ICML , 2020.
[15] Anant Gupta, Atul Ingle, Andreas Velten, and Mohit
Gupta. Photon-ﬂooded single-photon 3D cameras. In
CVPR , 2019.
[16] Miles Hansard, Seungkyu Lee, Ouk Choi, and
Radu Patrice Horaud. Time-of-ﬂight cameras: prin-
ciples, methods and applications . Springer Science &
Business Media, 2012.[17] Felix Heide, Steven Diamond, David B Lindell, and
Gordon Wetzstein. Sub-picosecond photon-efﬁcient
3D imaging using single-photon sensors. Scientiﬁc re-
ports , 2018.
[18] Carlos Hernandez, George V ogiatzis, and Roberto
Cipolla. Multiview photometric stereo. TPAMI , 2008.
[19] Quercus Hernandez, Diego Gutierrez, and Adrian
Jarabo. A computational model of a single-photon
avalanche diode sensor for transient imaging. arXiv
preprint arXiv:1703.02635 , 2017.
[20] Fang Hu, Peng He, Songlin Xu, Yin Li, and Cheng
Zhang. FingerTrak: Continuous 3d hand pose tracking
by deep learning hand silhouettes captured by minia-
ture thermal cameras on wrist. IMWUT , 4(2):1–24,
2020.
[21] Shengyu Huang, Zan Gojcic, Zian Wang, Fran-
cis Williams, Yoni Kasten, Sanja Fidler, Konrad
Schindler, and Or Litany. Neural LiDAR ﬁelds for
novel view synthesis. In CVPR , 2023.
[22] Ray A Jarvis. A perspective on range ﬁnding tech-
niques for computer vision. TPAMI , 1983.
[23] Sacha Jungerman, Atul Ingle, Yin Li, and Mohit
Gupta. 3D scene inference from transient histograms.
InECCV , 2022.
[24] Berk Kaya, Suryansh Kumar, Carlos Oliveira, Vittorio
Ferrari, and Luc Van Gool. Multi-view photometric
stereo revisited. In WACV , 2023.
[25] Kiriakos N Kutulakos and Steven M Seitz. A theory
of shape by space carving. IJCV , 2000.
[26] Jo ˜ao L Lagarto, Federica Villa, Simone Tisa, Franco
Zappa, Vladislav Shcheslavskiy, Francesco S Pavone,
and Riccardo Cicchi. Real-time multispectral ﬂuores-
cence lifetime imaging using single photon avalanche
diode arrays. Scientiﬁc Reports , 2020.
[27] Jongho Lee, Atul Ingle, Jenu V Chacko, Kevin W Eli-
ceiri, and Mohit Gupta. CASPI: collaborative photon
processing for active single-photon imaging. Nature
Communications , 2023.
[28] David B Lindell, Gordon Wetzstein, and Matthew
O’Toole. Wave-based non-line-of-sight imaging us-
ing fast fk migration. ACM Transactions on Graphics
(ToG) , 2019.
[29] Xinyang Liu, Yijin Li, Yanbin Teng, Hujun Bao,
Guofeng Zhang, Yinda Zhang, and Zhaopeng Cui.
Multi-modal neural radiance ﬁeld for monocular
dense slam with a light-weight tof sensor. In ICCV ,
2023.
[30] Anagh Malik, Parsa Mirdehghan, Sotiris Nousias,
Kiriakos N. Kutulakos, and David B. Lindell. Tran-
sient neural radiance ﬁelds for LiDAR view synthesis
and 3D reconstruction. NeurIPS , 2023.
[31] ST Microelectronics. VL53L8CH Datasheet , 2024.
5310
[32] ST Microelectronics. VL6180X Proximity and Ambi-
ent Light Sensing Module Datasheet , 2024.
[33] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,
Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng.
Nerf: Representing scenes as neural radiance ﬁelds for
view synthesis. In ECCV , 2020.
[34] Fangzhou Mu, Sicheng Mo, Jiayong Peng, Xiaochun
Liu, Ji Hyun Nam, Siddeshwar Raghavan, Andreas
Velten, and Yin Li. Physics to the rescue: Deep non-
line-of-sight reconstruction for high-speed imaging.
TPAMI , 2022.
[35] Thomas M ¨uller, Alex Evans, Christoph Schied, and
Alexander Keller. Instant neural graphics primitives
with a multiresolution hash encoding. ACM Transac-
tions on Graphics (ToG) , 2022.
[36] Mark Nishimura, David B. Lindell, Christopher Met-
zler, and Gordon Wetzstein. Disambiguating monocu-
lar depth estimation with a single transient. In ECCV ,
2020.
[37] Joseph Ortiz, Alexander Clegg, Jing Dong, Edgar Su-
car, David Novotny, Michael Zollhoefer, and Mustafa
Mukadam. iSDF: Real-time neural signed distance
ﬁelds for robot perception. In RSS, 2022.
[38] Matthew O’Toole, David B Lindell, and Gordon Wet-
zstein. Confocal non-line-of-sight imaging based on
the light-cone transform. Nature , 2018.
[39] Adithya K Pediredla, Aswin C Sankaranarayanan,
Mauro Buttafava, Alberto Tosi, and Ashok Veer-
araghavan. Signal processing based pile-up compen-
sation for gated single-photon avalanche diodes. arXiv
preprint arXiv:1806.07437 , 2018.
[40] Martin Poll ´ak, Marek Ko ˇciˇsko, Du ˇsan Pauli ˇsin, and
Petr Baron. Measurement of unidirectional pose ac-
curacy and repeatability of the collaborative robot
UR5. Advances in Mechanical Engineering , 12(12):
1687814020972893, 2020.
[41] Alice Ruget, Max Tyler, Germ ´an Mora Mart ´ın, Stir-
ling Scholes, Feng Zhu, Istvan Gyongy, Brent Hearn,
Steve McLaughlin, Abderrahim Halimi, and Jonathan
Leach. Pixels2Pose: Super-resolution time-of-ﬂight
imaging for 3D pose estimation. Science Advances ,
2022.
[42] Steven M Seitz, Brian Curless, James Diebel, Daniel
Scharstein, and Richard Szeliski. A comparison and
evaluation of multi-view stereo reconstruction algo-
rithms. In CVPR , 2006.
[43] Carter Sifferman, Yeping Wang, Mohit Gupta, and
Michael Gleicher. Unlocking the performance of
proximity sensors by utilizing transient histograms.
RA-L , 2023.
[44] Edgar Sucar, Shikun Liu, Joseph Ortiz, and Andrew
Davison. iMAP: Implicit mapping and positioning in
real-time. In ICCV , 2021.[45] Richard Szeliski. Computer vision: algorithms and
applications . Springer Nature, 2022.
[46] Chia-Yin Tsai, Kiriakos N Kutulakos, Srinivasa G
Narasimhan, and Aswin C Sankaranarayanan. The ge-
ometry of ﬁrst-returning photons for non-line-of-sight
imaging. In CVPR , 2017.
[47] Eric Veach. Robust Monte Carlo methods for light
transport simulation . Stanford University, 1998.
[48] Andreas Velten, Thomas Willwacher, Otkrist Gupta,
Ashok Veeraraghavan, Moungi G Bawendi, and
Ramesh Raskar. Recovering three-dimensional shape
around a corner using ultrafast time-of-ﬂight imaging.
Nature communications , 2012.
[49] Peng Wang, Lingjie Liu, Yuan Liu, Christian
Theobalt, Taku Komura, and Wenping Wang. Neus:
Learning neural implicit surfaces by volume render-
ing for multi-view reconstruction. NeurIPS , 2021.
[50] Shumian Xin, Sotiris Nousias, Kiriakos N Kutulakos,
Aswin C Sankaranarayanan, Srinivasa G Narasimhan,
and Ioannis Gkioulekas. A theory of fermat paths
for non-line-of-sight shape reconstruction. In CVPR ,
2019.
[51] Liu Xinyang, Li Yijin, Teng Yanbin, Bao Hujun,
Zhang Guofeng, Zhang Yinda, and Cui Zhaopeng.
Multi-modal neural radiance ﬁeld for monocular
dense slam with a light-weight tof sensor. In ICCV ,
2023.
[52] Wenqi Yang, Guanying Chen, Chaofeng Chen, Zhen-
fang Chen, and Kwan-Yee K. Wong. PS-NeRF:
Neural inverse rendering for multi-view photometric
stereo. In ECCV , 2022.
[53] Li Yijin, Liu Xinyang, Dong Wenqi, Zhou han,
Bao Hujun, Zhang Guofeng, Zhang Yinda, and Cui
Zhaopeng. DELTAR: Depth estimation from a light-
weight tof sensor and rgb image. In ECCV , 2022.
[54] Franco Zappa, Simone Tisa, Alberto Tosi, and Ser-
gio Cova. Principles and features of single-photon
avalanche diode arrays. Sensors and Actuators A:
Physical , 2007.
[55] Dongxu Zhao, Daniel Lichy, Pierre-Nicolas Per-
rin, Jan-Michael Frahm, and Soumyadip Sengupta.
MVPSNet: Fast generalizable multi-view photomet-
ric stereo. In ICCV , 2023.
5311
