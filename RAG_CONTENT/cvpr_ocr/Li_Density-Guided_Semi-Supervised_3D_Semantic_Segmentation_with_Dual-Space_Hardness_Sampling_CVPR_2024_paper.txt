Density-Guided Semi-Supervised 3D Semantic Segmentation
with Dual-Space Hardness Sampling
Jianan Li1,2, Qiulei Dong∗,1,2,3
1School of Artiﬁcial Intelligence, University of Chinese Academy of Sciences,
2State Key Laboratory of Multimodal Artiﬁcial Intelligence Systems,
Institute of Automation, Chinese Academy of Sciences
3Center for Excellence in Brain Science and Intelligence Technology, Chinese Academy of Sciences
lijianan211@mails.ucas.ac.cn, qldong@nlpr.ac.cn
Abstract
Densely annotating the large-scale point clouds is labo-
rious. To alleviate the annotation burden, contrastivelearning has attracted increasing attention for tacklingsemi-supervised 3D semantic segmentation. However ,existing point-to-point contrastive learning techniques inliterature are generally sensitive to outliers, resulting ininsufﬁcient modeling of the point-wise representations. Toaddress this problem, we propose a method named DDSemi
for semi-supervised 3D semantic segmentation, where adensity-guided contrastive learning technique is explored.This technique calculates the contrastive loss in a point-to-anchor manner by estimating an anchor for each classfrom the memory bank based on the ﬁnding that the clustercenters tend to be located in dense regions. In this tech-nique, an inter-contrast loss is derived from the perturbedunlabeled point cloud pairs, while an intra-contrast loss isderived from a single unlabeled point cloud. The derivedlosses could enhance the discriminability of the featuresand implicitly constrain the semantic consistency betweenthe perturbed unlabeled point cloud pairs. In addition,we propose a dual-space hardness sampling strategy topay more attention to the hard samples located in sparseregions of both the geometric space and feature space byreweighting the point-wise intra-contrast loss. Experimen-tal results on both indoor-scene and outdoor-scene datasetsdemonstrate that the proposed method outperforms thecomparative state-of-the-art semi-supervised methods.
1. Introduction
3D semantic segmentation [ 13,17,23,27,30] is a funda-
mental task in computer vision and plays an essential role
*Corresponding authorFigure 1. Illustration of the point-to-point contrast (top) and point-
to-anchor contrast (bottom). The circles represent the point fea-tures and the diamonds represent the anchors. Different colorsdenote different categories. The direction of the arrows shows the
force of pull or push in contrastive learning. The point-to-point
contrast is prone to be affected by the confusing points while thepoint-to-anchor contrast is robust to them.
in scene understanding. Most of the existing 3D segmen-
tation models in literature are trained in a fully-supervisedmanner, where the labor-intensive and time-consuming dataannotation is required. To address this issue, several semi-supervised methods [ 11,18,20,24,36], weakly-supervised
methods [ 25,26,28,43], and annotation-free methods
[5,6,39,46] have been explored. Among them, semi-
supervised 3D semantic segmentation has drawn growinginterest, where the training data contains a small amount ofdensely-labeled data and a large amount of unlabeled data.
A typical manner for handling the semi-supervised se-
mantic segmentation tasks [ 1,18,37,48] is to apply con-
trastive learning to explore the information encapsulated inthe unlabeled data while preserving the nutrition in the lim-ited densely-labeled data.
Originating from the classiﬁcation task, contrastive
learning has become a prevailing technique in many vi-sual tasks [ 7,8,12,15]. However, as revealed in [ 32],
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
3260
there exists a supervision gap between the classiﬁcation task
and the dense prediction tasks ( e.g., the segmentation task).
Contrastive learning in the classiﬁcation task tends to fo-cus on the most representative part for learning a discrim-inative representation, while the information contained in
a single element ( e.g., pixel or point) may not be repre-
sentative enough for effective contrast in dense predictiontasks. Moreover, as stated in [ 33], directly utilizing point-
to-point contrast usually results in insufﬁcient modeling ofthe point-wise representations, for some confusing pointsmay be sampled to construct the undesired pairs, as illus-
trated at the top part of Figure 1. The above issues motivate
us to investigate the following problem: How to ﬁnd efﬁca-cious supervisory signals for contrastive learning in semi-supervised 3D semantic segmentation?
To address this problem, we propose a method named
DDSemi for semi-supervised 3D semantic segmentation.
Inspired by the ﬁnding about clustering in [ 31] that the clus-
ter centers tend to be located in dense regions and data lo-cated in sparser regions is less representative, we explorea density-guided contrastive learning technique for the un-labeled data. Speciﬁcally, we estimate an anchor for each
class by using the high-density features stored in a memory
bank. The anchors are regarded as the supervisory signalsfor the point-to-anchor contrastive learning. Each point ispulled closer to its corresponding anchor and pushed awayfrom other anchors during the contrastive learning process,
as illustrated at the bottom part of Figure 1.
In the explored density-guided contrastive learning tech-
nique, an inter-contrast loss and an intra-contrast loss aredesigned. The inter-contrast loss utilizes the anchors esti-mated from one perturbed point cloud and the features ex-
tracted from another perturbed point cloud for contrastive
learning and vice versa, based on the assumption that se-mantic consistency should be maintained between the pointclouds under different perturbations. The intra-contrast lossutilizes the anchors and features from the same point cloudfor contrastive learning. The proposed contrastive learn-ing technique not only enhances the discriminability of thefeatures, but also implicitly constrains the semantic consis-tency between the perturbed point cloud pairs. Moreover,we propose a dual-space hardness sampling strategy for theunlabeled data to mine hard points in both the geometric
space and feature space. The hard points are deﬁned as the
points located in sparse regions and more attention is paidto them by reweighting the point-wise intra-contrast loss.
In summary, our contributions are as follows:
• We propose a density-guided contrastive learning tech-
nique, where an inter-contrast loss and an intra-contrastloss are designed in a point-to-anchor manner. This tech-nique could provide some insights to the application of
contrastive learning in semi-supervised segmentation.
• We propose a dual-space hardness sampling strategy topay more attention to the hard points in both the geomet-
ric space and feature space. This strategy is also density-guided and explicitly shrinks the sparse regions, which isbeneﬁcial to the segmentation performances.
• By integrating the above contrastive learning technique
and hardness sampling strategy, we propose a methodfor semi-supervised 3D semantic segmentation, namedDDSemi. The effectiveness of DDSemi is demonstratedby the experimental results in Section 4.
2. Related Work
2.1. Fully-supervised 3D Semantic Segmentation
The existing methods for fully-supervised 3D semantic seg-
mentation could be roughly divided into three categories:projection-based methods, voxel-based methods, and point-based methods.
The projection-based methods [ 19,29,34,42,45] gener-
ally project the point clouds into the image plane and use the2D Convolutional Neural Networks (CNNs) or transformerblocks to extract features. Kong et al .[19] proposed a full-
cycle framework and scalable training strategy to processthe LiDAR point clouds from the range view.
The voxel-based methods [ 10,14,22,44,49] divide the
3D points into regular voxels and extract features from thediscrete voxels. Lai et al .[22] designed the radial window
self-attention and exponential splitting to mitigate the infor-mation disconnection and limited receptive ﬁeld issues.
The point-based methods [ 16,21,38,40,47] take the
raw point clouds as input and extract point-wise features forsegmentation. Lai et al .[21] proposed the stratiﬁed strategy
to enlarge the receptive ﬁeld of the model and capture thelong-range contexts at a low computational cost.
2.2. Semi-supervised 3D Semantic Segmentation
As deﬁned in [ 41], semi-supervised 3D semantic segmen-
tation [ 11,18,20,24] aims at utilizing a small number
of densely-labeled point cloud frames and a large numberof unlabeled point cloud frames for model training, whichcould alleviate the annotation burden to some extent.
Deng et al .[11] focused on indoor-scene segmentation
and proposed to optimize the pseudo labels for the unla-
beled point using the superpoints generated by geometry-based and color-based region growing algorithms. Kong et
al.[20] focused on segmenting the outdoor-scene LiDAR
point clouds and proposed to leverage the spatial prior ofLiDAR point clouds to exploit the unlabeled data. Also
focusing on the outdoor-scene LiDAR point clouds, Li et
al.[24] utilized the reﬂectivity-prior descriptors to gener-
ate high-quality pseudo labels and made use of the unre-liable pseudo-labels for learning more discriminative rep-resentations. Jiang et al .[18] proposed the label-guided
point-to-point contrastive learning for the unlabeled points
3261
Shared Weights
Augmentation
Augmentation 
Augmentation
InitializeInitializeMemory 
Bank 1
Memory 
Bank 2Shared Weightsࣦ஼ா௟
Labeled Branch
Unlabeled BranchProjectorClassifier
Backbone NetworkUpdate
Update۾௨ଵ
۾௨ଶClassifier
Projector
ClassifierProjectorDHS
DHSࣦ௧௥௔௨భ
ࣦ௧௥௔௨మࣦ௧௘௥௨భ
ࣦ௧௘௥௨మ෡܇௨ଵ
෡܇௨ଶ
Figure 2. Architecture of the proposed DDSemi. It contains a labeled branch and an unlabeled branch. The weights of the backbone
networks, classiﬁers, and projectors in two branches are shared. ˆYu1and ˆYu2are the pseudo labels predicted by the classiﬁers. Pu1
andPu2are the anchors estimated from the memory banks. The DHS in the unlabeled branch denotes the proposed dual-space hardness
sampling strategy, which is elaborately introduced in Section 3.3.
to enhance the feature representation ability of the model.
Unlike existing method [ 18] that uses the point-to-point
contrast, we design the density-guided contrastive learningtechnique to avoid the adverse effect caused by the unde-sired pairs by conducting the point-to-anchor contrast learn-ing. In addition, unlike existing methods [ 11,18,20,24]
that treat all points equally, we propose the dual-space hard-ness sampling strategy to put more emphasis on the hard
points located in sparse regions of both the geometric spaceand feature space.
3. Methodology
3.1. Architecture
Figure 2depicts the architecture of the proposed DDSemi.
As seen from this ﬁgure, it contains a labeled branch and anunlabeled branch.
The labeled branch takes the labeled point clouds as in-
put. It consists of a backbone network for extracting fea-
tures from the input data, a classiﬁer for supervised segmen-
tation, and a projector for mapping the extracted features toa novel feature space, as shown at the top part of Figure 2.
The labeled branch is trained under the supervision of theground-truth labels via the cross-entropy loss L
l
CE.
The unlabeled branch takes the unlabeled point clouds as
input. It contains two streams, which share the same archi-tecture but use two different augmentation techniques. Each
stream contains a backbone network, a classiﬁer, a projec-tor, and a memory bank, as shown at the bottom part of Fig-ure2. The memory bank is utilized to estimate a representa-
tive anchor for each class. It is initialized with the featuresoutput by the projector in the labeled branch and updatedwith the features output by its corresponding projector inthe unlabeled branch. The density-guided contrastive learn-
ing technique contains the inter-contrast losses L
u1
ter,Lu2
ter
and the intra-contrast losses Lu1
tra,Lu2
tra, which are calcu-
lated between the anchors and features output by the pro-jectors. The dual-space hardness sampling strategy is usedfor reweighting each point in the intra-contrast losses to paymore attention to the hard points.
At the training stage, the labeled branch is ﬁrst pre-
trained. Then, the labeled branch and the unlabeled branchare trained jointly. At the inference stage, only the back-
bone network and classiﬁer are used for segmentation.
3.2. Density-guided Contrastive Learning
To effectively exploit the unlabeled points, we propose the
density-guided contrastive learning technique that calcu-
lates the contrastive loss in a point-to-anchor manner.
In this technique, a category-wise memory bank is built
in each stream of the unlabeled branch to store high-qualityfeatures. Then, an anchor is estimated for each class usingthe features stored in the memory bank. The anchors areutilized to construct the point-anchor pairs for contrastivelearning. This technique includes an inter-contrast losscalculated between the perturbed point cloud pairs and anintra-contrast calculated within the same point cloud. Inthis subsection, we will introduce the above parts in detail.
3262
Class C
瀖Memory Bank in the Last Batch
High-confidence Features in  the  Current BatchReplaceLocal FeaturesGlobal Features 
SimilaritiesGlobal Features 
in the Last BatchSelected Local Features
in the Last Batch Local Feature UpdateGlobal Feature Update
Global Features 
in the Current BatchClass 1
Randomly Select
Figure 3. Illustration of the update process of the category-wise memory bank. Different colors represent different categories. The light
colors represent the global part and the corresponding dark colors represent the local part. We ﬁrst progressively update the global featureswith the local features in the last batch. Then we update the local features with the high-conﬁdence features in the current batch.
Memory Bank Construction. Ideally, a memory bank
should contain the holistic information of the whole dataset
for estimating representative anchors and the distributionof the features in the memory bank should be consistentwith that of the latest features output by the projector. Tothis end, we construct a category-wise memory bank M=
{M
c}C
c=1in each stream of the unlabeled branch, where C
denotes the number of classes and Mcstores a set of global
features from previous batches and a set of local featuresfrom the current batch.
As mentioned in Section 3.1, the memory bank is initial-
ized with the features output by the projector in the labeledbranch. Then, the global and local features in the mem-ory bank are updated respectively at each training iteration.Figure 3illustrates the update process of the memory bank.
As seen from the right part of this ﬁgure, the global featuresare updated progressively. For class c, we ﬁrst randomly
select a local feature f
l
c,jfor the global feature fg
c,i. Then,
we calculate the cosine similarity βijbetweenfg
c,iandfl
c,j.
Finally, the updated global feature ˆfg
c,iis obtained by the
weighted sum of fg
c,iandfl
c,j. The above update process is
formulated as:
ˆfg
c,i=( 1−βi,j)·fg
c,i+βi,j·fl
c,j. (1)
As seen from the left part of Figure 3, the local features
are updated with high-conﬁdence features from the currentbatch in a Fist-In-First-Out (FIFO) manner.
Density-guided Anchor Estimation. Here, we develop
a density-guided anchor estimation strategy to estimate an
anchor for each class, based on the ﬁnding that the clustercenters tend to be located in dense regions. As seen from
Figure 4, given a feature f
c,iinMc, we ﬁrst search its k-
nearest neighbors Nf(fc,i)inMc. Then, we leverage the
cosine similarities between fc,iandNf(fc,i)to estimate the
densityd(fc,i), which is formulated as:
d(fc,i)=1
|Nf(fc,i)|/summationdisplay
fc,j∈N f(fc,i)fT
c,i·fc,j
/bardblfc,i/bardbl·/bardblfc,j/bardbl, (2)where/bardbl·/bardbl denotes the L2-Norm.
The anchor pcof classcis estimated by the features with
the top-Kdensities in its corresponding memory bank Mc:
pc=/summationtext
k∈Ωcd(fc,k)·fc,k/summationtext
k∈Ωcd(fc,k), (3)
whereΩcdenotes the index set of the features with the top-
Kdensities in Mc.
Density-guided Inter-contrast Loss. As seen from Fig-
ure 2, two different augmentation techniques are used for
the same unlabeled point cloud. Thus, the semantic in-formation should be consistent between the perturbed un-labeled point cloud pairs ( e.g., the anchors and semantic
predictions), which means that the anchors from one per-
turbed point cloud could serve as the supervisory signals for
another perturbed point cloud, and vice versa. Accordingto this assumption, the following point-wise inter-contrastlosses are designed:
L
u1
e,i=−logexp(fu1
i·pu2
ˆyu1
i/τ)
exp(/summationtextC
c=1fu1
i·pu2c/τ), (4)
Lu2
e,i=−logexp(fu2
i·pu1
ˆyu2
i/τ)
exp(/summationtextC
c=1fu2
i·pu1c/τ), (5)
瀖
k-Nearest Neighbor Graph瀖
Density RankingMemory Bank
Top-KDensities
Figure 4. Illustration of the density-guided anchor estimation. The
density of each feature is calculated within the memory bank based
on its k-nearest neighbors. Only the features with the top- Kden-
sities are used for estimating the anchors.
3263
wherefu1
iandfu2
idenote the features output by their cor-
responding projectors in the unlabeled branch, pu1
candpu2
c
denote the anchors of class cinPu1andPu2respectively,
ˆyu1
iandˆyu2
idenote the pseudo labels of the i-th point in two
streams, and τdenotes the temperature hyper-parameter.
In addition, since the point-anchor pairs are constructed
based on the pseudo labels and inaccurate pseudo labelsmay result in undesired contrast, we eliminate the pointswith low-conﬁdence predictions. Then, the inter-contrastloss for the point cloud is derived by combing the point-wise inter-contrast losses in Equation ( 4) and Equation ( 5):
Lu
ter=N/summationtext
i=11(wu1
i>γ e)·Lu1
e,i
N/summationtext
i=11(wu1
i>γ e)+N/summationtext
i=11(wu2
i>γ e)·Lu2
e,i
N/summationtext
i=11(wu2
i>γ e),(6)
whereNis the number of the unlabeled points, wu1
iand
wu2
iare the conﬁdences output by their corresponding clas-
siﬁers in the unlabeled branch, 1(·)is the indicator function,
andγeis the conﬁdence threshold in the inter-contrast loss.
Density-guided Intra-contrast Loss. The intra-contrast
loss is calculated between the features and anchors from thesame point cloud, which is explored to tighten the featuredistribution. Similar to Equation ( 4) and Equation ( 5), the
point-wise intra-contrast losses are formulated as:
L
u1
a,i=−logexp(fu1
i·pu1
ˆyu1
i/τ)
exp(/summationtextC
c=1fu1
i·pu1c/τ), (7)
Lu2
a,i=−logexp(fu2
i·pu2
ˆyu2
i/τ)
exp(/summationtextC
c=1fu2
i·pu2c/τ). (8)
Likewise, the points with low-conﬁdence predictions are
also eliminated here for reliable contrast.
3.3. Dual-space Hardness Sampling
The aforementioned density-guided contrastive learningtechnique focuses on the high-density points in the feature
space but neglects the low-density points. To make use
of the low-density points effectively, we propose the dual-space hardness sampling strategy that takes the structural in-formation of the point clouds into account by mining hard
points in both the geometric space and feature space.
Considering the uneven densities of the point clouds, we
deﬁne the hard points in the geometric space as the points
located in sparse regions, due to the paucity of spatial ad-jacency information. The geometric density of each pointis calculated according to its local structural information.Speciﬁcally, given the Cartesian coordinates of an unlabeledpointx
u
i, its geometric density dg(xu
i)is calculated based
on the number of its neighbors within a certain radius R:
dg(xu
i)=|Ng(xu
i,R)|−min
j∈Xu|Ng(xu
j,R)|
max
j∈Xu|Ng(xu
j,R)|−min
j∈Xu|Ng(xu
j,R)|,(9)whereNg(·,R)is the neighbor set of a given point within
the radius RandXudenotes the set of the unlabeled points.
Based on the ﬁnding in [ 31], we deﬁne the hard points
in the feature space as the points whose features are locatedin sparse regions. And the density of each point feature iscalculated in a point-to-memory fashion. Speciﬁcally, givena featuref
iwith its pseudo label ˆyi, we search its k-nearest
neighbors Nf(fi)in its corresponding memory bank Mˆyi,
and its density d(fi)is calculated according to Equation ( 2).
Considering that the hard points are less representative,
more emphasis needs to be put on them. Thus, we devise a
density-based weighting function as follows:
ri=max(1−α
2·[dg(xu
i)+d(fi)],/epsilon1)
1
N/summationtextN
j=1max(1−α
2·[dg(xu
i)+d(fi)],/epsilon1),(10)
whereαand/epsilon1are two predetermined hyper-parameters.
As seen from Equation ( 10), points with lower densities
will obtain larger weights and these weights are integratedinto the intra-contrast loss. Finally, the intra-contrast lossfor the point cloud is derived by combining the point-wise
intra-contrast losses in Equation ( 7) and Equation ( 8):
Lu
tra=N/summationtext
i=11(wu1
i>γ a)·ri·Lu1
a,i
N/summationtext
i=11(wu1
i>γ a)+N/summationtext
i=11(wu2
i>γ a)·ri·Lu2
a,i
N/summationtext
i=11(wu2
i>γ a),(11)
whereγais the conﬁdence threshold in Lu
tra.
3.4. Total Loss
As seen from Figure 2, the labeled branch is trained with the
cross-entropy loss Ll
CE and the unlabeled branch is trained
with the density-guided contrastive losses Lu
terandLu
tra.
The total loss is the weighted sum of the above losses:
Ltotal=Ll
CE+λterLu
ter+λtraLu
tra, (12)
whereλterandλtraare the weights of Lu
terandLu
tra.
4. Experiments
4.1. Datasets and Evaluation Metric
The following outdoor-scene and indoor-scene datasets are
used for evaluating the proposed DDSemi:•SemanticKITTI [3] is a large-scale 3D outdoor driving-
scene LiDAR dataset consisting of 22 sequences, amongwhich 10 sequences are used for training, 1 sequence isused for validation, and 11 sequences are used for testing.
•nuScenes [4] is a large-scale 3D outdoor driving-scene
LiDAR dataset, which contains 1000 scenes. Accordingto the ofﬁcial splitting, 850 scenes are used for trainingand validation, and 150 scenes are utilized for testing.
•S3DIS [2] is a 3D indoor-scene dataset, which contains
13 object classes and 6 areas. Following the common split[18], we utilize Area 5 as the validation set and adopt the
other ﬁve areas as the training set.
3264
MethodSemanticKITTI [ 3] nuScenes [ 4]
1% 10% 20% 50% 1% 10% 20% 50%
MeanTeacher [ 35] 45.4 57.1 59.2 60.0 51.6 66.0 67.1 71.7
CBST [ 50] 48.8 58.3 59.4 59.7 53.0 66.5 69.6 71.6
CPS [ 9] 46.7 58.7 59.6 60.5 52.9 66.3 70.0 72.5
LaserMix (Range View) [ 20] 43.4 58.8 59.4 61.4 49.5 68.2 70.6 73.0
LaserMix (V oxel) [ 20] 50.6 60.0 61.9 62.3 55.3 69.9 71.8 73.2
GPC [ 18] 54.1 62.0 62.5 62.8 ----
LiM3D [ 24] 58.4 62.2 63.1 63.6 ----
DDSemi 59.3 65.1 66.3 67.0 58.1 70.2 74.0 76.5
Table 1. Comparative results on the SemanticKITTI [ 3] and nuScenes [ 4] datasets with varying labeled ratios. All mIoU scores are given
in percentage (%). The best results are in bold and the second best results are marked with underlines .
As done in previous works [ 20,24], we use the mIoU
(mean Intersection over Union) as the evaluation metric.
4.2. Implementation Details
We follow the basic experimental settings of LaserMix [ 20]
and GPC [ 18] for evaluating the proposed DDSemi on the
outdoor-scene and indoor-scene datasets. We store 500 fea-tures per class in the memory bank, with the global part andthe local part accounting for half respectively. The kink-
nearest neighbors and Kin anchor estimation are set to 8
and 16. The hyper-parameters γ
e,γa,τ,α,/epsilon1,λter, and
λtraare set to 0.9, 0.75, 1, 0.9, 0.1, 0.1, and 1. More details
are introduced in the supplementary material.
4.3. Comparative Evaluation
We ﬁrst evaluate the proposed DDSemi on the outdoor-scene datasets (SemanticKITTI [ 3] and nuScenes [ 4]) in
comparison to the state-of-the-art (SoTA) methods (GPC[18], LaserMix [ 20], and LiM3D [ 24]) that are specially de-
signed for semi-supervised 3D semantic segmentation. We
also extend some classic semi-supervised learning methods
in the 2D domain to 3D segmentation for further compari-son, including MeanTeacher [ 35], CBST [ 50], and CPS [ 9].
As done in [ 20], we set the labeled ratio of the outdoor-
scene datasets as {1%,10%,20%,50%}, and the corre-
sponding results are reported in Table 1. As seen from this
MethodS3DIS [ 2]
5% 10% 20% 30% 40%
MeanTeacher [ 35] 46.3 53.3 60.1 61.5 62.6
CBST [ 50] 48.7 54.0 60.3 61.8 62.9
CPS [ 9] 48.5 54.4 60.9 62.0 63.4
SSS-Net [ 11] - 51.1 55.5 - -
GPC [ 18] 53.0 57.7 63.5 64.9 65.0
DDSemi 63.2 66.8 70.3 70.5 70.8
Table 2. Comparative results on the S3DIS dataset [ 2].table, the proposed DDSemi outperforms the comparative
methods on both two datasets.
We also evaluate the proposed DDSemi on the indoor-
scene dataset (S3DIS [ 2]), where the extended 2D meth-
ods (MeanTeacher, CBST, and CPS) and the SoTA meth-ods (SSS-Net [ 11] and GPC) in semi-supervised 3D indoor-
scene segmentation are compared. Note that LaserMix andLiM3D are specially designed for LiDAR point cloud seg-mentation, which leverage the spatial cues and the reﬂectiv-ity of LiDAR point clouds respectively, whereas the pointclouds in the S3DIS dataset are reconstructed from multi-view RGB-D images. Thus, LaserMix and LiM3D are notcompared in the indoor-scene experiments.
As done in [ 18], we set the labeled ratio of the indoor-
scene dataset as {5%,10%,20%,30%,40%}. The corre-
sponding results reported in Table 2show that the proposed
DDSemi achieves the best performances, which are consis-tent with the results in the outdoor-scene experiments and
Ground Truth
Ground Truth
Ground Truth
 GPCLaserMixLiM3D
DDSemiDDSemiDDSemi
Figure 5. Visualization of the semantic segmentation results on
the SemanticKITTI [ 3] (top), nuScenes [ 4] (middle), and S3DIS
[2] (bottom) datasets by our proposed DDSemi and the second-
best methods. All models are trained with 10% labeled data and
the highlighted areas are marked with blue boxes.
3265
further verify the effectiveness of our method.
Moreover, we visualize the segmentation results of the
proposed DDSemi and the second-best methods on the threedatasets in Figure 5. As seen from this ﬁgure, DDSemi out-
performs its second-best counterparts, which shows the ef-fectiveness of DDSemi from the qualitative perspective.
4.4. Ablation Studies
We conduct ablation studies on SemanticKITTI [ 3] to verify
the effectiveness of each key element in DDSemi.
Effectiveness of the involved components. In the pro-
posed DDSemi, the labeled data is trained with the cross-entropy loss L
l
CE, and the unlabeled data is trained with the
inter-contrast loss Lu
terand intra-contrast loss Lu
tra. The
dual-space hardness sampling (DHS) strategy is adopted toreweight each point in L
u
tra, which includes the hard point
mining in both the geometric space and feature space. Weprogressively add these components into training to verifytheir effectiveness. The experiments are conducted with10% labeled data and the results are reported in Table 3.
The results in the ﬁrst two rows of Table 3indicate that
the proposed inter-contrast loss is beneﬁcial to mine extrainformation from the unlabeled data by constraining the se-mantic consistency between the perturbed point cloud pairs.The result in the third row of Table 3shows that the intra-
contrast loss could further improve the segmentation perfor-mance. The results in the last three rows of Table 3demon-
strate the effectiveness of the proposed DHS strategy fromthe quantitative perspective. To further verify the effective-ness of the DHS strategy, we visualize the features in Fig-ure 6. As seen from this ﬁgure, the proposed DHS strategy
is beneﬁcial to the compactness of the intra-class featuredistribution and the separation of the inter-class feature dis-tribution, leading to better performances.
Effect of the contrastive learning strategy. To eval-
uate the effect of the contrastive learning strategy, wechange the proposed point-to-anchor strategy to the point-to-point strategy, and the corresponding results are reportedin Table 4. As seen from this table, the model trained
Ll
CE Lu
ter Lutra DHSmIoU (%)
GS FS
/check 56.1
/check/check 59.7
/check/check/check 61.3
/check/check/check /check 61.9
/check/check/check /check 63.8
/check /check /check /check /check 65.1
Table 3. Ablation studies of the involved elements in the proposed
method. DHS denotes dual-space hardness sampling. GS and FSstand for the geometric space and feature space respectively.w/o DHS w/ DHS
Figure 6. Visualization of the features sampled from the S3DISdataset [ 2]. 200 features are sampled for each category. DHS de-
notes the dual-space hardness sampling strategy. Different colorsrepresent different categories.
with the point-to-anchor strategy achieves better perfor-
mance, which demonstrates the effectiveness of the pro-
posed density-guided point-to-anchor contrastive learningtechnique.
Effect of the update strategy of the memory bank.
As mentioned in Section 3.2, the memory bank stores the
global and local features, which are updated in a progres-sive manner and in a FIFO manner respectively. We eval-uate the effect of the update strategy by replacing it with apure progressive strategy and a pure FIFO strategy. The cor-responding results reported in Table 5show that our strat-
egy achieves the best performance. This is mainly becausethe progressive strategy only focuses on holistic informa-tion and neglects the latest feature distribution. The FIFOstrategy only focuses on the features in the current batch andoverlooks the overall information of each class. Our strat-egy combines the advantages of these two complementarystrategies, which leads to better performance.
Effect of the size of the memory bank. We deﬁne the
size of the memory bank as the number of stored featuresper class. To investigate its effect, we evaluate the proposedmethod with the size set as {50,100,500,1000,5000}. Fig-
ure 7ashows the corresponding results. As seen from this
ﬁgure, a larger size improves the performance of the model
Strategy 1% 10% 20% 50%
Point-to-point 55.6 63.1 64.0 64.8
Point-to-anchor 59.3 65.1 66.3 67.0
Table 4. Ablation studies of the contrastive learning strategy.
Strategy 1% 10% 20% 50%
Progressive 58.7 64.3 65.5 66.3
FIFO 58.8 64.5 65.7 66.2
Ours 59.3 65.1 66.3 67.0
Table 5. Ablation studies of the update strategy.
3266
(a) The size of the memory bank. (b)Kin anchor estimation. (c)kink-nearest neighbors.
Figure 7. Ablation studies of the size of the memory bank, Kin anchor estimation, and kink-nearest neighbors, which are the number of
per-class features stored in the memory bank, the number of features used for anchor estimation, and the number of neighbors respectively.
at ﬁrst ( e.g., from50to500), because a stronger capacity
enables the memory bank to give a more comprehensive de-scription for each class. When the size increases to a certainlevel ( e.g.,1000 ,5000 ), the model is generally insensitive to
it. Because the memory bank is utilized to estimate anchorsfrom the features with the top- Kdensities, and thus only
those high-density features could make a difference in the
performance. Moreover, a large size brings more storagecost and computational cost when calculating the densities.Hence, we set the size as 500 here.
Effect of the estimation strategy of the anchors. We
utilize the features with the top- Kdensities to estimate the
anchors. To verify the effectiveness of our density-based es-timation strategy, we change it with a random strategy and
a conﬁdence-based strategy. The random strategy utilizes
the mean of Krandomly selected features in the memory
bank to estimate the anchors. The conﬁdence-based strat-egy utilizes the weighted mean of the features with the top-Kconﬁdences to estimate the anchors. The corresponding
results are reported in Table 6. As seen from this table,
our density-based strategy achieves the best performances,which proves the validity of the assumption in [ 31] and
demonstrates the superiority of our strategy.
Effect of Kin anchor estimation. Kdenotes the num-
ber of features utilized for the anchor estimation. Here, weevaluate the proposed method with K={4,8,16,64,128}
and show the results in Figure 7b. As seen from this ﬁg-
ure, theKthat is too small or too large may lead to per-
formance degradation. This is mainly because a smaller K
ensures the reliability of the features used for anchor esti-mation, while less information is contained in the anchors.
Strategy 1% 10% 20% 50%
Random 58.2 64.0 64.9 65.8
Conﬁdence-based 58.4 64.3 65.1 66.0
Ours 59.3 65.1 66.3 67.0
Table 6. Ablation studies of the estimation strategy of the anchors.A largerKindicates that the anchors contain information
from more features, but features with lower densities maynot be reliable enough for a valid estimation. Therefore, wesetK=1 6 with the best performances.
Effect of kink-nearest neighbors. kdenotes the num-
ber of neighbors and affects the calculation of density. Here,we evaluate the proposed method with k={2,4,8,16,32}
and depict the results in Figure 7c. As seen from this ﬁgure,
whenkis too small ( e.g.,2or4), the performances drop
evidently, due to the insufﬁcient perception of the vicinityof each point. The performances are relatively stable whenk={8,16,32}. A larger kfacilitates a more holistic per-
ception of the local region of each point, but the computa-
tional cost increases accordingly. Thus, we set k=8 for a
trade-off between the accuracy and computational cost.
5. Conclusion
In this work, we propose a method for semi-supervised 3Dsemantic segmentation, named DDSemi. In DDSemi, adensity-guided contrastive learning technique is explored,which calculates the contrastive loss in a point-to-anchor
manner. This technique contains an inter-contrast loss de-
rived from the perturbed point cloud pairs and an intra-contrast loss derived from a single point cloud. In addi-tion, a dual-space hardness sampling strategy is proposedto pay more attention to the hard points located in sparse
regions of both the geometric space and feature space byreweighting the point-wise intra-contrast loss. Experimen-tal results on three public datasets demonstrate that the pro-posed DDSemi outperforms the comparative methods.
Acknowledgements
This work was supported by the National Natural Science
Foundation of China (Grant Nos. 61991423, 62376269,U1805264), the Strategic Priority Research Program of theChinese Academy of Sciences (Grant No. XDA27040811).
3267
References
[1] Inigo Alonso, Alberto Sabater, David Ferstl, Luis Monte-
sano, and Ana C Murillo. Semi-supervised semantic seg-mentation with pixel-level contrastive learning from a class-wise memory bank. In ICCV , pages 8219–8228, 2021. 1
[2] Iro Armeni, Ozan Sener, Amir R Zamir, Helen Jiang, Ioannis
Brilakis, Martin Fischer, and Silvio Savarese. 3d semanticparsing of large-scale indoor spaces. In CVPR , pages 1534–
1543, 2016. 5,6,7
[3] Jens Behley, Martin Garbade, Andres Milioto, Jan Quen-
zel, Sven Behnke, Cyrill Stachniss, and Jurgen Gall. Se-mantickitti: A dataset for semantic scene understanding oflidar sequences. In ICCV , pages 9297–9307, 2019. 5,6,7
[4] Holger Caesar, V arun Bankiti, Alex H Lang, Sourabh V ora,
V enice Erin Liong, Qiang Xu, Anush Krishnan, Y u Pan,Giancarlo Baldan, and Oscar Beijbom. nuscenes: A mul-timodal dataset for autonomous driving. In CVPR , pages
11621–11631, 2020. 5,6
[5] Runnan Chen, Y ouquan Liu, Lingdong Kong, Xinge Zhu,
Y uexin Ma, Yikang Li, Y uenan Hou, Y u Qiao, and WenpingWang. Clip2scene: Towards label-efﬁcient 3d scene under-standing by clip. In CVPR , pages 7020–7030, 2023. 1
[6] Runnan Chen, Y ou-Chen Liu, Lingdong Kong, Nenglun
Chen, Xinge Zhu, Y uexin Ma, Tongliang Liu, and WenpingWang. Towards label-free scene understanding by vision
foundation models. In NeurIPS , 2023. 1
[7] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-
offrey Hinton. A simple framework for contrastive learning
of visual representations. In ICML , pages 1597–1607, 2020.
1
[8] Xinlei Chen, Saining Xie, and Kaiming He. An empiri-
cal study of training self-supervised vision transformers. InICCV , pages 9620–9629, 2021. 1
[9] Xiaokang Chen, Y uhui Y uan, Gang Zeng, and Jingdong
Wang. Semi-supervised semantic segmentation with crosspseudo supervision. In CVPR , pages 2613–2622, 2021. 6
[10] Christopher Choy, JunY oung Gwak, and Silvio Savarese. 4d
spatio-temporal convnets: Minkowski convolutional neuralnetworks. In CVPR , pages 3075–3084, 2019. 2
[11] Shuang Deng, Qiulei Dong, Bo Liu, and Zhanyi Hu.
Superpoint-guided semi-supervised semantic segmentationof 3d point clouds. In ICRA , pages 9214–9220, 2021. 1,
2,3,6
[12] Ye Du, Zehua Fu, Qingjie Liu, and Y unhong Wang. Weakly
supervised semantic segmentation by pixel-to-prototypecontrast. In CVPR , pages 4320–4329, 2022. 1
[13] Siqi Fan, Qiulei Dong, Fenghua Zhu, Yisheng Lv, Peijun
Ye, and Feiyue Wang. Scf-net: Learning spatial contextualfeatures for large-scale point cloud segmentation. In CVPR ,
pages 14499–14508, 2021. 1
[14] Benjamin Graham, Martin Engelcke, and Laurens van der
Maaten. 3d semantic segmentation with submanifold sparseconvolutional networks. CVPR , pages 9224–9232, 2018. 2
[15] Kaiming He, Haoqi Fan, Y uxin Wu, Saining Xie, and Ross
Girshick. Momentum contrast for unsupervised visual rep-resentation learning. In CVPR , pages 9729–9738, 2020. 1[16] Qingyong Hu, Bo Yang, Linhai Xie, Stefano Rosa, Y ulan
Guo, Zhihua Wang, Niki Trigoni, and Andrew Markham.Randla-net: Efﬁcient semantic segmentation of large-scalepoint clouds. In CVPR , pages 11108–11117, 2020. 2
[17] Di Huang, Sida Peng, Tong He, Honghui Yang, Xiaowei
Zhou, and Wanli Ouyang. Ponder: Point cloud pre-training
via neural rendering. In ICCV , pages 16089–16098, 2023. 1
[18] Li Jiang, Shaoshuai Shi, Zhuotao Tian, Xin Lai, Shu Liu,
Chi-Wing Fu, and Jiaya Jia. Guided point contrastive learn-
ing for semi-supervised point cloud semantic segmentation.InICCV , pages 6403–6412, 2021. 1,2,3,5,6
[19] Lingdong Kong, Y ouquan Liu, Runnan Chen, Y uexin Ma,
Xinge Zhu, Yikang Li, Y uenan Hou, Y u Qiao, and Ziwei Liu.
Rethinking range view representation for lidar segmentation.InICCV , pages 228–240, 2023. 2
[20] Lingdong Kong, Jiawei Ren, Liang Pan, and Ziwei Liu.
Lasermix for semi-supervised lidar semantic segmentation.
InCVPR , pages 21705–21715, 2023. 1,2,3,6
[21] Xin Lai, Jianhui Liu, Li Jiang, Liwei Wang, Hengshuang
Zhao, Shu Liu, Xiaojuan Qi, and Jiaya Jia. Stratiﬁed trans-
former for 3d point cloud segmentation. In CVPR , pages
8500–8509, 2022. 2
[22] Xin Lai, Y ukang Chen, Fanbin Lu, Jianhui Liu, and Jiaya
Jia. Spherical transformer for lidar-based 3d recognition. InCVPR , pages 17545–17555, 2023. 2
[23] Jianan Li and Qiulei Dong. Open-set semantic segmenta-
tion for point clouds via adversarial prototype framework. InCVPR , pages 9425–9434, 2023. 1
[24] Li Li, Hubert P . H. Shum, and Toby P . Breckon. Less is more:
Reducing task and model complexity for 3d point cloud se-mantic segmentation. In CVPR , pages 9361–9371, 2023. 1,
2,3,6
[25] Mengtian Li, Y uan Xie, Y unhang Shen, Bo Ke, Ruizhi Qiao,
Bo Ren, Shaohui Lin, and Lizhuang Ma. Hybridcr: Weakly-supervised 3d point cloud semantic segmentation via hybridcontrastive regularization. In CVPR , pages 14930–14939,
2022. 1
[26] Lizhao Liu, Zhuangwei Zhuang, Shangxin Huang, Xun-
long Xiao, Tianhang Xiang, Cen Chen, Jingdong Wang, andMingkui Tan. Cpcm: Contextual point cloud modeling forweakly-supervised point cloud semantic segmentation. InICCV , pages 18413–18422, 2023. 1
[27] Y ouquan Liu, Lingdong Kong, Jun Cen, Runnan Chen, Wen-
wei Zhang, Liang Pan, Kai Chen, and Ziwei Liu. Segmentany point cloud sequences by distilling vision foundationmodels. In NeurIPS , 2023. 1
[28] Zhengzhe Liu, Xiaojuan Qi, and Chi-Wing Fu. One thing
one click: A self-training approach for weakly supervised 3dsemantic segmentation. In CVPR , pages 1726–1736, 2021.
1
[29] Ryan Razani, Ran Cheng, Ehsan Taghavi, and Liu Bingbing.
Lite-hdseg: Lidar semantic segmentation using lite harmonicdense convolutions. In ICRA , pages 9550–9556, 2021. 2
[30] Damien Robert, Hugo Raguet, and Loic Landrieu. Efﬁcient
3d semantic segmentation with superpoint transformer. InICCV , pages 17195–17204, 2023. 1
3268
[31] Alex Rodriguez and Alessandro Laio. Clustering by fast
search and ﬁnd of density peaks. Science , 344:1492 – 1496,
2014. 2,5,8
[32] Wei Shen, Zelin Peng, Xuehui Wang, Huayu Wang, Jiazhong
Cen, Dongsheng Jiang, Lingxi Xie, Xiaokang Yang, and QiTian. A survey on label-efﬁcient deep image segmentation:Bridging the gap between weak supervision and dense pre-diction. IEEE TPAMI , 45:9284–9305, 2022. 1
[33] Xiaoxiao Sheng, Zhiqiang Shen, Gang Xiao, Longguang
Wang, Y ulan Guo, and Hehe Fan. Point contrastive predic-
tion with semantic clustering for self-supervised learning onpoint cloud videos. In ICCV , pages 16515–16524, 2023. 2
[34] Kshitij Sirohi, Rohit Mohan, Daniel B ¨uscher, Wolfram Bur-
gard, and Abhinav V alada. Efﬁcientlps: Efﬁcient lidarpanoptic segmentation. IEEE TOR , 38(3):1894–1914, 2022.
2
[35] Antti Tarvainen and Harri V alpola. Mean teachers are better
role models: Weight-averaged consistency targets improvesemi-supervised deep learning results. In NIPS , 2017. 6
[36] Ozan Unal, Dengxin Dai, Lukas Hoyer, Yigit Baran Can,
and Luc V an Gool. 2d feature distillation for weakly-andsemi-supervised 3d semantic segmentation. In WACV , pages
7336–7345, 2024. 1
[37] Xiaoyang Wang, Bingfeng Zhang, Limin Y u, and Jimin
Xiao. Hunting sparsity: Density-guided contrastive learn-ing for semi-supervised semantic segmentation. In CVPR ,
pages 3114–3123, 2023. 1
[38] Xiaoyang Wu, Yixing Lao, Li Jiang, Xihui Liu, and Heng-
shuang Zhao. Point transformer v2: Grouped vector atten-tion and partition-based pooling. In NeurIPS , pages 33330–
33342, 2022. 2
[39] Xiaoyang Wu, Xin Wen, Xihui Liu, and Hengshuang Zhao.
Masked scene contrast: A scalable framework for unsuper-vised 3d representation learning. In CVPR , pages 9415–
9424, 2023. 1
[40] Peng Xiang, Xin Wen, Y u-Shen Liu, Hui Zhang, Yi Fang,
and Zhizhong Han. Retro-fpn: Retrospective feature pyra-mid network for point cloud semantic segmentation. InICCV , pages 17826–17838, 2023. 2
[41] Aoran Xiao, Xiaoqin Zhang, Ling Shao, and Shijian Lu. A
survey of label-efﬁcient deep learning for 3d point clouds.arXiv preprint arXiv:2305.19812 , 2023. 2
[42] Chenfeng Xu, Bichen Wu, Zining Wang, Wei Zhan, Peter
V ajda, Kurt Keutzer, and Masayoshi Tomizuka. Squeeze-segv3: Spatially-adaptive convolution for efﬁcient point-cloud segmentation. In ECCV , pages 1–19, 2020. 2
[43] Cheng-Kun Yang, Ji-Jia Wu, Kai-Syun Chen, Y ung-Y u
Chuang, and Yen-Y u Lin. An mil-derived transformer forweakly supervised point cloud segmentation. In CVPR ,
pages 11830–11839, 2022. 1
[44] Y u-Qi Yang, Y u-Xiao Guo, Jian-Y u Xiong, Yang Liu,
Hao Pan, Peng-Shuai Wang, Xin Tong, and Baining Guo.Swin3d: A pretrained transformer backbone for 3d indoor
scene understanding, 2023. 2
[45] Yang Zhang, Zixiang Zhou, Philip David, Xiangyu Y ue, Ze-
rong Xi, Boqing Gong, and Hassan Foroosh. Polarnet: Animproved grid representation for online lidar point clouds se-mantic segmentation. In CVPR , pages 9601–9610, 2020. 2[46] Zihui Zhang, Bo Yang, Bing Wang, and Bo Li. Growsp:
Unsupervised semantic segmentation of 3d point clouds. In
CVPR , pages 17619–17629, 2023. 1
[47] Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip HS Torr, and
Vladlen Koltun. Point transformer. In ICCV , pages 16259–
16268, 2021. 2
[48] Y uanyi Zhong, Bodi Y uan, Hong Wu, Zhiqiang Y uan, Jian
Peng, and Y u-Xiong Wang. Pixel contrastive-consistentsemi-supervised semantic segmentation. In ICCV , pages
7273–7282, 2021.
1
[49] Xinge Zhu, Hui Zhou, Tai Wang, Fangzhou Hong, Y uexin
Ma, Wei Li, Hongsheng Li, and Dahua Lin. Cylindrical andasymmetrical 3d convolution networks for lidar segmenta-tion. In CVPR , pages 9939–9948, 2021. 2
[50] Yang Zou, Zhiding Y u, BVK Kumar, and Jinsong Wang. Un-
supervised domain adaptation for semantic segmentation via
class-balanced self-training. In ECCV , pages 289–305, 2018.
6
3269
