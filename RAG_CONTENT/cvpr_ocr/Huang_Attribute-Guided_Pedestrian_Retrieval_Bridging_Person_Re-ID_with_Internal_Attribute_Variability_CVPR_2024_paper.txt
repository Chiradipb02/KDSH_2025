Attribute-Guided Pedestrian Retrieval: Bridging Person Re-ID with Internal
Attribute Variability
Yan Huang1Zhang Zhang1,2∗Qiang Wu3Yi Zhong4Liang Wang1,2*
1Center for Research on Intelligent Perception and Computing (CRIPAC),
State Key Laboratory of Multimodal Artiﬁcial Intelligence Systems (MAIS),
Institute of Automation, Chinese Academy of Sciences, Beijing China.
2School of Artiﬁcial Intelligence, University of Chinese Academy of Sciences (UCAS).
3School of Electrical and Data Engineering, University of Technology Sydney, Australia.
4School of Information and Electronics, Beijing Institute of Technology, China.
huangyan.750@outlook.com, {zzhang,wangliang }@nlpr.ia.ac.cn,
qiang.wu@uts.edu.au, yi.zhong@bit.edu.cn
Abstract
In various domains such as surveillance and smart retail,
pedestrian retrieval, centering on person re-identiﬁcation
(Re-ID), plays a pivotal role. Existing Re-ID method-
ologies often overlook subtle internal attribute variations,
which are crucial for accurately identifying individuals with
changing appearances. In response, our paper introduces
the Attribute-Guided Pedestrian Retrieval (AGPR) task, fo-
cusing on integrating speciﬁed attributes with query im-
ages to reﬁne retrieval results. Although there has been
progress in attribute-driven image retrieval, there remains
a notable gap in effectively blending robust Re-ID models
with intra-class attribute variations. To bridge this gap, we
present the Attribute-Guided Transformer-based Pedestrian
Retrieval (ATPR) framework. ATPR adeptly merges global
ID recognition with local attribute learning, ensuring a co-
hesive linkage between the two. Furthermore, to effectively
handle the complexity of attribute interconnectivity, ATPR
organizes attributes into distinct groups and applies both
inter-group correlation and intra-group decorrelation reg-
ularizations. Our extensive experiments on a newly estab-
lished benchmark using the RAP dataset [ 32] demonstrate
the effectiveness of ATPR within the AGPR paradigm.
1. Introduction
Pedestrian retrieval plays a crucial role in various domains,
including security and smart retail. At its core lies the tech-
nique of person re-identiﬁcation (Re-ID), which focuses on
identifying individuals across camera views [ 19,23,63,69–
*Corresponding authors: Liang Wang, Zhang Zhang
1. Gender: Female
2. Standard Body Type
3. Black Hair
4. Wearing a Hoodie
5. Upper Clothing Color: Pink
6. Wearing Long Pants
7. Lower Clothing Color: Black
8. Shoe Color: Black
9. Not Carrying a Handbag1. Gender: Female
2. Standard Body Type
3. Black Hair
4. Wearing a Padded Jacket
5. Upper Clothing Color: Green
6. Wearing Long Pants
7. Lower Clothing Color: Black
8. Shoe Color: Black
9. Carrying a Handbag
Attributes of the query sample itself Desired attributes provided by witne ssQuery sampleI remember when I saw her, the person 
in the query sample image was wearing a 
hoodie with the color of pink, and was 
not carrying a handbag.
...Similarity ranking list without specified attributes 
...Similarity ranking list without specified attributes 
...Similarity ranking list with desired attributes
Rank1 Rank2 Rank3 Ranki Rankj Rank1 Rank2 Rank3 Ranki RankjwitnessWearing a Padded Jacket: (1->0)
Upper Clothing Color: Green: (1->0)
Carrying a Handbag: (1->0)
Wearing a Hoodie: (0->1)
Upper Clothing Color: Pink: (0->1)
Not Carrying a Handbag: (0->1)Change
Attribute label:
0: absence
1: presenceFigure 1. An illustration of the AGPR task. The ﬁgure showcases
how the additional attributes remembered by a witness but not
consistent with the queried image. Initial similarity rankings with-
out speciﬁc attributes are contrasted with attribute-guided rankings
based on the witness’s attribute descriptions.
71]. Although existing Re-ID methods demonstrate notable
accuracy and scalability [ 16,17,22,40,55,63], they tend
to overlook subtle, individual attribute variations, which are
essential for precise identiﬁcation.
Consider a series of subway thefts as an example. The
police presents witnesses with a mugshot of the suspect.
Due to the thief’s potential disguises or the limited ﬁeld
of view (FoV) of the mugshot, witnesses might offer var-
ied attribute descriptions. For instance, one might mention
a “green scarf and a tattoo on his left hand,” while another
recalls “sunglasses and a blue jacket.” This necessitates the
development of an efﬁcient method capable of merging the
old mugshot with attributes provided by witnesses to iden-
tify the suspect in CCTV footage. This retrieval with mul-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
17689
timodal queries is no doubt an effective method to improve
the searching results, underscoring the importance of inte-
grating Re-ID systems with detailed attribute information.
We term this innovative task as the Attribute-Guided Pedes-
trian Retrieval (AGPR) task. Fig. 1illustrates an example.
Attribute-driven image retrieval has advanced, particu-
larly in fashion [ 39,53] and face retrieval [ 6,12,38,65].
However, these innovations often do not address intra-class
attribute variability and lack focus on consistent ID recog-
nition. Similarly, while text-based person searches [ 1,7,
48,50] bridge textual descriptors and visual content, they
fall short in handling the complexities of attribute varia-
tions within an ID. While some studies have investigated
enhancing Re-ID systems by integrating attributes during
the training phase [ 36,42,52], these methods generally uti-
lize attribute labels to boost performance rather than ad-
dressing intra-ID attribute variations. Consequently, these
approaches are typically conﬁned to contexts where intra-
class attributes do not vary.
Confronted with these issues, a clear gap becomes ev-
ident: the need to develop a paradigm that seamlessly in-
tegrates person Re-ID with the variability of internal at-
tributes. We emphasize that while the cornerstone of per-
son Re-ID remains the identiﬁcation of the correct ID, in-
tegrating attribute guidance can enhance the speciﬁcity of
retrieval results. For instance, searching for an individual
‘wearing a red hat’ should yield not just any individual, but
speciﬁcally the person depicted in the queried image.
The primary challenge in AGPR lies in integrating nu-
anced internal attribute variations into person Re-ID match-
ing models. In real-world scenarios, individuals may
change their appearance, which can include alterations in
clothing, hairstyle, or accessories. Such variability poses
a signiﬁcant challenge, as these changes can lead to intra-
ID confusion, where the same individual is misidentiﬁed
across different camera perspectives or over time. Addi-
tionally, ensuring the Re-ID system’s robustness against at-
tribute variability, while maintaining high accuracy and ef-
ﬁciency, is another critical hurdle. Moreover, over-reliance
on speciﬁc attributes combinations could diminish the sys-
tem’s effectiveness. In diverse environments, individuals
may present with a wide range of unseen attribute combina-
tions, further compound the complexity of the task.
Current methodologies face signiﬁcant limitations when
addressing the above challenges. For instance, existing
clothing-change Re-ID models primarily concentrate on
adapting to variations in apparel [ 10,18,24,28,37,60,61].
Yet, the challenge of establishing the ID of a target individ-
ual without constraining speciﬁc attribute changes remains
formidable. Moreover, while existing attribute recognition
frameworks are adept at identifying speciﬁc attributes [ 49,
51,54], they typically lack a comprehensive approach to
learning and integrating ID information. These challengeshighlight a crucial gap in existing pedestrian retrieval sys-
tems – the need for a more holistic approach that considers
the capability to pinpoint targets with speciﬁed attributes.
To address these challenges, this paper introduces the
Attribute-Guided Transformer-based Pedestrian Retrieval
(ATPR) framework that effectively integrates global ID
recognition and local attribute learning. This dual-faceted
strategy ensures that while the system recognizes individ-
ual attribute changes, it still maintains a strong foundation
in ID recognition. ATPR is particularly adept at handling
intra-ID confusion by creating a coherent connection be-
tween a person’s attributes and their ID, even amidst al-
terations. Additionally, the framework employs attribute
correlation/decorrelation regularization techniques to man-
age the intricate relationships between different attributes.
This regularization helps in distinguishing individuals based
on their unique attribute features. Furthermore, the ATPR
framework’s efﬁcacy in handling intra-ID attribute variabil-
ity is validated by extensive experiments on a new evalua-
tion benchmark using the RAP dataset [ 32]. Our approach
allows for the speciﬁcation of attributes during the retrieval
process, enabling more precise and targeted Re-ID. The pri-
mary contributions of this paper are outlined as follows:
• We introduce the novel AGPR task in pedestrian retrieval,
utilizing ﬁne-grained attribute for more targeted image re-
trieval. This approach enhances search precision in appli-
cations requiring speciﬁc attribute identiﬁcation.
• We present the ATPR framework that effectively com-
bines global ID recognition with local attribute learning,
addressing intra-class attribute variation. It includes inter-
group attribute correlation and intra-group decorrelation
regularizations to manage attribute interconnectedness.
• A new AGPR evaluation benchmark is established using
the RAP dataset [ 32], featuring rich attribute annotations
and signiﬁcant variability. Extensive experiments demon-
strate our ATPR’s ability to enhance pedestrian image re-
trieval by focusing on speciﬁc attribute variations.
2. Related work
Attribute-Guided Image Retrieval is a key area in vision
research [ 2,26,44,64,74], with notable advancements in
fashion image retrieval and face image retrieval. Within
Fashion Image Retrieval , V o et al. [ 53] innovated a sys-
tem where queries consist of an input image paired with
textual descriptions detailing desired modiﬁcations. Liu
et al. [ 39] capitalized on pre-existing vision-and-language
frameworks to adapt visual features based on natural lan-
guage inputs. Both works utilize a shared architecture to
encode the query image, textual conditions, and target im-
age. Other notable contributions, such as [ 8,11,29,30,68],
empowered users or embedded natural language systems to
offer iterative feedback on retrieved items, thus reﬁning the
search outcomes. Turning to Face Image Retrieval , Gupta
17690
et al. [ 12] harnessed user feedback to categorize images as
either aligning with or deviating from their mental picture,
subsequently guiding the face images during retrieval. Za-
eemzadeh et al. [65] enhanced the input face query utilizing
facial attributes. Liu et al. [38] adopted facial attributes to
steer the generation of face visuals.
Contrary to the above works, which often overlooks the
need for ID consistency or attribute correlations, our AGPR
places emphasis on the detailed integration of individual
identity recognition with correlated person attributes. Our
AGPR differentiates itself in the image retrieval domain
by speciﬁcally addressing real-world necessities through a
thorough and precise person ID matching process.
Text-Based Person Search primarily utilizes textual de-
scriptions to identify individuals in images and videos [ 1,
3,7,41,48,50,57]. Wu et al. [57] enhanced the learning
of ﬁne-grained cross-modality connections through a color-
reasoning sub-task. Gao et al. [7] synchronized person
image sub-regions with their corresponding textual phrase
descriptions. Jiang et al. [27] showcased the applicabil-
ity of the full CLIP model [ 46] to the text-to-image person
retrieval task with minimal ﬁne-tuning. While text-based
person search tackles a cross-modal challenge (translating
textual attributes into visual cues), our AGPR intricately
merges Re-ID with speciﬁc person attributes. Rather than
depending exclusively on textual descriptions, our AGPR
places more emphasis on visual cues to identify individu-
als, further reﬁning retrieval based on detailed attributes.
Person Re-ID with Attribute aims to enhance identity
learning through multi-task learning [ 36,42,52]. Lin et
al.[36] learned a CNN embedding for both person Re-ID
and attribute recognition via an attribute re-weighting mod-
ule. Schumann et al. [47] incorporated the semantic details
of attributes into the CNN learning process for person Re-
ID. Zhang et al. [66] devised an attribute attentional block
to harness ﬁne-grained attribute attention, further enhancing
Re-ID performance. However, these approaches overlook
instances where individuals may alter an attribute.
Clothing-Change Person Re-ID considers scenarios
where a person might alter their clothing [ 10,13,15,21,
28,37,56,61,67]. These approaches predominantly
focus on adapting the network to overcome dress varia-
tions, thereby ensuring persistent recognition despite cloth-
ing changes [ 21,37]. Our AGPR distinguishes itself by not
only identifying individuals but also enhancing the retrieval
precision through an in-depth attribute-centric analysis.
Vehicle Re-ID with Attribute is centered on identify-
ing vehicles, often using attributes like mark, model, color,
or license plates [ 33,62,73]. Vehicle attributes are usu-
ally distinct and less prone to rapid change. In our AGPR,
the speciﬁcation and retrieval based on attributes are more
challenging due to the frequent and unpredictable nature of
human attribute changes.Table 1. Attribute groupings and their respective descriptions.
Group Description Group Description
a1Basic Personal Attributes a2Head and Shoulder
a3Upper Body Clothing a4Lower Body Clothing
a5Footwear a6Accessory
3. Methodology
3.1. Input data
As delineated in Fig. 2, our ATPR processes pedestrian im-
ages combined with corresponding attributes. We man-
ually categorize these attributes into six groups {ai|i=
1,2,...,6}, each containing distinct attributes ai
j, wherej
represents the count of attributes in a particular group. The
categorization is detailed in Tab. 1.
Before feeding pedestrian images to our model, they
are pre-processed with Openpose [ 4] to identify anatomi-
cal keypoints, then segmented into 16×16non-overlapping
patches{xp|p= 1,2,...,N}. Each patch (or token) is con-
verted into a 1D vector and undergoes linear projection for
dimensionality D. These vectors form token embeddings
for the transformer layers [ 5], with an additional [class] to-
ken embedding to discern global features.
3.2. Attribute Embedding
Patch-Attribute Association. Prior to processing token
embeddings through transformer layers, each token is asso-
ciated with a relevant attribute based on keypoints identiﬁed
via Openpose in the patch. For a patch with Nkkeypoints
(0≤Nk≤K, whereKis the maximum number of key-
points and 0 signiﬁes no keypoints), we calculate the Eu-
clidean distance from each keypoint to the patch’s center.
The closest keypoint determines the token’s linked attribute
group, such as a1for keypoints on the head or shoulder.
Attribute Embedding. We introduce Attribute Embed-
ding (AE), a learnable 1D vector, to integrate attribute infor-
mation into token embeddings. This approach is inspired by
the position and camera/viewpoint embeddings as outlined
in [5,14]. Unlike these methods, our AE speciﬁcally maps
tokens to relevant attributes for the AGPR task. Speciﬁcally,
attributes are categorized into six groups ( ai) (see Tab. 1).
Groupsa1anda6, representing basic personal attributes and
accessories, are initialized as AE A1andA6, respectively,
and added to the [class] token embedding ( xcls):
Z0
0=xcls+/summationtextj1
j=1A1
j+/summationtextj6
j=1A6
j, (1)
whereAi
j∈RDrepresents the learnable AE associated
with thej-thattribute in the i-thattribute group. j1andj6
denote the total counts of attributes in groups a1anda6,
respectively ( j6=0 if no accessory).
As illustrated in Fig. 2, other token embeddings pos-
sess corresponding local body attributes linked to groups ai
17691
a1
a2
a3
a4
a6
1 2 4 6 7 8 9
......0Transformer Layer
*a1a2a3a4a5Transformer Layer...Input data: image + attributes
a5
a3localizing keypoints
+ patch splitting
a6
Linear Projection of Flattened PatchesTransformer Layer
3 5
73
1 4 6a4a2
a3
98
a552Attribute-Guided 
Embedding Grouping* * * *
a2a3
a5a4Global Branch Local Attribute Branch
Patch-Attribute AssociationNo Attribute Group Linked
Empty embedding1. Gender: Female
2. Standard Body Type
1. Black Hair
1. Wearing a Padded Jacket
2. Upper Clothing Color: Green
1. Wearing Long Pants
2. Lower Clothing Color: Black
1. Shoe Type: Boots
2. Shoe Color: Black
1. Carrying a Handbaga5a6
a1Attribute Correlation/Decorrelation Regularization
a5
1a5
2a5
4
a5
3...*IGAD *IGAC
[*] Class Embedding [0-9] Position Embedding 0 * 9...a1a6a2a3a5a4 a[    ] Attribute EmbeddingiAttribute-Irrelevant 
Embedding Grouping 
Legend*
Transformer Layerl l
l-1
1fgLglb
fm1 fm2 fm3 fm4LlocLreg
*
***
Figure 2. The AGPR architecture. The ﬁgure highlights the processing of input data, consisting of an image and its associated attributes.
Key elements like global branch, local attribute branch, AEG, IGAC and IGAD, and attribute and position embeddings are illustrated.
wherei∈ {2,3,4,5}, or they might not have any linkage
if no keypoints are detected on the patch. For these embed-
dings, we initialize the learnable AE Aiand add it to the p-th
token embedding as:
Zp
0=xp+I·/summationtextji
j=1Ap→i
j,i∈ {2,3,4,5}, (2)
whereIis assigned the value 1 if the patch {xp|p=
1,2,...,N}has a corresponding linked attribute group (rep-
resented as p→iin the equation), and 0 otherwise, where
Nrepresents the total number of patches.
Lastly, the sequences provided as the input to trans-
former layers are expressed as:
Z0= [Z0
0;Z1
0;Z2
0;...;ZN
0]+P, (3)
whereP ∈R(N+1)×Drefers to the position embedding.
Mirroring the behavior in ViT [ 5], this position embedding
is also designed to be learnable.
3.3. Global and Local Attribute learning
The sequence Z0undergoes ltransformer layers to derive
feature representations. Inspired by [ 14], we implement two
branches atop the output of the l-1-thtransformer layer.
Global Branch. As shown in Fig. 2, the [class] token
output acts as a global feature representation, denoted as fg.
Aligning with the conventional ViT-based Re-ID methodol-
ogy [ 14], both the triplet loss ( Ltrip) and ID loss ( i.e., cross-
entropy loss, Lid) are applied on global features.
Lglb=Lid(fg)+Ltrip(fg); (4)
Local Attribute Branch. Given the intrinsic attribute
variation in our AGPR task, we craft a local attribute branchadept at intelligently assimilating local attribute informa-
tion alongside ID recognition. This design ensures that the
network effectively adapts to changes in an individual’s at-
tributes, maintaining the crucial link between evolving at-
tributes and the core ID information, thereby fulﬁlling the
objectives of the AGPR task. Speciﬁcally, assuming the
hidden feature of the l-1-thtransformer layer is denoted as
Zl−1= [Z0
l−1;Z1
l−1;Z2
l−1;...;ZN
l−1], our Attribute-guided
Embedding Grouping (AEG) amalgamates these hidden
features into groups, based on their corresponding attribute
embeddings. For instance, as indicated in Fig. 2,a3pos-
sesses two correlated hidden features merged together. To-
kens paired with empty embeddings are disregarded in the
local attribute branch. The token Z0
l−1(i.e., output of the
[class] token post the l-1 transformer layer) is appended to
each group and subsequently processed by the l-thTrans-
former layer of the local attribute branch. Notably, since a1
anda6represent global attributes merged with the [class]
token, they are excluded from the local attribute branch.
For the local attribute features ( fm), we deploy a binary
cross-entropy loss Lbinwhich predicts whether the attribute
should be marked as 1 or 0, based on its presence or ab-
sence in each group of the local branch. Moreover, ID and
triplet loss further bolster the learning of distinct ID features
within local attribute zones:
Lloc=1
M/summationtextM
i=1(Lid(fi
m)+Ltrip(fi
m)+Lbin(fi
m)),(5)
Functionality of the local attribute branch. The local
attribute branch operates on a dual learning mechanism, si-
multaneously acquiring knowledge about speciﬁc attributes
and ID information. This design ensures the network is not
solely reliant on static attributes. When attributes undergo
17692
changes, the foundational knowledge of the ID, reinforced
by integrating the [class] token’s output into every attribute
group post-AEG, aids in achieving consistent retrieval.
3.4. Attribute Regularization
Correlations within attribute groups often play a critical role
in vision tasks [ 25,59]. For inter-group correlations, such
connections provide invaluable contextual insights. For in-
stance, awareness of other correlated attributes, such as
pants type, can augment the accuracy in identifying a spe-
ciﬁc footwear type. Conversely, intra-group attributes do
not necessarily exhibit strong correlations always. For ex-
ample, within the attribute group a3, presented in Fig. 2
which illustrates upper body attributes, a “green” attribute
does not automatically imply its association with a “jacket”.
Consequently, dissociating these intra-group attributes is
imperative. To navigate these complexities, we devise
a novel regularization mechanism combining both Inter-
Group Attribute Correlation (IGAC) and Intra-Group At-
tribute Decorrelation (IGAD). This dual tactic seeks to op-
timize the balance between leveraging inter-group corre-
lations for enhanced recognition accuracy while ensuring
intra-group attributes remain uncorrelated, structured as:
Lreg=6/summationdisplay
i=1/summationdisplay
q,k∈Ai
q̸=k/vextenddouble/vextenddoubleAi
k−Ai
q/vextenddouble/vextenddouble
2
−λ6/summationdisplay
i,j=1
i̸=j∥1
|Ai|/summationdisplay
k∈AiAi
k−1
|Aj|/summationdisplay
q∈AjAj
q∥2.(6)
The strategy encompasses two main objectives. First,
within each attribute group (e.g., Ai), we compute pairwise
L2 distances among all unique attributes to ensure that intra-
group attributes remain decorrelated. This ensures that at-
tributes within the same group are less interconnected. Sec-
ond, when considering different attribute groups, we deter-
mine the average representation for each group and compute
the L2 distances between these attribute prototypes. Using
a negative sign, this term aspires to minimize discrepancies
between distinct attribute groups, accentuating their corre-
lations. The balancing factor, λ, mediates the dual goals of
intra-group decorrelation and inter-group correlation.
Introducing IGAC and IGAD regularizations offers a
more nuanced control and helps the network understand the
relationship between attributes. This is pivotal because even
if one attribute changes, the network can assess its rela-
tionship with other attributes and make informed decisions
about the ID association.
3.5. Total Objective
Total Objective. The ﬁnal loss function is:
L=Lglb+αLloc+βLreg, (7)whereαandβserve as hyperparameters determining the
weights of individual losses.
During inference , when provided with a query im-
age accompanied by speciﬁc attributes (potentially differ-
ing from the attributes evident in the image), we concate-
nate the global and the mean local attribute features f=
[fg,/summationtextM
i=1fim]to form the ﬁnal representation. As delin-
eated in Sec. 3.3, our local attribute branch can associate
the newly introduced attributes with the original ID, effec-
tively accomplishing the AGPR task.
4. Experiments
4.1. New AGPR benchmark based on RAP
Our experimental dataset for AGPR research meets three
key requirements: suitability for Re-ID tasks, intra-class
attribute variation, and a rich set of attribute labels. The
RAP dataset [ 32] is uniquely suited for this purpose, offer-
ing 2,589 pedestrian IDs across 26,638 images. The training
set includes 13,178 images with 1,295 unique IDs, and the
test set comprises 13,460 images with another 1,294 unique
IDs, including 7,202 query images and 6,258 gallery im-
ages. Each image is annotated with 156 attribute labels,
from which we selected 86 biological and appearance at-
tributes, categorized into six groups as outlined in Tab. 1.
These attributes, represented in an 86-dimensional vector
(0/1 indicating the absence/presence of an attribute), cover
aspects like gender, age, body type, clothing speciﬁcs, and
accessories. Analysis of the RAP dataset shows signiﬁcant
intra-ID attribute variation, with each ID averaging at least
four attribute changes, essential for our AGPR task. Note
that an arXiv paper [ 15] annotates a clothing-change Re-ID
dataset with 20 pedestrian attributes per image, but these
labels are not public, and the attribute set seems limited.
4.2. Implementation Details
All person images are resized to 256×128 pixels. Following
the established ViT-based Re-ID baseline protocols [ 14], the
training images undergo augmentation that includes random
horizontal ﬂipping, padding, random cropping, and random
erasing [ 72]. A patch size of 16×16 with a stride of 16
is used, resulting in 128 patches for each image as input.
The batch size is set to 64, comprising 4 images per ID.
The SGD optimizer is employed with a momentum of 0.9
and weight decay of 1e-4. The learning rate is initialized
at 0.008 and follows a cosine decay schedule. Initial ViT
weights are pre-trained on ImageNet-21K and then ﬁne-
tuned on ImageNet-1K. In Eq. 6,λis set to 1. In Eq. 7,
αis set to 1 and βis set to 0.05.
Evaluation Protocol. We employ standard cmc (e.g.,
rank-1) and mAP metrics for evaluation. Unlike traditional
settings that rely solely on the ID label for matching, the
AGPR task necessitates not just matching the ID label of
17693
Table 2. Comparison with state-of-the-art (SOTA) methods are
conducted, considering four different types of Re-ID approaches.
The results are obtained using the ofﬁcial released code of these
methods on the RAP benchmark.
Method mAP rank-1
1.Traditional Re-ID
EFL [ 9] 2.01 2.53
LOMO [ 35] 4.53 7.26
GOG [ 43] 19.06 31.49
JSTL [ 58] 14.80 29.94
MSCAN [ 31] 29.29 48.17
MuDeep [ 45] 26.26 45.97
HACNN [ 34] 47.96 70.69
2.Clothing-Change Re-ID
ReIDCaps [ 20] 45.40 64.80
CAL [ 10] 45.50 59.80
3.Re-ID + Attributes
PAR [ 36] 38.56 59.76
IDE-ATT-R [ 32] 38.21 59.25
4.ViT-based architecture
ViT baseline [ 5] 45.22 66.18
TransReID baselinee [ 14] 70.17 82.85
TransReID+CE [ 14] 70.72 83.21
TransReID+CE+JPM [ 14] 71.39 83.77
ATPR (Ours) 74.26 86.93
the image but also aligning with the speciﬁed attribute in-
put provided alongside the query image during testing. For
a match to be deemed correct, it must correspond to the cor-
rect ID and be consistent with the designated attribute label.
4.3. Comparison with SOTA
We ﬁrst compare our method under the common Re-ID
setting that does not specify the attribute of the query im-
age during testing. Tab. 2shows that in the ‘Traditional
Re-ID’ method section, HACNN achieves high mAP and
rank-1 accuracy. In the ‘Clothing-Change Re-ID’ method
section, ReIDCaps and CAL methods score similarly on
both metrics, but their performance is slightly worse than
HACNN. In the ‘Re-ID+Attributes’ method, PAR and IDE-
ATT-R show high performance even without speciﬁc de-
sign for intra-class attribute changes, and their mAP and
rank-1 metrics exceed some methods in ‘Traditional Re-ID’
methods. This highlights the role of attribute information
in the task. In the ‘ViT-based architecture’ method section,
the TransReID baseline method achieves the highest score
on the rank-1 metric, while the TransReID+CE and Tran-
sReID+CE+JPM methods also slightly improve the rank-1
metric. Finally, our ATPR achieves the highest mAP and
rank-1 metric scores. This indicates that combining at-
tributes, ViT architecture, and Re-ID tasks can achieve bet-
ter performance on our benchmark.
Tab. 3presents a comparison of different Re-ID meth-ods under different settings of intra-class attribute changes.
Among them, PAR, ReIDCaps, and TransReID are the best-
performing Re-ID methods in their respective sections in
Tab. 2. In this experiment, we provide the query im-
age along with the desired attributes (the attributes that
the query image, with the same ID but different attributes,
should possess in the gallery). These attributes are differ-
ent from the query image’s own attributes. Our goal is to
retrieve images with the same ID but different attributes.
We try assigning attribute changes from 10% to 90% of the
queries (s1-s5) and perform retrieval in the gallery. Observ-
ing the table, we can note the following points:
• Under all experimental conditions, our ATPR shows
higher mAP and rank-1 scores compared to the other
three existing methods. This indicates that our method
performs well on the AGPR task.
• As the proportion of query images with attribute changes
increases, the mAP and rank-1 scores of all methods grad-
ually decrease. Increasing the proportion of query im-
ages with attribute changes affects the performance of the
AGPR task, which is reasonable.
• Under the same experimental conditions (s1-s5), there are
differences in the mAP and rank-1 scores between differ-
ent methods compared to the results of s0 (non-AGPR
task). For example, under s1 (attribute change required),
mAP of the PAR method decreases by 4.23% compared to
the non-AGPR task (s0), while our method only decreases
by 3.93%. This result is based on the fact that our method
already has a much higher mAP than PAR under the s0
setting (74.26% vs. 38.56%). Although our method may
show slightly higher performance degradation in some
conditions, the magnitude of our method’s performance
decline in s1-s5 (difference between performance decline
and non-AGPR performance) is much lower. This high-
lights the robustness of our method on the AGPR task
relative to other methods.
4.4. Ablation Study
The ablation study is provided in Tab. 4:
Impact of different loss combinations: When using
only the global loss ( Lgb) or local loss ( Lloc), the model’s
performance is poor. For example, when using ‘only Lglb’,
the mAP is 43.78%. This suggests that the global loss ig-
nores important local attribute features. Similarly, using
Llocalone disregards the overall correlation among these at-
tributes. Combining both losses can comprehensively lever-
age their advantages, improving the performance. Adding
Lregfurther constrains the model’s training process. For ex-
ample, with the ‘ Lglb+Lloc+Lreg(IGAC)’ approach (only
using IGAC in Lreg), the mAP is 48.41%.
Impact of different attribute group combinations:
The way attribute groups are combined can inﬂuence
the model’s performance. For instance, merging at-
17694
Table 3. Comparative analyses of methods against varying levels of intra-class attribute (att.) changes. The robustness of each method and
our ATPR is evaluated under a spectrum of attribute change scenarios, ranging from no changes ( s0) to 90% alteration ( s1-s5). The ‘no
cross att.’ setting means we do not specify attributes for retrieval during testing.
Method s 0:no cross att. s 1:cross-att.:10% s 2:cross-att.:30% s 3:cross-att.:50% s 4:cross-att.:70% s 5:cross-att.:90%
mAP rank-1 mAP rank-1 mAP rank-1 mAP rank-1 mAP rank-1 mAP rank-1
PAR 38.56 59.76 34.33 55.22 31.35 51.72 27.74 44.35 23.22 37.48 19.57 31.14
|s0-si|, i∈[1...5] 0 0 ↓4.23↓4.54 ↓7.21↓8.04↓10.82↓15.41↓15.34↓22.28↓18.99↓28.62
ReIDCaps 45.40 64.80 41.20 60.70 38.60 57.60 30.70 47.50 25.60 36.80 19.20 30.10
|s0-si|, i∈[1...5] 0 0 ↓4.20↓4.10 ↓6.80↓7.20↓14.70↓17.30↓19.80↓28.00↓26.20↓34.70
TransReID 71.39 83.77 66.42 77.56 59.35 72.41 52.82 64.28 44.21 55.41 38.91 42.76
|s0-si|, i∈[1...5] 0 0 ↓4.97↓6.21↓12.04↓11.36↓18.57↓19.77↓27.18↓28.36↓32.48↓41.01
ATPR(Ours) 74.26 86.93 70.33 82.72 66.09 76.41 60.28 70.98 53.24 66.22 49.51 60.33
|s0-si|, i∈[1...5] 0 0 ↓3.93↓4.21 ↓8.17↓10.52↓13.98↓15.95↓21.02↓20.71↓24.75↓26.60
Table 4. Ablation Studies
Methods no cross att. cross-att.:90%
mAP rank-1 mAP rank-1
Different Loss Combinations
only Lglb 70.47 82.82 43.78 53.94
only Lloc 69.98 82.19 42.63 53.76
Lglb+Lloc 71.93 84.02 46.66 57.83
Lglb+Lloc+Lreg (IGAC) 73.17 85.88 48.41 59.10
Lglb+Lloc+Lreg (IGAD) 72.75 85.47 48.67 59.24
Different Attribute Group Combinations
a1,(a2,a3),(a4,a5),a673.26 85.92 48.91 59.26
a1,a2,(a3,a4,a5),a672.95 85.71 47.73 58.82
a1,(a2,a3,a4,a5),a671.48 84.18 47.65 58.47
Different Patch and Attributes Strategies
ﬁxed Patch-Att. Assoc. 73.01 85.82 48.46 58.89
w/o. adding AE 68.85 80.49 43.87 55.69
w/o. using [class] in AEG 70.74 83.03 45.22 56.73
Different Inference Strategies
inference: only fg 72.92 85.31 48.26 58.83
inference: only/summationtextM
i=1fim 71.33 84.92 47.95 58.19
ATPR 74.26 86.93 49.51 60.33
tribute groups a2witha3, anda4witha5(i.e.,
‘a1,(a2,a3),(a4,a5),a6’), results in an mAP of 48.91% and
a rank-1 score of 59.26%. Combining a3,a4, anda5(i.e.,
‘a1,a2,(a3,a4,a5),a6’) yields an mAP of 47.73% and a rank-
1 score of 58.82%. This suggests that different attribute
group combinations affect the model’s understanding of im-
age content. More generalized combinations may lead to
a diminished ability to learn speciﬁc local attribute infor-
mation, which in turn can impact the overall performance.
For example, amalgamating a2,a3,a4, anda5into a single
group (i.e., ‘ a1,(a2,a3,a4,a5),a6’) results in lower mAP and
rank-1 scores of 47.65% and 58.47%, respectively.
Impact of different patching and attribute strategies:
‘ﬁxed Patch-Att. Assoc.’: Using ﬁxed patch and attribute
association without human keypoint detection prevents themodel from dynamically adjusting patch and attribute in-
formation associations in different images. This approach
achieves an mAP and rank-1 of 48.46% and 58.89%, re-
spectively. ‘Without adding AE’ ‘Without using [class]
in AEG’: Not using AE or not using [class] after AEG
affects the model’s performance. For example, ‘without
adding AE’ achieves an mAP and rank-1 of only 43.87%
and 55.69%. Not using AE prevents the model from encod-
ing and learning attribute information, impacting its per-
formance. The approach ‘without using [class] in AEG’
achieves an mAP and rank-1 of 45.22% and 56.73%, re-
spectively, indicating that not using [class] after AEG af-
fects the model’s overall understanding of image content.
Different inference strategies: ‘inference: only fg’ ‘in-
ference: only/summationtextM
i=1fi
m’: Only using global features or av-
eraging all local information for inference results in poor
model performance. For example, the approach ‘inference:
onlyfg’ achieves an mAP and rank-1 of only 48.26% and
58.83%, respectively, lacking expression of local informa-
tion. The approach ‘inference: only/summationtextM
i=1fi
m’ achieves an
mAP and rank-1 of only 47.95% and 58.19%, respectively,
indicating that using only average attribute features for in-
ference overlooks global pedestrian features.
The last row presents the performance of our AGPR,
which achieves the best performance.
4.5. Qualitative Analyses
Fig. 3demonstrates our ATPR model’s capacity for han-
dling attribute-guided pedestrian retrieval. In the ﬁrst row,
without attribute changes, the ATPR model shows proﬁ-
cient baseline matching. The second and third rows present
the ATPR’s adaptability to attribute changes speciﬁed in
queries, which is central to the AGPR task. Despite the in-
troduction of new attributes, the ATPR model successfully
identiﬁes correct matches, as indicated by the red boxes.
This highlights the strength of the local attribute branch and
the AEG mechanism in maintaining accurate ID recognition
17695
Jacket
Top Color Brown
Short Sleeves
Top Color Black
Change Att....
...Similar Dissimilar
...
...Query
Short Sleeves
Top Color Grey
Top Color Black
... ...1
2 Change Att.
... ...Query
Other Types of Tops
Top Color Black
Leggings
Bottom Color BlackChange Att. 1
Sweater
Top Color orange
Top Color PinkChange Att. 2
Similar Dissimilar
... ...
... ...
Query Similar Dissimilar
... ...
... ...
 Shirt
Top Color BlueChange Att. 1
Change Att. 2
... ...Figure 3. Rankings with attribute changes. The ﬁrst row shows initial rankings without specifying desired attributes. Rows two and three
show rankings when attributes are modiﬁed (in blue) based on witness descriptions. Correct matches are marked with red boxes.
0 0.5 1 1.5 248.54949.5 vs. mAP (%)49.51
0 0.5 1 1.5 259.56060.5 vs. rank-1 (%)60.33
0 0.5 1 1.5 2464850 vs. mAP (%)49.51
0 0.5 1 1.5 2565860 vs. rank-1 (%)60.33
0 0.02 0.04 0.06 0.08 0.147484950 vs. mAP (%)49.51
0 0.02 0.04 0.06 0.08 0.158596061 vs. rank-1 (%)60.33
Figure 4. Sensitivity of hyperparameters λ(the ﬁrst row), α(the
second row), and β(the third row.).
amidst attribute variability. Our ATPR’s robustness is fur-
ther underscored by its consistent performance across vari-
ous attribute alterations, suggesting a well-balanced imple-
mentation of IGAC and IGAD within the approach. Over-
all, this ﬁgure substantiates our ATPR’s potential in prac-
tical AGPR applications, where individuals may alter their
appearance yet still need to be reliably identiﬁed.
4.6. Hyperparameter Analyses.
The hyperparameter λin Eq. 6determines the trade-off be-
tween IGAC and IGAD. As observed in Fig. 4, an increase
inλfrom 0.2 to 1 corresponds with a general increase in
both mAP and rank-1 accuracy, suggesting that amplify-
ing the emphasis on IGAC (which highlights inter-group
correlations) enhances the performance. At λ= 1, the
model achieves the highest mAP (49.51%) and rank-1 accu-
racy (60.33%), indicating an optimal balance between intra-
group decorrelation and inter-group correlation. A further
increase in λpast 1 leads to a decline in performance, im-
plying an overemphasis on inter-group correlations that po-
tentially undermines intra-group decorrelation.Similarly, αandβmodulate the inﬂuence of Llocand
Lregwithin the ﬁnal loss (Eq. 7). As shown in Fig. 4, an
upward trend in mAP and rank-1 accuracy is noted as α
increases from 0.2 to 1, peaking at α=1 with the highest
mAP and rank-1 accuracy, suggesting an ideal balance with
the global loss term. Performance drops when αexceeds 1,
indicating possible overﬁtting to local features due to exces-
sive weighting of the local loss component. For β, the mAP
and rank-1 accuracy improve consistently as βincreases up
to 0.05, after which they start to decrease. The peak perfor-
mance at β=0.05 implies that beyond this point, the regular-
ization may be too stringent, leading to potential model un-
derﬁtting from excessive attribute correlation/decorrelation.
5. Conclusion
This work presents an Attribute-Guided Transformer-based
Pedestrian Retrieval (ATPR) architecture that skillfully
merges global ID recognition with local attribute learning.
It effectively tackles the challenges posed by the newly
introduced Attribute-Guided Pedestrian Retrieval (AGPR)
task, speciﬁcally person Re-ID with intra-class attribute
variability. Our ATPR shows superior performance on the
proposed RAP benchmark. However, potential limitations
might emerge in scenarios featuring unseen attributes, or
signiﬁcant occlusions, all of which merit further investi-
gation. Future research directions may include extending
ATPR to a broader range of datasets and real-world condi-
tions, as well as incorporating additional modalities, such as
biometric or behavioral data, to enhance retrieval precision.
6. Acknowledge
This work was jointly supported National Science
and Technology Major Project (2022ZD0117901), Na-
tional Natural Science Foundation of China (62306311,
62373355, 62236010, and 62201061), Fellowship of
China Postdoctoral Science Foundation (2022T150698),
International Postdoctoral Exchange Fellowship Program
of China (YJ20210324), and Special Research Assistant
Program of Chinese Academy of Sciences (E2S9180301).
17696
References
[1] Surbhi Aggarwal, Venkatesh Babu Radhakrishnan, and Anir-
ban Chakraborty. Text-based person search via attribute-
aided matching. In WACV , pages 2617–2625, 2020. 2,3
[2] Kenan E Ak, Ashraf A Kassim, Joo Hwee Lim, and Jo Yew
Tham. Learning attribute representations with localization
for ﬂexible fashion search. In CVPR , pages 7708–7717,
2018. 2
[3] Min Cao, Yang Bai, Ziyin Zeng, Mang Ye, and Min Zhang.
An empirical study of clip for text-based person search.
arXiv preprint arXiv:2308.10045 , 2023. 3
[4] Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh.
Realtime multi-person 2d pose estimation using part afﬁnity
ﬁelds. In CVPR , pages 7291–7299, 2017. 3
[5] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale. In ICLR , 2021. 3,4,
6
[6] Deng-Ping Fan, Ziling Huang, Peng Zheng, Hong Liu, Xue-
bin Qin, and Luc Van Gool. Facial-sketch synthesis: a new
challenge. Machine Intelligence Research , 19(4):257–287,
2022. 2
[7] Chenyang Gao, Guanyu Cai, Xinyang Jiang, Feng Zheng,
Jun Zhang, Yifei Gong, Fangzhou Lin, Xing Sun, and Xiang
Bai. Conditional feature learning based transformer for text-
based person search. IEEE TIP , 31:6097–6108, 2022. 2,
3
[8] Sonam Goenka, Zhaoheng Zheng, Ayush Jaiswal, Rakesh
Chada, Yue Wu, Varsha Hedau, and Pradeep Natarajan.
Fashionvlp: Vision language transformer for fashion re-
trieval with feedback. In CVPR , pages 14105–14115, 2022.
2
[9] Douglas Gray and Hai Tao. Viewpoint invariant pedestrian
recognition with an ensemble of localized features. In ECCV ,
pages 262–275, 2008. 6
[10] Xinqian Gu, Hong Chang, Bingpeng Ma, Shutao Bai,
Shiguang Shan, and Xilin Chen. Clothes-changing person
re-identiﬁcation with rgb modality only. In CVPR , pages
1060–1069, 2022. 2,3,6
[11] Xiaoxiao Guo, Hui Wu, Yu Cheng, Steven Rennie, Gerald
Tesauro, and Rogerio Feris. Dialog-based interactive image
retrieval. In NeurIPS , 2018. 2
[12] Devansh Gupta, Aditya Saini, Sarthak Bhagat, Shagun Up-
pal, Rishi Raj Jain, Drishti Bhasin, Ponnurangam Ku-
maraguru, and Rajiv Ratn Shah. A suspect identiﬁcation
framework using contrastive relevance feedback. In WACV ,
pages 4361–4369, 2023. 2,3
[13] Ke Han, Yan Huang, Shaogang Gong, Liang Wang, and Tie-
niu Tan. 3d shape temporal aggregation for video-based
clothing-change person re-identiﬁcation. In ACCV , pages
2371–2387, 2022. 3
[14] Shuting He, Hao Luo, Pichao Wang, Fan Wang, Hao Li,
and Wei Jiang. Transreid: Transformer-based object re-
identiﬁcation. In ICCV , pages 15013–15022, 2021. 3,4,
5,6[15] Weizhen He, Shixiang Tang, Yiheng Deng, Qihao Chen,
Qingsong Xie, Yizhou Wang, Lei Bai, Feng Zhu, Rui Zhao,
Wanli Ouyang, et al. Retrieve anyone: A general-purpose
person re-identiﬁcation task with instructions. arXiv preprint
arXiv:2306.07520 , 2023. 3,5
[16] Han Huang, Yan Huang, and Liang Wang. Vi-diff: Unpaired
visible-infrared translation diffusion model for single modal-
ity labeled visible-infrared person re-identiﬁcation. arXiv
preprint arXiv:2310.04122 , 2023. 1
[17] Yan Huang, Jingsong Xu, Qiang Wu, Zhedong Zheng,
Zhaoxiang Zhang, and Jian Zhang. Multi-pseudo regularized
label for generated data in person re-identiﬁcation. IEEE
TIP, 28(3):1391–1403, 2018. 1
[18] Yan Huang, Qiang Wu, Jingsong Xu, and Yi Zhong.
Celebrities-reid: A benchmark for clothes variation in long-
term person re-identiﬁcation. In International Joint Confer-
ence on Neural Networks , pages 1–8. IEEE, 2019. 2
[19] Yan Huang, Qiang Wu, JingSong Xu, and Yi Zhong. Sbsgan:
Suppression of inter-domain background shift for person re-
identiﬁcation. In ICCV , pages 9527–9536, 2019. 1
[20] Yan Huang, Jingsong Xu, Qiang Wu, Yi Zhong, Peng
Zhang, and Zhaoxiang Zhang. Beyond scalar neuron:
Adopting vector-neuron capsules for long-term person re-
identiﬁcation. IEEE TCSVT , pages 3459–3471, 2019. 6
[21] Yan Huang, Qiang Wu, JingSong Xu, Yi Zhong, and ZhaoX-
iang Zhang. Clothing status awareness for long-term person
re-identiﬁcation. In ICCV , pages 11895–11904, 2021. 3
[22] Yan Huang, Qiang Wu, Jingsong Xu, Yi Zhong, and Zhaox-
iang Zhang. Unsupervised domain adaptation with back-
ground shift mitigating for person re-identiﬁcation. IJCV ,
129(7):2244–2263, 2021. 1
[23] Yan Huang, Zhang Zhang, Qiang Wu, Yi Zhong, and Liang
Wang. Enhancing person re-identiﬁcation performance
through in vivo learning. IEEE TIP , 2023. 1
[24] Yan Huang, Qiang Wu, Zhang Zhang, Caifeng Shan, Yi
Zhong, and Liang Wang. Meta clothing status calibration
for long-term person re-identiﬁcation. IEEE TIP , 2024. 2
[25] Dinesh Jayaraman, Fei Sha, and Kristen Grauman. Decor-
relating semantic visual attributes by resisting the urge to
share. In CVPR , pages 1629–1636, 2014. 5
[26] Ge-Peng Ji, Mingchen Zhuge, Dehong Gao, Deng-Ping Fan,
Christos Sakaridis, and Luc Van Gool. Masked vision-
language transformer in fashion. Machine Intelligence Re-
search , 20(3):421–434, 2023. 2
[27] Ding Jiang and Mang Ye. Cross-modal implicit relation rea-
soning and aligning for text-to-image person retrieval. In
CVPR , pages 2787–2797, 2023. 3
[28] Xin Jin, Tianyu He, Kecheng Zheng, Zhiheng Yin, Xu
Shen, Zhen Huang, Ruoyu Feng, Jianqiang Huang, Zhibo
Chen, and Xian-Sheng Hua. Cloth-changing person re-
identiﬁcation from a single image with gait prediction and
regularization. In CVPR , pages 14278–14287, 2022. 2,3
[29] Adriana Kovashka and Kristen Grauman. Attribute pivots for
guiding relevance feedback in image search. In ICCV , pages
297–304, 2013. 2
[30] Adriana Kovashka, Devi Parikh, and Kristen Grauman.
Whittlesearch: Image search with relative attribute feedback.
InCVPR , pages 2973–2980, 2012. 2
17697
[31] Dangwei Li, Xiaotang Chen, Zhang Zhang, and Kaiqi
Huang. Learning deep context-aware features over body and
latent parts for person re-identiﬁcation. In CVPR , pages 384–
393, 2017. 6
[32] Dangwei Li, Zhang Zhang, Xiaotang Chen, and Kaiqi
Huang. A richly annotated pedestrian dataset for person re-
trieval in real surveillance scenarios. IEEE TIP , 28(4):1575–
1590, 2018. 1,2,5,6
[33] Hongchao Li, Chenglong Li, Aihua Zheng, Jin Tang, and
Bin Luo. Attribute and state guided structural embedding
network for vehicle re-identiﬁcation. IEEE TIP , pages 5949–
5962, 2022. 3
[34] Wei Li, Xiatian Zhu, and Shaogang Gong. Harmonious at-
tention network for person re-identiﬁcation. In CVPR , pages
2285–2294, 2018. 6
[35] Shengcai Liao, Yang Hu, Xiangyu Zhu, and Stan Z Li. Per-
son re-identiﬁcation by local maximal occurrence represen-
tation and metric learning. In CVPR , pages 2197–2206,
2015. 6
[36] Yutian Lin, Liang Zheng, Zhedong Zheng, Yu Wu, Zhilan
Hu, Chenggang Yan, and Yi Yang. Improving person re-
identiﬁcation by attribute and identity learning. PR, 95:151–
161, 2019. 2,3,6
[37] Feng Liu, Minchul Kim, ZiAng Gu, Anil Jain, and Xiaoming
Liu. Learning clothing and pose invariant 3d shape represen-
tation for long-term person re-identiﬁcation. In ICCV , pages
19617–19626, 2023. 2,3
[38] Yunfan Liu, Qi Li, Qiyao Deng, Zhenan Sun, and Ming-
Hsuan Yang. Gan-based facial attribute manipulation. IEEE
TPAMI , 2023. 2,3
[39] Zheyuan Liu, Cristian Rodriguez-Opazo, Damien Teney, and
Stephen Gould. Image retrieval on real-life images with pre-
trained vision-and-language models. In ICCV , pages 2125–
2134, 2021. 2
[40] Andong Lu, Zhang Zhang, Yan Huang, Yifan Zhang, Cheng-
long Li, Jin Tang, and Liang Wang. Illumination distillation
framework for nighttime person re-identiﬁcation and a new
benchmark. IEEE TMM , 2023. 1
[41] Haoyu Lu, Yuqi Huo, Mingyu Ding, Nanyi Fei, and Zhiwu
Lu. Cross-modal contrastive learning for generalizable and
efﬁcient image-text retrieval. Machine Intelligence Re-
search , 20(4):569–582, 2023. 3
[42] Jinghao Luo, Yaohua Liu, Changxin Gao, and Nong Sang.
Learning what and where from attributes to improve person
re-identiﬁcation. In ICIP , pages 165–169. IEEE, 2019. 2,3
[43] Tetsu Matsukawa, Takahiro Okabe, Einoshin Suzuki, and
Yoichi Sato. Hierarchical gaussian descriptor for person re-
identiﬁcation. In CVPR , pages 1363–1372, 2016. 6
[44] Andrei Neculai, Yanbei Chen, and Zeynep Akata. Proba-
bilistic compositional embeddings for multimodal image re-
trieval. In CVPRW , pages 4547–4557, 2022. 2
[45] Xuelin Qian, Yanwei Fu, Yu-Gang Jiang, Tao Xiang, and
Xiangyang Xue. Multi-scale deep learning architectures for
person re-identiﬁcation. In ICCV , pages 5399–5408, 2017.
6
[46] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-
ing transferable visual models from natural language super-
vision. In ICML , pages 8748–8763, 2021. 3
[47] Arne Schumann and Rainer Stiefelhagen. Person re-
identiﬁcation by deep learning attribute-complementary in-
formation. In CVPRW , pages 20–28, 2017. 3
[48] Zhiyin Shao, Xinyu Zhang, Meng Fang, Zhifeng Lin, Jian
Wang, and Changxing Ding. Learning granularity-uniﬁed
representations for text-to-image person re-identiﬁcation. In
ACM MM , pages 5566–5574, 2022. 2,3
[49] Patrick Sudowe, Hannah Spitzer, and Bastian Leibe. Per-
son attribute recognition with a jointly-trained holistic cnn
model. In ICCVW , pages 87–95, 2015. 2
[50] Wei Suo, Mengyang Sun, Kai Niu, Yiqi Gao, Peng Wang,
Yanning Zhang, and Qi Wu. A simple and robust correla-
tion ﬁltering method for text-based person search. In ECCV ,
pages 726–742, 2022. 2,3
[51] Chufeng Tang, Lu Sheng, Zhaoxiang Zhang, and Xiaolin
Hu. Improving pedestrian attribute recognition with weakly-
supervised multi-scale attribute-speciﬁc localization. In
ICCV , pages 4997–5006, 2019. 2
[52] Chiat-Pin Tay, Sharmili Roy, and Kim-Hui Yap. Aanet: At-
tribute attention network for person re-identiﬁcations. In
CVPR , pages 7134–7143, 2019. 2,3
[53] Nam V o, Lu Jiang, Chen Sun, Kevin Murphy, Li-Jia Li, Li
Fei-Fei, and James Hays. Composing text and image for
image retrieval-an empirical odyssey. In CVPR , pages 6439–
6448, 2019. 2
[54] Xiao Wang, Shaofei Zheng, Rui Yang, Aihua Zheng, Zhe
Chen, Jin Tang, and Bin Luo. Pedestrian attribute recogni-
tion: A survey. PR, 2022. 2
[55] Junyi Wu, Yan Huang, Qiang Wu, Zhipeng Gao, Jianqiang
Zhao, and Liqin Huang. Dual-stream guided-learning via a
priori optimization for person re-identiﬁcation. Transactions
on Multimedia Computing, Communications, and Applica-
tions , 17(4):1–22, 2022. 1
[56] Junyi Wu, Yan Huang, Min Gao, Zhipeng Gao, Jianqiang
Zhao, Huiji Zhang, and Anguo Zhang. A two-stream hybrid
convolution-transformer network architecture for clothing-
change person re-identiﬁcation. IEEE TMM , 2023. 3
[57] Yushuang Wu, Zizheng Yan, Xiaoguang Han, Guanbin Li,
Changqing Zou, and Shuguang Cui. Lapscore: language-
guided person search via color reasoning. In ICCV , pages
1624–1633, 2021. 3
[58] Tong Xiao, Hongsheng Li, Wanli Ouyang, and Xiaogang
Wang. Learning deep feature representations with domain
guided dropout for person re-identiﬁcation. In CVPR , pages
1249–1258, 2016. 6
[59] Wenjia Xu, Yongqin Xian, Jiuniu Wang, Bernt Schiele, and
Zeynep Akata. Attribute prototype network for zero-shot
learning. In NeurIPS , 2020. 5
[60] Qize Yang, Ancong Wu, and Wei-Shi Zheng. Person re-
identiﬁcation by contour sketch under moderate clothing
change. IEEE TPAMI , pages 2029–2046, 2019. 2
[61] Zhengwei Yang, Meng Lin, Xian Zhong, Yu Wu, and Zheng
Wang. Good is bad: Causality inspired cloth-debiasing for
cloth-changing person re-identiﬁcation. In CVPR , pages
1472–1481, 2023. 2,3
17698
[62] Yue Yao, Liang Zheng, Xiaodong Yang, Milind Naphade,
and Tom Gedeon. Simulating content consistent vehicle
datasets with attribute descent. In ECCV , pages 775–791.
Springer, 2020. 3
[63] Mang Ye, Jianbing Shen, Gaojie Lin, Tao Xiang, Ling
Shao, and Steven CH Hoi. Deep learning for person re-
identiﬁcation: A survey and outlook. IEEE TPAMI , pages
2872–2893, 2021. 1
[64] Qian Yu, Feng Liu, Yi-Zhe Song, Tao Xiang, Timothy M
Hospedales, and Chen-Change Loy. Sketch me that shoe. In
CVPR , pages 799–807, 2016. 2
[65] Alireza Zaeemzadeh, Shabnam Ghadar, Baldo Faieta, Zhe
Lin, Nazanin Rahnavard, Mubarak Shah, and Ratheesh
Kalarot. Face image retrieval with attribute manipulation.
InICCV , pages 12116–12125, 2021. 2,3
[66] Jianfu Zhang, Li Niu, and Liqing Zhang. Person re-
identiﬁcation with reinforced attribute attention selection.
IEEE TIP , 30:603–616, 2020. 3
[67] Peng Zhang, Jingsong Xu, Qiang Wu, Yan Huang, and Xi-
anye Ben. Learning spatial-temporal representations over
walking tracklet for long-term person re-identiﬁcation in the
wild. IEEE TMM , 23:3562–3576, 2020. 3
[68] Bo Zhao, Jiashi Feng, Xiao Wu, and Shuicheng Yan.
Memory-augmented attribute manipulation networks for in-
teractive fashion search. In CVPR , pages 1520–1528, 2017.
2
[69] Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jing-
dong Wang, and Qi Tian. Scalable person re-identiﬁcation:
A benchmark. In ICCV , pages 1116–1124, 2015. 1
[70] Liang Zheng, Yi Yang, and Alexander G Hauptmann. Per-
son re-identiﬁcation: Past, present and future. arXiv preprint
arXiv:1610.02984 , 2016.
[71] Zhedong Zheng, Liang Zheng, and Yi Yang. Unlabeled sam-
ples generated by gan improve the person re-identiﬁcation
baseline in vitro. In ICCV , pages 3754–3762, 2017. 1
[72] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and
Yi Yang. Random erasing data augmentation. In AAAI , pages
13001–13008, 2020. 5
[73] Chaoran Zhuge, Yujie Peng, Yadong Li, Jiangbo Ai, and
Junru Chen. Attribute-guided feature extraction and aug-
mentation robust learning for vehicle re-identiﬁcation. In
CVPRW , pages 618–619, 2020. 3
[74] Xiao-Long Zou, Tie-Jun Huang, and Si Wu. Towards a new
paradigm for brain-inspired computer vision. Machine Intel-
ligence Research , 19(5):412–424, 2022. 2
17699
