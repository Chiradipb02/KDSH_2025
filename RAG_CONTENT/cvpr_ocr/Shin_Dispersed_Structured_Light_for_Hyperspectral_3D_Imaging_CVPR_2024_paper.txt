Dispersed Structured Light for Hyperspectral 3D Imaging
Suhyun Shin1Seokjun Choi1Felix Heide2Seung-Hwan Baek1
1POSTECH2Princeton University
430 530 630
430430nmnm
490490nmnm
 500500nmnm
 510510nmnm
 520520nmnm
 530530nmnm
 540540nmnm
 550550nmnm
 560560nmnm
 570570nmnm
580580nmnm
 590590nmnm
 600600nmnm
 610610nmnm
 620620nmnm
 630630nmnm
 640640nmnm
 650650nmnm
 660660nmnm
440440nmnm
 450450nmnm
 460460nmnm
 470470nmnm
 480480nmnm
900
850
800
750
700
650
600
550
22222
1
Ours
Ours
1
1
2
2GT
GT
Imaging systemScene(a) (b) (c) (d)
(e)
Intensity
0
Wavelength [nm][mm]
Figure 1. We introduce dispersed structured light, a low-cost high-quality hyperspectral 3D imaging method. By placing a diffraction
grating ﬁlm on a conventional camera-projector setup, we disperse structured-light patterns. Analyzing the images captured under the
dispersed structured light enables accurate hyperspectral 3D reconstruction. (a) Capture conﬁguration, (b) estimated hyperspectral image
in sRGB, (c) comparison with spectroradiometer measurements, (d) estimated depth map, (e) estimated hyperspectral image.
Abstract
Hyperspectral 3D imaging aims to acquire both depth
and spectral information of a scene. However , exist-
ing methods are either prohibitively expensive and bulky
or compromise on spectral and depth accuracy. In thispaper , we present Dispersed Structured Light (DSL), acost-effective and compact method for accurate hyperspec-tral 3D imaging. DSL modiﬁes a traditional projector-camera system by placing a sub-millimeter thick diffrac-
tion grating ﬁlm front of the projector . This conﬁguration
enables dispersing structured light based on light wave-length. To utilize the dispersed structured light, we devisea model for dispersive projection image formation and aper-pixel hyperspectral 3D reconstruction method. We val-idate DSL by instantiating a compact experimental proto-type. DSL achieves spectral accuracy of 18.8 nm full-widthhalf-maximum (FWHM) and depth error of 1 mm, outper-forming prior work on practical hyperspectral 3D imag-ing. DSL promises accurate and practical hyperspectral 3Dimaging for diverse application domains, including com-
puter vision and graphics, cultural herit age, g eology, and
biology.1. Introduction
Hyperspectral 3D imaging aims to capture both depth andspectrum per pixel. Allowing for geometric-spectral analy-sis of a scene [ 4,21–23,44], this imaging modality has po-
tential applications across diverse domains, including foodripeness detection [ 41], mineral detection [ 42], art authenti-
cation [ 36], classiﬁcation [ 26], and cultural heritage preser-
vation [ 19].
Despite of the potential in capturing rich geometric and
spectral information, existing hyperspectral 3D imagingmethods are often impractical, due to the high instrumen-
tation costs, large form factor, and low accuracy. Speciﬁ-cally, high-end systems such as using coded-aperture snap-shot spectral imaging systems (CASSI) and liquid-crystal
tunable ﬁlters provide accurate depth and spectral infor-mation, however mandate high cost and large form fac-tor [ 7,14,19,37,44–46,48]. In contrast, affordable hy-
perspectral 3D imaging systems suffer from low accuracyof depth and spectrum [ 4,15,20–23,29,35].
In this paper, we exploit dispersion, a phenomenon
where light rays are spatially redirected according to theirwavelength either by refraction or diffraction. Using theoptical dispersion, we present dispersed structured light
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
24997
aiming for practical and accurate hyperspectral 3D imag-
ing. DSL augments a conventional projector-camera systemwith diffraction grating ﬁlm, which adds a negligible costand size of around 10 USD with a sub-millimeter thickness.The diffraction grating in front of the projector spatially dis-perses broadband projector light based on its wavelength,resulting in a dispersed pattern consisting of several diffrac-tion orders [ 13].
Speciﬁcally, zero-order diffraction permits light to pass
through the diffraction grating as if the grating is notpresent, enabling us to exploit it for 3D imaging. First-order diffraction changes the direction of a light ray based
on wavelength, resulting in dispersed structured light pat-
terns that aid hyperspectral reconstruction. We develop amodel for dispersive projection image formation and a per-pixel hyperspectral 3D reconstruction method using multi-order diffractions.
DSL achieves an average depth error of 1 mm and
spectral FWHM of 18.8 nm in the visible spectrum, out-
performing existing practical hyperspectral 3D imagingmethod [ 21] that has a spectral FWHM of 83 nm.
In summary, we make the following contributions.
• We present dispersed structured light that enables com-
pact, low-cost, and high-quality hyperspectral 3D imag-ing by augmenting a projector-camera setup with adiffraction grating ﬁlm.
• We develop a model for disperisve projection image
formation and per-pixel hyperspectral 3D reconstructionmethod that considers both zero-order and ﬁrst-orderdiffractions.
• We perform extensive evaluations and show that DSL
achieves an average depth error of 1 mm and spectral
FWHM of 18.8 nm, outperforming the state-of-the-art af-
fordable hyperspectral 3D imaging methods.
2. Related Work
Hyperspectral 3D Imaging Existing work on hyperspec-
tral 3D imaging typically combines the ﬁeld of hyperspec-tral imaging [ 5,24,25,27,47] with depth imaging [ 15,
38,44]. Speciﬁcally, researchers have paired CASSI [ 43]
with structured light [ 7,19], time-of-ﬂight camera [ 37],
light-ﬁeld camera [ 45], and stereo [ 44]. An alternative to
CASSI is the use of spectral bandpass ﬁlters with struc-tured light [ 4,14] or a light ﬁeld camera [ 48]. Xu et al.[ 46]
present a hyperspectral projector that incorporates collima-tion optics, diffraction grating, and digital micro-mirror de-vice. Although these systems acquire accurate hyperspec-
tral 3D information, their increased complexity due to relay
lenses, narrow-band spectral ﬁlters, and plate-based disper-sion optics results in a high building cost and large formfactor.
Several methods have explored hyperspectral 3D imag-
ing with compact setups. Baek et al. [ 4] propose a diffrac-tive optical element (DOE) producing a point spread func-
tion (PSF) that varies with scene spectrum and depth, facil-itating single-shot hyperspectral 3D imaging through bluranalysis. However, the depth and spectral reconstruction
accuracy are limited due to the low-frequency characteris-
tics of the PSF. Li et al. [ 21,23] employ a projector-camera
setup to capture a scene with varying trichromatic projectorprimaries with known spectra, allowing for hyperspectralreconstruction. Despite its practicality, the spectral accu-racy is limited by large spectral bandwidth of the projectorprimary spectra, resulting in 83 nm FWHM. In comparison,the proposed DSL enables accurate hyperspectral 3D imag-ing with 18.8 nm FWHM and average depth error of 1 mmby dispersing projector light at a cost and form factor on-parwith conventional structured light systems.
Structured Light Structured light techniques project il-
lumination patterns onto a scene and analyze the reﬂectedlight using a camera [ 8]. The projected patterns enable
establishing correspondence between camera and projectorpixels for 3D imaging. DOEs, combined with narrow-bandcoherent laser, have often been employed as cost-effectivecomponents for generating structured light [ 2,18,32]. Em-
ploying a conventional projector instead of the DOE-basedlaser illumination allows for using multiple patterns in aprogrammable manner, signiﬁcantly enhancing depth accu-racy [ 9,38]. V arious structured light patterns have been
proposed to facilitate 3D imaging robust to global illumina-tion [ 11,31], light transport analysis [ 10,33], and energy-
efﬁcient 3D imaging [ 34]. Introducing polarizing optics to a
conventional projector further enhances its capability, mak-ing 3D imaging of translucent objects and polarimetric lighttransport analysis feasible [ 1,17]. The proposed DSL aug-
ments structured light by only placing a diffraction gratingﬁlm in front of a projector, enabling accurate hyperspectral3D imaging with a compact setup.
Dispersive Optics for Cameras and Projectors Disper-
sive optics, such as prism and diffraction grating, have beenoften employed in the design of cameras and displays, inparticular for hyperspectral imaging. CASSI employs dis-
persive optics and a coded mask to obtain masked spectral
images with a wavelength-dependent translation [ 43]. Us-
ing a prism and a coded mask without relay lenses enables
video-rate hyperspectral imaging [ 6]. Computed tomogra-
phy imaging spectrometers leverage multi-order dispersionfrom a diffraction grating for hyperspectral imaging [ 30].
Hostettler et al.[ 16] use a prism and a mask to implement
a trichromatic color projector based on dispersion. Mohanet al. [ 28] employ a diffraction grating and an attenuation
mask to control the spectral power distribution of projec-tor light. Recently, Sheinin et al.[ 39,40] use a diffraction
grating and a line camera to track fast-moving sparse scene
24998
points. Our DSL augments a projector-camera system only
with a diffraction grating ﬁlm, allowing to keep the formfactor compact and cost low. Instead of directly maskingthe spectrum emitted from the projector using additionalcomponents, we analyze and exploit how structured lightpatterns are dispersed and illuminate a scene.
3. Dispersive Projection Image Formation
Imaging Setup We devise the proposed DSL imaging
system with the goals of compactness and affordability.To this end, we combine a conventional trichromatic cam-era (FLIR GS3-U3-32S4C-C) with a trichromatic projector(LG PH30N) and a diffraction-grating ﬁlm (Edmund #54-509) mounted in front. This conﬁguration makes light fromthe projector undergo dispersion, and, as such, patternsemitted from the projector are spatially dispersed dependingon wavelength. Figure 2shows our experimental prototype
and a captured image with a projector pattern composed of
four squares. For zero-order diffraction, light does not ex-perience wavelength-dependent changes, and the projectedsquares retain their original shapes. For ﬁrst-order diffrac-tions, each square undergoes wavelength-dependent shift.
3.1. Image Formation
Background on Diffraction Grating Diffraction grating
consists of repetitive, even-spaced micron-scale groovescharacterized by the groove density g. The interaction of
light with the grating incurs wavelength-dependent diffrac-tion [ 13], which is modeled in the geometric-optics perspec-
tive as a redirection of the light path:
dg(v,m,λ)=(−mgλ+v
x/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
dx,vy/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
dy,/radicalBig
1−d2x−d2y),(1)
wheremis the diffraction order, v=(vx,vy,vz)is the
incident light vector. x-axis here is aligned with the groove
direction and λis the wavelength. dg(v,m,λ)is the ray di-
rection after diffraction. In addition to the direction change,the diffraction grating introduces a change in light intensitydepending on diffraction order and wavelength, represented
as diffraction efﬁciency η
m,λ [13].
We use a diffraction grating which creates positive and
negative ﬁrst-order diffractions appearing to the left andright sides with respect to the zero-order component asshown in Figure 2(c). Note that higher-order diffractions
are not detected inside of our camera FoV .
Forward Model For a projector pattern P(q,c), whereq
is a projector pixel and cis a color channel (R, G, or B), the
light intensity L(q,λ)emitted for each wavelength λis
L(q,λ)=/summationdisplay
cΩproj
c,λP(q,c), (2)
Camera Projector
Diffraction
grating
(a) Experimental protot ype
(c) Example of dispersed structured li ght captur e(b) Schematic dia gramScene
Camera ima ge
+1 order 
diffraction
0 orde r 
diffraction
Object
Projector patter n
Projector
Diffraction grating
Cam era
Figure 2. Experimental prototype. (a) & (b) Our prototype con-
sists of an RGB projector equipped with a diffraction grating ﬁlm,and an RGB camera. (c) An example projector pattern and its cor-responding captured image, exhibiting clear ﬁrst-order diffraction.
whereΩproj
c,λis the projector emission function shown in Fig-
ure 3(a).
The light then passes through the diffraction grating and
splits into multiple light rays with varying propagation di-rections dg(v,m,λ), depending on diffraction order mand
wavelength λ. Here, we focus on a ray corresponding to
wavelength λand diffraction order m. The ray will prop-
agate and eventually illuminate a scene point Sat depthz.
The reﬂected light from the scene point will be captured bya camera pixel p. We then establish a correspondence func-
tionψthat maps the camera pixel p, depthz, diffraction
orderm, and wavelength λto the corresponding projector
pixelq
m,λ that emitted the light ray illuminating the scene
point:
qm,λ=ψ(p,z,m,λ ). (3)
We proceed to describe our full image formation before de-
scribing how we model the correspondence function ψ.
The intensity at the camera pixel pand color channel c
then can be modeled as
I(p,c)=/summationdisplay
λ∈ΛΩcam
c,λH(p,λ)1
d(p)21/summationdisplay
m=−1ηm,λL(qm,λ,λ)
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
Dispersed structured light,
(4)
24999
+1 order -1 order
(a) Spectral response
/emission functionsNormalized
spectral intensity
450 500 550 600 650
Wavelength [nm]0.00.20.40.60.81.0Camera Projector
(d) Spatially varying dispersio n0 200 400 600 8005004003002001000Row [px]
Column [px]500
100
(b) Image formationCamera ProjectorGrating Camera
centerProjector centerObject Zero-order
First-order
(c) Depth dependency in first-order
diffraction correspondence450 500 550 600 650
370620
600 650 700 750 800 850 900
Depth [mm]
[nm]
Figure 3. Image formation. (a) Camera response function and projector emission function. (b) Schematic diagram of image formation.
(c) Depth dependency of the correspondence function ψform=1 and a camera pixel p. (d) Spatially-varying correspondence map for
depth 700 mm and wavelength 430 nm for the ﬁrst-order diffractions m=−1,1.
whereΩcam
c,λis the camera response function shown in Figure
3(a),Λ=[λ1,···,λ47]is the set of 47 wavelengths sam-
pled from 430 nm to 660 nm with 5 nm interval. H(p,λ)is
the hyperspectral intensity, which we aim to estimate. d(p)
is the propagation distance from the projector to the scenepoint S, as such, the term 1/d(p)
2describes inverse-square
law.
The dispersed structured light term in Equation ( 4) de-
scribes the light intensity projected to the scene point S
by aggregating zero- and ﬁrst-order light energy L(qm,λ,λ)
weighted by the corresponding diffraction efﬁciency ηm,λ.
In the following, we model the correspondence function ψ
introduced in Equation ( 3).
3.2. Correspondence under Dispersion
Zero-order Diffraction For zero-order diffracted light
(m=0 ), light transport can be analyzed as if no diffrac-
tion grating exists [ 13]: given a camera pixel pand its depth
z, we obtain the corresponding projector pixel q0,λfor any
visible wavelength λby applying perspective unprojection
and projection [ 12]:
q0,λ=ψ(p,z,0,λ) (5)
=project (unproject (p,z)), (6)
where unproject (·)is the perspective unprojection from
a camera pixel pto a 3D point Susing the depth z.
project (·)is the perspective projection from the 3D point
Sto the projector.
First-order Diffractions Correspondence for ﬁrst-order
diffractions ( m=−1or1) is more challenging to model.
Due to the direction change by diffraction grating, direct
perspective projection from the scene point Sto the projec-
tor is no longer valid. Instead, we need to identify a point r
on the diffraction grating where the light ray from the pro-jector center t
projdiffracts at rand reaches to the scene point
S. This is illustrated in Figure 3(b) and by using Equa-
tion ( 1) it can be formulated as
minimize
r/bardbldg(˙−−→tprojr,m,λ)−˙−→rS/bardbl2, (7)where˙−→xrepresents the normalized vector of−→x. While
the formulation is intuitive, solving Equation ( 7) over the
entire 3D volume, for each wavelength λand diffraction
orderm, is prone to calibration errors and is computation-
ally demanding when approached with iterative numericalmethods like the Newton-Raphson method or root-ﬁndingalgorithms.
Hence, instead of directly solving Equation ( 7), we de-
velop a data-driven model for efﬁcient and accurate corre-spondence mapping of the ﬁrst-order diffractions. We pre-pare samples: ﬁrst-order corresponding projector pixel po-sitionsq
/prime
m,λ for wavelengths λ/prime∈Sλ, grid-sampled camera
pixelsp/prime∈Sp, and depths z/prime∈Sz. Section 5describes
the sample acquisition. We then ﬁt an exponential functionto the samples for modeling depth dependency of the corre-spondence function ψand perform linear interpolation for
spatial and spectral dimensions. As the ﬁrst-order diffrac-tion in our prototype occurs along the horizontal axis, andour hyperspectral reconstruction only uses horizontal corre-spondence, we only model the horizontal coordinate of thecorrespondence function, i.e., the column index. More de-tails on our data-driven correspondence model can be foundin the Supplemental Document.
Our data-driven ﬁrst-order correspondence model ob-
tains a mean reprojection error of 1px and can be efﬁcientlyevaluated with tabulation in O(1). Figure 3(c) shows the
depth dependency of the modeled correspondence functionψ. Figure 3(d) shows the spatially-varying correspondence
map for depth 700 mm, wavelength 430 nm, and the diffrac-
tion orders m=1 and−1.
4. Hyperspectral 3D Reconstruction
With the image formation model from above in hand, we
estimate a depth map using binary-code structured lightand then reconstruct the hyperspectral image using scanline
structured light.
4.1. Depth Reconstruction
For depth estimation, we use binary-code structuredlight patterns shown in Figure 4and represented as
25000
(c) Quantitative anal ysis(a) DSLSimulated images under binary-code patterns
(b) Conventional SLDepth error [cm]
Standard deviation0.0 0.01 0.02 0.040.100.110.120.130.140.150.16DSL Conventional SL DSL Conventional SLIntens ity
Binary code pattern index 0 5 10 15 30 35 40 20 25020406080
noise std = 0.01
depth error = 0.104 Our experimental setting
0.03
Figure 4. Binary decoding under dispersion. Simulated images
under the binary-code patterns for (a) DSL and (b) conventional
SL. (c) Intensity of a camera pixel with DSL and conventional SL.(d) Depth error of binary-code decoding for DSL and conventionalSL with varying Gaussian noise.
[P1,...,P Kb]:Pi(q,∀c)=bit(q,i), where bit(q,i)re-
turns thei-th binary bit of the decimal horizontal and verti-
cal location of the projector pixel q, encoding the projector
pixel location as a binarized code [ 9].Kb=4 0 is the num-
ber of patterns. Then, each camera pixel pobserves the ﬂuc-
tuating intensity values under these binary-code patterns,producing an observation vector [I
1(p,∀c),...,I Kb(p,∀c)]
shown in Figure 4(c).
Binary-code Decoding From the observation vector, we
aim to estimate the zero-order correspondence q0,λ, which
will facilitate the estimation of depth zthrough trian-
gulation using the zero-order correspondence function
ψ(p,z,0,λ). Note that zero-order correspondence has no
dependency on wavelength λ. For thei-th projector pattern
Pi, the conventional decoding method binarizes the cap-
tured intensity Iiinto 0 or 1 based on its intensity level
by using RGB-to-gray conversion and thresholding with a
constantτ. The binary code is then decoded to a decimal
number, which is the location of the corresponding projec-tor pixelq
0,λ.
Validation of Binary-code Decoding for DSL An im-
portant question is whether the conventional binary decod-ing works for DSL in the presence of dispersed light. Weprovide both mathematical and experimental veriﬁcation.
In theory, this can be conﬁrmed if the intensity lit by thezero-order diffracted light exceed the intensity when not re-
ceiving the zero-order diffracted light. We derive the mini-mum captured intensity I
on
minwhen a scene point is only illu-minated by the zero-order structured light. Also, we model
the maximum captured intensity Ioff
max when the zero-order
structured light does not illuminate the point and the ﬁrst-order light illuminates. The quantities are written as
I
on
min(p,c)=/summationdisplay
λΩcam
c,λH(p,λ)η0,λ/summationdisplay
c/primeΩproj
c/prime,λ, (8)
Ioff
max(p,c)=/summationdisplay
λΩcam
c,λH(p,λ)/summationdisplay
m=1,−1ηm,λ/summationdisplay
c/primeΩproj
c/prime,λ.(9)
We then compare both quantities, giving an observation
that the inequality Ioff
max<Ion
minholds in our setup conﬁgu-
ration, because for all wavelength λthe following inequal-
ity holdsη0,λ/summationtext
c/primeΩproj
c/prime,λ>/summationtext
m=1,−1ηm,λ/summationtext
c/primeΩproj
c/prime,λ. This
validates the use of conventional binary-code decoding forDSL in theory. Refer to the Supplemental Document for thederivation details.
Next, we further validate the applicability of binary de-
coding on 10
6simulated samples with random hyperspec-
tral reﬂectance for a planar object with Gaussian measure-ment noise. Example observation vectors with and withoutconsidering ﬁrst-order diffractions are shown in Figure 4(c).
We set the Gaussian standard deviation 0.01corresponding
to the real-world noise in our hardware measurements. Theaverage depth error after decoding, shown in Figure 4(d), is
1.04mm in the presence of ﬁrst-order diffractions. This ex-
periment further validates the use of binary-code decodingfor DSL.
4.2. Hyperspectral Reconstruction
Once depth zis obtained, we proceed to estimate per-pixel
hyperspectral intensity H=[H(p,λ1),···,H(p,λN)]∈
RN×1. To this end, we use the scanline structured light
patterns[P1,,...,P Ks]that scans through the whole column
with a line width w.Ks= 318 is the number of scanline
patterns. Figure 5shows that the ﬁrst-order diffracted light
from the i-th scanline pattern produces a narrow-band il-
lumination spread across multiple columns. Consequently,scanning columns from left to right enables illuminatingeach scene point with every narrow-band light, facilitatinghigh-quality hyperspectral reconstruction.
Pixel Intensity under Scanline Patterns For thei-th
scanline pattern, a camera pixel preceives either zero-order
diffraction, ﬁrst-order diffractions, or no illumination at all.This simpliﬁes our image formation as
I
i(p,c)=⎧
⎪⎨
⎪⎩/summationtext
λΩcam
c,λH(p,λ)η0,λL(q0,λ,λ)for zero order ,
Ωcam
c,λH(p,λ)ηm,λL(qm,λ,λ) for ﬁrst orders ,
0 otherwise .
(10)
Figure 5(c) shows the intensity graph of a camera
pixelpfor varying index of the scanline patterns:
25001
-1 order
(a) White-pattern RAW image (b) 90th scanline image 
(c) 220th scanline image (d) Intensity graph of a single pixel 0.00.20.3
0.151015Intensity
Pattern indexRed Green Blue
50 100 150 200 2500 order
-1 order
0 order
Figure 5. Hyperspectral imaging with scanline illuminations.
Captured images under (a) white pattern and (b)&(c) scanline pat-
terns. Narrow-band illumination over multiple columns is shown
in (b), which originates from the ﬁrst-order diffraction. (d) Pixelintensity with respect to varying scanline pattern index.
[I1(p,c),···,IKs(p,c)]. For the ﬁrst-order cases ( m=
−1or1), the intensity stems from a speciﬁc wavelength λ,
hence narrow-band illumination.
Index Mapping for First-order Diffraction We exploit
ﬁrst-order diffractions for accurate hyperspectral recon-
struction. We ﬁnd the scanline pattern index iof which
m-order diffracted light with wavelength λilluminates the
camera pixel p. In fact, we already obtained this index map-
ping in the form of the correspondence function ψ:
δm,i=px2index (ψ(p,z,m,λ i)), (11)
whereδm,i is the corresponding scanline pattern index and
px2index is the conversion function that simply returns
the scanline pattern index that lights up at the input projec-
tor pixel location.
We then collect the intensity captured under narrow-band
illumination for all target wavelengths in [λ1,···,λN]for
each diffraction order m:
Im=[Iδm,1(p,c),···,Iδm,N(p,c)]∈RN×3. (12)
Optimization We formulate a per-pixel optimization
problem for accurate hyperspectral reconstruction:
argmin
H1/summationdisplay
m=−1κm/bardblAmH−Im/bardbl2
2
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
Data term+κλ/bardbl∇λH/bardbl2
2/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Regularization term,(13)
whereκmis the spatially-varying per-pixel balancing
weight for the diffraction order m. Refer to the Supple-
mental Document how we set the balancing weight. Amisthe system matrix deﬁned as
Am=/braceleftBigg/summationtext
λΩcam
c,ληm,λL(qm,λ,λ)for zero order ,
Ωcam
c,ληm,λL(qm,λ,λ) for ﬁrst orders .(14)
∇λis the gradient operator along the spectral axis. The ﬁrst
term in Equation ( 13) accounts for the reconstruction loss
across the multiple diffraction orders. The second term isthe spectral smoothness prior [ 3]. We use gradient descent
for the optimization, which takes three minutes to converge.Our hyperspectral reconstruction method operates on a per-pixel basis and exploits ﬁrst-order diffractions, enabling ac-curate hyperspectral image reconstruction.
5. Calibration
We calibrate the image formation parameters of the projec-tor, camera, and diffraction grating, as brieﬂy described in
the following. A detailed description of the calibration pro-cedure can be found in the Supplemental Document.
Diffraction Efﬁciency To calibrate the diffraction efﬁ-
ciencyη
m,λ, we measure the intensity of m-order diffracted
light at each wavelength λprojected onto a Spectralon sam-
ple. We use spectral bandpass ﬁlters at 10nm intervals,
from430 nm to660 nm. Diffraction efﬁciency is then com-
puted as the intensity ratio of each ﬁrst-order wavelengthmeasurement over the zero-order intensity.
Spectral Response and Emission Functions The projec-
tor spectral emission function, Ω
proj
λ,c, was obtained by pro-
jecting red, green, and blue dots onto a Spectralon target,measuring the reﬂected radiance with a spectroradiome-
ter (JETI Specbos 1211), and normalizing the results withSpectralon reﬂectance. For the camera response function,Ω
cam
λ,c, we use the data provided by the camera manufacturer.
For both emission and response functions, we perform re-ﬁnements of which details can be found in the SupplementalDocument. Figure 3(a) shows the calibrated functions.
First-order Correspondence Model To calibrate the
ﬁrst-order correspondence model described in Section 3.2,
we acquired images of ﬂat Spectralon surfaces at ﬁve dif-ferent depth positions with scanline illumination patternspresent. These images were captured using multiple band-pass ﬁlters. For sampled camera pixels, denoted p
/prime,w e
identiﬁed the corresponding projector pixel q/prime
m,λ from the
captured images. Using the samples, we obtain the data-
driven corresponding model ψ.
6. Assessment
Hyperspectral 3D Reconstruction DSL enables accu-
rate hyperspectral 3D imaging. We estimate a hyperspectral
25002
480530580 630
Wavelen gth [nm]1.0
0.00.20.40.60.8Intensit y2
480530580 630
Wavelen gth [nm]1.0
0.00.20.40.60.8Intensit y3
4805305806301.0
0.00.20.40.60.8Intensit y
Wavelen gth [nm]
Ours
GT1
430nm-660nm
10nm interval(d)430nm
480nm
530nm
580nm 590nm540nm 550nm
600nm 610nm 620nm
630nm560nm 570nm490nm 500nm440nm 450nm 460nm
510nm 520nm470nm
640nm 650nm 660nm
900
850
800
750
700
650
600
1
2
3(a) (b)
(c)
Figure 6. Hyperspectral 3D imaging. (a) Reconstructed hy-
perspectral image visualized in sRGB, (b) reconstructed depthmap, (c) estimated hyperspectral intensity for three different pointscompared with spectroradiometer measurements, (d) estimatedhyperspectral image.
image with 46 channels, from 430 nm to 660 nm at 5 nm in-
tervals, along with a depth map. Figures 1and 6shows
reconstruction results for two real-world scenes.
Reconstruction of High-frequency Spectral Curves
Figure 7shows the results of our DSL in comparison with Li
et al. [ 21], state-of-the-art practical hyperspectral 3D imag-
ing method. We captured a scene containing nine band-pass spectral ﬁlters, each with a bandwidth of 10 nm. DSLaccurately identiﬁes the center wavelengths and achievesan average FWHM of 18.8 nm. In contrast, Li et al. [ 21]
rely on broadband RGB illuminations of projector, result-ing in a signiﬁcantly broader FWHM of 83 nm, due to thelimited capability in differentiating high-frequency spectral
features. This performance gap mainly originates from ouruse of ﬁrst-order diffractions. We test the DSL withoutusing the ﬁrst-order term in Equation ( 13). As expected,
the resuting spectra is overly smooth with 50 nm FWHMand cannot detect the high-peak spectral features. This is
aligned with the results of Li et al.[ 21], demonstrating the
importance of using ﬁrst-order diffractions.
(b) Reconstructed sRGB (c) Reconstructed depth
Li et al. Ours Ours without first-order GTIntensity
(e) Reconstructed h yperspectral intensit y of the spectral filtersLi et al.Ours
Ours without first-order diffractions
(d) Reconstructed hyperspectral image(a) Filter layout
900
850
800
750
700
650
600
550[mm]
470
460
450570
560
550 600610620
1
 0
1
01
0
1
0
430 530 630
 430 530 630
470nm470nm
 610nm610nm
 600nm600nm
 460nm460nm
 450nm450nm
 570nm570nm
 560nm560nm
 550nm550nm
 620nm620nm
430 530 630
[nm]
450nm
460nm
470nm
550nm
560nm
570nm
600nm
610nm
620nm
Figure 7. High-frequency spectral imaging. (a) Filter layout,
(b) reconstructed hyperspectral image in sRGB, (c) reconstructeddepth, (d) reconstructed hyperspectral images, (e) spectral curves
for the nine bandpass ﬁlters.
Colorchecker and Metamerism Figure 8shows the
spectral accuracy of our DSL measured on a ColorCheckerand metameric fake and real leaves. For smooth spectralcurves of color patches, DSL successfully reconstructs thehyperspectral intensity. Also, DSL enables telling cleardifference between fake and real leaves. We obtained theground-truth intensity using a spectroradiometer.
Depth Accuracy To assess the depth accuracy of our ex-
perimental prototype, we assess both relative and absolutedepth errors. Figure 9(a) show that DSL accurately esti-
mates the distance between two boxes, with a marginal error
25003
(a) Reconstructed
hyperspectral ima ge in sR GB(b) Reconstructed depth ma p
(d) Colorchecker evaluatio n(c) Metamerism of real plant and fake plan t1
00.20.40.60.8
430 480 530 580 630
Wavelength [nm]Real leaf
1
00.20.40.60.8
430 480 530 580 630Fake leaf
Wavelength [nm]
900
850
800
750
700
650
600
550[mm]
GTT
OursOursHyperspectral intensit y Hyperspectra l intens ity
Wavelen gth[nm]0
0001.0
0
0.5
5
0
0001.0
0
0.5
5
0
001.0
00
0.5
555
0
01.0
00
0.5
5
430
430 630
630430 630430 630430 630430 630430 630
GT
Ours
FakeReal
Figure 8. Evaluation on a ColorChecker and metameric sam-
ples. (a) Reconstructed hyperspectral image in sRGB, (b) recon-
structed depth map, (c) spectra of metameric samples (real andfake leaves), (d) reconstructed spectra of each color patch.
of only 1 mm. Figure 9(b) evaluates the absolute depth error
by capturing a planar object mounted on a linear translationstage (Thorlabs #LTS150C). Across the working range ofthe translation stage with a 10 mm step size, DSL achieves
an average depth error of 1.35mm. Note that these exper-
imental results are aligned with our synthetic experimentsshown in Figure 4.
7. Conclusion
In this paper, we introduced DSL, an accurate, low-cost,
and compact hyperspectral 3D imaging method. Ourdispersion-aware image formation, per-pixel hyperspectral3D reconstruction, and calibration enables accurate hyper-
spectral 3D imaging. DSL can be implemented in an af-fordable experimental prototype by using a diffraction grat-
ing adding sub-millimeter thickness at a cost of 10 USD.Our experimental prototype achieves depth error of 1 mm
White-pattern
RAW SceneGround truth 
measurement
950
900
850
800
750
700
650
600
550
[mm]
Reconstructed depth 
Linear stage system GT:38mm Est.:39mm
850
800
750
700Depth [mm]
Position index02 46 8 10 12 14(a) Relative depth evaluation
(b) Absolute depth evalutionBox object
GT
Ours
Figure 9. Depth accuracy. We estimate (a) the distance between
two boxes and (b) the depth of the box mounted on a translation
stage. We obtain the depth errors of 1 mm for the relative-depth
experiment and 1.35 mm for the absolute depth experiment.
and spectral FWHM of 18.8 nm, outperforming prior work
on practical hyperspectral 3D imaging. We ﬁnd that DSLmakes a step towards practical hyperspectral 3D imagingfor applications beyond computer vision and graphics.
Limitations and Future Work While accurate and low-
cost, our current experimental prototype takes around10 minutes to capture a scene, restricting it to static scenes.Also, the low intensity of ﬁrst-order diffracted light limitsthe working depth range to be less than a meter.
For handling dynamic scenes and increasing depth range,
we leave developing a light-efﬁcient capture system andjoint depth-spectrum reconstruction method as an interest-ing direction for future research. Another unexplored direc-tion is to ﬁnd the optimal design of the diffraction pattern
for efﬁcient and accurate hyperspectral 3D imaging. Re-
cent differentiable optimization of imaging systems couldbe employed to achieve the goal. Lastly, the core princi-ple of DSL can be applied to other spectral ranges beyondvisible spectrum.
Acknowledgements Seung-Hwan Baek was
supported by Korea NRF (RS-2023-00211658,2022R1A6A1A03052954), Korea IITP MSIT (No.2019-0-01906, Artiﬁcial Intelligence Graduate School Program-POSTECH), and POSCO. Felix Heide was supported byan NSF CAREER Award (2047359), a Packard FoundationFellowship, a Sloan Research Fellowship, a Sony Y oungFaculty Award, a Project X Innovation Award, and anAmazon Science Research Award.
25004
References
[1] Seung-Hwan Baek and Felix Heide. Polarimetric spatio-
temporal light transport probing. ACM Trans. Graph. ,4 0
(6):1–18, 2021. 2
[2] Seung-Hwan Baek and Felix Heide. Polka lines: Learning
structured illumination and reconstruction for active stereo.InIEEE Conf. Comput. Vis. Pattern Recog. , pages 5757–
5767, 2021. 2
[3] Seung-Hwan Baek, Incheol Kim, Diego Gutierrez, and
Min H Kim. Compact single-shot hyperspectral imaging us-ing a prism. ACM Trans. Graph. , 36(6):1–12, 2017. 6
[4] Seung-Hwan Baek, Hayato Ikoma, Daniel S Jeon, Y uqi
Li, Wolfgang Heidrich, Gordon Wetzstein, and Min HKim. Single-shot hyperspectral-depth imaging with learneddiffractive optics. In Int. Conf. Comput. Vis. , pages 2651–
2660, 2021. 1,2
[5] Y uanhao Cai, Jing Lin, Xiaowan Hu, Haoqian Wang, Xin
Y uan, Y ulun Zhang, Radu Timofte, and Luc V an Gool.Coarse-to-ﬁne sparse transformer for hyperspectral image re-construction. In Eur . Conf. Comput. Vis. , pages 686–704.
Springer, 2022. 2
[6] Xun Cao, Xin Tong, Qionghai Dai, and Stephen Lin. High
resolution multispectral video capture with a hybrid camerasystem. In IEEE Conf. Comput. Vis. Pattern Recog. , pages
297–304. IEEE, 2011. 2
[7] Elkin D ´ıaz, Jaime Meneses, and Henry Arguello. Hyperspec-
tral+ depth imaging using compressive sensing and struc-
tured light. In 3D Image Acquisition and Display: Tech-
nology, Perception and Applications , pages 3M3G–6. Optica
Publishing Group, 2018. 1,2
[8] Jason Geng. Structured-light 3d surface imaging: a tutorial.
Adv. Opt. Photon. , 3(2):128–160, 2011. 2
[9] Jason Geng. Structured-light 3d surface imaging: a tutorial.
Adv. Opt. Photon. , 3(2):128–160, 2011. 2,5
[10] Mohit Gupta, Y uandong Tian, Srinivasa G Narasimhan, and
Li Zhang. (de) focusing on global light transport for active
scene recovery. In IEEE Conf. Comput. Vis. Pattern Recog. ,
pages 2969–2976. IEEE, 2009. 2
[11] Mohit Gupta, Qi Yin, and Shree K Nayar. Structured light in
sunlight. In Int. Conf. Comput. Vis. , pages 545–552, 2013. 2
[12] Richard Hartley and Andrew Zisserman. Multiple view ge-
ometry in computer vision . Cambridge university press,
2003. 4
[13] Eugene Hecht. Optics (F ourth Edition) . Addison-Wesley,
2001. 2,3,4
[14] Stefan Heist, Chen Zhang, Karl Reichwald, Peter
K¨uhmstedt, Gunther Notni, and Andreas T ¨unnermann.
5d hyperspectral imaging: fast and accurate measurement ofsurface shape and spectral characteristics using structuredlight. Optics express , 26(18):23366–23379, 2018. 1,2
[15] Keita Hirai, Ryosuke Nakahata, and Takahiko Horiuchi.
Measuring spectral reﬂectance and 3d shape using multi-primary image projector. In Image and Signal Processing:
7th International Conference, ICISP 2016, Trois-Rivi `eres,
QC, Canada, May 30-June 1, 2016, Proceedings 7 , pages
137–147. Springer, 2016. 1,2[16] Rafael Hostettler, Ralf Habel, Markus Gross, and Wojciech
Jarosz. Dispersion-based color projection using maskedprisms. Comput. Graph. F orum , 34(7), 2015. 2
[17] Daniel S Jeon, Andr ´eas Meuleman, Seung-Hwan Baek, and
Min H Kim. Polarimetric itof: Measuring high-ﬁdelity depththrough scattering media. In IEEE Conf. Comput. Vis. Pat-
tern Recog. , pages 12353–12362, 2023. 2
[18] Gyeongtae Kim, Y eseul Kim, Jooyeong Y un, Seong-Won
Moon, Seokwoo Kim, Jaekyung Kim, Junkyeong Park,Trevon Badloe, Inki Kim, and Junsuk Rho. Metasurface-
driven full-space structured light for three-dimensionalimaging. Nature Communications , 13(1):5920, 2022. 2
[19] Min H Kim, Todd Alan Harvey, David S Kittle, Holly Rush-
meier, Julie Dorsey, Richard O Prum, and David J Brady. 3dimaging spectroscopy for measuring hyperspectral patternson solid objects. ACM Trans. Graph. , 31(4):1–11, 2012. 1,
2
[20] Masahiro Kitahara, Takahiro Okabe, Christian Fuchs, and
Hendrik PA Lensch. Simultaneous estimation of spectral re-ﬂectance and normal from a small number of images. InVISAPP (1) , pages 303–313, 2015. 1
[21] Chunyu Li, Y usuke Monno, Hironori Hidaka, and Masatoshi
Okutomi. Pro-cam ssfm: Projector-camera system for struc-ture and spectral reﬂectance from motion. In Int. Conf. Com-
put. Vis. , pages 2414–2423, 2019. 1,2,7
[22] Chunyu Li, Y usuke Monno, and Masatoshi Okutomi. Spec-
tral mvir: Joint reconstruction of 3d shape and spectral re-ﬂectance. In IEEE Int. Conf. Comput. Photo. , pages 1–12.
IEEE, 2021.
[23] Chunyu Li, Y usuke Monno, and Masatoshi Okutomi. Deep
hyperspectral-depth reconstruction using single color-dotprojection. In IEEE Conf. Comput. Vis. Pattern Recog. , pages
19770–19779, 2022. 1,2
[24] Lingen Li, Lizhi Wang, Weitao Song, Lei Zhang, Zhiwei
Xiong, and Hua Huang. Quantization-aware deep optics fordiffractive snapshot hyperspectral imaging. In IEEE Conf.
Comput. Vis. Pattern Recog. , pages 19780–19789, 2022. 2
[25] Miaoyu Li, Ying Fu, Ji Liu, and Y ulun Zhang. Pixel adaptive
deep unfolding transformer for hyperspectral image recon-struction. In Int. Conf. Comput. Vis. , pages 12959–12968,
2023. 2
[26] S Mahesh, DS Jayas, J Paliwal, and NDG White. Hyperspec-
tral imaging to classify and monitor quality of agriculturalmaterials. Journal of Stored Products Research , 61:17–26,
2015. 1
[27] Maksim Makarenko, Arturo Burguete-Lopez, Qizhou Wang,
Fedor Getman, Silvio Giancola, Bernard Ghanem, and An-drea Fratalocchi. Real-time hyperspectral imaging in hard-ware via trained metasurface encoders. In IEEE Conf. Com-
put. Vis. Pattern Recog. , pages 12692–12702, 2022. 2
[28] Ankit Mohan, Ramesh Raskar, and Jack Tumblin. Agile
spectrum imaging: Programmable wavelength modulationfor cameras and projectors. In Comput. Graph. F orum , pages
709–717. Wiley Online Library, 2008. 2
[29] Giljoo Nam and Min H Kim. Multispectral photometric
stereo for acquiring high-ﬁdelity surface normals. IEEE
computer graphics and applications , 34(6):57–68, 2014. 1
25005
[30] F. Narea-Jim ´enez, J. Castro-Ramos, J. J. S ´anchez-Escobar,
and A. Mu noz Morales. Assessment of a computed tomog-
raphy imaging spectrometer using an optimized expectation-
maximization algorithm. Appl. Opt. , 61(20):6076–6085,
2022. 2
[31] Shree K Nayar, Gurunandan Krishnan, Michael D Gross-
berg, and Ramesh Raskar. Fast separation of direct andglobal components of a scene using high frequency illumi-nation. In ACM Trans. Graph. , pages 935–944. ACM New
Y ork, NY , USA, 2006. 2
[32] Yibo Ni, Sai Chen, Y ujie Wang, Qiaofeng Tan, Shumin Xiao,
and Y uanmu Y ang. Metasurface for structured light projec-tion over 120 ﬁeld of view. Nano Letters , 20(9):6719–6724,
2020. 2
[33] Matthew O’Toole, John Mather, and Kiriakos N Kutulakos.
3d shape and indirect appearance by structured light trans-port. In IEEE Conf. Comput. Vis. Pattern Recog. , pages
3246–3253, 2014. 2
[34] Matthew O’Toole, Supreeth Achar, Srinivasa G Narasimhan,
and Kiriakos N Kutulakos. Homogeneous codes for energy-efﬁcient illumination and imaging. ACM Trans. Graph. ,3 4
(4):1–13, 2015. 2
[35] Keisuke Ozawa, Imari Sato, and Masahiro Y amaguchi. Hy-
perspectral photometric stereo for a single capture. JOSA A ,
34(3):384–394, 2017. 1
[36] Adam Polak, Timothy Kelman, Paul Murray, Stephen Mar-
shall, David JM Stothard, Nicholas Eastaugh, and FrancisEastaugh. Hyperspectral imaging combined with data clas-siﬁcation techniques as an aid for artwork authentication.Journal of Cultural Heritage , 26:1–11, 2017. 1
[37] Hoover Rueda-Chacon, Juan F Florez-Ospina, Daniel L Lau,
and Gonzalo R Arce. Snapshot compressive tof+ spectralimaging via optimized color-coded apertures. IEEE Trans.
Pattern Anal. Mach. Intell. , 42(10):2346–2360, 2019. 1,2
[38] Simon Schreiberhuber, Jean-Baptiste Weibel, Timothy Pat-
ten, and Markus Vincze. Gigadepth: Learning depth fromstructured light with branching neural networks. In Eur .
Conf. Comput. Vis. , 2022. 2
[39] Mark Sheinin, Dinesh N Reddy, Matthew O’Toole, and
Srinivasa G Narasimhan. Diffraction line imaging. In Eur .
Conf. Comput. Vis. , pages 1–16. Springer, 2020. 2
[40] Mark Sheinin, Matthew O’Toole, and Srinivasa G
Narasimhan. Deconvolving diffraction for fast imaging ofsparse scenes. In IEEE Int. Conf. Comput. Photo. , pages 1–
10. IEEE, 2021. 2
[41] Da-Wen Sun. Hyperspectral imaging for food quality analy-
sis and control . Elsevier, 2010. 1
[42] Mahesh Kumar Tripathi and H Govil. Evaluation of aviris-ng
hyperspectral images for mineral identiﬁcation and mapping.
Heliyon , 5(11), 2019. 1
[43] Ashwin Wagadarikar, Renu John, Rebecca Willett, and
David Brady. Single disperser design for coded aperturesnapshot spectral imaging. Applied optics , 47(10):B44–B51,
2008. 2
[44] Lizhi Wang, Zhiwei Xiong, Guangming Shi, Wenjun Zeng,
and Feng Wu. Simultaneous depth and spectral imaging witha cross-modal stereo system. IEEE Transactions on Circuitsand Systems for Video Technology , 28(3):812–817, 2016. 1,
2
[45] Zhiwei Xiong, Lizhi Wang, Huiqun Li, Dong Liu, and Feng
Wu. Snapshot hyperspectral light ﬁeld imaging. In IEEE
Conf. Comput. Vis. Pattern Recog. , pages 3270–3278, 2017.
2
[46] Yibo Xu, Anthony Giljum, and Kevin F Kelly. A hyperspec-
tral projector for simultaneous 3d spatial and hyperspectralimaging via structured illumination. Optics Express , 28(20):
29740–29755, 2020. 1,2
[47] Xuanyu Zhang, Y ongbing Zhang, Ruiqin Xiong, Qilin Sun,
and Jian Zhang. Herosnet: Hyperspectral explicable recon-struction and optimal sampling deep network for snapshotcompressive imaging. In IEEE Conf. Comput. Vis. Pattern
Recog. , pages 17532–17541, 2022. 2
[48] Kang Zhu, Y ujia Xue, Qiang Fu, Sing Bing Kang, Xilin
Chen, and Jingyi Y u. Hyperspectral light ﬁeld stereo match-ing. IEEE Trans. Pattern Anal. Mach. Intell. , 41(5):1131–
1143, 2018. 1,2
25006
