HEAL-SWIN: A Vision Transformer On The Sphere
Oscar Carlsson∗abJan E. Gerken∗aHampus LinanderaHeiner SpießcFredrik Ohlssond
Christoffer PeterssoneaDaniel Perssona
Abstract
High-resolution wide-angle fisheye images are becom-
ing more and more important for robotics applications
such as autonomous driving. However, using ordinary
convolutional neural networks or vision transformers on
this data is problematic due to projection and distor-
tion losses introduced when projecting to a rectangular
grid on the plane. We introduce the HEAL-SWIN trans-
former, which combines the highly uniform Hierarchi-
cal Equal Area iso-Latitude Pixelation (HEALPix) grid
used in astrophysics and cosmology with the Hierarchical
Shifted-Window (SWIN) transformer to yield an efficient
and flexible model capable of training on high-resolution,
distortion-free spherical data. In HEAL-SWIN, the nested
structure of the HEALPix grid is used to perform the patch-
ing and windowing operations of the SWIN transformer, en-
abling the network to process spherical representations with
minimal computational overhead. We demonstrate the su-
perior performance of our model on both synthetic and real
automotive datasets, as well as a selection of other image
datasets, for semantic segmentation, depth regression and
classification tasks. Our code is publicly available1.
1. Introduction
High-resolution fisheye cameras are among the most com-
mon and important sensors in modern intelligent vehi-
cles [42]. Due to their non-rectilinear mapping functions
and large field of view, fisheye images are highly distorted.
Moreover, the most commonly used large-scale computer
∗Equal contribution
aDepartment of Mathematical Sciences, Chalmers University of Tech-
nology, University of Gothenburg, SE-412 96 Gothenburg, Sweden
bCorresponding author, email: osccarls@chalmers.se
cNeural Information Processing, Science of Intelligence, Technical
University Berlin, DE-10623 Berlin, Germany
dDepartment of Mathematics and Mathematical Statistics, Ume ˚a Uni-
versity, SE-901 87 Ume ˚a, Sweden
eZenseact, SE-417 56 Gothenburg, Sweden
1https://github.com/JanEGerken/HEAL-SWIN
<latexit sha1_base64="cQJewpwtZcpJ6NEe50L9/ReM0bc=">AAADT3ichVJNTxRBEH07q8LiF8jFhMvEjcbTZsYoeCRBDBcTTFzAADEzQ+/S2fnKdC8GNvsruMKP4ugv4WIMr8vGRFHpTW9Vv6r3qrqm0zrXxkbRt1bQvnP33sxsZ+7+g4ePHs8vPNky1bjJVD+r8qrZSROjcl2qvtU2Vzt1o5IizdV2Olpz8e0j1RhdlZ/sca32i2RY6oHOEkvo857VhTJhvPxlvhv1IlnhTSf2Thd+bVYLrafYwwEqZBijgEIJSz9HAsPfLmJEqIntY0KsoaclrjDFHLljZilmJERH/B/ytOvRkmenaYSdsUrO3ZAZ4jn3e1FMme2qKvqG9jv3iWDDf1aYiLLr8Jg2pWJHFD8Qtzhkxm3Mwmde93I7093KYoC3chvN/mpB3D2zXzrvGGmIjSQSYl0yh9RI5XzECZS0fXbgpnytEMqND2gTsUpUSq+YUK+hddN3/bgef8YVvspkC7mRY04kVojGQCrk5E2Ju+9Y+O/2P7Yh72/sQ3Y/5RuL/3xRN52tV714uffm4+vu6gv/2maxhGd4yRe1glVsYJMzyFj3FGc4Dy6Cy+BH26cGLe8s4rfV7lwB56CrDA==</latexit>⇥16
<latexit sha1_base64="H68/dxKI2UsseIb13+4XGgtKcZA=">AAADTnichVLLThtBECwveRhIwiMXpFxWWKCcrDWCJEckSMQFCSQMlgCh3WVsRt6XdsZExPJPcE0+iis/khOI1DRLpITXWOPuqe6q6entqEi0sUFwWfPGXrx89bo+PjH55u27qemZ2V2TD8pYteM8yctOFBqV6Ey1rbaJ6hSlCtMoUXtRf83F905VaXSe7dizQh2mYS/TXR2HllDnwOpUGX/5aLoRNANZ/n2nVTkNVGsrn6nN4QDHyBFjgBQKGSz9BCEMf/toIUBB7BBDYiU9LXGFESbIHTBLMSMk2ud/j6f9Cs14dppG2DFvSbhLMn0scH8TxYjZ7lZF39Becf8QrPfoDUNRdhWe0UZUHBfFTeIWJ8x4jplWmXe1PM90r7Lo4ou8RrO+QhD3zvivzjojJbG+RHx8lcweNSI5n7IDGW2bFbgu3yn48uJj2lCsEpWsUgypV9K67rt6XI23cYXv0tlUXuSYQ4mlotGVGxLyRsTdd0yr7/YU25D3EPuE1Y84Y63/J+q+s7vUbH1qrmwvN1YXq2mr4wPm8ZET9Rmr2MAWe+Am6Rw/8cu78H57197NbapXqzjv8c8aq/8BTCWrzg==</latexit>⇥4
<latexit sha1_base64="WGRhOxYokfOXVAs1tZNL6VNKYck=">AAADd3ichVLbbtNAEJ3EXFpzS+EFiQciIqq+kNpFXB4rcREvSEUibaU4qmxn4q5ir631plEb5TP4Gl7hI/gUJB44O3WQoEA32szsmTlnZ8abVLmqbRB8a7W9K1evXV9b92/cvHX7Tmfj7n5dzkzKg7TMS3OYxDXnSvPAKpvzYWU4LpKcD5LpKxc/OGFTq1J/tKcVj4o402qi0tgCOupsRwlnSi9S1pbN0p8rPS7nUTR88rSyI79WZ+xHrMerhKNOL+gHsroXnbBxetSsvXKjdZ8iGlNJKc2oICZNFn5OMdX4DSmkgCpgI1oAM/CUxJmW5IM7QxYjIwY6xX+G07BBNc5OsxZ2iltybANmlx5jvxXFBNnuVoZfw/7APhMs++cNC1F2FZ7CJlBcF8X3wC0dI+MyZtFkrmq5nOm6sjShl9KNQn2VIK7P9JfOa0QMsKlEuvRGMjNoJHI+wQQ07AAVuCmvFLrS8Rg2FsuiohvFGHoG1k3f1eNqPI8zzWWyhXTkmAuJFaIxkRty8JbA3Xcsmu/2P3YN3t/Yx6jevbHwzxd10dnf6YfP+88+7PR2N5vXtkYP6BFt4UW9oF16R3uYQUqf6DN9oa/t795Db9PbOk9ttxrOPfpteeFP7li5vw==</latexit>windowsize
<latexit sha1_base64="Ft6bx/o8LJGZzlKi7UM7FU4VCGA=">AAADT3ichVJdaxNBFD3ZaJtU+xH7IvQlGIQiGDaFfjxGbKWFKhXNh8RSdreTdMl+sbuJtCG/oq/6o3z0l/SlFM9cp4JG2wmTe+fce869c3fcJPCz3LZ/FKzig4dz86XywqPHi0vLK5Un7SwepZ5qeXEQp13XyVTgR6qV+3mgukmqnNANVMcdvtbxzlilmR9HH/PzRB2HziDy+77n5IQ+7e+9Onz5oXPw7mSlZtdtWdVZp2GcGsw6iiuFp/iMU8TwMEIIhQg5/QAOMv56aMBGQuwYE2IpPV/iClMskDtilmKGQ3TI/wFPPYNGPGvNTNgeqwTcKZlVPOd+I4ous3VVRT+jvea+EGzw3woTUdYdntO6VCyL4lviOc6YcR8zNJm3vdzP1LfK0ceO3MZnf4kg+p7eb51dRlJiQ4lUsSeZA2q4ch5zAhFtix3oKd8qVOXGp7SOWCUqkVF0qJfS6unrfnSPv+IKX2SyodxIMycSC0WjLxUC8qbE9XcMzXe7i52R9y/2Gbuf8o01/n5Rs057o97Yqm++36g1X5jXVsIanmGdL2obTezjiDPwWPcSX/HN+m5dWTdFk2oVjLOKP1ax/BN7FqqR</latexit>HEAL-SWIN<latexit sha1_base64="G8ES7BJ6M+eZ5PcIQIPR60J+4Wg=">AAADSnichVLJSgNBEH0Z9327CF6CQRAPYSK4HAUX9KAoGiOoyMzYSYbMxsxE0ZBf8Kof5Q/4G95ED74uR8G9Q6eqX9V7XVXTduS5SWqaDzmjo7Oru6e3r39gcGh4ZHRs/DAJm7Gjyk7ohfGRbSXKcwNVTt3UU0dRrCzf9lTFbqzqeOVCxYkbBgfpVaROfasWuFXXsVIN7Ve2ds5GC2bRlJX/7pQyp4Bs7YZjuUmc4BwhHDThQyFASt+DhYS/Y5RgIiJ2ihaxmJ4rcYU2+sltMksxwyLa4H+Np+MMDXjWmomwHd7iccdk5jHDvSGKNrP1rYp+QvvCfS1Y7dcbWqKsK7yitanYJ4rbxFPUmfEf088y32v5n6m7SlHFsnTjsr5IEN2n86GzxkhMrCGRPNYls0YNW84XnEBAW2YFesrvCnnp+JzWEqtEJcgULerFtHr6uh5d41tc4VIm60tHmtmSmC8aVbnBI69NXH9HP/tuf7ET8n5i11l9m2+s9PVFfXcO54ulxeLC3nxhZS57bb2YwjRm+aKWsIJN7HIGDjVvcIs74954NJ6M57dUI5dxJvBpdXS+AiRpqhc=</latexit>SWIN
Figure 1. Our HEAL-SWIN model uses the nested structure of
the HEALPix grid to lift the windowed self-attention of the SWIN
model onto the sphere.
vision and autonomous driving datasets do not contain fish-
eye images. For these reasons, fisheye images have received
much less attention than rectilinear images in the literature.
Despite the distortions introduced by the mapping func-
tion, the traditional approach for dealing with this kind of
data is to use standard (flat) convolutional neural networks
which are adjusted to the distortions and either preprocess
the data [16, 17, 26, 36, 46, 50] or deform the convolu-
tion kernels [47]. However, these approaches struggle to
capture the inherent spherical geometry of the images since
they operate on a flat approximation of the sphere. Errors
and artifacts arising from handling the strong and spatially
inhomogeneous distortions are particularly problematic in
safety-critical applications such as autonomous driving.
Utilizing spherical representations is an approach taken
by some models [8, 10, 19] which lift convolutions to the
sphere. These models rely on a rectangular grid in spherical
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
6067
coordinates, namely the Driscoll–Healy grid [15], to per-
form efficient Fourier transforms. However, this approach
has several disadvantages for high-resolution data: First, the
sampling in this grid is not uniform but much denser at the
poles, necessitating very high bandwidths to resolve fine de-
tails around the equator. Second, the Fourier transforms in
the aforementioned models require tensors in the Fourier
domain of the rotation group SO(3) which scale with the
third power of the bandwidth, limiting the resolution. Third,
the Fourier transform is very tightly coupled to the input do-
main: If the input data lies only on a half-sphere, as is often
the case for fisheye images, the definition of the convolu-
tional layers needs to be changed to use this data efficiently.
As a novel way to address all these problems at the same
time, we propose to combine an adapted vision transformer
with the Hierarchical Equal Area iso-Latitude Pixelisation
(HEALPix) grid [23]. The HEALPix grid was developed
for capturing the high-resolution measurements of the cos-
mic microwave background performed by the MAP and
PLANCK satellites featuring a uniform distribution of grid
points on the sphere that associates the same area to each
pixel. This is in contrast to most other grids used in the lit-
erature like the Driscoll–Healy grid or the icosahedral grid.
In our model, which we call HEAL-SWIN, we use
a modified version of the Hierarchical Shifted-Window
(SWIN) transformer [37] to learn directly on the HEALPix
grid with minimal computational overhead. The SWIN
transformer performs attention over blocks of pixels called
windows which aligns well with the nested structure of
the HEALPix grid (Figure 1). To distribute information
globally, the SWIN transformer shifts the windows in ev-
ery other layer, creating overlapping regions. In HEAL-
SWIN, we employ the same principles but tailor them to fit
the structure of the HEALPix grid. In particular, we pro-
pose two different strategies for shifting windows on the
sphere: Either aligned with the hierarchical structure of the
HEALPix grid or in a spiral from one pole to the other.
Besides excellent performance, an additional benefit of
using HEAL-SWIN is that the attention layers do not re-
quire a Fourier transform and can therefore easily deal with
high-resolution data and with data which covers only part of
the sphere, resulting in significant efficiency gains. This is
of central importance to dealing with high-resolution fish-
eye images which cover about half of the sphere, leaving
half of the input unused in spherical models which rely on
operating on the entire sphere.
In order to verify the efficacy of our proposed model,
we train HEAL-SWIN on a number of different computer
vision tasks: classification, depth estimation and seman-
tic segmentation of fisheye images from a diverse range
of datasets. In these experiments, we put particular em-
phasis on tasks from the autonomous driving domain for
which high-resolution input images and very precise out-mean ±std. dev.
HS S567←Chamfer distance
HS S0.810.820.830.840.85mIoU→
Figure 2. Chamfer distance (lower is better) and mIoU (higher is
better) for HEAL-SWIN (HS) and SWIN (S). Details are provided
in Section 4.3 and in Section 4.1.2.
puts in terms of 3D information are critical, necessitating
geometry-aware representations. To the best of our knowl-
edge, we are the first to treat fisheye images in automotive
applications as spherical signals.
In particular, we perform fisheye-image-based depth es-
timation and compare the resulting predicted 3D point
clouds to the corresponding ground truth point clouds. In
this way, we can assess the quality of the learned repre-
sentations for downstream tasks that rely on accurate 3D
information, such as general obstacle detection and colli-
sion avoidance, allowing us to isolate the effect of using
spherical representations on the HEALPix grid. We show
that our model outperforms the flat SWIN transformer in
this setting on the SynWoodScape dataset [45] of computer-
generated fisheye images of street scenes, see Figure 2
(left), establishing the benefit of using HEALPix represen-
tations. For the semantic segmentation task, we confirm the
superior performance of our model on both the real-world
WoodScape dataset [49] and on the SynWoodScape dataset,
see Figure 2 (right). For both depth estimation and semantic
segmentation we optimize the loss directly on the sphere.
Having shown that the addition of spherical represen-
tations is beneficial for high-resolution image datasets, we
verify that our model is competitive with the state of the art
in spherical image classification and semantic segmentation
of indoor scenes.
Although we focus in this work on fisheye images, our
model is not specific to this kind of problem but can be
trained on any high-resolution spherical data, as provided
for instance by satellites mapping the sky or the earth. In
particular, deep spherical models (and in particular trans-
formers) were recently shown to outperform physical mod-
els for weather prediction tasks [3, 32, 35, 40], opening an-
other important application domain for our model.
Our main contributions are as follows:
• We construct the HEAL-SWIN transformer which op-
erates on high-resolution spherical representations by
combining the spherical HEALPix grid with an adapted
SWIN transformer. By exploiting the similar hierarchi-
cal structures in HEALPix and SWIN, we construct win-
6068
dowing and shifting mechanisms for the HEALPix grid
which efficiently deal with data that only covers part of
the sphere.
• We treat fisheye images in automotive applications for the
first time as distortion-free spherical signals. We demon-
strate the superiority of this approach for depth estimation
and semantic segmentation on both synthetic and real au-
tomotive datasets.
• To compare HEAL-SWIN to other models operating on
spherical representations, we benchmark on the Stanford
2D-3D-S indoor fisheye dataset [1] and find that our
model outperforms comparable spherical models.
2. Related work
The transformer architecture was introduced in [48], ex-
tended to the Vision Transformer (ViT) in [14], and fur-
ther refined in [37] to the Shifted-Window (SWIN) trans-
former based on attention in local windows combined with
window shifting to account for global structure. A first
step towards spherical transformer models has been pro-
posed in [7] where various spherical grids are used to extract
patches on which the ViT is applied, and in [4] where icosa-
hedral grid sampling is combined with the Adaptive Fourier
Neural Operator (AFNO, see [24]) architecture for spatial
token mixing to account for the geometry of the sphere.
Compared to previous spherical transformer designs, such
as [7], our model constitutes a significant improvement by
incorporating local attention and window shifting to accom-
modate high-resolution images in the spherical geometry,
without requiring the careful construction of accurate graph
representations of the spherical geometry.
The SWIN paradigm has been incorporated into trans-
formers operating on 3D point clouds (e.g. LiDAR depth
data) by combining voxel based models (see [39]) with
sparse [20] and stratified [33] local attention mechanisms.
In [34], spherical geometry is used to account for long range
interactions in the point cloud to create a transformer based
on local self-attention in radial windows. Closer in spirit
to our approach is [25], where the point cloud is projected
to the sphere, and partitioned into neighborhoods. Local
self-attention and patch merging is then applied to the cor-
responding subsets of the point cloud, and shifting of sub-
sets is achieved by rotations of the underlying sphere. Our
HEAL-SWIN model inherits windowing and shifting from
the SWIN architecture, but in contrast to the point cloud
based approaches we handle data native to the sphere and
use the HEALPix grid to construct a hierarchical sampling
scheme which minimizes distortions, allows for the han-
dling of high-resolution data, and, in addition, incorporates
new shifting strategies specifically adapted to the HEALPix
grid.
Several Convolutional Neural Network (CNN) based
models have been proposed to accommodate spherical data.As mentioned above, [8, 10, 19] use an equirectangular grid
and implement convolutions in Fourier space, while [31]
applies CNNs directly to the HEALPix partitions of the
sphere. DeepSphere is a graph-based CNN which is ro-
tationally equivariant for radial filters [11]. The underly-
ing graph allows for a non-uniform sampling which can be
beneficial for certain applications. DeepSphere has been
specialized to the HEALPix grid [41] but cannot exploit the
nested grid in the same way as HEAL-SWIN which em-
ploys windowed self-attention.
Other works that consider spherical CNNs combined
with the HEALPix grid include [16, 29]. Compared to pre-
vious works combining CNNs and the HEALPix grid, the
windowed self-attention equips our model with the ability
to efficiently encode long-range interactions. Moreover,
transformers are known to be able to benefit from large
datasets where CNNs reach their capacity limits.
Due to the general shortage of computer vision datasets
involving fisheye images, it is common to evaluate pro-
posed methods by creating spherical versions of MNIST
[12] and SYNTHIA [44]. In 2021, the first set of com-
puter vision tasks for real-world automotive fisheye images
was released for the WoodScape dataset [49]. In 2022,
the SynWoodScape [45] dataset was released, consisting of
synthetic fisheye images generated using the driving simu-
lator CARLA [13].
Outside of the automotive domain, a common bench-
mark for fishey image data is the Stanford 2D-3D dataset [1]
of indoor scenes. Previous works that, like our proposed
HEAL-SWIN model, perform evaluation in the spherical
domain for this dataset include; [50] based on an icosahe-
dral grid sampling, [27] which constructs structured graph
convolutions to incorporate the spherical geometry, [29]
where convolutional kernels are parameterised using differ-
ential operators on the sphere, and [19] which uses spin-
weighted spherical harmonics to perform spherical convo-
lutions in the Fourier domain.
3. HEAL-SWIN
We propose to combine the SWIN-transformer [37, 38]
with the HEALPix grid [23] resulting in the HEAL-SWIN-
transformer which is capable of training on high-resolution
images on the sphere. In this section, we describe the struc-
ture of the HEAL-SWIN model in detail.
3.1. The SWIN transformer
The SWIN-transformer is a computationally efficient vision
transformer which attends to windows that are shifted from
layer to layer, enabling a global distribution of information
while mitigating the quadratic scaling of attention in the
number of pixels.
In the first layer of the SWIN-transformer, squares of
pixels are joined into tokens called patches to reduce the ini-
6069
0
17 18
19 20
21 22
2324
25 26
27 28
29 30
31
6362 6160 5958 5756
5554 5352 5150 4948
4746 4544 4342 4140
3938 3736 353432
33Figure 3. Grid shifting scheme for window size 16: The windows
before the shift are framed in red and the patches are numbered in
the nested scheme. After a shift by half a window size, the patches
are divided into the windows framed in blue, so that e.g. patch 0
becomes patch 12 after the shift. The hashed regions are masked
in the attention layer. The patches hashed horizontally correspond
to the pixels marked in yellow in Figure 4 (left). They are filled
with patches hashed vertically which correspond to the pixels lost
in the center of Figure 4 (left).
tial resolution of the input images. Each following SWIN-
layer consists of two transformer blocks which perform at-
tention over squares of patches called windows . The win-
dows are shifted along the patch-grid axes by half a window
size, before the attention for the second transformer block
is computed. In this way, information is distributed across
window boundaries. To down-scale the spatial resolution,
two-by-two blocks of patches are periodically merged.
An important detail in this setup is that at the boundary of
the image, shifting creates partially-filled windows. Here,
the SWIN-transformer fills up the windows with patches
from other partially-filled windows and then performs a
masked version of self-attention which does not attend to
pixel pairs which originated from different regions of the
original.
For the depth-estimation and segmentation tasks we con-
sider in this work, we use a UNet-like variant of the SWIN-
transformer [3, 5] which extends the encoding layers of the
original SWIN-transformer by corresponding decoding lay-
ers connected via skip connections. The decoding layers
are identical to the encoding layers, only the patch merging
layers are replaced by patch expansion layers which expand
one patch into a two-by-two block of patches such that the
output of the entire model has the same resolution as the
input.
3.2. The HEALPix grid
In the HEAL-SWIN-transformer, the patches are not as-
sociated to an underlying rectangular pixel grid as in the
original SWIN-transformer, but to the HEALPix grid on
the sphere. The HEALPix grid is constructed from twelve
equal-area, four-sided polygons (quadrilaterals) of different
shapes which tessellate the sphere (drawn in red in the top-left sphere of Figure 1). These are subdivided along their
edges nsidetimes to yield a high-resolution partition of the
sphere into npix= 12·n2
sideequal-area, iso-latitude quadri-
laterals (the nside= 2grid is drawn in white in the top-left
sphere of Figure 1). To allow for a nested (hierarchical)
grid structure, nsideneeds to be a power of two. The pixels
of the grid are then placed at the centers of the quadrilat-
erals. The resulting positions are sorted in a list either in
thenested ordering descending from the iterated subdivi-
sions of the base-resolution quadrilaterals, as illustrated in
Figure 3, or in a ring ordering which follows rings of equal
latitude from one pole to the other. Given this data structure,
we use a one-dimensional version of the SWIN transformer
which operates on these lists. For retrieving the positions of
the HEALPix pixels at a certain resolution, translating be-
tween the nested- and ring indexing and interpolating in the
HEALPix grid, we use the Python package healpy [51].
Since for our experiments, we consider images taken by
fisheye cameras which cover only half of the sphere, we use
a modification of the HEALPix grid, where we only use the
pixels in eight out of the twelve base-resolution quadrilat-
erals which we will call base pixels . These cover approxi-
mately half of the sphere and allow for an efficient handling
of the input data, in contrast to many methods used in the
literature which require a grid covering the entire sphere.
The restriction to the first eight base pixels is performed by
selecting the first8/12entries in the HEALPix grid list in
nested ordering.
3.3. HEAL-SWIN
In HEAL-SWIN we adapt the patching-, windowing- and
shifting mechanisms of the SWIN transformer to the
HEALPix grid, enabling the transformer to operate on an
inherently spherical representation of the data.
3.3.1 Patches and windows
The nested structure of the HEALPix grid aligns very well
with the patching, windowing, patch-merging and patch-
expansion operations of the SWIN transformer. Corre-
spondingly, the modifications to the SWIN transformer re-
sult in a minimal computational overhead.
The input data is provided as a list in the nested ordering
described in Section 3.2 above. Therefore, we start from
a one-dimensional version of the model (in contrast to the
usual two-dimensional version used for images). Then, the
patching of the input pixels amounts to joining npatch con-
secutive pixels into a patch, where npatch is a power of
four. Due to the nested ordering and the homogeneity of
the HEALPix grid, the resulting patches cover quadrilateral
areas of the same size on the sphere. Similarly, to partition
patches into windows over which attention is performed,
nwinconsecutive patches are joined together, where nwin
6070
Figure 4. Grid (left) and spiral (right) shifting strategies for the
HEAL-SWIN transformer, projected onto the plane for visualiza-
tion. The grid lines outline the eight base pixels used for this
dataset, arrows indicate the directions in which pixels move. High-
lighted regions are masked in the attention layers. Note that in grid
shifting, pixels at boundaries of colliding base pixels are moved to
the outer edge. In ring shifting, distortions are introduced towards
the pole (center). For better visibility, the amount of shifting in
these images is exaggerated.
is again a power of four. E.g. in Figure 3, a window size
ofnwin= 16 is illustrated with patches 0−15in the first
window, patches 16−31in the second window etc. For
patch merging, we can similarly merge n= 4kconsecutive
patches in the HEALPix list for downscaling and expand
n= 4kpatches for upscaling.
3.3.2 Shifting
As mentioned above, to distribute information globally in
the image, the SWIN transformer shifts the windows by half
a window size along both image axes in every second atten-
tion layer. We have experimented with two different ways
of performing this shifting in the HEALPix grid.
The most direct generalization of the shifting in the pixel
grid of the original SWIN-transformer is a shifting in the
HEALPix grid along the axes of the quadrilaterals of the
base-resolution pixels, cf. Figure 3 and Figure 4 (left). We
call this grid shifting and shift by half a window in both
directions. Similarly to the original SWIN shifting scheme,
there are boundary effects at the edge of the half sphere cov-
ered by the grid. Additionally, due to the alignment of the
base pixels relative to each other, the shifting necessarily
clashes at some base-pixel boundaries in the interior of the
image. As in the original SWIN transformer, both of these
effects are handled by reshuffling the problematic pixels to
fill up all windows and subsequently masking the attention
mechanism to not attend to pixel pairs which originate from
different regions of the sphere. The corresponding pixels
are highlighted in yellow in Figure 4.
In the spiral shifting scheme, we first convert the nested
ordering into a ring ordering and then perform a roll op-
eration by nshift on that list. Finally, we convert back to
the nested ordering. In this way, windows are shifted along
the azimuthal angle by nshift pixels, with slight distortions
which grow larger towards the poles due to the decreased
length of circles of constant latitude, cf. Figure 4 (right).A shift by half a window is achieved with a shift size of
nshift=√nwin/2.2
As in the grid shifting scheme, we encounter boundary
effects. In the spiral shifting, they occur at the pole and at
the boundary of the half sphere covered by the grid. These
effects are again handled by reshuffling the pixels and mask-
ing the attention mechanism appropriately. In this scheme,
there are no boundary effects in the interior of the image.
Both shifting strategies can be implemented as precom-
puted indexing operations on the list holding the HEALPix
features and are therefore efficient. In ablation studies we
found that the spiral shifting outperforms the grid shifting
slightly (see section 4.2).
3.3.3 Relative position bias
In the SWIN-transformer, an important component that
adds spatial information is the relative position bias B
which is added to the query-key product in the attention
layers: Att (Q, K, V ) =SoftMax (QK⊤/√
d+B)V .This
bias is a learned value which depends only on the differ-
ence vector between the pixels in a pixel pair, i.e. all pixel
pairs with the same relative position receive the same bias
contribution.
In the HEALPix grid, the pixels inside each base
quadrilateral are arranged in an approximately rectan-
gular grid which we use to compute the relative posi-
tions of pixel pairs for obtaining the relative position
bias mapping. Consequently, in HEAL-SWIN, B∈
Rnwin×nwinwith values taken from a learnable matrix ˆB∈
R(2√nwin−1)×(2√nwin−1)according to
Bij=ˆBx(i)−x(j)+√nwin, y(i)−y(j)+√nwin, (1)
where (x(i), y(i))are the Cartesian coordinates of the pixel
iin the window, e.g. in Figure 3, pixel 11 would have co-
ordinates x(11) = 1 ,y(11) = 0 . We share the same rela-
tive position bias table across all windows, so in particular
also across base quadrilaterals. We also experimented with
an absolute position embedding after the patch embedding
layer but observed no benefit for performance.
4. Experiments
To verify the performance of our model, we trained the
HEAL-SWIN and the SWIN transformer on challenging
realistic datasets of fisheye camera images of both street-
and indoor scenes. We perform semantic segmentation and
monocular depth estimation and furthermore test our model
on the standard classification problem of spherical MNIST
digits.
We show that HEAL-SWIN reaches better predictions
than the non-spherical version of the model in all direct
2Since nwinis a power of four, the square root is always an integer.
6071
Table 1. Mean intersection over union on the sphere for seman-
tic segmentation of fisheye street scenes with HEAL-SWIN and
SWIN, averaged over three runs.
Model Dataset mIoU
HEAL-SWIN Large SynWoodScape 0.947
SWIN Large SynWoodScape 0.918
HEAL-SWIN Large+AD SynWoodScape 0.841
SWIN Large+AD SynWoodScape 0.809
HEAL-SWIN WoodScape 0.628
SWIN WoodScape 0.617
comparisons and is competitive on standard benchmarks for
spherical models.
Due to space limitations, the details of the experiments
on spherical MNIST are relegated to Appendix B.
4.1. Semantic segmentation of fisheye street scenes
We compare the performance of HEAL-SWIN on
HEALPix-projected fisheye images to that of the SWIN
transformer on the original, i.e. flat and distorted im-
ages. The architecture and training hyperparameters were
fixed by ablation studies for semantic segmentation on the
WoodScape dataset unless stated otherwise.
As a baseline, we use the SWIN transformer in a 12-layer
configuration similar to the “tiny” configuration SWIN-T
from the original paper [37] with a patch size of 2×2and a
window size of 8×8, adapted to the size of our input images.
Since both tasks require predictions of the same spatial di-
mensions as the input, we mirror the SWIN encoder in a
SWIN decoder and add skip connections in a SWIN-UNet
architecture, resulting in a model of around 41M param-
eters. We found the improved layer-norm placement and
cosine attention introduced in [38] to be very effective and
use them in all our models.
For the HEAL-SWIN models, we use the same con-
figurations as for the SWIN model with a patch size of
npatch = 4, mirroring the 2×2on the flat side and a win-
dow size of nwin= 64 , mirroring the 8×8on the flat
side. For shifting, we use the spiral shifting introduced in
Section 3.3.2 with a shift size of 4, corresponding roughly
to half windows. Again, we mirror the encoder and add
skip connections to obtain a UNet-like architecture. A table
with the spatial feature dimensions throughout the network
can be found in Appendix C. The shared model configura-
tion gives our HEAL-SWIN model the same total parameter
count as the SWIN model.
Figure 5. Example of segmentation using SWIN (left) and HEAL-
SWIN (right) on the Woodscape dataset of real automotive images.
Overlays correspond to predicted segmentation masks. The pedes-
trian (overlayed in red) is only recognized by HEAL-SWIN.
4.1.1 Real-world images
We evaluate our models using the real-world WoodScape
dataset3[49] consisting of 8234 fisheye images of street
scenes recorded in various locations in the US, Europe and
China. The images are presented as flat pixel grids together
with calibration data which we use to project the input data
and ground truth segmentation masks onto the sphere.
Although using only eight out of the twelve base pixel of
the HEALPix grid allows for an efficient representation of
the fisheye images, some image pixels are projected to re-
gions outside of the coverage of our subset of the HEALPix
grid; see the hatched regions of Figure 8 in Appendix A.
However, the affected pixels lie at the corners of the image,
making the tradeoff well worth it for the autonomous driv-
ing tasks considered here. We restrict evaluation to the eight
base pixels.
For WoodScape, we exclude the void class from the
mean but keep it in the loss, different from the 2021 CVPR
competition [43] as explained in Appendix A. As shown
in Table 1, the HEALPix version of the SWIN transformer
outperforms the baseline with otherwise identical training
scheme and model architecture supporting the claimed ben-
efit of spherical representations. See Figure 5 for an ex-
ample of the qualitative improvement of HEAL-SWIN over
SWIN for the task of pedestrian segmentation.
We investigated whether HEAL-SWIN outperforms the
baseline in certain regions of the image and concluded that
HEAL-SWIN outperforms SWIN everywhere and not just
in a particular (e.g. very distorted) region of the image. This
is likely the case since HEAL-SWIN can benefit from dis-
torted and undistorted training regions equally, leading to a
better overall performance.
4.1.2 Synthetic images
In order to remove the effects of the labeling inaccu-
racies, we perform the analogous experiments with the
3For WoodScape, we noticed inconsistencies in the semantic labels, see
Appendix A for further details.
6072
Table 2. Inference times for semantic segmentation models on a
single A40 GPU. Measurements are taken from first to last model
operation in a forward pass with an input tensor of batch size one
available on the GPU. Mean and standard deviation over 200 iter-
ations with 10 iteration warm-up.
Resolution Pixels time / pixel
HEAL-SWIN 8×256.025.2·105297±26ns
SWIN 640×768 4 .9·105296±39ns
same HEAL-SWIN model and SWIN baseline on the
SynWoodScape [45] dataset of 2000 fisheye images from
synthetic street scenes generated using the driving sim-
ulator CARLA [13]. We use two different subsets out
of the 25 classes provided in the dataset. All excluded
classes are mapped to void. In the first subset, which we
call Large SynWoodScape , we train on 8 classes which
cover large areas of the image, like building ,ego-vehicle ,
road etc. obtaining a dataset which lacks a lot of fine
details and hence minimizes projection effects between
the flat projection and HEALPix. For the second subset,
Large+AD SynWoodScape , we include further classes rele-
vant to autonomous driving, like pedestrian ,traffic light and
traffic sign to create a more realistic dataset with 12 classes
and featuring finer details; see Figure 8 in Appendix A for a
sample. More details about the datasets are in Appendix A.
The performance results on the different datasets are
summarized in Table 1 and show that HEAL-SWIN outper-
forms the baseline in all cases. Figure 2 (right) shows the
results on Large+AD SynWoodScape .
Inference time Due to the efficient handling of the spher-
ical data in HEAL-SWIN, the inference time is the same
as the baseline model for the target resolution, cf. Table 2.
See Appendix Table 11 for an ablation over resolutions. At
lower resolutions, the memory layout in the HEALPix grid
enables faster inference times for HEAL-SWIN compared
to SWIN. For a comparison of the computational benefits of
the SWIN architecture compared to CNNs we refer to [37].
Dataset size ablation We also study the effects of the size
of the dataset by training on different subsets of the training
data. We train on the Large+AD SynWoodScape class sub-
set. Within a run, we use exactly the same subset for both
models, HEAL-SWIN and SWIN, while strictly increasing
the subset when moving to a larger training set. All mod-
els are trained entirely from scratch until convergence and
evaluated using the entire validation set. We find that the
HEAL-SWIN model can make better use of larger training
sets than the SWIN model, as the difference in performance
becomes larger the more training data is used as shown in
Figure 6.
0.0 0.2 0.4 0.6 0.8 1.0
Fraction of the training data0.50.60.70.8mIoUModel
HEAL-SWIN
SWIN
Run
1
2Figure 6. Semantic Segmentation for varying training set sizes.
Performance is measured as the mean intersection over union
(higher is better) computed on the HEALPix grid (spherical
mIoU).
Table 3. Performance of spherical models on the Stanford 2D-3D
segmentation task of indoor fisheye images evaluated using the
official three-fold cross-validation.
Model mIoU mAcc
Gauge CNN [9] 39.4 55.9
UGSCNN [29] 38.8 54.7
HexRUNet [50] 43.3 58.6
SphCNN [18, 19] 40.2 52.8
Spin-SphCNN [19] 41.9 55.6
HEAL-SWIN (Ours) 44.3 61.9
4.2. Semantic segmentation of indoor fisheye images
To enable a comparison between our model and other mod-
els operating on spherical representations, we trained a ver-
sion of HEAL-SWIN with about 1.5M parameters on the se-
mantic masks of the Stanford 2D-3D-S dataset [1] of 1413
RGB-D fisheye images of indoor scenes. We project the
data to a HEALPix grid of resolution nside= 64 , corre-
sponding to 49kpixels. For this task, we use all twelve base
pixels of the grid. For details on the data, model architecture
and training scheme, see Appendix A and C.
In Table 3, we compare the performance of HEAL-
SWIN to the performance of similarly-sized spherical mod-
els on the same dataset trained in a similar fashion (e.g.
without data augmentation). HEAL-SWIN outperforms
comparable models in this class. We use the same data pre-
processing and weighted loss as [50].
Ablations To investigate how the model performance de-
pends on the HEALPix-specific hyper parameters, we per-
form ablations over patch size, window size, shift size and
shift strategy, as summarized in Appendix Table 9. The best
model was obtained for window size nwin= 16 , patch size
npatch = 4, and a shift size of nshift= 2 with spiral shift-
ing.
6073
Figure 7. Depth-map ground truth for the image in Appendix Fig-
ure 8 (left) and corresponding point cloud (right). The red arrow
indicates the orientation of the camera.
4.3. Depth estimation
Estimating distances to obstacles and other road users is an
important task for 3D scene understanding and route plan-
ning in autonomous driving. In monocular depth estima-
tion, pixel-wise distance maps are predicted from camera
images. For the SynWoodScape dataset, pixel-perfect depth
maps are available on which we train our models. We use
the same architectures and training procedures as in Sec-
tion 4.1 and set the number of output channels to one.
The model is trained using an L2loss and the depth data
is standardized to have zero mean and unit variance; in ad-
dition, the sky is masked out during training and evaluation.
In order to preserve the common high-contrast edges all re-
sampling is done using nearest neighbor interpolation.
In order to capture the quality of 3D scene predictions
of the different models, we evaluate the depth estimations
in terms of point clouds. More specifically, we generate
a point cloud from the ground truth depth values by com-
puting azimuthal and polar angles for each pixel from the
calibration information of the camera and scaling the corre-
sponding vectors on the unit sphere with the depth values,
cf. Figure 7 (right). Similarly, the SWIN predictions are
transformed into a point cloud, as are the HEAL-SWIN pre-
dictions, for which we use the pixel positions in HEALPix
for the spherical angles. Comparing the predicted point
clouds to the full-resolution ground truth point cloud leads
to an evaluation scheme which is sensitive to the 3D in-
formation in the predicted depth values. These in turn are
essential for downstream tasks.
To compare the predicted point cloud Ppredto the ground
truth point cloud Pgt, we use the Chamfer distance [2].
From the results depicted in Figure 2 (left),4it is evident
that the point clouds predicted by HEAL-SWIN match the
ground truth point cloud better than the point clouds pre-
dicted by the SWIN transformer, indicating that the HEAL-
SWIN model has indeed learned a better 3D representation
4For completeness, we want to mention that one HEAL-SWIN run per-
formed very differently from the others with a Chamfer distance of 6.784.
According to Chauvenet’s criterion this run should be classified as an out-
lier and is therefore not included in the figure.of the spherical images.
5. Conclusion
We constructed the efficient spherical vision tranformer
HEAL-SWIN, combining the HEALPix spherical grid with
the SWIN transformer. We showed superior performance of
our model on the sphere, in comparison to a baseline SWIN
model, for depth estimation and semantic segmentation on
automotive and indoor fisheye images.
Although showing high performance already in its
present form, HEAL-SWIN still has ample room for im-
provement. Firstly, in the presented setup, the grid is cut
along base pixels to cover half of the sphere, leaving parts
of the image uncovered while parts of the grid are unused.
This could be improved by descending with the boundary
into the nested structure of the grid and adapting the shift-
ing strategy accordingly. Secondly, the relative position bias
currently does not take into account the different base pix-
els around the poles and around the equator. This could be
solved by a suitable correction deduced from the grid struc-
ture. Thirdly, the UNet-like architecture we base our setup
on, is not state-of-the-art in tasks like semantic segmenta-
tion. Adapting a modern vision transformer decoder head
to HEALPix could boost performance even further.
Finally, our setup is not yet equivariant with respect to
rotations of the sphere. For equivariant tasks like seman-
tic segmentation or depth estimation, a considerable perfor-
mance boost can be expected from making the model equiv-
ariant [21]. In this context it would also be very interest-
ing to investigate equivariance with respect to local trans-
formations. This has been thoroughly analyzed for CNNs
in [6, 9, 22] and a gauge equivariant transformer has been
proposed in [28].
Acknowledgments and Disclosure of Funding
We are very grateful to Jimmy Aronsson and Joakim
Johnander for discussions, feedback on the manuscript and
for collaborations in the initial stages of this project.
The work of O.C., J.G. and D.P. is supported by the Wal-
lenberg AI, Autonomous Systems and Software Program
(WASP) funded by the Knut and Alice Wallenberg Foun-
dation. J.G. was supported by the Berlin Institute for the
Foundations of Learning and Data (BIFOLD). H.S. work
has been funded by the Deutsche Forschungsgemeinschaft
(DFG, German Research Foundation) under Germany’s Ex-
cellence Strategy – EXC 2002/1 “Science of Intelligence”
– project number 390523135. The computations were en-
abled by resources provided by the National Academic In-
frastructure for Supercomputing in Sweden (NAISS) and
the Swedish National Infrastructure for Computing (SNIC)
at C3SE partially funded by the Swedish Research Council
through grant agreements 2022-06725 and 2018-05973.
6074
References
[1] I. Armeni, A. Sax, A. R. Zamir, and S. Savarese. Joint 2D-
3D-Semantic Data for Indoor Scene Understanding. Arxiv
e-prints arXiv:1702.01105 , 2017. 3, 7, 1
[2] H.G. Barrow and J.M. Tenenbaum. Parametric correspon-
dence and chamfer matching: Two new techniques for image
matching. In Proceedings of the International Joint Con-
ferences on Artificial Intelligence (IJCAI) , pages 659–670.
MIT, 1977. 8
[3] Kaifeng Bi, Lingxi Xie, Hengheng Zhang, Xin Chen, Xiao-
tao Gu, and Qi Tian. Accurate medium-range global weather
forecasting with 3D neural networks. Nature , 619(7970):
533–538, 2023. 2, 4
[4] Salva R ¨uhling Cachay, Peetak Mitra, Haruki Hirasawa,
Sookyung Kim, Subhashis Hazarika, Dipti Hingmire, Phil
Rasch, Hansi Singh, and Kalai Ramea. Climformer – a
spherical transformer model for long-term climate projec-
tions. In Proceedings of the Machine Learning and the Phys-
ical Sciences Workshop, NeurIPS 2022 , 2022. 3
[5] Hu Cao, Yueyue Wang, Joy Chen, Dongsheng Jiang, Xi-
aopeng Zhang, Qi Tian, and Manning Wang. Swin-Unet:
Unet-like pure transformer for medical image segmentation.
InComputer Vision – ECCV 2022 Workshops. ECCV 2022 ,
pages 205–218. Springer International Publishing, 2022. 4
[6] Miranda CN Cheng, Vassilis Anagiannis, Maurice Weiler,
Pim de Haan, Taco S Cohen, and Max Welling. Covariance
in physics and convolutional neural networks. 2019. 8
[7] Sungmin Cho, Raehyuk Jung, and Junseok Kwon. Spherical
transformer. Arxiv e-prints arXiv:2202.04942 , 2022. 3, 6
[8] Oliver Cobb, Christopher G. R. Wallis, Augustine N. Mavor-
Parker, Augustin Marignier, Matthew A. Price, Mayeul
d’Avezac, and Jason McEwen. Efficient generalized spher-
ical CNNs. In Proceedings of the International Conference
on Learning Representations (ICLR) , 2021. 1, 3
[9] Taco Cohen, Maurice Weiler, Berkay Kicanaoglu, and Max
Welling. Gauge equivariant convolutional networks and
the Icosahedral CNN. In Proceedings of the International
Conference on Machine learning (ICML) , pages 1321–1330.
PMLR, 2019. 7, 8, 3, 6
[10] Taco S. Cohen, Mario Geiger, Jonas K ¨ohler, and Max
Welling. Spherical CNNs. In Proceedings of the Inter-
national Conference on Learning Representations (ICLR) ,
2018. 1, 3, 6
[11] Micha ¨el Defferrard, Martino Milani, Fr ´ed´erick Gusset, and
Nathana ¨el Perraudin. DeepSphere: A graph-based spherical
CNN. In Proceedings of the International Conference on
Learning Representations (ICLR) , 2020. 3
[12] Li Deng. The mnist database of handwritten digit images for
machine learning research [best of the web]. IEEE Signal
Processing Magazine , 29(6):141–142, 2012. 3
[13] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio
Lopez, and Vladlen Koltun. CARLA: An open urban driving
simulator. In Proceedings of the 1st Annual Conference on
Robot Learning , pages 1–16, 2017. 3, 7
[14] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image
is woth 16x16 words: Transformers for image recognition
at scale. In Proceedings of the International Conference on
Learning Representations (ICLR) , 2021. 3
[15] J. R. Driscoll and D. M. Healy. Computing Fourier trans-
forms and convolutions on the 2-sphere. Advances in Applied
Mathematics , 15:202–250, 1994. 2
[16] Haikuan Du, Hui Cao, Shen Cai, Junchi Yan, and Siyu
Zhang. Spherical Transformer: Adapting spherical signal
to CNNs. Arxiv e-prints arXiv:2101.03848 , 2021. 1, 3
[17] Marc Eder and Jan-Michael Frahm. Convolutions on spher-
ical images. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR) Work-
shops , pages 1–5. IEEE, 2019. 1
[18] Carlos Esteves, Christine Allen-Blanchette, Ameesh Maka-
dia, and Kostas Daniilidis. Learning SO(3) Equivariant Rep-
resentations with Spherical CNNs. In Proceedings of the Eu-
ropean Conference on Computer Vision (ECCV) , pages 52–
68, 2018. 7, 3, 6
[19] Carlos Esteves, Ameesh Makadia, and Kostas Daniilidis.
Spin-weighted spherical CNNs. In Advances in Neural Infor-
mation Processing Systems , pages 8614–8625. Curran Asso-
ciates Inc., 2020. 1, 3, 7, 6
[20] Lue Fan, Ziqi Pang, Tianyuan Zhang, Yu-Xiong Wang, Hang
Zhao, Feng Wang, Naiyan Wang, and Zhaoxiang Zhang.
Embracing single stride 3d object detector with sparse trans-
former. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR) , pages
8448–8458. IEEE, 2022. 3
[21] Jan E. Gerken, Oscar Carlsson, Hampus Linander, Fredrik
Ohlsson, Christoffer Petersson, and Daniel Persson. Equiv-
ariance versus augmentation for spherical images. In Pro-
ceedings of the International Conference on Machine Learn-
ing (ICML) , pages 7404–7421. PMLR, 2022. 8
[22] Jan E Gerken, Jimmy Aronsson, Oscar Carlsson, Hampus
Linander, Fredrik Ohlsson, Christoffer Petersson, and Daniel
Persson. Geometric deep learning and equivariant neural
networks. Artificial Intelligence Review , 56:14605–14662,
2023. 8
[23] K. M. Gorski, E. Hivon, and B. D. Wandelt. Analysis is-
sues for large CMB data sets. Arxiv eprints arXiv:astro-
ph/9812350 , 1998. 2, 3
[24] John Guibas, Morteza Mardani, Zongyi Li, Andrew Tao, An-
ima Aanandkumar, and Bryan Catanzaro. Adaptive Fourier
neural operators: Efficient token mixers for transformers.
InProceedings of the International Conference on Learning
Representations (ICLR) , 2022. 3
[25] Xindong Guo, Yu Sun, Rong Zhao, Liqun Kuang, and Xie
Han. SWPT: Spherical window-based point cloud trans-
former. In Computer Vision – ACCV 2022. ACCV 2022 ,
pages 396–412. Springer International Publishing, 2023. 3
[26] Niv Haim, Nimrod Segol, Heli Ben-Hamu, Haggai Maron,
and Yaron Lipman. Surface Networks via general covers. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision (ICCV) , pages 632–641. IEEE, 2019. 1
[27] D. Hart, M. Whitney, and B. Morse. Interpolated se-
lectionconv for spherical images and surfaces. In 2023
6075
IEEE/CVF Winter Conference on Applications of Computer
Vision (WACV) , pages 321–330. IEEE Computer Society,
2023. 3
[28] Lingshen He, Yiming Dong, Yisen Wang, Dacheng Tao, and
Zhouchen Lin. Gauge equivariant transformer. In Neural In-
formation Processing Systems , pages 27331–27343. Curran
Associates, Inc., 2021. 8
[29] Chiyu Jiang, Jingwei Huang, Karthik Kashinath, Prabhat,
Philip Marcus, and Matthias Nießner. Spherical CNNs on
unstructured grids. In Proceedings of the International Con-
ference of Learning Representations (ICLR) , 2019. 3, 7, 1,
2, 6
[30] Risi Kondor, Zhen Lin, and Shubhendu Trivedi. Clebsch–
Gordan Nets: A Fully Fourier Space Spherical Convolutional
Neural Network. In Advances in Neural Information Pro-
cessing Systems . Curran Associates, Inc., 2018. 6
[31] Krachmalnicoff, N. and Tomasi, M. Convolutional neural
networks on the HEALPix sphere: A pixel-based algorithm
and its application to CMB data analysis. A&A , 628:A129,
2019. 3
[32] Thorsten Kurth, Shashank Subramanian, Peter Harring-
ton, Jaideep Pathak, Morteza Mardani, David Hall, An-
drea Miele, Karthik Kashinath, and Anima Anandkumar.
FourCastNet: Accelerating Global High-Resolution Weather
Forecasting Using Adaptive Fourier Neural Operators. In
Proceedings of the Platform for Advanced Scientific Com-
puting Conference , pages 1–11, Davos Switzerland, 2023.
ACM. 2
[33] Xin Lai, Jianhui Liu, Li Jiang, Liwei Wang, Hengshuang
Zhao, Shu Liu, Xiaojuan Qi, and Jiaya Jia. Stratified trans-
former for 3d point cloud segmentation. In 2022 IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR) , pages 8490–8499. IEEE, 2022. 3
[34] Xin Lai, Yukang Chen, Fanbin Lu, Jianhui Liu, and Jiaya
Jia. Spherical transformer for LiDAR-based 3d recognition.
Arxiv e-prints arXiv:2303.12766 , 2023. 3
[35] Remi Lam, Alvaro Sanchez-Gonzalez, Matthew Willson,
Peter Wirnsberger, Meire Fortunato, Ferran Alet, Suman
Ravuri, Timo Ewalds, Zach Eaton-Rosen, Weihua Hu,
Alexander Merose, Stephan Hoyer, George Holland, Oriol
Vinyals, Jacklynn Stott, Alexander Pritzel, Shakir Mohamed,
and Peter Battaglia. GraphCast: Learning skillful medium-
range global weather forecasting, 2022. 2
[36] Yeonkun Lee, Jaeseok Jeong, Jongseob Yun, Wonjune Cho,
and Kuk-Jin Yoon. SpherePHD: Applying CNNs on a spher-
ical PolyHeDron representation of 360 degree images. Arxiv
e-prints arXiv:1811.08196 , 2019. 1
[37] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng
Zhang, Stephen Lin, and Baining Guo. Swin transformer:
Hierarchical vision transformer using shifted windows. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision (ICCV) , pages 9992–10002. IEEE, 2021.
2, 3, 6, 7
[38] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie,
Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, Furu
Wei, and Baining Guo. Swin transformer v2: Scaling up
capacity and resolution. In Proceedings of the IEEE/CVFInternational Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 11999–12009. IEEE, 2022. 3, 6
[39] Jiageng Mao, Yujing Xue, Minzhe Niu, Haoyue Bai, Jiashi
Feng, Xiaodan Liang, Hang Xu, and Chunjing Xu. V oxel
transformer for 3d object detection. In Proceedings of the
IEEE/CVF International Conference on Computer Vision
(ICCV) , pages 3144–3153. IEEE, 2021. 3
[40] Tung Nguyen, Johannes Brandstetter, Ashish Kapoor,
Jayesh K. Gupta, and Aditya Grover. ClimaX: A founda-
tion model for weather and climate. In Proceedings of the
40th International Conference on Machine Learning , pages
25904–25938. PMLR, 2023. 2
[41] Nathana ¨el Perraudin, Micha ¨el Defferrard, Tomasz Kacprzak,
and Raphael Sgier. DeepSphere: Efficient spherical con-
volutional neural network with HEALPix sampling for cos-
mological applications. Astronomy and Computing , 27:130–
146, 2019. 3
[42] Yeqiang Qian, Ming Yang, and John M. Dolan. Survey on
fish-eye cameras and their applications in intelligent vehi-
cles. IEEE Transactions on Intelligent Transportation Sys-
tems, 23:22755–22771, 2022. 1
[43] Saravanabalagi Ramachandran, Ganesh Sistu, John McDon-
ald, and Senthil Yogamani. Woodscape Fisheye Semantic
Segmentation for Autonomous Driving – CVPR 2021 Om-
niCV Workshop Challenge. 2021. 6, 1
[44] German Ros, Laura Sellart, Joanna Materzynska, David
Vazquez, and Antonio M. Lopez. The synthia dataset: A
large collection of synthetic images for semantic segmenta-
tion of urban scenes. In 2016 IEEE Conference on Computer
Vision and Pattern Recognition (CVPR) , pages 3234–3243,
2016. 3
[45] Ahmed Rida Sekkat, Yohan Dupuis, Varun Ravi Kumar,
Hazem Rashed, Senthil Yogamani, Pascal Vasseur, and Paul
Honeine. SynWoodScape: Synthetic surround-view fisheye
camera dataset for autonomous driving. IEEE Robotics and
Automation Letters , 7:8502–8509, 2022. 2, 3, 7, 1
[46] Mehran Shakerinava and Siamak Ravanbakhsh. Equivariant
networks for pixelized spheres. In Proceedings of the 38th
International Conference on Machine Learning (ICML) ,
pages 9477–9488. PMLR, 2021. 1
[47] Keisuke Tateno, Nassir Navab, and Federico Tombari.
Distortion-aware convolutional filters for dense prediction in
panoramic images. In Computer Vision – ECCV 2018 , pages
732–750. Springer International Publishing, 2018. 1
[48] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N. Gomez, and Łukasz Kaiser. At-
tention is all you need. In Advances in Neural Information
Processing Systems . Curran Associates, Inc., 2017. 3
[49] S. Yogamani, C. Hughes, J. Horgan, G. Sistu, S. Chennu-
pati, M. Uricar, S. Milz, M. Simon, K. Amende, C. Witt, H.
Rashed, S. Nayak, S. Mansoor, P. Varley, X. Perrotton, D.
Odea, and P. Perez. Woodscape: A multi-task, multi-camera
fisheye dataset for autonomous driving. In Proceedings of
the IEEE/CVF International Conference on Computer Vision
(ICCV) , pages 9307–9317. IEEE, 2019. 2, 3, 6, 1
[50] Chao Zhang, Stephan Liwicki, William Smith, and Roberto
Cipolla. Orientation-Aware Semantic Segmentation on
6076
Icosahedron Spheres. In Proceedings of the IEEE/CVF Inter-
national Conference on Computer Vision , pages 3533–3541,
2019. 1, 3, 7, 2, 6
[51] Andrea Zonca, Leo Singer, Daniel Lenz, Martin Reinecke,
Cyrille Rosset, Eric Hivon, and Krzysztof Gorski. healpy:
equal area pixelization and spherical harmonics transforms
for data on the sphere in Python. Journal of Open Source
Software , 4:1298, 2019. 4
6077
