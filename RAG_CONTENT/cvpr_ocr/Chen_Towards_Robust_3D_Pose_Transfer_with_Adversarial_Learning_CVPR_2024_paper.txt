Towards Robust 3D Pose Transfer with Adversarial Learning
Haoyu Chen1Hao Tang2Ehsan Adeli3Guoying Zhao1,3∗
1CMVS, Finland2CMU, USA3Stanford University, USA
*Corresponding Author
{chen.haoyu, guoying.zhao }@oulu.fi, bjdxtanghao@gmail.com, eadeli@stanford.edu
Identity Ground truth
Identity Transferred result Source pose  
(point cloud with
Gaussian noise)Ground truthIdentity Ground truth
Identity Source pose  
(raw scan)Ground truthTransferred result Transferred result
Transferred result
Source pose
(mesh)
Source pose  
(adversarial point cloud)
Figure 1. Examples of our 3D pose transfer results on various pose sources, show strong robustness and generalizability. The pose source
includes clean mesh (top left) and point clouds with Gaussian noise (bottom left) from SMPL-NPT dataset [41], the adversarial sample
of point cloud generated by our method (top right), and raw scan (bottom right) from DFAUST dataset [5]. Identity meshes are from the
SMPL-NPT dataset [41] and the FAUST [3] (bottom right) dataset. Our method can achieve promising pose transfer performance even on
the extremely challenging incomplete raw scan (bottom right). See more results and details in the Supplementary Materials.
Abstract
3D pose transfer that aims to transfer the desired pose
to a target mesh is one of the most challenging 3D gener-
ation tasks. Previous attempts rely on well-defined para-
metric human models or skeletal joints as driving pose
sources. However, to obtain those clean pose sources,
cumbersome but necessary pre-processing pipelines are in-
evitable, hindering implementations of the real-time appli-
cations. This work is driven by the intuition that the robust-
ness of the model can be enhanced by introducing adver-
sarial samples into the training, leading to a more invulner-
able model to the noisy inputs, which even can be further
extended to directly handling the real-world data like raw
point clouds/scans without intermediate processing. Fur-
thermore, we propose a novel 3D pose Masked Autoencoder
(3D-PoseMAE), a customized MAE that effectively learns
3D extrinsic presentations (i.e., pose). 3D-PoseMAE facil-
itates learning from the aspect of extrinsic attributes by si-
multaneously generating adversarial samples that perturb
the model and learning the arbitrary raw noisy poses via a
multi-scale masking strategy. Both qualitative and quanti-
tative studies show that the transferred meshes given by ournetwork result in much better quality. Besides, we demon-
strate the strong generalizability of our method on various
poses, different domains, and even raw scans. Experimental
results also show meaningful insights that the intermediate
adversarial samples generated in the training can success-
fully attack the existing pose transfer models.
1. Introduction
As a promising and challenging task, 3D pose transfer has
been consistently drawing research attention from the com-
puter vision community [10, 27, 37, 41]. The task aims
at transferring a source pose to a target identity mesh and
keeping the intrinsic attributes (i.e., shape) of the identity
mesh. Aside from pure research interests, transferring de-
sired poses to target 3D models has various potential appli-
cations in the film industry, games, AR/VR, etc [8, 11, 12].
To achieve data-driven learning, existing 3D pose trans-
fer methods rely on different prerequisites to the data
sources, which severely limits their further real-world im-
plementations. Firstly, many existing 3D pose transfer
methods [30, 45] cannot directly be generalized to unseen
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
2295
Baseline
networks
Mpose Mid
3D-PoseMAE
(train mode)
Back pr opagationMadv_pose
Traditional pipeline Adversarial learning-based pipeline (ours)3D-PoseMAE
(eval mode)
Adversarial
gradient
Detach
Mpose
MGT~
Detach
Mid
PertubationBack pr opagation
Mresult
MGT Mresult
MGT Mresult~~
Scope of applicationCompar ed
methodsOurs
Watertight
mesh
Simple
perturbation
(Gaussian noise)
Adversarial
attacks
Real-world data
(Point
clouds/scans)Difficulty
Existing adversarial
attacking methodsPT adversarial
functionFigure 2. Left: The existing methods can deal with simple perturbations such as Gaussian noises but fail to handle harder inputs in real-
world cases. Middle: The traditional pipeline used in previous methods [10, 27, 37, 41] for 3D pose transfer. The model is trained with
clean mesh inputs without considering the robustness to the noisy inputs. We use the symbol ∼to generally refer to the loss term, which
differs according to the actual condition. Right: Our method. Our method utilizes the strength of adversarial learning to enhance the
robustness and generalizability of the model. It consists of an adversarial sample generating flow(top part in red) and a pose transferring
flow(bottom part in green). The two flows happen iteratively during the adversarial training and the adversarial samples are calculated
on-the-fly. Note that Mid,Mpose,Mresult , andMGTstand for the identity, pose, generated meshes, and ground truths, the same as below.
target meshes, and training on the target meshes is in-
evitable for them to learn the priors of the target shape.
Secondly, some studies [1, 19, 27] assume that the paired
correspondences between the pose and identity meshes are
given, which also involves extra manual efforts to obtain.
Lastly, all previous attempts of 3D pose transfer rely on pre-
processed and clean source poses to drive the target meshes
[41, 51]. However, acquiring clean data is cumbersome and
necessary pre-processing pipelines are inevitable. For in-
stance, to register raw human scans to well-defined para-
metric human models (e.g., SMPL series [4, 29, 34]), it will
take roughly 1-2 minutes to process merely a single frame
[44], hindering the real-time implementations.
Inspired by the scaling successes of adversarial learning
in the CV community [2, 7, 15, 18, 21, 25] for the robust-
ness of the models, we experiment with applying adversar-
ial training to 3D pose transfer tasks. As shown in Fig. 2,
Existing methods [10, 41] already show certain robustness
to the noisy input (Gaussian noise), however, experimen-
tal results show that they are still vulnerable when we di-
rectly apply them to real-world point clouds or scan inputs.
We suspect there is a domain gap between the synthesized
noises and real-world data distribution. Thus, as shown in
Fig. 1, we wish to utilize the strength of adversarial learning
to enhance the robustness and generalizability of the model
with more challenging adversarial attacks, making it go be-
yond so that conducting pose transfer on unseen domains or
even directly from raw scans can be possible.
However, although the idea is intuitive, it’s not feasible
to naively extend existing adversarial training algorithms
[40, 47, 50] to the 3D pose transfer task . Primarily, the cur-rent methods [46, 50] generate adversarial samples based on
discriminative adversarial functions, and to our knowledge,
there is no adversarial attack proposed specifically for the
3D generative tasks (i.e., how to quantitatively justify if a
sample harms the generated results). Moreover, previous
approaches using 3D adversarial samples, such as [46, 50],
do so by taking them as pre-computed input data and leav-
ing them untouched for the entire training procedure. This
protocol is not practical for our task, as models need to
learn the latent pose space via gradients. Thus, we proposed
an adversarial learning framework with a new pose transfer
(PT) adversarial function and on-the-fly computation of ad-
versarial samples, which enables the successful application
of adversarial training for the generative models.
Another novel ingredient in this paper is inspired by the
recent powerful capability of masked autoencoding (MAE)
architectures [20] in the computer vision community. We
adopt the idea of MAE to 3D pose transfer task by im-
plementing a new model, called 3D-PoseMAE, that em-
pathizes the learning of extrinsic presentations (i.e., pose).
Specifically, unlike any existing 3D MAE-based models
[32, 49, 52] that merely put efforts into depicting spatial
local regions to capture the geometric and semantic depen-
dence, 3D-PoseMAE exploits a multi-scale masking strat-
egy to aggregate consistent sampling regions across scales
for robust learning of extrinsic attributes. Besides, we ob-
serve that local geometric details (wrinkles, small tissues,
etc.) from the pose sources are intuitively unessential for
pose learning. Thus, we argue that traditional 3D spatial-
wise attention/correlation operation [10, 37] might involve
a lot of redundant geometric information for pose learning.
2296
Instead, we adopt a progressive channel-wise attention op-
eration to the 3D-PoseMAE so that the attention is operated
gradually and fully on the latent pose code, making the pre-
sentations more compact and computationally efficient.
The contributions are summarized as follows:
• We work on the robustness problem of 3D pose transfer.
To our knowledge, it is the first attempt made to approach
3D pose transfer from the aspect of adversarial learning.
As a result, we provide a new research entry that gener-
ates adversarial samples to simulate noisy inputs and even
raw scans so that conducting end-to-end pose transfer on
raw scans and point clouds is made possible.
• We introduce a novel adversarial learning framework cus-
tomized for the 3D pose transfer task with a novel PT
adversarial function and on-the-fly computation of adver-
sarial samples in backpropagation. It’s the first time that
on-the-fly computation of adversarial samples appears in
a 3D generative deep learning pipeline. Notably, we
show that our proposed adversarial function can be easily
plugged into other existing adversarial attack methods .
• We propose a novel MAE-based architecture for 3D pose
transfer with carefully designed components to capture
the extrinsic attributions with a multi-scale masking strat-
egy and a progressive channel-wise attention operation.
The 3D-PoseMAE shows encouraging performances in
both computational efficiency and generative ability.
• Intensive experimental results on various datasets and
data sources show that our proposed method achieves
promising performances with substantial robustness to
noisy inputs and the generalizability to noisy raw scans
from the real world. Code will be made available.
2. Related Work
Data-driven 3D Pose Transfer. Data-driven 3D pose trans-
fer aims to transfer given source poses to target shapes by
learning the correspondence between the pose and shape au-
tomatically. On the one hand, parametric human model-
based methods could bring impressive generated results
[30, 45, 56], but their superb performances largely rely on
the need for the priors of the target shape, which limits their
generalization ability to unseen target meshes. Besides, reg-
istering raw human scans to well-defined parametric human
models is also cumbersome and time-consuming [44]. On
the other hand, some linear blending skinning (LBS) based
works [1, 27] can be extended to unseen target meshes,
but they assume that annotated landmarks/mesh points or
T-posed target meshes are given, which is also a strong
condition. Lastly, to our knowledge, all existing meth-
ods are within a strong prerequisite that pre-processed and
clean source poses are available to drive the target meshes
[10, 13, 19, 37, 41]. However, to our knowledge, no effort
has been made to study the robustness of the 3D pose trans-
fer to the noisy inputs and even raw scans.3D Adversarial Learning. Adversarial attacks [7, 15] have
drawn considerable research attention in the 2D vision mod-
els as their severe threat to real-world deployments since
it was proposed [18]. When it comes to the 3D field, the
attack-generating algorithms could be roughly sorted into
Carlini& Wagner (C&W) [6], and Projected Gradient De-
scent (PGD) [18] groups. C&W-based attacks [40, 47, 50]
switch the min-max trade-off problem of adversarial train-
ing into jointly minimizing the perturbation magnitude and
adversarial loss of attacks. But C&W attacks all suffer from
time-consuming issues due to the binary search and opti-
mization iteration. Meanwhile, PGD attacks [16, 28] set
the perturbation magnitude as a fixed constraint in the opti-
mization procedure, which can achieve the attack in a much
shorter time. However, the attack form is limited to point-
shifting, unlike C&W attacks that can perform adding or
dropping operations on the point clouds. On the other hand,
although some efforts have been made in related tasks such
as pose estimation, and motion retargeting [42, 51], there
is no strategy proposed specifically for attacking the 3D
generative tasks, including pose transfer (i.e., it is unad-
dressed how to define a successful attack to a generated
point cloud/mesh). Besides, research efforts [26, 39, 46]
have been made to defend against those attacks on point
cloud data for various tasks. However, those approaches
conduct the defense training directly on 3D pre-computed
adversarial samples. To our knowledge, there is no existing
3D deep learning pipeline that jointly generates adversarial
samples and learns the defense of generative models yet.
Deep Learning Models on Point Cloud. The pioneer-
ing and representative deep learning models on point cloud
include PointNet, PointNet++, and Dynamic Graph CNN
(DGCNN) [35, 36] as being widely used as benchmark
models on various 3D tasks. In the past two years, with the
trend of Transformer-based and MAE-based architectures
[20] in the computer vision community, many 3D-variant
models [32, 48, 49, 52, 54] have been proposed. An anal-
ogy to the patches in the ViT [17] and MAE [20] in the 2D
field, existing 3D MAE-based architectures [53] represent a
point cloud as a set of point tokens/proxies, making it into
a set-to-set translation problem. Attention operators will
be applied to depict different spatial local regions to better
capture the local geometric and semantic dependence for
the tasks of 3D classification, semantic segmentation, etc.
We argue that the 3D pose transfer task is different, and the
learning focus of the pose source should be made on the ex-
trinsic presentations (i.e., pose). Meanwhile, the traditional
3D spatial-wise attention/correlation operation [10, 37] can
capture much intrinsic (i.e., shape) information, which is
inevitable for tasks like 3D classification, and semantic seg-
mentation. However, this is partially inefficient for the task
of 3D pose transfer as the detailed geometric information is
redundant for learning the pose correlations.
2297
Target identity
γ
InsNorm3D-PoseMAE decoder 3D-PoseMAE framework
Zid
Channel-wise
Cross-Attention
Zposek v q
Conv1d
InsNorm
Conv1d
InsNorm
Conv1d
Multi-scale
Masked encoder
Conv1d
Conv1d
 Conv1d
 Conv1d
3D-PoseMAE
decoder
Conv1d
Conv1d3D-PoseMAE
decoder
Conv1d
Tanh
[N, 3 ] [N, 3 ][N, C ]
Conv1d3D-PoseMAE
decoder
Conv1d
Conv1d
[N, C/2 ]
[N, C ]
[N, C ][N, C ][N, C/4 ]
[N, C/2 ]
[N, C ][N, C/2 ][N, C/4 ][N, C/4 ][N, 3 ]
[N, C' ] [N, C' ][N, C' ]
[N, C' ] [N, C' ][C', C' ][N, C' ][N, C' ][N, C' ][N, C' ][N, C' ]Multi-scale masked encoder
Multi-scale
Masked encoder
Mini
PointNetMini
PointNetMini
PointNet
Aggregating
[N, 3 ] [N/2, 3 ] [N/4, 3 ]
Ratio
Masking
Tiling
[φN,3] [φN/2,3] [φN/4,3]
Ratio
Masking
Ratio
Masking[1,C] [1,C] [1,C][1,C][N,C]
Mid
Sour ce pose
clean/adversarialMposeMresultTransferr ed resultZ Latent pose code
Target identityMidMid
MidMidFigure 3. An overlook of our 3D-PoseMAE. The left part is the whole architecture of the 3D-PoseMAE. The middle and right parts
illustrate the architectural details of one multi-scale masked encoder and one 3D-PoseMAE decoder, respectively. The 3D-PoseMAE
borrows the idea from the work of [20] but is extensively extended to 3D data processing, especially for the 3D pose transfer task. Note
thatZstands for the encoded pose feature. Zpose andZidstand for the specific encoded pose features from pose and identity. Subscripts
are the dimensional shape of variables.
3. Methodology
We first present a general introduction to the whole pipeline
of the proposed method as shown in Fig. 2. Compared to
the traditional 3D pose transfer pipeline, we introduce ad-
versarial learning. As shown in Fig. 2, our method consists
of an adversarial sample generation procedure (top part in
red flow) and a pose transferring (bottom part in green flow)
procedure. In the top adversarial sample generation flow,
the proposed 3D-PoseMAE model will be set as eval mode
for obtaining the gradient of the data. By using the gradient
of the output of 3D-PoseMAE, we can obtain the pertur-
bation to the meshes, resulting in adversarial samples. In
the bottom pose transferring flow, the 3D-PoseMAE model
works the same as traditional 3D pose transfer models in a
train mode, but the pose inputs are replaced with adversar-
ial samples, leading to the adversarial training. Note that the
inputs MposeandMidof 3D-PoseMAE need to be detached
to ensure the backpropagation of the two flows works with-
out interference.
Next, the overall architecture of the 3D-PoseMAE net-
work together with each component will be introduced, fol-
lowed by the implementation details of adversarial training.
3.1. 3D-PoseMAE
An overview of the 3D-PoseMAE is in Fig. 3. Similar to
previous works [10, 37, 41], 3D-PoseMAE takes a source
pose (in the form of a mesh/point clouds) and a target mesh
as input, and outputs a corresponding pose transferred re-
sult. In the following, we introduce each component of the
3D-PoseMAE in an order following the data stream.
Multi-scale Masked 3D Encoder. Analogy to the original
MAE [20], existing 3D MAE-based architectures [32, 49,
52] invariably chose to split a given point cloud into multi-ple subsets to conduct the masking. However, this protocol
involves feature extraction on each subset for later atten-
tion and aggregation, which is computationally demanding
and only works on small-scale meshes/point clouds (up to
8,192 points). It cannot be extended to tasks with large-size
meshes, such as 3D pose transfer, where the size of the tar-
get mesh can be more than 27,000 vertices. Thus, we pro-
pose a novel multi-scale masking strategy to boost extrinsic
attribute learning. Specifically, as shown in Fig. 3, we re-
gard the input point cloud Mpose as the 1-th scale. For the
i-th scale, 1≤i≤3, we randomly downsample the points
by2(i−1)times, resulting in a group of the S-scale repre-
sentations of the source pose. Then, we adopt masking at a
ratio of ϕto the point cloud at each scale, so that the model
will be pushed to learn the same extrinsic attribute shared
from those scaled representations via Mini-pointNet [36].
Next, we aggregate the resulting latent pose code from all
scales and form the embedding vector Zwith dimension C.
At last, we tail the size of Zfrom [1, C]to[N, C]for better
integration with the identity mesh (N is the vertex number
of the identity mesh, which is flexible according to given
target meshes) and feed it into the 3D-PoseMAE decoders.
3D-PoseMAE Decoder with Channel-wise Attention.
After obtaining the latent codes of pose and identity Zpose
andZid, we need to adopt the Transformer backbone to in-
tegrate two latent codes and conduct the decoding and gen-
erating. As there are many existing 3D transformer back-
bones [10, 32, 54] and the work of GC-Transformer [10]
is proposed specifically for 3D pose transfer, we implement
our 3D-PoseMAE decoder based on GC-Transformer archi-
tecture as shown in the right part of Fig. 3. However, our
3D-PoseMAE decoder has a core difference from any ex-
isting 3D Transformer, which is the design of the attention
operation. To learn the correlations of given meshes, previ-
2298
ous models conduct the attention operation over the spatial
channel to perceive the geometric information from the two
meshes. Our intuition is that many redundant local geomet-
ric representations (wrinkles, small tissues) from the source
pose are not essential for pose learning. In other words,
we only need the compact pose representations from source
poses, it would be encouraging to conduct channel-wise at-
tention where the integration will be fully achieved on the
compact pose spaces.
With the above observation, we propose a channel-
wise cross-attention module in our 3D-PoseMAE decoder.
Firstly, we construct the channel-wise attention map Abe-
tween qandkwith the following formula (the subscripts
indicate the matrix size):
AC′×C′= Softmax( qT
C′×NkN×C′), (1)
where the representations qkare generated from embed-
ding vectors via different 1D convolution layers (the same
asvbelow), with the same size as [N, C′](Nfor vertex
number and C′for the channel dimension in the current de-
coder). Thus, the size of the resulting attention map be-
comes [C′, C′]. Next, we can obtain the refined latent em-
bedding Z′
posewith following
Z′
pose=γAC′×C′vT
C′×N+Zpose, (2)
where γis a learnable parameter. The remaining design
of the 3D-PoseMAE decoder is consistent with the GC-
Transformer [10], and the whole network structure is pre-
sented in Fig. 3. Please refer to the Appendix for more
details on network design and parameter setting.
As we can see, by implementing such a channel-wise at-
tention operation, the network can enjoy two major benefits.
Firstly, the size of the intermediate attention map in the net-
work changes from [N, N ]to[C′, C′]. Thus, the model size
will be substantially reduced. Especially when the process-
ing mesh size is huge, e.g., vertex number Ncould be up
to 27,000 while channel size C′is fixed no more than 1,024
in practice. Secondly, the integration of the pose and target
information will be more compact with the cross-attention
conducted fully channel-wise, avoiding touching the redun-
dant spatial information brought from the source pose.
Preserving Fine-grained Geometry. Note that, to preserve
the fine-grained spatial geometric information of the target
meshes, we modify the traditional normalization layer into
InstanceNorm layer [22, 41]. By the gradual integration in
the 3D-PoseMAE, the fine-grained geometry can be refined
by the compact pose representations from the channel-wise
attention mechanism, see more in the Appendix.
Optimization . To train the 3D-PoseMAE for the pose
transfer task, we define the full objective function as below:
Lfull=Lrec+λedgeLedge, (3)where LrecandLedge are the two losses used as our full
optimization objective, as reconstruction loss and edge loss,
the same as following existing baselines [37, 41]. λedge is
the corresponding weight of edge loss.
3.2. Adversarial Training
An overview of our adversarial learning-based pipeline is
on the right side of Fig. 2. Below, we introduce the motiva-
tion and problem definition of the adversarial training in the
pose transfer task, followed by the implementing details.
Motivation. It’s proven in various computer vision tasks
[2, 7, 21, 43, 57] that introducing adversarial samples to
perturb the neural networks during the training can enhance
networks’ robustness and increase their generalizability and
adaptability to other/unseen domains. It’s intuitive to think
of applying a similar scheme to the 3D pose transfer task
so that transferring poses from unseen domains or even di-
rectly from raw scans can be made possible. However, this
would not work naively by transferring the existing 3D ad-
versarial attack methods to our task due to several issues.
Below, we will discuss each issue and illustrate how we ap-
proach to the solution.
Problem Definition. We define the problem of adversar-
ial training in the 3D pose transfer task as follows. Given
a source pose Mpose and an identity mesh Mid, let𭟋be
the target pose transfer model (e.g., 3D-PoseMAE) with pa-
rameters θ. Then 𭟋(Mpose, Mid;θ)is the pose transferred
mesh generated by the model. For an ideal model we should
get:𭟋(Mpose, Mid;θ) =MGT, with MGTas the ground
truth mesh. Then, the problem is converted to an inequality
problem: adversarial attack method fneeds to generate an
adversarial sample Madvpose=f(Mpose)that can satisfy:
𭟋(Madvpose, Mid;θ)̸=MGT. (4)
To solve the inequality problem analogy to Eq. (4), exist-
ing 3D adversarial attack methods for 3D object classifi-
cation tasks [16, 23, 40, 47, 50, 55] convert the inequal-
ity into a minimal optimization problem with adversarial
loss term ||argmaxc𭟋(xadv;θ)c−t||by forcing the predic-
tion to an adversarial sample xadvclose to a target class t
which is different than the correct one, resulting a success-
ful attack. This is known as targeted attacks. However, this
discriminative-based adversarial function cannot directly be
applied to generative tasks (e.g., 3D pose transfer) as a con-
tinuous latent space is required to present the pose code.
Untargeted Adversarial Attack for Pose Transfer. Some
non-targeted attack methods [31, 38] convert the above ad-
versarial loss term for targeted attacks into a reversed form
of||argmaxc𭟋(xadv;θ)c−t||as a minimal optimization
problem by pushing away the prediction from the correct
class. Inspired by this, we construct a novel PT adversarial
2299
function for our 3D generative task in an intuitive way:
fadv=||𭟋(Mpose, Mid;θ)−MGT||−1. (5)
By minimizing the above term, we can push the gen-
erated results from the model away from the ground truth
mesh, resulting in an attack effect, which is proposed for
the first time for the 3D pose transfer.
The Magnitude of Attacks. To guarantee the adversarial
sample is visually similar to the clean data, adversarial at-
tack methods [16, 23, 40, 47, 50, 55] deploy norms (C&W
based attacks) or predefined threshold budget (PGD-based
methods) to restrict the perturbations small enough. While
in our case, the perturbation ought be strong enough so that
the model can handle it without the need to consider the in-
visibility issue of the perturbation since the goal is not to
obtain effective attacks but a robust model. While the mag-
nitude of the adversarial attack is too large, it can be easily
defended by simple pre-processing such as statistical outlier
removal (SOR) [46]. Thus, to make it closer to real-world
applications, we add SOR as pre-processing to all the ad-
versarial samples to filter out those easy samples.
Adversarial Training for Robustness. Lastly, after con-
firming the adversarial loss function as Eq. (5), we can
merge it into the existing adversarial attack methods to gen-
erate adversarial pose meshes for 3D pose transfer:
Madvpose=Mclean pose+
ϕ[fadv(𭟋(Mpose, Mid;θ), MGT)],(6)
where ϕcan be any existing adversarial attack methods,
such as PGD or FGM [28, 47] to generate the perturba-
tions. As mentioned, C&W-based attacks can provide better
adversarial samples with less visibility. However, they all
suffer from time-consuming issues due to the binary search
and heavy optimization iteration. According to our prelimi-
nary implementation, merging C&W-based attacks [47] (the
perturbation attack with original parameter setting) into the
pose transfer learning will extend the training time by more
than 900 times, which is not feasible. Thus, we deploy
PGD-based attacks [16], including the several variants of
fast gradient method (FGM) attacks and PGD attacks, into
the framework for the adversarial training. Taking FGM at-
tacks as an example, our final objective function to generate
the adversarial samples is defined as below:
Madvpose=Mclean pose+
ϵ·sign(∇fadv(𭟋(Mpose, Mid;θ), MGT)])),(7)
where ϵis the magnitude of the FGM attacks. We encourage
readers to refer to the Appendix and [16, 18] for a detailed
explanation and implementation of adversarial attacks on
3D data. The training pipeline is demonstrated in Algo-
rithm 1 by taking the FGM attack as an example. With thisAlgorithm 1 Adversarial training with FGM for 3D pose
transfer.
Input: N: Total epoch number for training.
λ: The threshold to determine a successful attack.
ϵ: The budget for FGM-based attacks.
Mpose: The source pose.
Mid: The identity mesh.
MGT: The ground truth mesh.
θ: The parameter of target model 𭟋to attack.
Output: θ′: The updated parameter of target model 𭟋.
forepoch inNdo
set𭟋aseval()
Mid.detach ();Mpose.detach ()
Mresult =𭟋(Mpose, Mid;θ)
Ladv=fadv(Mresult , MGT)
Ladv.backward ()
gradient adv=Mpose.grad.detach ()
∆ =gradient adv×ϵ
Madvpose=Mpose+ ∆
set𭟋astrain ()
Mresult =𭟋(Madvpose, Mid;θ)
Lrec=||Mresult−MGT||
Lrec.backward ()
end for
pipeline, we achieve on-the-fly computation of adversarial
samples, which enables the generative model to cover the
whole latent pose space via gradients. It’s worth emphasiz-
ing that our method generates adversarial attacks directly
based on the gradients of arbitrary given meshes, with no
need to utilize SMPL models in the training.
4. Experiments
In this section, we first present quantitative evaluations of
3D-PoseMAE with two protocols for both clean sample
training and adversarial training. One step further, we qual-
itatively visualize the strong generalization ability of our
method as well as the intermediate-generated adversarial
samples. Lastly, we perform ablation studies to evaluate
the effectiveness of our methods.
Datasets. (1) SMPL-NPT [41] is a synthesized dataset
containing 24,000 body meshes generated via the SMPL
model [4] by random sampling in the parameter space. 16
different identities paired with 400 different poses are pro-
vided for training. At the testing stage, 14 new identities
are used. Those 400 poses in the training set paired with
those new identities will be used as the “seen” protocol and
200 new poses for “unseen” protocols. The models are only
trained on the SMPL-NPT dataset and will be generalized
to other datasets. (2) FAUST [3] is a well-known 3D human
body scan dataset. It provides both FAUST registrations fits
the SMPL body model with 6,890 vertices and also the raw
2300
MethodsPMD↓(×10−4)
Seen Unseen
DT [19] 7.3 7.2
NPT-MP [41] 2.1 12.7
NPT [41] 1.1 9.3
3D-CoreNet [37] 0.8 -
GC-Transformer [10] 0.6 4.0
3D-PoseMAE (Ours) 0.6 3.4
Table 1. Performance comparison with other methods on SMPL-
NPT dataset with training on clean samples .
scans. We use it for qualitative evaluation. (3) DFAUST [5]
dataset contains high-resolution 4D scans of 10 human sub-
jects performing 14 different body motions. We use it for
both qualitative and quantitative evaluation.
Implementation Details. Our algorithm is implemented
in PyTorch [33]. All the experiments are carried out on a
server with four Nvidia V olta V100 GPUs with 32 GB of
memory and Intel Xeon processors. We train our networks
for 400 epochs with a learning rate of 0.00005 and Adam
optimizer [24]. The batch size is fixed as 4 for all settings. It
takes around 15 hours for pure training (clean samples) on
3D-PoseMAE and 40 hours for adversarial-based training
(with FGM attack, which is the fastest) on 3D-PoseMAE.
Please refer to the Appendix for more details on parameter
settings and other attacks.
The adversarial training of the whole framework is to
seek a balance between the magnitude of the adversarial
samples and the generative model’s robustness. Although
it can be controlled by adjusting hyper-parameters such as
the attacking budget ϵ, it’s intuitive to assume that introduc-
ing adversarial samples into the early stage of the training
would not contribute as the transferring results are still de-
generated and easily fall into local minima and numerical
errors in gradient computation. Thus, similar to many exist-
ing methods [9, 14] that have trade-offs in training, we con-
duct the training with two stages. For the first 200 epochs,
only clean samples are used to stabilize the model and avoid
local minima, and after 200 epochs, the adversarial training
starts with adversarial samples added.
4.1. Quantitative Evaluation
SOTA Comparison on Clean Samples. We start the eval-
uation with training models on clean samples, following
theclassical evaluation protocol for 3D pose transferring
from [41] to train the model on the SMPL-NPT dataset.
We evaluate the resulting model with Point-wise Mesh Eu-
clidean Distance (PMD) as the evaluation metric:
PMD =1
|V|X
v∥Mresult−MGT∥2
2, (8)
where Mresult andMGT are the point pairs from the
ground truth mesh MGTand generated one Mresult . The
corresponding experimental results are presented in Table 1.MethodPMD↓(×10−4)
w/o Adversarial Training w/ Adversarial Training
NPT-MP [41] 307.1 62.3
NPT [41] 237.6 59.0
GC-Transformer [10] 105.2 19.3
3D-PoseMAE (Ours) 77.6 16.9
Table 2. Performance comparison with other methods on SMPL-
NPT dataset evaluated with adversarial samples . Adversarial
Training means the models are trained with adversarial samples.
Datasets Domains NPT 3D-PoseMAE (Ours)
DFAUST [5] Raw scan 25.21 13.70
SMPL-NPT [41] Gaussian noise 12.66 6.13
Table 3. Quantitative evaluation across datasets. Models are both
trained on SMPL-NPT in an adversarial manner.
As we can see, our 3D-PoseMAE achieves the SOTA per-
formance with the lowest PMD ( ×10−4) of: 0.6 and 3.4 on
“seen” and “unseen” settings. We denote PMD ( ×10−4) as
PMD for simplicity in the following. Note that, at this stage,
all the methods are trained only with clean samples (no ad-
versarial training/samples involved yet) to make a classical
evaluation the same as in previous work.
SOTA Comparison on Adversarial Samples. We next
conduct the experiments by evaluating models on adver-
sarial samples to verify the performances of models from
the robustness aspect. We continually use the PMD as the
evaluation metric. To build the testing set of adversarial
samples, we directly utilize the “seen” testing set from the
SMPL-NPT dataset and generate attack samples (using the
same attacking strategy, i.e., the FGM) to attack the victim
models. The experimental results are presented in Table 2,
where we report the result of using the FGM attack with
an attacking budget of 0.08. More experiments with dif-
ferent attack types can be found in the Appendix. When
it comes to the pose transferring with adversarial samples,
all the methods’ performances will suffer from the degen-
eration problem compared to using the clean pose as the
source. Especially for models without adversarial training,
the pose transferring performance will drop dramatically,
proving the vulnerability of those “clean” models.
Adversarial Training Improves All the Methods. To
make a fair comparison, we also conducted the adversar-
ial training of two baseline methods, NPT [41] and GC-
Transformer [10], with exactly the same setting for the ad-
versarial training pipeline. As we can see in “w/ adversarial
training” setting of Table 2, the robustness of all the com-
pared methods has been enhanced considerably. It is worth
noting that our 3D-PoseMAE performans the best in both
of the strategies, demonstrating it as a strong baseline.
Evaluation on Noisy Inputs and Raw Scans. We present
extra evaluations on raw scans from the DFAUST dataset
and meshes with Gaussian noises on the SMPL-NPT dataset
in Table 3. Our 3D-PoseMAE outperforms the compared
methods by a large margin.
2301
Source pose  
(scan)Target NPT
 3D-PoseMAE (Ours)Figure 4. The performance of our method and compared method
[41] on an unseen raw scan from the DFAUST dataset [5]. We
can see that the compared method failed to handle the raw scan
as a source pose, leading to an arbitrary-generated pose while our
method can preserve the original pose in a better visual effect.
Model Correlation module Model size Pose Source Runtime
NPT [41] - 24.2M Mesh 0.0044s
3D-CoreNet [37] Correlation matrix 93.4M Mesh 0.0255s
GC-Transformer [10] Spatial-attention 48.1M Mesh 0.0056s
3D-PoseMAE (Ours) Channel-attention 40.7M Mesh/Raw scan 0.0048s
Table 4. Model size and runtime of different methods.
Runtime & Model Size. We present Table 4 to demonstrate
the computational attribute of 3D-PoseMAE. The runtime
is obtained by taking the average inference times in the
same experimental settings. As shown in the table, the
3D-CoreNet method [37] takes the longest time and largest
size compared to other deep learning-based methods. The
NPT method [41] has the shortest inference time as there
is no correlation module involved thus, the generation per-
formance is degraded. 3D-PoseMAE achieves notable im-
provements, while the inference time is also encouraging.
4.2. Qualitative Evaluation
Robustness to Various Pose Sources. Our final goal of
introducing adversarial training is to enhance the robust-
ness of the model so that it can be generalized to various
noisy and complicated situations. Thus, we also qualita-
tively display the generality of 3D-PoseMAE over various
pose sources, as shown in Fig. 1. Besides, we compare
our method with the existing pose transfer method [41] in
Fig. 4, to directly transfer pose from raw scans, and the en-
couraging performance proves that adversarial training can
effectively improve the robustness of the models.
Visualization of Perturbations. As shown in Fig. 5,
we visualize the latent codes and corresponding samples
(clean&adversarial). The perturbation causes meaningful
pose distribution changes in latent space, even with small
magnitude (see 0.0008) that are hard to observe for humans.
Attacking Effects as By-products. From Fig. 5, intrigu-
ingly, we find that the majority of generated perturbation
happened to be located on the body parts that are consistent
with the key kinetic positions, such as knees, elbows, and
feet, where proven to be easy parts to add attacks.
4.3. Ablation Study
Clean vs. Adversarial Training. We verify the efficacy
of adversarial training against the adversarial attacks/noisy
w/o adversarial training w/ adversarial training
Latent codes
Clean sample 1
Clean sample 2
Adversarial sample 1
Adversarial sample 1
Clean samplesAdversarial sample with
smaller  magnitude (0.0008)Adversarial sample with
larger  magnitude (0.008)
Figure 5. Visualization of latent pose space and the corresponding
poses. Please zoom in for details due to the page limit.
Component Vanilla + Multi-scale masking + Channel Attention
PMD↓(×10−4) 4.0 3.8 3.4
Table 5. Ablation study by progressively enabling each compo-
nent. The rightmost is from the full 3D-PoseMAE.
inputs in Table 2. We can see that when being attacked by
adversarial samples, all the existing methods enjoy bene-
fits from adversarial training compared to the ones being
trained with clean samples, e.g., 19.3 vs. 105.2 for GC-
Transformer. The results also prove that generated ad-
versarial samples can successfully attack the pure models ,
leading to degenerated results, e.g., the performance of the
NPT model degenerates dramatically with PMD from 1.1
(clean pose source) to 237.6 (attacking pose source).
Each Component. We verify the contributions made from
each component in the 3D-PoseMAE in Table 5. We disable
all the key components as a Vanilla model and enable each
step by step, showing the contributions can consistently im-
prove the generative performance of the method.
5. Conclusion
We work on the robustness problem of the 3D pose transfer,
especially on unseen domains and raw noisy inputs from the
aspect of adversarial learning. We propose a novel adver-
sarial learning framework customized for 3D pose transfer
with a new adversarial function and on-the-fly computation
of adversarial samples implementation. We further propose
the 3D-PoseMAE with two novelties: a multi-scale masking
strategy and a progressive channel-wise attention operation.
Experimental results on various data sources show that our
method achieves promising performances with substantial
robustness to noisy inputs. We show that this work makes
end-to-end 3D pose transfer on real-world scans possible.
Acknowledgment . This work was supported by the
Research Council of Finland (former Academy of Finland)
for Academy Professor project EmotionAI (grants 336116,
345122) and ICT 2023 project TrustFace (grant 345948),
FARIA (The Finnish-American Research & Innovation
Accelerator) project, Infotech Oulu, and the University
of Oulu & Research Council of Finland Profi 7 (grant
352788). As well, the authors wish to acknowledge CSC –
IT Center for Science, Finland, for computational resources.
2302
References
[1] Kfir Aberman, Peizhuo Li, Dani Lischinski, Olga Sorkine-
Hornung, Daniel Cohen-Or, and Baoquan Chen. Skeleton-
aware networks for deep motion retargeting. TOG , 39(4):
62–1, 2020. 2, 3
[2] Muhammad Awais, Fengwei Zhou, Hang Xu, Lanqing
Hong, Ping Luo, Sung-Ho Bae, and Zhenguo Li. Adversar-
ial robustness for unsupervised domain adaptation. In ICCV ,
pages 8568–8577, 2021. 2, 5
[3] Federica Bogo, Javier Romero, Matthew Loper, and
Michael J Black. Faust: Dataset and evaluation for 3d mesh
registration. In CVPR , 2014. 1, 6
[4] Federica Bogo, Angjoo Kanazawa, Christoph Lassner, Peter
Gehler, Javier Romero, and Michael J Black. Keep it smpl:
Automatic estimation of 3d human pose and shape from a
single image. In ECCV , 2016. 2, 6
[5] Federica Bogo, Javier Romero, Gerard Pons-Moll, and
Michael J Black. Dynamic faust: Registering human bod-
ies in motion. In CVPR , 2017. 1, 7, 8
[6] Nicholas Carlini and David Wagner. Towards evaluating the
robustness of neural networks. In 2017 ieee symposium on
security and privacy (sp) , pages 39–57. Ieee, 2017. 3
[7] Nicholas Carlini, Anish Athalye, Nicolas Papernot, Wieland
Brendel, Jonas Rauber, Dimitris Tsipras, Ian Goodfellow,
Aleksander Madry, and Alexey Kurakin. On evaluating
adversarial robustness. arXiv preprint arXiv:1902.06705 ,
2019. 2, 3, 5
[8] Haoyu Chen, Hao Tang, Nicu Sebe, and Guoying Zhao. An-
iformer: Data-driven 3d animation with transformer. BMVC ,
2021. 1
[9] Haoyu Chen, Hao Tang, Henglin Shi, Wei Peng, Nicu Sebe,
and Guoying Zhao. Intrinsic-extrinsic preserved gans for
unsupervised 3d pose transfer. In ICCV , pages 8630–8639,
2021. 7
[10] Haoyu Chen, Hao Tang, Zitong Yu, Nicu Sebe, and Guoying
Zhao. Geometry-contrastive transformer for generalized 3d
pose transfer. In AAAI , pages 258–266, 2022. 1, 2, 3, 4, 5, 7,
8
[11] Haoyu Chen, Henglin Shi, Xin Liu, Xiaobai Li, and Guoying
Zhao. Smg: A micro-gesture dataset towards spontaneous
body gestures for emotional stress state analysis. Interna-
tional Journal of Computer Vision , 2023. 1
[12] Haoyu Chen, Hao Tang, Radu Timofte, Luc V Gool, and
Guoying Zhao. Lart: Neural correspondence learning with
latent regularization transformer for 3d motion transfer.
NeurIPS , 36, 2024. 1
[13] Jinnan Chen, Chen Li, and Gim Hee Lee. Weakly-supervised
3d pose transfer with keypoints. In CVPR , pages 15156–
15165, 2023. 3
[14] Luca Cosmo, Antonio Norelli, Oshri Halimi, Ron Kimmel,
and Emanuele Rodol `a. Limp: Learning latent shape repre-
sentations with metric preservation priors. ECCV , 2020. 7
[15] Francesco Croce and Matthias Hein. Reliable evalua-
tion of adversarial robustness with an ensemble of diverseparameter-free attacks. In ICML , pages 2206–2216. PMLR,
2020. 2, 3
[16] Xiaoyi Dong, Dongdong Chen, Hang Zhou, Gang Hua,
Weiming Zhang, and Nenghai Yu. Self-robust 3d point
recognition via gather-vector guidance. In CVPR , pages
11513–11521, 2020. 3, 5, 6
[17] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale. ICLR , 2021. 3
[18] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy.
Explaining and harnessing adversarial examples. arXiv
preprint arXiv:1412.6572 , 2014. 2, 3, 6
[19] Thibault Groueix, Matthew Fisher, Vladimir G Kim,
Bryan C Russell, and Mathieu Aubry. 3d-coded: 3d cor-
respondences by deep deformation. In ECCV , 2018. 2, 3,
7
[20] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr
Doll´ar, and Ross Girshick. Masked autoencoders are scalable
vision learners. In CVPR , pages 16000–16009, 2022. 2, 3, 4
[21] Jiaxing Huang, Dayan Guan, Aoran Xiao, and Shijian Lu.
Rda: Robust domain adaptation via fourier adversarial at-
tacking. In ICCV , pages 8988–8999, 2021. 2, 5
[22] Xun Huang and Serge Belongie. Arbitrary style transfer in
real-time with adaptive instance normalization. In ICCV ,
pages 1501–1510, 2017. 5
[23] Jaeyeon Kim, Binh-Son Hua, Thanh Nguyen, and Sai-Kit
Yeung. Minimal adversarial examples for deep learning on
3d point clouds. In ICCV , pages 7797–7806, 2021. 5, 6
[24] Diederik P. Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. In ICLR , 2015. 7
[25] Hui Kuurila-Zhang, Haoyu Chen, and Guoying Zhao. Adap-
tive adversarial norm space for efficient adversarial training.
BMVC , 2023. 2
[26] Kaidong Li, Ziming Zhang, Cuncong Zhong, and Guanghui
Wang. Robust structured declarative classifiers for 3d point
clouds: Defending adversarial attacks with implicit gradi-
ents. In CVPR , pages 15294–15304, 2022. 3
[27] Zhouyingcheng Liao, Jimei Yang, Jun Saito, Gerard Pons-
Moll, and Yang Zhou. Skeleton-free pose transfer for styl-
ized 3d characters. In ECCV . Springer, 2022. 1, 2, 3
[28] Daniel Liu, Ronald Yu, and Hao Su. Extending adversarial
attacks and defenses to deep 3d point cloud classifiers. In
ICIP , pages 2279–2283. IEEE, 2019. 3, 6
[29] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard
Pons-Moll, and Michael J Black. Smpl: A skinned multi-
person linear model. TOG , 34(6):1–16, 2015. 2
[30] Qianli Ma, Jinlong Yang, Siyu Tang, and Michael J. Black.
The power of points for modeling humans in clothing. In
ICCV , pages 10974–10984, 2021. 1, 3
[31] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar
Fawzi, and Pascal Frossard. Universal adversarial perturba-
tions. In CVPR , pages 1765–1773, 2017. 5
[32] Yatian Pang, Wenxiao Wang, Francis EH Tay, Wei Liu,
Yonghong Tian, and Li Yuan. Masked autoencoders
for point cloud self-supervised learning. arXiv preprint
arXiv:2203.06604 , 2022. 2, 3, 4
2303
[33] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,
James Bradbury, Gregory Chanan, Trevor Killeen, Zeming
Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An
imperative style, high-performance deep learning library. In
NeurIPS , 2019. 7
[34] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani,
Timo Bolkart, Ahmed A. A. Osman, Dimitrios Tzionas, and
Michael J. Black. Expressive body capture: 3D hands, face,
and body from a single image. In CVPR , pages 10975–
10985, 2019. 2
[35] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.
Pointnet: Deep learning on point sets for 3d classification
and segmentation. In CVPR , pages 652–660, 2017. 3
[36] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J
Guibas. Pointnet++: Deep hierarchical feature learning on
point sets in a metric space. NeurIPS , 30, 2017. 3, 4
[37] Chaoyue Song, Jiacheng Wei, Ruibo Li, Fayao Liu, and Gu-
osheng Lin. 3d pose transfer with correspondence learning
and mesh refinement. NeurIPS , 34, 2021. 1, 2, 3, 4, 5, 7, 8
[38] Jiawei Su, Danilo Vasconcellos Vargas, and Kouichi Sakurai.
One pixel attack for fooling deep neural networks. IEEE
Transactions on Evolutionary Computation , 23(5):828–841,
2019. 5
[39] Jiachen Sun, Weili Nie, Zhiding Yu, Z Morley Mao, and
Chaowei Xiao. Pointdp: Diffusion-driven purification
against adversarial attacks on 3d point cloud recognition.
arXiv preprint arXiv:2208.09801 , 2022. 3
[40] Tzungyu Tsai, Kaichen Yang, Tsung-Yi Ho, and Yier Jin.
Robust adversarial objects against deep learning models. In
AAAI , pages 954–962, 2020. 2, 3, 5, 6
[41] Jiashun Wang, Chao Wen, Yanwei Fu, Haitao Lin, Tianyun
Zou, Xiangyang Xue, and Yinda Zhang. Neural pose transfer
by spatially adaptive instance normalization. In CVPR , 2020.
1, 2, 3, 4, 5, 6, 7, 8
[42] Jiahang Wang, Sheng Jin, Wentao Liu, Weizhong Liu, Chen
Qian, and Ping Luo. When human pose estimation meets ro-
bustness: Adversarial algorithms and benchmarks. In CVPR ,
pages 11855–11864, 2021. 3
[43] Ruibin Wang, Yibo Yang, and Dacheng Tao. Art-point: Im-
proving rotation robustness of point cloud classifiers via ad-
versarial rotation. In CVPR , pages 14371–14380, 2022. 5
[44] Shaofei Wang, Andreas Geiger, and Siyu Tang. Locally
aware piecewise transformation fields for 3d human mesh
registration. In CVPR , pages 7639–7648, 2021. 2, 3
[45] Shaofei Wang, Marko Mihajlovic, Qianli Ma, Andreas
Geiger, and Siyu Tang. Metaavatar: Learning animatable
clothed human models from few depth images. In NeurIPS ,
pages 2810–2822, 2021. 1, 3
[46] Ziyi Wu, Yueqi Duan, He Wang, Qingnan Fan, and
Leonidas J Guibas. If-defense: 3d adversarial point cloud de-
fense via implicit function based restoration. arXiv preprint
arXiv:2010.05272 , 2020. 2, 3, 6
[47] Chong Xiang, Charles R Qi, and Bo Li. Generating 3d ad-
versarial point clouds. In CVPR , pages 9136–9144, 2019. 2,
3, 5, 6
[48] Honghui Yang, Tong He, Jiaheng Liu, Hua Chen, Boxi Wu,
Binbin Lin, Xiaofei He, and Wanli Ouyang. Gd-mae: gener-ative decoder for mae pre-training on lidar point clouds. In
CVPR , pages 9403–9414, 2023. 3
[49] Xumin Yu, Yongming Rao, Ziyi Wang, Zuyan Liu, Jiwen
Lu, and Jie Zhou. Pointr: Diverse point cloud completion
with geometry-aware transformers. In ICCV , pages 12498–
12507, 2021. 2, 3, 4
[50] Jinlai Zhang, Lyujie Chen, Binbin Liu, Bo Ouyang, Qizhi
Xie, Jihong Zhu, Weiming Li, and Yanmei Meng. 3d
adversarial attacks beyond point cloud. arXiv preprint
arXiv:2104.12146 , 2021. 2, 3, 5, 6
[51] Jiaxu Zhang, Junwu Weng, Di Kang, Fang Zhao, Shaoli
Huang, Xuefei Zhe, Linchao Bao, Ying Shan, Jue Wang, and
Zhigang Tu. Skinned motion retargeting with residual per-
ception of motion semantics & geometry. In CVPR , pages
13864–13872, 2023. 2, 3
[52] Renrui Zhang, Ziyu Guo, Peng Gao, Rongyao Fang, Bin
Zhao, Dong Wang, Yu Qiao, and Hongsheng Li. Point-
m2ae: Multi-scale masked autoencoders for hierarchical
point cloud pre-training. arXiv preprint arXiv:2205.14401 ,
2022. 2, 3, 4
[53] Renrui Zhang, Ziyu Guo, Peng Gao, Rongyao Fang, Bin
Zhao, Dong Wang, Yu Qiao, and Hongsheng Li. Point-m2ae:
multi-scale masked autoencoders for hierarchical point cloud
pre-training. NeurIPS , 35:27061–27074, 2022. 3
[54] Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip HS Torr, and
Vladlen Koltun. Point transformer. In ICCV , pages 16259–
16268, 2021. 3, 4
[55] Tianhang Zheng, Changyou Chen, Junsong Yuan, Bo Li, and
Kui Ren. Pointcloud saliency maps. In ICCV , pages 1598–
1606, 2019. 5, 6
[56] Keyang Zhou, Bharat Lal Bhatnagar, and Gerard Pons-
Moll. Unsupervised shape and pose disentanglement for 3d
meshes. In Computer Vision–ECCV 2020: 16th European
Conference, Glasgow, UK, August 23–28, 2020, Proceed-
ings, Part XXII 16 , pages 341–357. Springer, 2020. 3
[57] Congcong Zhu, Xiaoqiang Li, Jide Li, and Songmin Dai. Im-
proving robustness of facial landmark detection by defending
against adversarial attacks. In ICCV , pages 11751–11760,
2021. 5
2304
