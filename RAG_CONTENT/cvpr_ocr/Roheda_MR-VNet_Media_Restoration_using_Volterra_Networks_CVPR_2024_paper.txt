MR-VNet: Media Restoration using Volterra Networks
Siddharth Roheda, Amit Unde, Loay Rashid
Samsung Research Institute
Bengaluru, India
{sid.roheda, amit.unde, loay.rashid }@samsung.com
Abstract
This research paper presents a novel class of restoration
network architecture based on the Volterra series formu-
lation. By incorporating non-linearity into the system re-
sponse function through higher order convolutions instead
of traditional activation functions, we introduce a general
framework for image/video restoration. Through exten-
sive experimentation, we demonstrate that our proposed
architecture achieves state-of-the-art (SOTA) performance
in the field of Image/Video Restoration. Moreover, we es-
tablish that the recently introduced Non-Linear Activation
Free Network (NAF-NET) can be considered a special case
within the broader class of Volterra Neural Networks. These
findings highlight the potential of Volterra Neural Networks
as a versatile and powerful tool for addressing complex
restoration tasks in computer vision.
1. Introduction
In our increasingly visual-oriented world, media such as
images and videos play a crucial role in conveying informa-
tion, expressing emotions, and preserving memories. How-
ever, images/videos are susceptible to distortions during the
process of capturing (eg. sensor noise, blur, zoom, bad ex-
posure), saving/sharing (eg. compression, down-sampling),
and editing (eg. un-natural artefacts). It is crucial to re-
store such degraded images so as to prevent loss of infor-
mation and ensure that the best visual quality is delivered
to users. This is done through techniques such as image
de-noise, de-blur, compression reduction, super-resolution,
etc. Recently, Convolutional Neural Networks (CNNs) cou-
pled with ample training data and computational resources
has driven remarkable progress in image restoration algo-
rithms [5, 7, 24, 29, 30]. The basic building block in a CNN
is a convolutional layer followed by an activation function.
The convolution operation provides local connectivity and
translational in-variance, while the activation function intro-
duces non-linearity into the network. Following this, Trans-
former networks that use a more dynamic alternative of self-
Figure 1. Comparison of PSNR (dB) and Computational Com-
plexity (GMACs) of the various models on the GoPro Dataset.
attention were introduced to resolve some of the shortcom-
ings of CNNs. Namely, instead of relying on convolutional
filters that have static weights and cannot adapt to input con-
tent, self-attention allows calculation of response at a given
pixel by weighted sum of all other positions. The drawback
of such methods is that they are extremely heavy and diffi-
cult to train. This also makes the analysis and tractability of
such networks elusive. More recently, there was an interest
shown in activation free networks [4] that are lighter and
more powerful than the traditional CNNs. In our work we
explore introduction of non-linearity in the network through
interaction between the pixels of an image. This is done
by performing higher order convolutions to augment linear
convolutions. We utilize the well-established V olterra Se-
ries [21] to accomplish this task.
Contributions: In this paper we propose a novel archi-
tecture tailored for image and video restoration that utilizes
the recently proposed V olterra layers [18] to optimally in-
troduce non-linearities in the restoration process. We design
a U-Net like architecture integrated with V olterra layers to
achieve high quality reconstruction while minimizing com-
putational overhead. Our study demonstrates that using the
V olterra formulation with the proposed lossless/lossy ap-
proximation results in significantly reduced network com-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
6098
plexity and depth compared to traditional CNNs to achieve
similar performance levels. We also showcase that the
recently proposed Non-linear Activation Free NETworks
(NAFNet) [4] are a special case of the V olterra Formula-
tion.
2. Related Work
2.1. Volterra Filter
The V olterra Filter, as proposed in [21], serves as an
approximation for capturing the non-linear relationship be-
tween the input at time t, denoted as xt, and the correspond-
ing output yt. Mathematically, this relationship is expressed
as follows:
yt=L−1X
τ1=0W1
τ1xt−τ1+L−1X
τ1,τ2=0W2
τ1,τ2xt−τ1xt−τ2
+...+L−1X
τ1,τ2,...,τK=0WK
τ1,τ2,...,τKxt−τ1xt−τ2...xt−τK,(1)
where Lrepresents the filter memory or filter length, and
WKis the weight matrix corresponding to the Kthorder
term. Notably, the computational complexity of this for-
mulation experiences exponential growth with an increase
in the desired filter order. Specifically, a Kthorder filter
with length Lnecessitates solving LKequations. It is note-
worthy that the first term in the equation corresponds to the
linear convolutional layer commonly employed in Convolu-
tional Neural Networks (CNNs).
The distinctive feature of Equation 1 is the incorporation
of non-linearities through higher order convolutions, as op-
posed to the traditional activation function. For instance, the
second term in Equation 1 specifically refers to a second-
order convolution.
2.2. Volterra Filters in Deep Learning
The potential of V olterra Filters in learning non-linear
functions is vast, suggesting their potential utility in enhanc-
ing the performance of deep learning models. In the study
by [31], the authors introduced a second-order V olterra filter
to augment non-linearity, supplementing traditional activa-
tion functions. The application of a second-order V olterra
Filter was also exemplified in facial recognition tasks [9].
Despite demonstrating the efficacy of V olterra Filters, the
exploration was constrained to second-order non-linearities,
primarily due to computational complexity limitations.
In another study [18], a cascaded approach to V olterra
Filtering was proposed to address the challenge of Video
Action Recognition. This approach involved cascading lay-
ers of second-order filters, resulting in a filter with sig-
nificantly higher complexity. The research showcased theproficiency of V olterra Filters in learning non-linear infor-
mation from data, all the while demanding lower compu-
tational resources compared to conventional Convolutional
Neural Networks that rely on activation functions for intro-
ducing non-linearities.
2.3. Image/Video Restoration and Enhancement
Image and video restoration is a crucial task involv-
ing the enhancement of images or videos that have suf-
fered degradation during capture or storage, ultimately de-
livering clean, sharp visuals to enhance user experience.
Recent contributions in the literature, as exemplified by
[5,11,13,20,29], have leveraged the U-Net architecture [19]
and extended its capabilities for performance improvement.
However, such extensions often come at the cost of in-
creased model complexity. This escalation in complexity,
ranging from the incorporation of multiple U-Net stages to
the integration of transformers for image restoration, has re-
sulted in a notable surge in model intricacies for achieving
marginal PSNR/SSIM improvements.
In contrast, a Non-Linear Activation Free Network
(NAFNet) was introduced in [4], challenging the prevail-
ing notion that high complexities are indispensable. This
work argued that a simple baseline network can achieve
comparable results. Moreover, empirical evidence from this
study demonstrated that non-linear activation functions may
be dispensable, substitutable by straightforward element-
wise multiplication. This finding aligns with the assertion
in [18], where V olterra Filters replaced activation functions
for Action Recognition tasks.
On a different front, Video Restoration has garnered
increased attention in the research community. Video
Restoration tasks present greater complexity than image
restoration, given their multi-frame structure and tempo-
ral interdependencies among frames. Addressing both spa-
tial and temporal aspects is crucial to ensuring flicker-
free restoration and maintaining continuity across video
frames. Recent research in Video Restoration, as observed
in the use of LSTMs and RNNs [17, 22], emphasizes ef-
fective exploitation of temporal information. Alternatively,
transformer architectures [10] have been employed to con-
sider frame order and ensure coherence in restored videos.
V olterra Filters [21] offer a promising formulation for ex-
ploring the temporal dynamics of videos, enabling the gen-
eration of non-linear interactions to enhance both spatial
and temporal non-linearity.
3. Problem Formulation
Consider a collection of degraded images denoted as
XD=x1
D, x2
D, ..., xN
Dof particular interest. The objec-
tive is to devise a restoration function, G:RH×W×C→
RH×W×C, capable of recovering a set of clean images,
XR=x1
R, x2
R, ..., xN
RfromXD. This restorative func-
6099
tion is structured as a composition of an encoder, a middle
block, and a decoder, expressed as follows:
G=FD◦FM◦FE. (2)
4. Proposed Solution
4.1. Volterra Filters for Restoration
In our proposed methodology, we employ the second-
order V olterra formulation to implement a V olterra layer.
Specifically, the zthlayer of the V olterra Neural Network
(VNN) processes the input Xz−1as follows:
Xz=Vz(Xz−1) =F1
z(Xz−1) +F2
z(Xz−1),(3)
whereF1
zandF2
zrepresent the first and second-order
convolution operations in the zthlayer. For video data
tasks, a spatio-temporal (3D) version of the V olterra Fil-
ter is employed to compute the feature value at location
[t, m 1, m2], given by Equation 4:
Xz
t
m1m2
=Vz
Xz−1
t−L:t+L
m1−p1:m1+p1
m2−p2:m2+p2


=X
τ1,σ11,σ21W1τ1σ11σ21xt−τ1m1−σ11m2−σ21
+X
τ1,σ11,σ21τ2,σ12σ22W2τ1σ11σ21τ2σ12σ22xτ1m1−σ11m2−σ21xt−τ2m1−σ12m2−σ22,
(4)
where τi∈[−L, L],σ1j∈[−p1, p1], and σ2j∈
[−p2, p2]represent temporal and spatial translations (hor-
izontal and vertical directions), respectively. Notably, the
first term corresponds to the linear convolution utilized in
Convolutional Neural Networks (CNNs), while the second
term introduces non-linearity in the network instead of re-
lying on a conventional activation function.
For image data, where only spatial translations (2D) are
applicable, the feature value at location [m1, m2]in feature
mapXzis computed using a 2D version of the V olterra Fil-
ter, as expressed in Equation 5:
Xz[m1m2]=Vz 
Xz−1m1−p1:m1+p1
m2−p2:m2+p2!
=X
σ11,σ21W1
[σ11σ21]xhm1−σ11m2−σ21i
+X
σ11,σ21σ12σ22W2
[σ11σ21][σ12σ22]xhm1−σ11m2−σ21ixhm1−σ12m2−σ22i,(5)where σ1j∈[−p1, p1]andσ2j∈[−p2, p2]represent
spatial translations in the horizontal and vertical directions,
respectively.
We incorporate these V olterra filter layers in a cascaded
fashion, following the simple U-Net architecture. The cas-
cading of the layers as defined in Equations 4 and 5 leads to
the approximation of a higher-order V olterra filter.
Proposition 1. [Higher Order Complexity] [18] If Z
second-order filters are cascaded, the resulting V olterra
Network achieves an effective order of Kz= 22Z−1.
The implementation of a Kthorder V olterra Filter, as
specified in Equation 1, necessitatesPK
k=1(L.P1.P2)kpa-
rameters. Where P1= 2p1+1,P2= 2p2+1andL= 1in
case of images. The adoption of cascaded 2ndorder filters
emerges as a more resource-efficient approach for imple-
menting the V olterra formulation, significantly reducing the
required parameters [18].
Proposition 2. [VNN Complexity] [18] The complexity
of a cascaded Kth
zorder V olterra filter is given by:
ZX
z=1
(Lz.P1zP2z+ (Lz.P1zP2z)2)
. (6)
As previously mentioned, our approach eschews explicit
activation functions like ReLU, tanh, etc. The non-linearity
is learned as a function of the input data. Proposition 3 high-
lights that the VNN formulation is proficient in approximat-
ing well-known activation functions, implying that VNN of-
fers a generalized activation encompassing ReLU, sigmoid,
tanh, etc., as special cases.
Proposition 3. [Generalized Activation] A V olterra Filter,
as described in Equation 1, provides an approximation to
any continuous function.
Proof. Employing the Taylor expansion, any non-linear
function, σ(.), can be expressed as:
σ(x) =c0+c1x+...+cKxK+...c∞x∞. (7)
Specifically, a Sigmoid can be written as:
σsigmoid (x) =1
1 +ex=1
2+1
4x−1
48x2+1
480x5...(8)
From Equation 1, the V olterra Filter formulation can ef-
fectively approximate such an expansion upto a finite order
when L=p1=p2= 1,
σV F=w0+w1x+w2x2+...+wKxK, (9)
6100
Figure 2. High level Block Diagram for implementation of the proposed VNN Image Restoration Model
where wkis learned during the training process, ren-
dering it a more generalized activation function capable of
learning data-dependent non-linearities. When wkin Equa-
tion 9 is assigned the values corresponding to the Taylor ex-
pansion coefficients of a specific activation function (such
as the sigmoid in Equation 8), it assumes the role of a prede-
fined activation function, representing a special case within
the broader framework.
4.2. Realization of Higher Order Convolutions
4.2.1 Lossless Approximation
We will now elaborate on the implementation of the higher-
order kernels outlined in Equation 3. The V olterra Formu-
lation from Equation 3 can be expressed as follows:
Xz=W1
z⋆ Xz−1+X
s1,s2W2
zs1,s2⋆ Xz−1.Ss1,s2(Xz−1),
(10)
Here, s1ands2denote spatial shifts in the input feature
mapXz−1, andSs1,s2represents the feature map circularly
shifted along its rows and columns by s1ands2respec-
tively. The formulation described in Equation 10 enables
the parallel implementation of second order V olterra kernel
using 2D convolutions as part of PyTorch [16] or Tensor-
flow [1].
Consider the scenario where P1=P2=P. To im-
plement the second term in Equation 10, P2convolutions
would be required. To mitigate redundancy and model com-
plexity, we discard the symmetric terms, resulting inPC2<
P2convolutions. This approach reduces the model’s com-
plexity by eliminating redundant information, thus servingas a lossless approximation of the exact kernel. In the sub-
sequent section, we delve into a lossy approximation for the
second-order V olterra kernel.
4.2.2 Lossy Approximation
Implementing higher-order convolution can incur signifi-
cant costs despite the lossless optimization techniques dis-
cussed in Section 4.2.1. To address this, we employ the con-
cept of separable kernels to approximate the second-order
convolution. In the 2D case, the second-order filter is real-
ized as follows:
W2
P1×P2×P1×P2=QX
q=1W2
aqP1×P2×1W2
bq1×P1×P2,(11)
where Qis the desired rank of approximation, P1=
2p1+ 1 andP2= 2p2+ 1. A similar separable kernel
approach was explored in [3]. However, authors of [3] only
considered a 1strank ( Q= 1) approximation, leading to
sub-optimal performance as detailed in Tables 5 and 6.
With this approximation, the V olterra Net Block can be
implemented as follows:
W1
z⋆ Xz−1+QX
q=1W2
aq⋆ Xz−1.W2
bq⋆ Xz−1. (12)
This reduces the complexity of network asPZ
z=1
(Lz.P1z.P2z) + 2 Q(Lz.P1z.P2z)
, which is
significantly lower than Equation 6.
6101
Figure 3. NAFNet Block on the left, MR-VNN block (lossy approximation) on the right
Proposition 4. [Second Order Approximation] The lossy
approximation in Equation 11 is a Qthrank approximation
of the exact quadratic kernel.
Proof. Let’s examine a 1-D V olterra Filter with length L.
The quadratic weight matrix, W2, in such a case has di-
mension L×L, and Equation 11 becomes:
W2 (Q)
L×L=QX
q=1W2
aqL×1W2
bq1×L. (13)
Singular Value Decomposition of W2leads to:
W2=UΣVT, (14)
whereUandVareL×Lmatrices, and Σis a diagonal
matrix with singular values of W2. Consequently, Equa-
tion 14 becomes:
W2=LX
q=1uqσqvT
q, (15)
where the qthbasis of UandVare represented by uq
andvqrespectively, and σqis the qthsingular value of W2.
AQthrank approximation is then given as:
W2(Q)=QX
q=1uqσqvT
q (16)
=QX
q=1ˆuqvq, (17)
where ˆuq=uq.σq. IfW2
aq= ˆuqandW2
bq=vT
q,W2(Q)=QX
q=1W2
aqW2
bq. (18)
This proves that the approximation detailed in Equa-
tion 11 is indeed a Qthrank approximation of the exact
quadratic kernel, W2.
Proposition 5. [Special Case of NAFNet] The Non-
Linear Activation Free Net (NAFNet) is a special case of
V olterra Neural Networks, characterized by W1
z=β.Iand
Q= 1.
Proof. Consider the tensor Xz−1W×H×Cto be the input to
thezthNAF-Block. The NAF-Block processes the input
by convolving it with a kernel, WzNAF, yielding the in-
termediate output, LzW×H×2C=WzNAF⋆ Xz−1. Subse-
quently, Lzis split into LzaW×H×CandLzbW×H×C, which
are multiplied to produce the simple gate output, Mz=
Lza.Lzb. Finally, the NAF-Block output is obtained as
Xz=Mz+βXz−1, where βis a scaling factor. As WzNAF
is a depthwise convolution with groups = 2c, it can be ex-
pressed as the product of separable convolutions:
WzNAF=WzaNAF.WzbNAF, (19)
Consequently, Mzis computed as,
Mz=WzaNAF⋆ Xz−1.WzbNAF⋆ Xz−1 (20)
=Lza.Lzb.
Setting Q= 1 in Equation 11 yields the expression in
Equation 20, demonstrating that the simple gate used in
NAFNet Blocks is a 1strank approximation of the quadratic
V olterra kernel.
6102
Method MIMO-
UNet [7]HINet
[5]MAXIM
[20]Restormer
[27]UFormer
[23]Deep-
RFT
[13]MPR-
Net [8]NAF-
Net [4]MR-VNet-
LYA (Q=2)MR-VNet-
LYA (Q=4)MR-
VNet-
LLA
PSNR 32.68 32.71 32.86 32.92 32.97 33.23 33.31 33.69 33.85 33.93 34.04
SSIM 0.959 0.959 0.961 0.961 0.967 0.963 0.964 0.967 0.967 0.967 0.969
GMACs 1235 170.7 169.5 140 89.5 187 778.2 65 47 70 96
Table 1. De-Blurring Performance on GoPro Dataset. Best results are bold , second best are underlined .
Method MPR-
Net [8]MIR-
Net
[28]NBNet
[6]UFormer
[23]MAXIM
[20]HINet
[5]Restormer
[27]NAF-
Net [4]MR-VNet-
LYA (Q=2)MR-VNet-
LYA (Q=4)MR-
VNet-
LLA
PSNR 39.71 39.72 39.75 39.89 39.96 39.99 40.02 40.30 40.39 40.43 40.58
SSIM 0.958 0.959 0.959 0.960 0.960 0.958 0.960 0.962 0.962 0.963 0.963
GMACs 588 786 88.8 89.5 169.5 170.7 140 65 47 70 96
Table 2. De-Noising Performance on SIDD Dataset. Best results are bold , second best are underlined .
Furthermore, a residual is added to the simple gate out-
put to obtain the final output, which can be written as
Xz=Mz+β.I⋆ Xz−1. Consequently, the NAF-Block
emerges as a special case of the V olterra Networks with
W1
z=β.IandQ= 1.
The comparison of a NAFNet Block and a MR-VNet
(Lossy Approximation) Block is depicted in Figure 3
4.3. Model Architecture
We design a U-Net inspired architecture for approximat-
ing the Encoder-Decoder functions. The input image under-
goes initial processing through the Encoder function, FE:
RW×H×C→Rw×h×c, comprising 4 encoder blocks. Each
encoder block consists of 4 V olterra Layers, implemented
as described by Equation 5 utilizing the Lossless and Lossy
approximations from Sections 4.2.1 and 4.2.2. Strided con-
volutions are employed in consecutive Encoder blocks to
reduce resolution, resulting in a latent space configuration
ofh=H/8andw=W/8. The middle block in the la-
tent space, FM:Rh×w×c→Rh×w×c, is implemented
using a single V olterra Layer. Finally, the decoder func-
tion,FD:Rh×w×c→RH×W×C, transforms the encoded
features back into the image space. The Decoder, sym-
metric in design to the Encoder with 4 blocks, has only 1
V olterra Layer per block. Each encoder block is connected
to its corresponding decoder block via a residual connec-
tion. For video restoration problems, the same architecture
is retained, but 3D V olterra Filters are used instead of 2D, as
detailed in Equation 4. The entire architectural arrangement
is illustrated in Figure 2.
5. Experiments
To assess the efficacy of the proposed V olterra Restora-
tion Network, we conduct experiments targeting prevalent
degradations in images and videos:•Motion-Blur : This degradation, arising from camera
or subject motion, is addressed by training and testing
the restoration network on the GoPro [14] and Reds
[15] datasets for image deblurring.
•Camera Sensor Noise : We aim to mitigate noise
introduced by the camera sensor during image/video
capture. Evaluation is performed using the SIDD [2]
and CRVD [25] datasets.
We present two iterations of our proposed V olterra
Layer-based architecture: MR-VNet-LYA, employing the
Lossy approximation discussed in Section 4.2.2, and MR-
VNet-LLA, incorporating the proposed Lossless approxi-
mation. Subsequently, we subject our method to a com-
prehensive evaluation against state-of-the-art (SOTA) al-
gorithms in the domains of Image and Video Restoration.
Evaluation metrics such as Peak Signal to Noise Ratio
(PSNR) and Structural Similarity Index Measure (SSIM)
are employed for a thorough assessment. Additionally, we
conduct a detailed comparative analysis of the computa-
tional complexities associated with the proposed methods,
quantified in terms of Giga Multiply-Add Computations
(GMACs).
The efficacy of our proposed method is substantiated
through comparisons with various SOTA techniques preva-
lent in the literature. A detailed quantitative assessment for
De-Blurring is presented in Tables 1 and 3, utilizing the Go-
Pro and REDS datasets, respectively. The results under-
score the superior performance of our proposed model, out-
performing existing SOTA methods in terms of both PSNR
and SSIM metrics.
In the realm of image denoising, our proposed method
outperforms alternative approaches, as evidenced by the
quantitative results presented in Table 2 on the SIDD
dataset. Notably, our technique demonstrates superior de-
6103
Figure 4. De-Blurring Results on GoPro.
Figure 5. De-Noising Results on SIDD. Left to Right: Noisy Image, NAFNet, MR-VNet (Ours)
Method PSNR SSIM GMACs
MPRNet [8] 28.79 0.811 776.7
HiNet [5] 28.83 0.862 170.7
MAXIM [20] 28.93 0.865 169.5
NAFNet [4] 29.09 0.867 65
MR-VNet-LYA (Q=4) 29.79 0.868 70
MR-VNet-LLA 29.92 0.869 96
Table 3. De-Blurring Performance on REDS Datasetnoising efficacy, as reflected in higher Peak Signal to Noise
Ratio (PSNR) and Structural Similarity Index Measure
(SSIM) scores compared to competing methods.
Furthermore, our exploration extends to the challenging
domain of video denoising, detailed in Table 4. In this sce-
nario, we utilize 3D convolution as detailed in Equation 4
and exclusively assess the performance of our Lossy ap-
proximation due to its practical efficiency compared to the
computationally demanding exact kernel. Our results un-
derscore the effectiveness of the proposed method in achiev-
ing superior denoising outcomes in video data, showcasing
its competitive edge over existing methodologies.
6104
Method PSNR SSIM GMACs
EMVD-L [12] 41.00 0.983 2543
RVIDEFormer [26] 41.29 0.984 287
LLVD-L [17] 41.41 0.984 117
MR-VNet-LYA 41.93 0.985 163
Table 4. De-Noising Performance on CRVD Dataset. Best perfor-
mance is bold , second best is underlined .
Kernel Rank PSNR SSIM GMACs
Q=1 (Special case of NAFNet) 33.50 0.965 36
Q=2 33.85 0.966 47
Q=4 33.93 0.967 70
Q=8 33.97 0.967 115
Table 5. Effect of the rank of 2ndorder Kernel for GoPro
Kernel Rank PSNR SSIM GMACs
Q=1 (Special case of NAFNet) 40.21 0.962 36
Q=2 40.39 0.963 47
Q=4 40.43 0.963 70
Q=8 40.46 0.964 115
Table 6. Effect of the rank of 2ndorder Kernel for SIDD
These compelling quantitative results affirm the efficacy
of our proposed method in addressing both image and video
denoising challenges, positioning it as a robust and ad-
vanced solution in comparison to contemporary approaches.
5.1. Ablation Study
In our comprehensive ablation study, we rigorously an-
alyze the individual contributions of various components
within the proposed architecture.
We first investigate the impact of selecting the rank of
the kernel in the context of lossy approximation, as demon-
strated on the GoPro and SIDD datasets, as detailed in Ta-
bles 5 and 6. Notably, when the kernel is constrained to
rank 1, it aligns with the specific case observed in NAFNet.
As we escalate the rank, the approximation becomes pro-
gressively accurate, converging towards the precision of the
exact kernel. However, this refinement comes at the expense
of heightened complexity.
In Table 7, we demonstrate the impact of width of the ar-
chitecture in terms of channels while keeping the quadratic
filter rank constant.
In Table 8, we present the impact of incorporating ac-
tivation functions alongside the 2ndorder kernel. Our
observations indicate that, while ReLU and Sigmoid ac-
tivation confer a marginal improvement in the case of
NAFNet ( 1stRank approximation), the same effect is notWidth PSNR SSIM GMACs
16 32.80 0.961 8
24 32.92 0.963 18
32 33.18 0.966 32
48 33.93 0.967 70
Table 7. Effect of width of the architecture when Q=4 (GoPro
Dataset)
Activation NAFNet MR-VNet
Identity 39.96 40.43
ReLU 39.98 40.40
GELU 39.97 40.39
Sigmoid 39.99 40.40
SiLU 39.96 40.39
Table 8. Impact of using activation functions. The performance is
evaluated in terms of PSNR on the SIDD Dataset
discernible when employing a higher rank approximation
of the V olterra Filter. This suggests that the 1stRank kernel
utilized in NAFNet is insufficient to closely approximate the
exact quadratic kernel. Consequently, the activation func-
tion introduces some non-linearity to the model, resulting in
improved performance. However, with a closer approxima-
tion in the form of a higher rank kernel, none of the activa-
tion functions appear to contribute additional non-linearity
beyond what the model has already learned.
6. Conclusion
In conclusion, our research introduces the Media
Restoration-V olterra Network (MR-VNet) as a novel ap-
proach to image and video restoration. Leveraging higher-
order V olterra filters, the proposed architecture demon-
strates promising capabilities in addressing common image
and video degradations, such as motion blur and camera
sensor noise. Through the development of two architectural
variants, VNN-LYA and VNN-LLA, employing lossy and
lossless approximations, respectively, we offer a compre-
hensive exploration of the network’s performance.
Our experimental evaluations, conducted on diverse
datasets including GoPro, Reds, SIDD, and CRVD, show-
case the effectiveness of MR-VNet in comparison to state-
of-the-art algorithms. Notably, MR-VNet exhibits compet-
itive results in terms of Peak Signal to Noise Ratio (PSNR)
and Structural Similarity Index Measure (SSIM). Further-
more, we analyze the impact of activation functions on the
network’s performance, revealing insights into their efficacy
in conjunction with higher-order V olterra filters.
6105
References
[1] Mart ´ın Abadi, Ashish Agarwal, Paul Barham, Eugene
Brevdo, Zhifeng Chen, Craig Citro, Greg S Corrado, Andy
Davis, Jeffrey Dean, Matthieu Devin, et al. Tensorflow:
Large-scale machine learning on heterogeneous distributed
systems. arXiv preprint arXiv:1603.04467 , 2016. 4
[2] Abdelrahman Abdelhamed, Stephen Lin, and Michael S
Brown. A high-quality denoising dataset for smartphone
cameras. In Proceedings of the IEEE conference on com-
puter vision and pattern recognition , pages 1692–1700,
2018. 6
[3] Monami Banerjee, Rudrasis Chakraborty, Jose Bouza, and
Baba C Vemuri. V olterranet: A higher order convolutional
network with group equivariance for homogeneous mani-
folds. IEEE Transactions on Pattern Analysis and Machine
Intelligence , 44(2):823–833, 2020. 4
[4] Liangyu Chen, Xiaojie Chu, Xiangyu Zhang, and Jian Sun.
Simple baselines for image restoration. In European Confer-
ence on Computer Vision , pages 17–33. Springer, 2022. 1, 2,
6, 7
[5] Liangyu Chen, Xin Lu, Jie Zhang, Xiaojie Chu, and Cheng-
peng Chen. Hinet: Half instance normalization network for
image restoration. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
182–192, 2021. 1, 2, 6, 7
[6] Shen Cheng, Yuzhi Wang, Haibin Huang, Donghao Liu,
Haoqiang Fan, and Shuaicheng Liu. Nbnet: Noise basis
learning for image denoising with subspace projection. In
Proceedings of the IEEE/CVF conference on computer vi-
sion and pattern recognition , pages 4896–4906, 2021. 6
[7] Sung-Jin Cho, Seo-Won Ji, Jun-Pyo Hong, Seung-Won Jung,
and Sung-Jea Ko. Rethinking coarse-to-fine approach in sin-
gle image deblurring. In Proceedings of the IEEE/CVF inter-
national conference on computer vision , pages 4641–4650,
2021. 1, 6
[8] Xiaojie Chu, Liangyu Chen, Chengpeng Chen, and Xin Lu.
Improving image restoration by revisiting global information
aggregation. In European Conference on Computer Vision ,
pages 53–71. Springer, 2022. 6, 7
[9] Ritwik Kumar, Arunava Banerjee, Baba C Vemuri, and
Hanspeter Pfister. Trainable convolution filters and their ap-
plication to face recognition. IEEE transactions on pattern
analysis and machine intelligence , 34(7):1423–1436, 2011.
2
[10] Jingyun Liang, Jiezhang Cao, Yuchen Fan, Kai Zhang,
Rakesh Ranjan, Yawei Li, Radu Timofte, and Luc Van Gool.
Vrt: A video restoration transformer. arXiv preprint
arXiv:2201.12288 , 2022. 2
[11] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc
Van Gool, and Radu Timofte. Swinir: Image restoration us-
ing swin transformer. In Proceedings of the IEEE/CVF inter-
national conference on computer vision , pages 1833–1844,
2021. 2
[12] Matteo Maggioni, Yibin Huang, Cheng Li, Shuai Xiao,
Zhongqian Fu, and Fenglong Song. Efficient multi-stage
video denoising with recurrent spatio-temporal fusion. InProceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 3466–3475, 2021. 8
[13] Xintian Mao, Yiming Liu, Fengze Liu, Qingli Li, Wei Shen,
and Yan Wang. Intriguing findings of frequency selection for
image deblurring. In Proceedings of the AAAI Conference on
Artificial Intelligence , volume 37, pages 1905–1913, 2023.
2, 6
[14] Seungjun Nah, Tae Hyun Kim, and Kyoung Mu Lee. Deep
multi-scale convolutional neural network for dynamic scene
deblurring. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 3883–3891,
2017. 6
[15] Seungjun Nah, Sanghyun Son, Suyoung Lee, Radu Timofte,
and Kyoung Mu Lee. Ntire 2021 challenge on image de-
blurring. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pages 149–165,
2021. 6
[16] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,
James Bradbury, Gregory Chanan, Trevor Killeen, Zeming
Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An im-
perative style, high-performance deep learning library. Ad-
vances in neural information processing systems , 32, 2019.
4
[17] Loay Rashid, Siddharth Roheda, and Amit Unde. Llvd:
Lstm-based explicit motion modeling in latent space for
video denoising. 2, 8
[18] Siddharth Roheda and Hamid Krim. Conquering the cnn
over-parameterization dilemma: A volterra filtering ap-
proach for action recognition. In Proceedings of the AAAI
Conference on Artificial Intelligence , volume 34, pages
11948–11956, 2020. 1, 2, 3
[19] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-
net: Convolutional networks for biomedical image segmen-
tation. In Medical Image Computing and Computer-Assisted
Intervention–MICCAI 2015: 18th International Conference,
Munich, Germany, October 5-9, 2015, Proceedings, Part III
18, pages 234–241. Springer, 2015. 2
[20] Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang,
Peyman Milanfar, Alan Bovik, and Yinxiao Li. Maxim:
Multi-axis mlp for image processing. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 5769–5780, 2022. 2, 6, 7
[21] Vito V olterra. Theory of functionals and of integral and
integro-differential equations. (No Title) , 1959. 1, 2
[22] Wei Wang, Xin Chen, Cheng Yang, Xiang Li, Xuemei Hu,
and Tao Yue. Enhancing low light videos by exploring high
sensitivity camera noise. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages 4111–
4119, 2019. 2
[23] Zhendong Wang, Xiaodong Cun, Jianmin Bao, Wengang
Zhou, Jianzhuang Liu, and Houqiang Li. Uformer: A general
u-shaped transformer for image restoration. In Proceedings
of the IEEE/CVF conference on computer vision and pattern
recognition , pages 17683–17693, 2022. 6
[24] Qingyang Xu, Chengjin Zhang, and Li Zhang. Denoising
convolutional neural network. In 2015 IEEE International
Conference on Information and Automation , pages 1184–
1187. IEEE, 2015. 1
6106
[25] Huanjing Yue, Cong Cao, Lei Liao, Ronghe Chu, and Jingyu
Yang. Supervised raw video denoising with a benchmark
dataset on dynamic scenes. In Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition ,
pages 2301–2310, 2020. 6
[26] Huanjing Yue, Cong Cao, Lei Liao, and Jingyu Yang. Rvide-
former: Efficient raw video denoising transformer with a
larger benchmark dataset. arXiv preprint arXiv:2305.00767 ,
2023. 8
[27] Syed Waqas Zamir, Aditya Arora, Salman Khan, Mu-
nawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang.
Restormer: Efficient transformer for high-resolution image
restoration. In Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition , pages 5728–5739,
2022. 6
[28] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar
Hayat, Fahad Shahbaz Khan, Ming-Hsuan Yang, and Ling
Shao. Learning enriched features for real image restoration
and enhancement. In Computer Vision–ECCV 2020: 16th
European Conference, Glasgow, UK, August 23–28, 2020,
Proceedings, Part XXV 16 , pages 492–511. Springer, 2020.
6
[29] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar
Hayat, Fahad Shahbaz Khan, Ming-Hsuan Yang, and Ling
Shao. Multi-stage progressive image restoration. In Pro-
ceedings of the IEEE/CVF conference on computer vision
and pattern recognition , pages 14821–14831, 2021. 1, 2
[30] Kai Zhang, Wangmeng Zuo, Shuhang Gu, and Lei Zhang.
Learning deep cnn denoiser prior for image restoration. In
Proceedings of the IEEE conference on computer vision and
pattern recognition , pages 3929–3938, 2017. 1
[31] Georgios Zoumpourlis, Alexandros Doumanoglou, Nicholas
Vretos, and Petros Daras. Non-linear convolution filters for
cnn-based learning. In Proceedings of the IEEE international
conference on computer vision , pages 4761–4769, 2017. 2
6107
