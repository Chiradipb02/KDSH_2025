Scaling Up Dynamic Human-Scene Interaction Modeling
Nan Jiang1,2˚, Zhiyuan Zhang1,2˚, Hongjie Li1, Xiaoxuan Ma3, Zan Wang4,
Yixin Chen2, Tengyu Liu2, Yixin Zhu1
, Siyuan Huang2
1Institute for AI, Peking University2National Key Lab of General AI, BIGAI3School of Computer Science, CFCS, Peking University
4Beijing Institute of Technology‹Equal contributors
 yixin.zhu@pku.edu.cn, syhuang@bigai.ai
https://jnnan.github.io/trumans/
ϭϬϬ^ĐĞŶĞƐ
ǇŶĂŵŝĐ
KďũĞĐƚϭϱͲ,ŽƵƌDŽĂƉ
ĂƚĂƐĞƚ
'ĞŶĞƌĂƚĞ
DŽƚŝŽŶƐŽĨ
/ŶĨŝŶŝƚĞůĞŶŐƚŚ^ĐĞŶĞͲǁĂƌĞ,K/
DŽƚŝŽŶ^ǇŶƚŚĞƐŝƐ
EĂƚƵƌĂů
ŽůůŝƐŝŽŶ
ǀŽŝĚĂŶĐĞ
Figure 1. Overview of TRUMANS dataset and our Human-Scene Interaction ( HSI ) framework . We introduce the most extensive motion-
captured HSI dataset, featuring diverse HSI s precisely captured in 100 scene conﬁgurations. Beneﬁting from TRUMANS , we propose a novel
method for real-time generation of HSIs with arbitrary length, surpassing all baselines and exhibiting superb zero-shot generalizability.
Abstract
Confronting the challenges of data scarcity and ad-
vanced motion synthesis in HSI modeling, we introduce
theTRUMANS (Tracking H umanActio nsi n Scenes) dataset
alongside a novel HSI motion synthesis method. TRUMANS
stands as the most comprehensive motion-captured HSI
dataset currently available, encompassing over 15 hours of
human interactions across 100 indoor scenes. It intricately
captures whole-body human motions and part-level object
dynamics, focusing on the realism of contact. This dataset is
further scaled up by transforming physical environments into
exact virtual models and applying extensive augmentations
to appearance and motion for both humans and objects while
maintaining interaction ﬁdelity. Utilizing TRUMANS , we de-vise a diffusion-based autoregressive model that efﬁciently
generates Human-Scene Interaction ( HSI ) sequences of any
length, taking into account both scene context and intended
actions. In experiments, our approach shows remarkablezero-shot generalizability on a range of 3D scene datasets
(e.g., PROX, Replica, ScanNet, ScanNet++), producing mo-
tions that closely mimic original motion-captured sequences,
as conﬁrmed by quantitative experiments and human studies.
1. Introduction
The intricate interplay between humans and their environ-
ment is a focal point in Human-Scene Interaction ( HSI )[12],
spanning diverse facets from object-level interaction [ 2,25]
to scene-level planning and interaction [ 1,15,16,18]. While
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
1737
signiﬁcant strides have been made, the ﬁeld is notably hin-
dered by a scarcity of high-quality datasets. Early datasets
like PiGraphs [ 39] and PROX [ 16] initiated the exploration
but are constrained by scalability and data quality. MoCap
datasets [ 14,30] prioritize high-quality human motion cap-
ture using sophisticated equipment like VICON. However,
they often lack in capturing diverse and immersive HSI s.
Scalable datasets recorded via RGBD videos offer broader
utility but are impeded by lower quality in human pose and
object tracking. The advent of synthetic datasets [ 1,3,4,55]
provides cost efﬁciency and adaptability but fails to encap-
sulate the full spectrum of realistic HSI s, particularly in
capturing dynamic 3D contacts and object tracking.
To address these challenges, this work ﬁrst introduces
theTRUMANS (Tracking H umanActio nsi n Scenes) dataset.
TRUMANS emerges as the most extensive motion-captured
HSI dataset, encompassing over 15 hours of diverse hu-
man interactions across 100 indoor scenes . It captures
whole-body human motions and part-level object dynamics
with an emphasis on the realism of contact. This dataset is
further enhanced by digitally replicating physical environ-
ments into accurate virtual models. Extensive augmentations
in appearance and motion are applied to both humans and
objects, ensuring high ﬁdelity in interaction.
Next, we devise a computational model tackling the above
challenges by taking both scene and action as conditions.Speciﬁcally, our model employs an autoregressive condi-tional diffusion with scene and action embeddings as con-
ditional input, capable of generating motions of arbitrary
length. To integrate scene context, we develop an efﬁcient lo-
cal scene perceiver by querying the global scene occupancy
on a localized basis, which demonstrates robust proﬁciency
in 3D-aware collision avoidance while navigating cluttered
scenes. To incorporate frame-wise action labels as condi-
tions, we integrate temporal features into action segments,
empowering the model to accept instructions anytime while
adhering to the given action labels. This dual integration of
scene and action conditions enhances the controllability of
our method, providing a nuanced interface for synthesizing
plausible long-term motions in 3D scenes.
We conducted a comprehensive cross-evaluation of both
theTRUMANS dataset and our motion synthesis method.
Comparing TRUMANS with existing ones, we demonstrate
thatTRUMANS markedly improves the performance of cur-
rent state-of-the-art approaches. Moreover, our method, eval-
uated both qualitatively and quantitatively, exceeds existing
motion synthesis methods in terms of quality and zero-shot
generalizability on unseen 3D scenes, closely approximating
the quality of original motion-captured data. Beyond motion
synthesis, TRUMANS has been benchmarked for human pose
and contact estimation tasks, demonstrating its versatilityand establishing it as a valuable asset for a broad range of
future research endeavors.Summarized in Fig. 1, our work signiﬁcantly advances
HSI modeling. Our contributions are threefold: (i) The in-
troduction of TRUMANS , an extensive MoCap HSI dataset
capturing a wide array of human behaviors across 100 in-door scenes, noted for its diversity, quality, and scalability.
(ii) The development of a diffusion-based autoregressive
method for the real-time generation of
HSI s, adaptable to
any length and conditioned on 3D scenes and action labels.
(iii) Through extensive experimentation, we demonstrate the
robustness of TRUMANS and our proposed methods, capable
of generating motions that rival MoCap quality, outperform-
ing existing baselines, and exhibiting exceptional zero-shot
generalizability in novel environments.
2. Related Work
HSI Datasets Capturing human motions in 3D scenes is
pivotal, with an emphasis on the quality and scale of human
interactions. Early work focused on capturing coarse 3D hu-
man motions using 2D keypoints [ 33] or RGBD videos [ 39].
To improve quality and granularity, datasets like PROX [ 16]
use scene scans as constraints to estimate SMPL-X parame-
ters [ 36] from RGBD videos. However, these image-based
motion capture methods often result in noisy 3D poses.
Recent efforts have incorporated more sophisticated sys-
tems like IMU or optical MoCap ( e.g., VICON) [ 14,15,
17,22,30,61], providing higher quality capture but lim-
ited in scalability. These are typically constrained to static
scenes [ 15,17,55] or single objects [ 2,22,61], not fully
representing complex real-world HSI s such as navigating
cluttered spaces or managing concurrent actions.
Synthetic datasets [ 1,4,55] have attempted to ﬁll this gap.
Notable examples like BEDLAM [ 3] and CIRCLE [ 1]h a v e
been acknowledged for their cost efﬁciency and adaptability.
These datasets integrate human motion data into synthetic
scenes but fail to fully capture the range of realistic 3D HSI s,
particularly in terms of dynamic object poses within their
simulated environments.
Addressing these shortcomings, our work achieves a
unique balance of quality and scalability. We replicate syn-
thetic 3D environments in an optical motion capture setting,
facilitating both accurate capture of humans and objectsin complex
HSI s and providing photorealistic renderings.
This approach not only enhances the ﬁdelity of the captured
interactions but also extends the range of scenarios and envi-
ronments that can be realistically simulated.
HSI Generation HSI generation involves single-frame hu-
man body [ 27,60,62] and temporal motion sequences [ 1,17,
21,26,32,35,52–54,57], utilizing models like conditional
V ariational Auto-Encoder ( cV AE )[43] and diffusion mod-
els [ 19,42,44]. Recent advancements focus on generat-
ing arbitrary-length human motions through autoregressive
methods [ 4,7,17,31,47,59] and anchor frame genera-
1738
Table 1. Comparison of TRUMANS with existing HSI datasets. TRUMANS differs by providing a diverse collection of HSI s, encompassing
over 15 hours of interaction across 100 indoor scenes, along with photorealistic RGBD renderings in both multi-view and ego-view.
Datasets Hours MoCapHuman Dynamic No. of ContactRGBD SegmentationMulti- Ego-
Representation Object Scenes Annotations view view
GT A IM [ 4] 9.3 skeleton 10 
PiGraphs [ 39] 2.0 skeleton 30 
PROX [ 16] 0.9 SMPL-X 12  
GRAB [ 46] 3.8  SMPL-X  - 
SAMP [ 17] 1.7  SMPL-X - 
RICH [ 20] 0.8 SMPL-X 5  
BEHA VE [ 2] 4.2 SMPL  -  
CHAIRS [ 22] 17.3  SMPL-X  -  
COUCH [ 61] 3.0  SMPL  -  
iReplica [ 15] 0.8  SMPL  7   
CIRCLE [ 1] 10.0  SMPL-X 9 
TRUMANS 15.0  SMPL-X  100     
tion [ 37,52]. Additionally, enhancing generation control-
lability has involved semantic guidance, such as action la-
bels [ 63] and language descriptions [ 55,56].
In comparison, our work contributes a conditional gener-
ative model with an autoregressive mechanism to generate
arbitrary-length motions, combining diffusion model capa-
bilities with improved controllability in HSI generation.
3.TRUMANS Dataset
This section introduces TRUMANS , the most comprehensive
MoCap dataset dedicated to 3D HSI s thus far. TRUMANS
offers not only accurate 3D ground truths but also photoreal-
istic renderings accompanied by various 2D ground truths,
suitable for various perceptual tasks in HSI . This section
details the dataset’s statistics, data capture process, post-
processing method, and our augmentation pipeline.
3.1. Dataset Statistics
TRUMANS encompasses 15 hours of high-quality motion-
captured data, featuring complex HSI s within 3D scenes,
where humans interact with clustered environments and dy-
namic objects. Captured at a rate of 30 Hz using the state-of-
the-art VICON MoCap system, the dataset comprises a total
of 1.6 million frames. The HSI interactions in TRUMANS
include 20 different types of common objects, ensuring aminimum of 5 distinct instances per type. The object cate-gories encompass a range from static items like sofas and
beds to dynamic objects such as bottles, and even articulated
items including laptops and cabinets. TRUMANS incorpo-
rates performances from 7 participants (4 male and 3 fe-
male), who enacted various actions across 100 indoor scenes.
These scenes span a variety of settings, such as dining rooms,
living rooms, bedrooms, and kitchens, among others. For a
comprehensive comparison of the TRUMANS dataset with
existing HSI datasets, please refer to Tab. 1.3.2. Scene-aware Motion Capture
Aiming to capture realistic and diverse Human-Scene Interac-
tion ( HSI ) within 3D scenes, our approach emphasizes both
data quality and diversity. We initiate this process by replicat-
ing 3D scenes and objects sourced from the 3D-FRONT [ 10]
dataset and BlenderKit [ 6] within the physical environment
housing our MoCap devices. To ensure the naturalness of
human interactions during motion capture, we meticulously
create real-world placeholders that correspond to the affor-
dances of the objects in the synthetic environment. All mov-
able objects are tagged with markers compatible with the
VICON system, enabling precise tracking of their poses. Ac-
tors undergo training to familiarize themselves with interact-
ing with these placeholders. During the capturing sessions,
actors are prompted to perform actions randomly selected
from a pre-deﬁned pool, ensuring a variety of interactions.
Post-capture, the human poses are converted into the
SMPL-X format [ 36], employing a vertex-to-vertex opti-
mization technique. This method is instrumental in calculat-
ing vertex-to-vertex distances between the human meshesand object meshes, facilitating accurate per-vertex contactannotations. We utilize Blender [
5] to render multi-view
photorealistic RGBD videos, segmentation masks, and ego-
centric videos. To further diversify the renderings, we in-corporate over 200 digital human models from Character
Creator 4 [ 38], ensuring that objects strategically placed in
scene backgrounds enhance the scene’s realism without im-
peding human movement. For a detailed exposition of our
capture and processing pipeline, refer to Appendix B.4 .
3.3. MoCap Data Augmentation
Our data augmentation pipeline is designed to adapt human
motions to changes in 3D scene objects, ensuring physical
plausibility and accuracy in HSI . This process is vital in com-
plex scenarios with concurrent or successive interactions;
1739
Increased 
Chair’s Height
Decreased 
Bed’s Height
(b) Augmented motion sequence (a) Original motion sequence
Figure 2. Data augmentation for motion generation. This example highlights how human motion is adjusted to accommodate variations
in object sizes. Speciﬁcally, the chair’s height is increased, and the bed’s height is decreased, each by 15cm. Our augmentation method
proﬁciently modiﬁes human motion to maintain consistent interactions despite these changes in object dimensions.
see Fig. 2. The pipeline consists of three main steps for
integrating altered human motions into diverse 3D settings.
Calculate Target Joint We identify contact points be-
tween human joints and object meshes, and locate corre-
sponding points on transformed or replaced objects. This
step crucially adjusts the target joint’s position to maintain
the original interaction’s contact relationship, ensuring re-
alistic human-object interactions despite changes in object
dimensions or positions.
Reﬁne Trajectory To smooth out abrupt trajectory
changes from the ﬁrst step or Inverse Kinematic ( IK) compu-
tations, we apply temporal smoothing to joint offsets, itera-
tively adjusting weights in adjacent frames. This reﬁnement
is critical for maintaining seamless motion, particularly in
scenarios with multiple object interactions. Further details
and theoretical background are discussed in Appendix B.5 .
Recompute Motion with IK In the ﬁnal step, we recom-
pute human motion using the smoothed trajectories with an
enhanced CCD-based [ 24]IK solver. This solver applies
clipping and regularizations to bone movements, ensuringnatural motion ﬂuidity. Bones further from the root joint
have increased rotational limits, reducing jitteriness and en-
hancing motion realism. For a complete description of these
methods, refer to Appendix B.5 .
4. Method
Utilizing the comprehensive TRUMANS dataset, we develop
an autoregressive motion diffusion model. This model gener-
ates HSI s that are not only physically plausible in 3D scenes
but also highly controllable through frame-wise action la-
bels, capable of producing sequences of arbitrary length in
real-time .
4.1. Problem Formulation and Notations
Given a 3D scene S, a goal location G, and action labels
A, our objective is to synthesize a human motion sequence
tHiuL
i“1of arbitrary length L. When interacting with dy-
namic objects P, we also estimate the corresponding object
pose sequence tOiuL
i“1.Human Human motion is represented as a sequence of
parameterized human meshes tHiuusing the SMPL-X
model [ 36]. The motion is initially generated as body joints
locations tXiuL
i“1, whereXiPRJˆ3represents J“24se-
lected joints. These are ﬁtted into the SMPL-X pose parame-
tersθ, global orientation φ, hand poses h, and root translation
r, resulting in the posed human mesh HPR10475 ˆ3.
Conditions We formalize three types of conditions in
our motion synthesis: 3D scene, goal location, and ac-tion labels. The 3D scene is represented by a voxel grid
SPt0,1uNxˆNyˆNz, with1indicating reachable locations.
Goal locations are 2D positions GPR2for navigation, or
3DR3for joint-speciﬁc control. Action labels are multi-hot
vectors APt0,1uLˆNA, indicating distinct actions.
Object When dynamic objects are involved, the object is
represented by its point cloud Pin canonical coordinates
and its global rotation Rand translation T. The dynamic
object sequence tOiuL
i“1is then represented by sequences
of rotations and translations tRi,TiuL
i“1.
4.2. Autoregressive Motion Diffusion
Our model architecture is illustrated in Fig. 3. Our goal
is to generate human motions that are not only physically
plausible in 3D scenes but also highly controllable by frame-
wise action labels, achieving arbitrary length in real time.
We employ an autoregressive diffusion strategy where a long
motion sequence is progressively generated by episodes ,
each deﬁned as a motion segment of Lepiframes. Based on
the approach by Shaﬁr et al. [40], successive episodes are
generated by extending from the ﬁnal kframes of the prior
episode. For each new episode, the ﬁrst kframes are set
based on the previous episode’s last kframes, with the noise
on these transition frames zeroed out using a mask Mtrans .
Our model aims to inpaint the remainder of each episode by
ﬁlling in the unmasked frames.
To ensure precise control over character navigation and
detailed interactions in each episode, we segment the over-
all goal Ginto discrete subgoals, represented as tGiuNepi
i“1,
whereNepi denotes the number of episodes. For navigation,
each subgoal GiPR2dictates the desired xy-coordinates of
1740
(a) Auto-regressive Inference (b) Diffusion Model
Action
SceneAction
Scene·  ·  ·
Action Label
ͲͲͳͲͲͲ ···
ͲͲͳͲͳͲ ···
Scene
Scen e
Positional 
EmbeddingCanonicalize
+Element-wise AdditionOptimize Object
&
Joint to SMPL-X
(a) Auto
 regressive  Inferen
·  ·  ··  ·  ·
ൈࢀ
Episode 3
·  ·  ··  ·  ·
ൈࢀ
Episode 1(d) Scene Encoder
Voxelize
 oder
 H
WC
Vision 
Transformer
Scene Embedding·  ·  ··  ·  ··  ·  ·
ൈࢀ
Episode 2c(c) Action Encoder
1
2
3
N-1
N+Ͳ ͲͲͲͳͲ
ͲͲͳͲͲͲ
ͳͲ
ͲͲͳ ͳ
ͲͲͳͲ ͳͲ· · ·
· · ·
· · ·
· · ·
· · ·· · ·· · ·· · ·· · ·· · ·
ͲͲͳͲ ͳͲ · · ·ͲͲ Ͳ
ͲͲ ͲͲͲ
Ͳ
ͲͲ Ͳ Ͳ
ͲͲ Ͳ Ͳ· · ·
· · ·
· · ·
· · ·
· · ·· · ·· · ·· · ·· · ·· · ·ͲǤͲͻ
ͲǤͳʹ
ͲǤͳͷ
ͲǤ͵͵
ͲͲ Ͳ Ͳ · · · ͲǤ͵͸ͲǤͳ
ͲǤʹ
ͲǤͺ
ͲǤͻAction
TimeAction Label DatasetProgress Indicator
(In Training Process)
Transformer 
EncoderAction Embedding
+ + + + +
Linear
12 N 3 N -1·  ·  ·Linearࣕ૚ࣕ૛ࣕ૜ ࡺࣕି૚ࡺࣕ ·  ·  ·
Transformer Encoder
+ + + + + +
࢚MLP·  ·  ·
MLP
·  ·  ·࢙࢔ࢇ࢚࢘ࡹ·  ·  ·
·  ·  ·
෩࢚ࢄ૚෩࢚ࢄ૛·  ·  ·
෩࢚ࢄ૜·  ·  ·
෩ࡺ࢚ࢄି૚·  ·  ·
෩ࡺ࢚ࢄ·  ·  ·1
2
3
J-1
J
࢒ࢇ࢕ࢍࡹ
Figure 3. Model architecture. (a) Our model employs an autoregressive diffusion sampling approach to generate arbitrary long-sequence
motions. (b) Within each episode, we synthesize motion using DDPM integrated with a transformer architecture, taking the human joint
locations as input. (c)(d) Action and scene conditions are encoded and forwarded to the ﬁrst token, guiding the motion synthesis process.
the character’s pelvis at an episode’s conclusion. Mirroring
the masking approach used in Mtrans , we align the pelvis’s
xy-coordinate in the episode’s ﬁnal frame to the respective
subgoal, simultaneously masking the corresponding diffu-
sion noise. As the z-coordinate is unspeciﬁed, the model is
trained to infer the appropriate pelvis height based on the
scene setup, such as making the character sit when the sub-
goal indicates a chair’s location. This principle also governs
ﬁne-grained interactions, like grasping or pushing, where the
subgoal GiPR3is set to the precise 3D location, aligning the
relevant hand joint to Giand masking joint noise accordingly.
This speciﬁc masking on the subgoals is denoted as Mgoal .
We devise a conditional diffusion model for generating
motions within each episode. This process involves sampling
from a Markov noising process tXtuT
t“0. Starting with the
original human joint data X0drawn from the data distribu-
tion, Gaussian noise is added to the components of X0not
masked by M “Mtrans YMgoal . The unmasked compo-
nents, represented as p1´M qdXtor˜Xt(where dis the
Hadamard product), undergo a forward noising process
qp˜Xt|˜Xt´1q“Np˜Xt;?αt˜Xt´1,p1´αtqIq, (1)
withαtPp0,1qdenoting hyper-parameters related to the
variance schedule.
Motion data generation within our model employs a re-
versed diffusion process to gradually denoise ˜XT. Consis-
tent with established diffusion model training methodolo-
gies, noise /epsilon1tis applied to obtain ˜Xt, and a neural network
/epsilon1θp˜Xt,t,S,Aqis constructed to approximate this noise. The
learning objective for /epsilon1θfollows a simple objective [ 19]
L“E˜X0„qp˜X0|Cq,t„r1,Ts›››/epsilon1´/epsilon1
θp˜Xt,t,S,Aq›››2
2. (2)We adopt the Transformer model architecture [ 48],
wherein the ﬁrst token encodes information about the diffu-
sion step, scene, and action, and subsequent tokens repre-
sent the noisy joint locations for each frame in the currentepisode. Throughout the sampling process, the model pre-
dicts the noise applied to each joint element. Once this sam-
pling phase concludes, the joint locations are translated into
SMPL-X parameters via a lightweight MLP . This translation
is further reﬁned through an optimization process, ensuringaccurate alignment with the human joint data.
Upon generating the human motion sequence tHiuL
i“0,
we optimize the trajectory of the interacting object tOiuL
i“0
to ensure natural Human-Object Interactions ( HOI s). To
enhance the realism of the interaction, we further ﬁne-tune
the object’s pose in each frame to minimize the variance in
distance between the object and the interacting hand [ 11].
4.3. Local Scene Perceiver
As illustrated in Fig. 3(d), the local scene perceiver is essen-
tial for embedding the local scene context, serving as a con-
dition for motion generation. This component analyzes the
scene using a local occupancy grid centered around the sub-
goal location for the current episode. Starting with the global
occupancy grid Sof the scene, where each cell’s boolean
value indicates reachability (1 for reachable, 0 otherwise), we
focus on the i-th episode’s subgoal Gi“px,y,z qorpx,y q.A
local occupancy grid is constructed around px,y q, extending
vertically from 0 to 1.8m. The grid’s orientation aligns with
the yaw of the agent’s pelvis at the episode’s start, and cell
values are derived by querying the global occupancy grid.
The voxel grid is encoded using a Vision Transformer
(ViT) [ 9]. We prepare the tokens by dividing the local occu-
1741
pancy grid into patches along the xy-plane, considering the
z-axis as feature channels. These patches are then fed into
the ViT model. The resulting scene embedding from this
process is utilized as the condition for the diffusion model.
Discretizing the scene into a grid format is a necessary
trade-off to boost training efﬁciency and practicality in our
HSI method. Although directly generating the local occu-
pancy grid from the scene mesh in real-time is technically
feasible, it substantially prolongs training time. For instance,
employing the checksign function from Kaolin results in a
training process that is approximately 300 times slower, ren-
dering it impractical. Despite this simpliﬁcation, our empiri-
cal results demonstrate that the quality of motion generation
is not signiﬁcantly impacted by this approximation.
4.4. Frame-wise Action Embedding
Our method distinguishes itself from prior approaches by
incorporating frame-wise action labels into the long-term
motion synthesis process, rather than generating a complete
motion sequence from a singular action description. In our
framework, a particular action can span multiple episodes,
necessitating the model’s capability to comprehend the evo-
lution and progression of an action over time.
To enhance our model’s understanding of action progres-
sion, we incorporate a progress indicator Aind PRLepiˆNA
into the frame-wise action labels, as depicted in Fig. 3(c).
This indicator is realized by appending a real number nP
r0,1sto the original action labels, representing the action’s
advancement from start to ﬁnish. As a result, action labels
take on values in 0Yr1,2spost-addition. For instance, dur-
ing a drinking action from frame itoj, we modify the p0,1q
label by adding a value that linearly progresses from 0 to
1 across this interval. Thus, at the onset of drinking (frame
i), the label is augmented to 1, gradually increasing to 2
by frame j, the action’s conclusion. This nuanced labeling
enables our model to seamlessly handle actions that span
multiple episodes, signiﬁcantly enhancing the realism and
ﬂuidity of the synthesized motion sequences.
The ﬁnal action embedding is obtained by processing the
progress-augmented action label APRLepiˆNAthrough a
Transformer encoder. Each frame’s action label AiPRNA
is treated as an individual token in the Transformer’s input.
The feature output from the last token is then passed through
an MLP to generate the ﬁnal action embedding.
5. Experiments
This section presents our evaluation of both TRUMANS and
our proposed motion synthesis method, focusing on action-
conditioned HSI generation. Additionally, we demonstrate
howTRUMANS contributes to advancements in state-of-the-
art motion synthesis methods.5.1. Experiment Settings
Our experimental evaluation of HSI generation quality is con-
ducted under two distinct settings: static and dynamic . The
static setting assesses synthesized motions in environments
without dynamic interactable objects, concentrating on lo-
comotion and interactions with static objects. Conversely,the dynamic setting evaluates motion synthesis involving
interactions with dynamic objects. In both scenarios, we
compare the performance of methods trained on TRUMANS
with those trained on existing datasets [ 46,62], offering
a thorough insight into both the model’s efﬁcacy and the
dataset’s impact.
5.2. Baselines and Ablations
Baselines– static setting We compare TRUMANS with
PROX [ 62], a dataset featuring human activities in indoor
scenes. To ensure a fair comparison, we retain only the loco-
motion and scene interaction of static objects in TRUMANS ,
such as sitting and lying down. Baseline methods for this
setting include cV AE [ 52], SceneDiff [ 21], and GMD [ 23].
Baselines– dynamic setting We compare TRUMANS with
GRAB [ 46], known for capturing full-body grasping actions
with human and object pose sequences. Here, the focus is on
motions of interaction with dynamic objects, like drinking
water and making phone calls, present in both datasets. We
compare our method against IMoS [ 11] and GOAL [ 47],
reproduced using their original implementations.
Ablations In our ablative studies, we examine the impact
of disabling the action progress indicator Aind in our model.
Additionally, to assess the signiﬁcance of our data aug-
mentation technique, we perform experiments using a non-
augmented version of TRUMANS . For reference, our standard
experiments employ the augmented TRUMANS , where each
object is transformed into two different variations.
Our evaluation encompasses 10 unseen indoor scenes
sourced from PROX [ 16], Replica [ 45], Scannet [ 8], and
Scannet++ [ 58]. These scenes are adapted to the require-
ments of different methods, with modiﬁcations including
conversion to point cloud format, voxelization, or maintain-
ing their original mesh format. To evaluate the diversity of
the synthesized motions, each method is tasked with gener-
ating ﬁve unique variations for each trajectory.
Furthermore, we conduct a qualitative comparison of our
method with other recent approaches, such as SAMP [ 17],
DIMOS [ 64], LAMA [ 25], and Wang et al. [54], based on the
feasibility of reproducing these methods. Detailed ﬁndings
from this comparison are discussed in Appendix A.4 .
5.3. Evaluation Metrics
In the static setting, we employ Contact and Penetration met-
rics, as recommended by Zhao et al. [64], to evaluate foot
slide and object penetration issues in synthesized motions.
1742
(a) Start from anywhere in any pose
(d) Long term motion with multiple milestones(b) Interact dynamically with the objects.
(c) Avoid colliding with objects in the scene
Read a book
 Open a cabinet
Pick up the phone
from the table and
make a phone call
Pick up the bottle
from the table and
drink
Figure 4. Visualization of motion generation. Leveraging local scene context and action instructions as conditions, our method demonstrates
its proﬁciency in (a) initiating motion given the surrounding environment, (b) dynamically interacting with objects, (c) avoiding collisions
during motion progression, and (d) robustly synthesizing long-term motion. The depicted scenes are selected from PROX, Replica, and
FRONT3D-test datasets, none of which were included in the training phase. For qualitative results, please refer to the Supplementary Video .
These metrics measure the degree to which the synthesized
motions conform to the speciﬁed scene. For the dynamic set-
ting, we utilize FID and Diversity metrics, commonly used in
language and action-guided motion generation tasks [ 11,48].
These metrics measure the quality and diversity of HOI mo-
tion generation involving various small objects.
Additionally, we introduce a novel MoCap-differentiating
human study for evaluation. Participants are presented with
ﬁve sequences, one of which is motion-captured, and areasked to identify the MoCap sequence. The likelihood of
correctly identifying the MoCap sequence serves as an indi-
cator of the synthesized motion’s realism. We quantify this
aspect through the Success Rate of Discrimination (SucRate-Dis), reﬂecting the percentage of participants who accurately
identify the MoCap sequence.
5.4. Results and Analysis
Fig. 4showcases our method’s qualitative strengths. It
adeptly manages complex scene conﬁgurations, including
initiating context-aware motion, avoiding collisions during
movement, and generating extended motions, especially in
HOI scenarios involving dynamic object interaction.
In the static setting (Tab. 2), our method, trained on
TRUMANS , surpasses baselines across most metrics. Notably,
disabling data augmentation leads to increased penetration,suggesting the efﬁcacy of augmented data in producing phys-
ically plausible motions. Compared to models trained on
PROX, ours shows signiﬁcant improvements, highlighting
TRUMANS as a high-quality resource for HSI research.
Table 2. Evaluation of locomotion and scene-level interaction .
We compare performances on TRUMANS and PROX [16].
Method Cont. Ò Pene meanÓ Pene maxÓ Dis. suc. Ó
Wang et al. [ 52] 0.969 1.935 14.33 0.581
SceneDiff [ 21] 0.912 1.691 17.48 0.645
GMD [ 23] 0.931 2.867 21.30 0.871
Ours 0.992 1.820 11.74 0.258
Ours w/o aug. 0.991 2.010 15.52 -
Wang et al. [ 52] 0.688 4.935 34.10 0.903
SceneDiff [ 21] 0.712 3.267 27.48 0.935
GMD [ 23] 0.702 4.867 38.30 0.968
Ours 0.723 4.820 31.74 0.903
Tab. 3illustrates results in the dynamic setting, where
our approach excels in 3D HOI generation. High penetration
rates with GRAB-trained methods indicate its limitations
in scene-adherent HOI motions, while TRUMANS captures
more detailed interactions. The absence of the progress in-
dicator Aind leads to method failure, as evidenced by the
ablation study.
1743
Table 3. Evaluation of object-level interaction. We compare per-
formances on TRUMANS and GRAB [46]. The deﬁnition of “Real”
follows the one deﬁned in Tevet et al. [ 48]
Method FID Ó Diversity Ñ Pene sceneÓ Dis. suc. Ó
Real-TRUMANS - 2.734 - -
GOAL [ 47] 0.512 2.493 34.10 0.801
IMoS [ 11] 0.711 2.667 37.48 0.774
Ours 0.313 2.693 11.74 0.226
Ours -Aind 2.104 1.318 10.62 1.000
Real-GRAB [ 46] - 2.155 - -
GOAL [ 47] 0.429 2.180 44.09 0.801
IMoS [ 11] 0.410 2.114 41.50 0.774
Ours 0.362 2.150 34.41 0.516
Human studies further afﬁrm the quality of our method.
Only about a quarter of participants could distinguish our
synthesized motions from real MoCap data, nearly aligning
with the 1/5 SucRateDis of random guessing. This suggests
that our synthesized motions are nearly indistinguishablefrom high-quality MoCap data. Comparative evaluations
with recent methods [ 17,25,54,64] show our model’s supe-
riority, outperforming the second-best model by over 30%
in support rate. For more detailed results, please refer to the
Supplementary Video .
Real-time Control Our method can sample an episode
of motion (1.6 seconds at 10 FPS) in 0.7 seconds on an
A800 GPU. This efﬁciency enables uninterrupted long-term
motion generation with a consistent control signal. For new
control signals, to minimize the 0.7-second delay, we imple-
ment an incremental sampling strategy: initially, 2 frames aresampled immediately, followed by sampling 4 frames during
their execution, increasing exponentially until 16 frames are
sampled. This approach ensures a balance between real-time
control and smooth motion continuity. Please refer to our
Supplementary Video for a visual demonstration.
5.5. Additional Image-based Tasks
TRUMANS , with its photo-realistic renderings and per-vertex
3D contact annotations, is also suited for various image-
based tasks. We focus on its application in 3D human mesh
estimation and contact estimation.
3D Human Mesh Estimation For reconstructing 3D hu-
man body meshes from input images, we utilize the state-of-
the-art method [ 29] as a baseline. We evaluate if including
TRUMANS in training enhances performance on the 3DPW
dataset [ 50]. Following Ma et al. [29], we report MPJPE,
P A-MPJPE, and MPVE for the estimated poses and meshes.
3D Contact Estimation This task involves predicting per-
vertex 3D contact on the SMPL mesh [ 28] from an input
image. We compare TRUMANS against RICH [ 20] and DA-
MON [ 49], both featuring vertex-level 3D contact labelswith RGB images. Utilizing BSTRO [ 20] for RICH and
DECO [ 49] for DAMON, we measure precision, recall, F1
score, and geodesic error following the literature [ 20,49].
Results and Analysis Quantitative results in Tab. 4re-
veal that integrating TRUMANS with 3DPW signiﬁcantly
improves human mesh estimation. Contact estimation out-
comes, presented in Tab. 5, show enhanced performance with
TRUMANS , particularly in reducing geodesic error. These re-
sults suggest that combining synthetic data from TRUMANS
with real-world data substantially beneﬁts image-based tasks.
For detailed experimental insights, see Appendix A.5 .
Table 4. Performance of Ma et al. [29]trained on 3DPW [ 50]
combined with TRUMANS in different ratios.
Training Data MPVE Ó MPJPE Ó P A-MPJPE Ó
3DPW [ 50] 101.3 88.2 54.4
3DPW+T (2:1) 88.8 77.2 46.4
3DPW+T (1:1) 78.5 78.5 46.4
Table 5. Performance of BSTRO [20] and DECO [49] trained
onRICH [20] and DAMON [49] combined with TRUMANS ,
respectively.
Training Data Prec Ò RecÒ F1Ò geo err Ó
RICH [ 20] 0.6823 0.7427 0.6823 10.27
R+T (2:1) 0.7087 0.7370 0.6927 9.593
R+T (1:1) 0.7137 0.7286 0.6923 9.459
DAMON [ 49]0.6388 0.5232 0.5115 25.06
D+T (2:1) 0.6472 0.5237 0.5148 21.54
D+T (1:1) 0.6701 0.4806 0.4972 18.87
6. Conclusion
We introduce TRUMANS , a large-scale mocap dataset, along-
side a novel motion synthesis method, addressing scalability,
data quality, and advanced motion synthesis challenges in
HSI modeling. As the most comprehensive dataset in its
category, TRUMANS encompasses diverse human interac-
tions with dynamic and articulated objects within 100 indoor
scenes. Our diffusion-based autoregressive motion synthesis
method, leveraging TRUMANS , is capable of real-time gen-
eration of HSI sequences of arbitrary length. Experimental
results indicate that the motions generated by our method
closely mirror the quality of the original MoCap data.
Acknowledgment The authors would like to thank
NVIDIA for their generous support of GPUs and hardware.
This work is supported in part by the National Science and
Technology Major Project (2022ZD0114900) and the Bei-
jing Nova Program.
1744
References
[1] Joao Pedro Ara ´ujo, Jiaman Li, Karthik V etrivel, Rishi Agar-
wal, Jiajun Wu, Deepak Gopinath, Alexander William Clegg,
and Karen Liu. Circle: Capture in rich contextual environ-
ments. In Conference on Computer Vision and Pattern Recog-
nition (CVPR) , 2023. 1,2,3
[2] Bharat Lal Bhatnagar, Xianghui Xie, Ilya A Petrov, Cristian
Sminchisescu, Christian Theobalt, and Gerard Pons-Moll.
Behave: Dataset and method for tracking human object in-teractions. In Conference on Computer Vision and Pattern
Recognition (CVPR) , 2022. 1,2,3,A2
[3] Michael J Black, Priyanka Patel, Joachim Tesch, and Jinlong
Y ang. Bedlam: A synthetic dataset of bodies exhibiting de-
tailed lifelike animated motion. In Conference on Computer
Vision and Pattern Recognition (CVPR) , 2023. 2
[4] Zhe Cao, Hang Gao, Karttikeya Mangalam, Qi-Zhi Cai, Minh
V o, and Jitendra Malik. Long-term human motion prediction
with scene context. In European Conference on Computer
Vision (ECCV) , 2020. 2,3
[5] Blender Online Community. Blender - a 3d modelling and
rendering package, 2018. 3
[6] Blender Online Community. Blenderkit. https://www.
blenderkit.com/ , 2023. 3,A3
[7] Enric Corona, Albert Pumarola, Guillem Aleny `a, and
Francesc Moreno-Noguer. Context-aware human motionprediction. In Conference on Computer Vision and Pattern
Recognition (CVPR) , 2020. 2
[8] Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber,
Thomas Funkhouser, and Matthias Nießner. Scannet: Richly-
annotated 3d reconstructions of indoor scenes. In Conference
on Computer Vision and Pattern Recognition (CVPR) , 2017.
6
[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is
worth 16x16 words: Transformers for image recognition at
scale. In International Conference on Learning Representa-
tions (ICLR) , 2021. 5
[10] Huan Fu, Bowen Cai, Lin Gao, Ling-Xiao Zhang, Jiaming
Wang, Cao Li, Qixun Zeng, Chengyue Sun, Rongfei Jia, Bin-
qiang Zhao, et al. 3d-front: 3d furnished rooms with layouts
and semantics. In International Conference on Computer
Vision (ICCV) , 2021. 3,A3
[11] Anindita Ghosh, Rishabh Dabral, Vladislav Golyanik, Chris-
tian Theobalt, and Philipp Slusallek. Imos: Intent-driven
full-body motion synthesis for human-object interactions. In
Computer Graphics F orum , 2023. 5,6,7,8
[12] James J Gibson. The perception of the visual world. Houghton
Mifﬂin, 1950. 1
[13] Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji,
Xingyu Li, and Li Cheng. Generating diverse and natural 3d
human motions from text. In Conference on Computer Vision
and Pattern Recognition (CVPR) , 2022. A1
[14] Vladimir Guzov, Aymen Mir, Torsten Sattler, and Gerard
Pons-Moll. Human poseitioning system (hps): 3d humanpose estimation and self-localization in large scenes frombody-mounted sensors. In Conference on Computer Vision
and Pattern Recognition (CVPR) , 2021. 2
[15] Vladimir Guzov, Julian Chibane, Riccardo Marin, Y annan
He, Y unus Saracoglu, Torsten Sattler, and Gerard Pons-Moll.
Interaction replica: Tracking human–object interaction and
scene changes from human motion. In International Confer-
ence on 3D Vision (3DV) , 2023. 1,2,3,A2
[16] Mohamed Hassan, V asileios Choutas, Dimitrios Tzionas, and
Michael J Black. Resolving 3d human pose ambiguities
with 3d scene constraints. In International Conference on
Computer Vision (ICCV) , 2019. 1,2,3,6,7
[17] Mohamed Hassan, Duygu Ceylan, Ruben Villegas, Jun Saito,
Jimei Y ang, Yi Zhou, and Michael Black. Stochastic scene-
aware motion prediction. In International Conference on
Computer Vision (ICCV) , 2021. 2,3,6,8,A1,A2
[18] Mohamed Hassan, Partha Ghosh, Joachim Tesch, Dimitrios
Tzionas, and Michael J Black. Populating 3d scenes by learn-
ing human-scene interaction. In Conference on Computer
Vision and Pattern Recognition (CVPR) , 2021. 1
[19] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-
sion probabilistic models. In Advances in Neural Information
Processing Systems (NeurIPS) , 2020. 2,5
[20] Chun-Hao P Huang, Hongwei Yi, Markus H ¨oschle, Matvey
Safroshkin, Tsvetelina Alexiadis, Senya Polikovsky, Daniel
Scharstein, and Michael J Black. Capturing and inferring
dense full-body human-scene contact. In Conference on Com-
puter Vision and Pattern Recognition (CVPR) , 2022. 3,8,
A2
[21] Siyuan Huang, Zan Wang, Puhao Li, Baoxiong Jia, Tengyu
Liu, Yixin Zhu, Wei Liang, and Song-Chun Zhu. Diffusion-
based generation, optimization, and planning in 3d scenes.
InConference on Computer Vision and Pattern Recognition
(CVPR) , 2023. 2,6,7,A1
[22] Nan Jiang, Tengyu Liu, Zhexuan Cao, Jieming Cui, Zhiyuan
Zhang, Yixin Chen, He Wang, Yixin Zhu, and Siyuan Huang.
Full-body articulated human-object interaction. In Interna-
tional Conference on Computer Vision (ICCV) , 2023. 2,3
[23] Korrawe Karunratanakul, Konpat Preechakul, Supasorn Suwa-
janakorn, and Siyu Tang. Guided motion diffusion for control-
lable human motion synthesis. In Conference on Computer
Vision and Pattern Recognition (CVPR) , 2023. 6,7,A1
[24] Ben Kenwright. Inverse kinematics–cyclic coordinate descent
(ccd). Journal of Graphics Tools , 2012. 4
[25] Jiye Lee and Hanbyul Joo. Locomotion-action-manipulation:
Synthesizing human-scene interactions in complex 3d envi-
ronments. In International Conference on Computer Vision
(ICCV) , 2023. 1,6,8,A1
[26] Jiaman Li, Jiajun Wu, and C Karen Liu. Object motion guided
human motion synthesis. arXiv preprint arXiv:2309.16237 ,
2023. 2
[27] Xueting Li, Sifei Liu, Kihwan Kim, Xiaolong Wang, Ming-
Hsuan Y ang, and Jan Kautz. Putting humans in a scene:Learning affordance in 3d indoor environments. In Confer-
ence on Computer Vision and Pattern Recognition (CVPR) ,
2019. 2
[28] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard
Pons-Moll, and Michael J. Black. SMPL: A skinned multi-
1745
person linear model. ACM Transactions on Graphics (TOG) ,
2015. 8
[29] Xiaoxuan Ma, Jiajun Su, Chunyu Wang, Wentao Zhu, and
Yizhou Wang. 3d human mesh estimation from virtual mark-
ers. In Conference on Computer Vision and Pattern Recogni-
tion (CVPR) , 2023. 8
[30] Christian Mandery, ¨Omer Terlemez, Martin Do, Nikolaus
V ahrenkamp, and Tamim Asfour. The kit whole-body human
motion database. In International Conference on Robotics
and Automation (ICRA) , 2015. 2
[31] Wei Mao, Miaomiao Liu, Richard Hartley, and Mathieu Salz-
mann. Contact-aware human motion forecasting. In Advances
in Neural Information Processing Systems (NeurIPS) , 2022.
2
[32] Aymen Mir, Xavier Puig, Angjoo Kanazawa, and Gerard
Pons-Moll. Generating continual human motion in diverse
3d scenes. In International Conference on 3D Vision (3DV) ,
2023. 2
[33] Aron Monszpart, Paul Guerrero, Duygu Ceylan, Ersin Y umer,
and Niloy J Mitra. imapper: interaction-guided scene mapping
from monocular videos. ACM Transactions on Graphics
(TOG) , 2019. 2
[34] Gyeongsik Moon and Kyoung Mu Lee. I2l-meshnet: Image-
to-lixel prediction network for accurate 3d human pose and
mesh estimation from a single rgb image. In European Con-
ference on Computer Vision (ECCV) , 2020. A2
[35] Liang Pan, Jingbo Wang, Buzhen Huang, Junyu Zhang, Hao-
fan Wang, Xu Tang, and Y angang Wang. Synthesizing physi-
cally plausible human motions in 3d scenes. In International
Conference on 3D Vision (3DV) , 2023. 2
[36] Georgios Pavlakos, V asileios Choutas, Nima Ghorbani,
Timo Bolkart, Ahmed AA Osman, Dimitrios Tzionas, and
Michael J Black. Expressive body capture: 3d hands, face,and body from a single image. In Conference on Computer
Vision and Pattern Recognition (CVPR) , 2019. 2,3,4,A4
[37] Huaijin Pi, Sida Peng, Minghui Y ang, Xiaowei Zhou, and
Hujun Bao. Hierarchical generation of human-object inter-
actions with diffusion probabilistic models. In International
Conference on Computer Vision (ICCV) , 2023. 3
[38] Reallusion. Character creator 4. https://www.
reallusion.com/character-creator/ , 2023. 3,
A3
[39] Manolis Savva, Angel X Chang, Pat Hanrahan, Matthew
Fisher, and Matthias Nießner. Pigraphs: learning interaction
snapshots from observations. ACM Transactions on Graphics
(TOG) , 2016. 2,3
[40] Y onatan Shaﬁr, Guy Tevet, Roy Kapon, and Amit H Bermano.
Human motion diffusion as a generative prior. arXiv preprint
arXiv:2303.01418 , 2023. 4
[41] Vicon Software. Shogun. http s://www. vicon.com/
software/shogun/ , 2023. A3
[42] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,
and Surya Ganguli. Deep unsupervised learning using
nonequilibrium thermodynamics. In International Conference
on Machine Learning (ICML) , 2015. 2
[43] Kihyuk Sohn, Honglak Lee, and Xinchen Y an. Learning
structured output representation using deep conditional gener-ative models. In Advances in Neural Information Processing
Systems (NeurIPS) , 2015. 2
[44] Y ang Song and Stefano Ermon. Generative modeling by
estimating gradients of the data distribution. In Advances in
Neural Information Processing Systems (NeurIPS) , 2019. 2
[45] Julian Straub, Thomas Whelan, Lingni Ma, Y ufan Chen, Erik
Wijmans, Simon Green, Jakob J Engel, Raul Mur-Artal, Carl
Ren, Shobhit V erma, et al. The replica dataset: A digital
replica of indoor spaces. arXiv preprint arXiv:1906.05797 ,
2019. 6
[46] Omid Taheri, Nima Ghorbani, Michael J Black, and Dimitrios
Tzionas. Grab: A dataset of whole-body human grasping of
objects. In European Conference on Computer Vision (ECCV) ,
2020. 3,6,8
[47] Omid Taheri, V asileios Choutas, Michael J. Black, and Dim-
itrios Tzionas. GOAL: Generating 4D whole-body motion
for hand-object grasping. In Conference on Computer Vision
and Pattern Recognition (CVPR) , 2022. 2,6,8,A1
[48] Guy Tevet, Sigal Raab, Brian Gordon, Y oni Shaﬁr, Daniel
Cohen-or, and Amit Haim Bermano. Human motion diffusion
model. In International Conference on Learning Representa-
tions (ICLR) , 2022. 5,7,8
[49] Shashank Tripathi, Agniv Chatterjee, Jean-Claude Passy,
Hongwei Yi, Dimitrios Tzionas, and Michael J Black. Deco:
Dense estimation of 3d human-scene contact in the wild. In
International Conference on Computer Vision (ICCV) , 2023.
8,A2
[50] Timo V on Marcard, Roberto Henschel, Michael J Black, Bodo
Rosenhahn, and Gerard Pons-Moll. Recovering accurate 3d
human pose in the wild using imus and a moving camera. In
European Conference on Computer Vision (ECCV) , 2018. 8,
A2
[51] Dongkai Wang and Shiliang Zhang. 3d human mesh recovery
with sequentially global rotation estimation. In International
Conference on Computer Vision (ICCV) , 2023. A2
[52] Jiashun Wang, Huazhe Xu, Jingwei Xu, Sifei Liu, and Xiao-
long Wang. Synthesizing long-term 3d human motion and
interaction in 3d scenes. In Conference on Computer Vision
and Pattern Recognition (CVPR) , 2021. 2,3,6,7,A1
[53] Jingbo Wang, Sijie Y an, Bo Dai, and Dahua Lin. Scene-aware
generative network for human motion synthesis. In Confer-
ence on Computer Vision and Pattern Recognition (CVPR) ,
2021.
[54] Jingbo Wang, Y u Rong, Jingyuan Liu, Sijie Y an, Dahua Lin,
and Bo Dai. Towards diverse and natural scene-aware 3d
human motion synthesis. In Conference on Computer Vision
and Pattern Recognition (CVPR) , 2022. 2,6,8,A1
[55] Zan Wang, Yixin Chen, Tengyu Liu, Yixin Zhu, Wei Liang,
and Siyuan Huang. Humanise: Language-conditioned hu-
man motion generation in 3d scenes. In Advances in Neural
Information Processing Systems (NeurIPS) , 2022. 2,3
[56] Zeqi Xiao, Tai Wang, Jingbo Wang, Jinkun Cao, Wenwei
Zhang, Bo Dai, Dahua Lin, and Jiangmiao Pang. Uni-
ﬁed human-scene interaction via prompted chain-of-contacts.
arXiv preprint arXiv:2309.07918 , 2023. 3
[57] Sirui Xu, Zhengyuan Li, Y u-Xiong Wang, and Liang-Y an
Gui. InterDiff: Generating 3d human-object interactions with
1746
physics-informed diffusion. In International Conference on
Computer Vision (ICCV) , 2023. 2
[58] Chandan Y eshwanth, Y ueh-Cheng Liu, Matthias Nießner, and
Angela Dai. Scannet++: A high-ﬁdelity dataset of 3d indoor
scenes. In International Conference on Computer Vision
(ICCV) , 2023. 6
[59] Jianrong Zhang, Y angsong Zhang, Xiaodong Cun, Shaoli
Huang, Y ong Zhang, Hongwei Zhao, Hongtao Lu, and XiShen. T2m-gpt: Generating human motion from textual de-
scriptions with discrete representations. In Conference on
Computer Vision and Pattern Recognition (CVPR) , 2023. 2
[60] Siwei Zhang, Y an Zhang, Qianli Ma, Michael J Black, and
Siyu Tang. Generating person-scene interactions in 3d scenes.
InInternational Conference on 3D Vision (3DV) , 2020. 2
[61] Xiaohan Zhang, Bharat Lal Bhatnagar, Sebastian Starke,
Vladimir Guzov, and Gerard Pons-Moll. Couch: Towards
controllable human-chair interactions. In European Confer-
ence on Computer Vision (ECCV) , 2022. 2,3
[62] Y an Zhang, Mohamed Hassan, Heiko Neumann, Michael J
Black, and Siyu Tang. Generating 3d people in scenes with-
out people. In Conference on Computer Vision and Pattern
Recognition (CVPR) , 2020. 2,6
[63] Kaifeng Zhao, Shaofei Wang, Y an Zhang, Thabo Beeler, and
Siyu Tang. Compositional human-scene interaction synthesis
with semantic control. In European Conference on Computer
Vision (ECCV) , 2022. 3
[64] Kaifeng Zhao, Y an Zhang, Shaofei Wang, Thabo Beeler, and
Siyu Tang. Synthesizing diverse human motions in 3d indoor
scenes. In International Conference on Computer Vision
(ICCV) , 2023. 6,8,A1
[65] Yi Zhou, Connelly Barnes, Jingwan Lu, Jimei Y ang, and Hao
Li. On the continuity of rotation representations in neural
networks. In Conference on Computer Vision and Pattern
Recognition (CVPR) , 2019. A1
1747
