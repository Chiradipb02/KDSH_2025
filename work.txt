Result 1: {'text': 'data', 'metadata': {'created_at': 1736932571, 'modified_at': 1736932571, 'owner': 'sayandeep', 'path': 'RAG_CONTENT/conferences/cvpr_ocr/Li_Real-Time_Exposure_Correction_via_Collaborative_Transformations_and_Adaptive_Sampling_CVPR_2024_paper.txt', 'size': 48686, 'seen_at': 1737191136, 'data': 'Real-Time Exposure Correction via Collaborative Transformations and\nAdaptive Sampling\nZiwen Li1, Feng Zhang1, Meng Cao2, Jinpu Zhang1, Yuanjie Shao3, Yuehuan Wang1*, Nong Sang1\n1National Key Laboratory of Multispectral Information Intelligent Processing Technology,\nSchool of Artificial Intelligence and Automation, Huazhong University of Science and Technology\n2Mohamed bin Zayed University of Artificial Intelligence\n3School of Electronic Information and Communication, Huazhong University of Science and Technology\n{D201980722,fengzhangaia,shaoyuanjie,yuehwang,nsang }@hust.edu.cn, {mengcaopku,zjphust }@gmail.com\nAbstract\nMost of the previous exposure correction methods learn\ndense pixel-wise transformations to achieve promising re-\nsults, but consume huge computational resources. Re-\ncently, Learnable 3D lookup tables (3D LUTs) have demon-\nstrated impressive performance and efficiency for image\nenhancement. However, these methods can only perform\nglobal transformations and fail to finely manipulate local\nregions. Moreover, they uniformly downsample the input\nimage, which loses the rich color information and limits the\nlearning of color transformation capabilities. In this pa-\nper, we present a collaborative transformation framework\n(CoTF) for real-time exposure correction, which integrates\nglobal transformation with pixel-wise transformations in\nan efficient manner. Specifically, the global transforma-\ntion adjusts the overall appearance using image-adaptive\n3D LUTs to provide decent global contrast and sharp de-\ntails, while the pixel transformation compensates for local\ncontext. Then, a relation-aware modulation module is de-\nsigned to combine these two components effectively. In ad-\ndition, we propose an adaptive sampling strategy to pre-\nserve more color information by predicting the sampling\nintervals, thus providing higher quality input data for the\nlearning of 3D LUTs. Extensive experiments demonstrate\nthat our method can process high-resolution images in real-\ntime on GPUs while achieving comparable performance\nagainst current state-of-the-art methods. The code is avail-\nable at https://github.com/HUST-IAL/CoTF .\n1. Introduction\nExposure correction [1] is a fundamental problem in\nthe field of computational photography and computer\n*Corresponding author\nResolutionFLOPs \n256251221024254.8G\n0.6G1.8G219.3G\nInput 3D LUT Ours\n(a) (b)3D LUTSID\nOurs3D LUTSID\nOurs\n13.7GRGB\n[ 1,  1,  1 ]\n[ 1,  1,  1 ]\n[ 1,  1,  1 ]RGB\n[ 13, 12, 9 ]\n[ 13, 12, 9 ]\n[ 13, 12, 9 ]RGB\n[ 24, 25, 24 ]\n[ 29, 26, 22 ]\n[ 31, 27, 22 ]Figure 1. Comparison of different transformation methods. (a)\nshows the computational effort of the different methods. We\ncan see that the computational effort of the pixel transforma-\ntion method SID increases significantly with resolution, while our\nmethod remains efficient at high resolution. (b) shows the pixel\nmapping relations for different transformations. 3D LUT performs\na fixed global transformation based on pixel values, resulting in\nsome unsatisfactory local contrast. While our method considers\nthe pixel context and yields favorable results.\nvision, and has been extensively studied over the last few\ndecades. Its purpose is to automatically correct over- or\nunderexposed images taken under undesirable lighting\nconditions. Exposure correction plays an important role\nin many applications such as autonomous driving [14] and\nvideo understanding [3–8].\nRecently, with the rapid development of deep learning,\nmany learning-based exposure correction methods [1, 15,\n16, 18, 36] have been proposed and achieved promising per-\nformance. However, most of them elaborate complex net-\nwork structures and learn dense pixel-wise transformations\nwith the computational burden proportional to the resolu-\ntion of the input image. This leads to the huge computations\nand endure the curse of dimensionality, when confronted\nwith high-resolution images. For example, in Figure 1(a),\nwhen processing 256×256images, SID [9] requires only\n13.7G FLOPs of floating point operations. When the image\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n2984\n(a) Input image\n    (1080× 1620)(b) Entropy map (c) Uniform sampling\n(256× 256)(d) Adaptive sampling\n(256× 256)\nFigure 2. Illustration of non-uniform distribution of color informa-\ntion. We use local entropy to reflect the richness of colors. From\nthe entropy map, we can see that the color information is unevenly\ndistributed. In addition, we show that our adaptive sampling re-\ntains more color information than uniform sampling.\nscale raise to 1024×1024 , the computation demand rises to\n219.3G FLOPs accordingly, which is unacceptable for real-\nworld and real-time applications on mobile devices. Thus,\nour goal is to design a network that achieves comparable en-\nhancement quality but with fast speeds and low operations.\nAn intuitive way to reduce the cost of pixel-wise\ntransformations is to apply a downsampling step before\nenhancement, but this may lose high-frequency detail and\nlead to blurring effects. Another efficient alternative is to\nresort to the global transformation, such as 3D Lookup\nTable (3D LUT). The 3D LUT simulates arbitrary nonlin-\near functions by predicting the control points of a curve,\nproviding strong color translation capabilities. It is also an\nefficient data structure that replaces complex calculations\nwith fast lookup operations. Recently, some excellent\nworks [33, 42, 49] have utilized neural networks to predict\nimage-adaptive 3D LUTs, which achieved pleasing image\nquality and efficient computation. However, as pointed out\nby [49], the 3D LUT performs global transformations and\nignores the local context of each pixel, and thus cannot\nfinely manipulate the pixel transformations in local regions,\nleading to globally sub-optimal enhancement results. As\nshown in Figure 1(b), the mapping ability of 3D LUT is\nlimited by the fixed transformation of pixel values, which\nproduces some unsatisfactory contrast in local areas.\nMoreover, existing learnable 3D LUT methods use bi-\nlinear downsampling to reduce the image resolution to save\ncomputation resources. This way samples pixels uniformly\naccording to a predetermined fixed position without consid-\nering the adaptation to the image content. Indeed, it is easy\nto observe that the distribution of color information in an\nimage is spatially inhomogeneous . As shown in Figure 2(b),\nwe use local entropy to visualize the color distribution in an\nimage. The sky region is flat with less color information,\nwhile the building region is colorful. Uniform sampling in\ncolorful regions may lose useful color information, while\ncolor redundancy occurs in flat regions. The color conver-\nsion capability of a learnable 3D LUT depends heavily on\nthe color information in the input image, so uniform down-sampling can limit the learning of the 3D LUT and lead to\nsubsequent performance degradation.\nTo alleviate the above problems, in this work, we pro-\npose a collaborative transformations framework (CoTF) for\nreal-time exposure correction that integrates global transfor-\nmation and pixel-wise transformation in an efficient way.\nThe idea is inspired by professional retouchers, who usu-\nally make global adjustments and then fine-tune local ar-\neas. In particular, the global transformation predicts image-\nadaptive 3D LUTs to adjust the appearance holistically, ob-\ntaining decent global contrast and sharp edge details. The\npixel-wise transformation uses an encoder-decoder to ex-\ntract the fine-grained local context, which can be performed\nat low resolution to ensure efficiency. In order to effec-\ntively combine these two components, we design a relation-\naware modulation (RAM) module to compensate local con-\ntrast information for global transformation results via cross-\nresolution interaction. In addition, we propose an adap-\ntive sampling strategy to retain more color information dur-\ning downsampling. It predicts an image-adaptive sampling\ngrid, which enables dense sampling in colorful regions and\nsparse sampling in flat regions, as shown in Figure 2(d). In\nthis way, we provide higher-quality input data for the learn-\ning of 3D LUTs to enhance color transformation capabil-\nity. Benefiting from the above design, our approach is able\nto achieve a good balance between performance and effi-\nciency, which can process high-resolution images in real\ntime on GPUs. Extensive experiments on several expo-\nsure correction datasets demonstrate that our method out-\nperforms the state-of-the-art methods both qualitatively and\nquantitatively.\nOverall, our main contributions are as follows:\n• We present a collaborative transformations framework\n(CoTF) that integrates the advantages of global transfor-\nmation and pixel-wise transformation in an efficient man-\nner. In addition, we design a relation-aware modulation\nmodule to modulate global transformation results with lo-\ncal context via cross-resolution interaction.\n• We propose an adaptive sampling strategy to retain more\ncolor information and provide more higher-quality input\ndata for the learning of 3D LUTs.\n• Extensive experiments demonstrate that the proposed\nmethod outperforms the existing state-of-the-art methods\non performance and efficiency.\n2. Related Work\n2.1. Exposure Correction\nExposure correction methods can be broadly categorized\ninto traditional and learning-based methods. Traditional\nmethods use histogram equalization [29, 56], curve map-\nping [47] and Retinex models [10, 13, 23, 32] to adjust\ncontrast and brightness. However, these methods rely on\n2985\nENC\nBasis LUTs Fusion\nLookup and Interpolation\nRAM\nw1       w2       w3\nBilinear \nDownsample\nAdaptive \nDownsample\nPixel -wise  Transformation\nGlobal  Transformation\nConv3× 3 Conv1× 1Depth -wise \nConv3× 3\nENC module RAM module\nENC\nRAM\nInput\nOutput\n𝐼𝑔𝑙𝑜𝑙𝑟 \n𝐼𝑝𝑖𝑥𝑙𝑟 \nFigure 3. Illustration of proposed collaborative transformations framework (CoTF). It consists of three main components, 1) the global\ntransformation based on the learnable 3D LUT, 2) the pixel-wise transformation in low resolution space and 3) the relation-aware modu-\nlation module to perform cross-resolution interactions. For the global transformation, we design an adaptive sampling strategy to provide\nhigher quality input data.\nhand-crafted priors and may not be robust enough to tackle\ncomplex scenarios.\nLearning-based methods are rapidly evolving due to the\npowerful learning capabilities of deep neural networks.\nSome methods combine neural networks with physical\nmodels, including Retienx models [26, 31, 38, 39, 46,\n52, 53, 55] and curve mapping [12, 22]. For example,\nRetinexNet [38] introduces subnets to decompose illumi-\nnation and reflection components and enhance them sepa-\nrately. ZeroDCE [12] proposes a higher-order pixel-wise\ncurve to enhance underexposed images. Another class of\nmethods [9, 19, 34, 35, 40, 41, 44, 45, 54] learns the pixel\nmapping relationship between degraded and clear images.\nFor example, DRBN [44] proposes to decompose images\ninto different bands and recombine them under perceptual\nguidance. However, these methods mainly focus on enhanc-\ning underexposed images, ignoring various exposure scenes\nin practical applications.\nRecently, some works have built a single model to cor-\nrect both overexposed and underexposed images. Afifi\net al. [1] presented a large-scale dataset and designed a\nmulti-scale Laplace pyramid network. To reduce the rep-\nresentation gap across exposures, CMEC [28], ENC [15],\nand ECLNet [17] map features to exposure-invariant space.\nHuang et al. [16] propose a Fourier-based network with\ncomplementary interactions in the spatial and frequency do-\nmains. Wang et al. [30] proposed local color distributions to\ndeal with non-uniform illumination. Wang et al. [36] pro-\nposed decoupling contrast enhancement and detail restora-\ntion in convolutional operations. Huang et al. [18] proposed\nto learn the sample relations and perform joint optimiza-\ntion in a mini-batch. CuDi [21] proposes curve distillationto extract knowledge from large curve-based teacher net-\nworks. CLIP-LIT [24] proposes a prompt learning frame-\nwork including prompt initialization, enhancement network\ntraining and prompt refinement. Unlike these methods that\nuse only pixel-wise transformations, in this work, we effec-\ntively unify the global transformation and pixel-wise trans-\nformations in a framework that is flexible and scalable for\nhigh-resolution images.\n2.2. Lookup Tables\n3D LUTs enable efficient color mapping and are widely\nused in camera imaging pipelines and photo editing soft-\nware. Recently, learnable LUT methods [25, 43, 49, 50]\nhave sprung up for image enhancement. Zeng et al. [49]\nwere the first to propose image-adaptive 3D LUTs, which\nconsists of several basic 3D LUTs and adaptive weights.\nWang et al. [33] proposed spatial-aware 3D LUT consid-\nering spatial information. AdaINT [42] learns adaptive in-\ntervals to achieve more flexible sample point allocation for\n3D LUT. However, all these methods use bilinear downsam-\npling to reduce resolution, which is a kind of uniform sam-\npling that may lose rich color information. In contrast, our\nproposed adaptive sampling strategy is able to retain more\ncolor information at a given size. In addition, unlike these\nmethods that only utilize the LUTs, we efficiently integrate\nLUT-based global transformation and pixel-wise transfor-\nmation to correct exposure collaboratively.\n3. Method\nThe pipeline for CoTF is shown in Figure 3. We first down-\nsample the high-resolution image Ihrto obtain Ilr\ngloandIlr\npix\n2986\nConv\nPoolingF\nConv\nConv\nConvConv\nSLN\nF\nF LNSR LN\nLNLayer Normalization\nFlattenSoftmax\nReshapeS\nF RFout\n𝐹𝑝𝑖𝑥𝑙𝑟 \n𝐹𝑔𝑙𝑜ℎ𝑟 Figure 4. Illustration of the Relation-Aware Modulation (RAM)\nmodule, which modulates global transformation results with local\ncontexts via cross-resolution interactions.\nto perform global and pixel-wise transformation in the low-\nresolution space to reduce the computational complexity\nand memory burden. The global transformation predicted at\nlow resolution can be flexibly scaled to high-resolution im-\nages, while the low-resolution pixel transformation focuses\non low-frequency local context. After the global and pixel-\nwise transformations, we design a cross-resolution RAM\nmodule that compensates the global transformation results\nwith fine-grained local context.\n3.1. Global Transformation\nAs a typical global transformations tool, 3D LUT can flex-\nibly express nonlinear mappings and adjust attributes, such\nas lighting, hue and saturation. A 3D LUT can be repre-\nsented as a 3D array of size N3that discretizes each dimen-\nsion of the RGB color space into Nbins, where index-value\npairs are used as input-output pairs. When transforming, 3D\nLUT use the color (r, g, b )of the input pixel as an index to\nlook up the nearest neighbor point, and then compute the\ntransformed color using trilinear interpolation.\nSince different exposures (e.g., under- and overexposure)\nrequire different 3D LUTs, we utilize multiple 3D LUTs\n{Tm}M\nm=1to handle various lighting conditions. We fol-\nlow the practice of [49] to adaptively fuse these 3D LUTs.\nIt contains two sub-mappings, one for predicting the ba-\nsis 3D LUTs, and the other for learning content-dependent\nweights {wm}M\nm=1. These basis 3D LUTs are linearly com-\nbined with adaptive weights to obtain the image-adaptive\n3D LUT, which flexibly covers the transformation space\nfrom different exposures to normal exposures. We use the\nlearned 3D LUT to transform high-resolution inputs Ihrto\nyield globally enhanced results ˆIhr\nglowith sharp details.\n3.2. Pixel-wise Transformation\nThe color conversion of 3D LUTs works only on pixel val-\nues, which may lead to undesirable results in local areas.\nIn contrast, pixel-wise transformation adjusts pixels with\nreference to the local context. Since 3D LUT preserves\nhigh-frequency details well, pixel-wise transformation fo-\ncuses only on low-frequency content and can be performed\nat low resolutions to reduce computational burden.\nWe employ an encoder-decoder consisting of pointwise\nAdaptive \nDownsampleSampling Intervals (x)\nSampling Points (x, y)Sampling Coordinates (x)Sampling Coordinates (y)Sampling Intervals (y)\n ContextBilinear \nDownsampleFigure 5. Illustration of adaptive sampling strategy. We first use a\nsmall CNN to extract the context of a low-resolution image. Then\nwe learn the sampling interval and convert it into sampling coor-\ndinates to adaptively downsample the original image.\nand depthwise convolutions to accomplish the pixel trans-\nformation. Besides, we introduce a simplified ENC mod-\nule [15] to reduce the discrepancy between different expo-\nsure features Fex, which is expressed as:\nˆFex= [IN(Fex), Fex], (1)\neFex=Sigmoid (FC(GAP (ˆFex)))·ˆFex, (2)\nwhere [·], IN, and GAP denote concatenation, instance nor-\nmalization, and global average pooling, respectively. After\npixel-wise transformation, we can obtain a low resolution\nresult ˆIlr\npixwith good local contrast.\n3.3. Relation-Aware Modulation\nAfter global and pixel-wise transformation, we obtain a\nhigh-resolution result ˆIhr\ngloand a low-resolution result ˆIlr\npix,\nwhich are complementary inherently. However, these two\ntransformations share inconsistent resolutions and charac-\nteristics. Therefore, directly upsampling ˆIlr\npixand then sim-\nply blending it with ˆIhr\nglocan lead to blurring effects and\nsub-optimal performance.\nTo address this issue, we design a lightweight Relation-\nAware Modulation (RAM) module that modulates global\ntransformation results with local contexts via cross-\nresolution interactions, as depicted in Figure 4. To avoid\nthe loss of local context information and repeated extrac-\ntion of features, we use the last feature map Flr\npixof the\nencoder-decoder instead of the image ˆIlr\npix. We use a con-\nvoluation with kernel size 3×3to extract the features Fhr\nglo\nof the ˆIhr\ngloand expand the channel dimensions to be con-\nsistent with Flr\npix. Subsequently, we pool Fhr\ngloand compute\nthe cross-attention with Flr\npixto obtain the relation map A,\nwhich reflects their information relationship. This operation\nis defined as:\nA=Softmax (Pooling (Fhr\nglo)×Flr\npix). (3)\n2987\nInspired by [48], we compute it along the channel dimen-\nsion to reduce complexity. Then we use Ato modulate the\nfeatures Fhr\ngloto dynamically aggregate local contexts, i.e.\nFout=FFN (Fhr\nglo×A+Fhr\nglo), (4)\nwhere we learn the residuals to stabilize the training and\nuse feed-forward networks (FFN) to obtain a better feature\nrepresentation.\n3.4. Adaptive Sampling\nThe previous learnable 3D LUT methods use uniform sam-\npling to reduce the image resolution, which limits the learn-\ning of 3D LUTs. To address this issue, in this work, we pro-\npose an adaptive sampling strategy to preserve more color\ninformation during downsampling.\nA naive way to perform adaptive sampling is to learn\nthe sampling coordinates directly, but this is hard to op-\ntimize because it is non-differentiable. We add two con-\nstraints: 1) sampling covers the entire spatial range (0,1),\nand 2) maintaining monotonically incrementality of the co-\nordinates. In this way, we can learn the sampling intervals\ninstead of learning the coordinates directly. Note that here\nwe choose the horizontal and vertical directions (denoted by\nXandY) as two separate sampling directions.\nAs shown in Figure 5, we employ a lightweight CNN to\nlearn sampling intervals at low resolution, which preserves\nthe original color distribution. Assuming a given sample\nsize of Kx×Ky,i.e., there are KxandKysampling points\nalong the XandYdirections, respectively, which means\nthat we need to learn K{x,y}−1sampling intervals P{x,y}\nin each direction. Next, we use Softmax to normalize\nthe interval, ensuring that the samples cover the entire im-\nage without exceeding the range. Subsequently, we convert\ntheK{x,y}−1normalized sampling intervals ˆP{x,y}into\nKsampling points Q{x,y}via an accumulation operation.\nSince the value of each interval is positive, the accumulation\noperation ensures monotonic increment of the coordinates.\nFinally, the sampling grid Gis obtained by computing\nthe Cartesian product of the X- and Y-direction coordinates,\nwhich is denoted as G=Qx⊗Qy={(Qx,i, Qy,j)|i∈\n{1,2, ..., K x}, j∈ {1,2, ..., K y}}. We downsample the\noriginal image by applying the sampling grid, which adapts\nto the image content. Compared with uniform sampling,\nadaptive sampling is a superior strategy to densely sample\ncolorful regions and sparsely sample flat regions. In this\nway, more color information can be retained during down-\nsampling, which provides higher quality data and thus im-\nproves the color translation capability of the 3D LUTs.\nIt is worth noting that we only applied the adaptive sam-\npling strategy to the global transformation. This is because\nthe 3D LUT is a spatially independent model, i.e., the color\ntransform is only related to the color values and not to theposition. In contrast, the pixel-wise transformation is a spa-\ntially correlated model that requires positional consistency,\nso we still use bilinear downsampling for it.\n4. Experiments\n4.1. Experimental settings\nDatasets. We evaluate proposed method on three datasets,\nincluding two exposure correction datasets, ( i.e., MSEC [1]\nand SICE [2]), and a non-uniform illumination dataset ( i.e.,\nLCDP [30]). The MSEC [1] dataset renders images using\nrelative EVs of -1.5 to +1.5 and contains a total of 17675\ntraining images, 750 validation images, and 5905 test im-\nages. Following the settings of [15] for SICE, we treat\nthe second and second-last exposure levels as underexposed\nand overexposed images, and the middle exposure levels as\nground truth. It contains 1000 training images, 24 valida-\ntion images and 60 test images. The LCDP dataset exhibits\nnon-uniform illumination due to both overexposure and un-\nderexposure occurring in single images. It contains 1415\ntraining images, 100 validation images, and 218 test im-\nages.\nImplementation Details. We use the small CNN in [49]\nas a backbone for global transformation and adaptive sam-\npling, which contains only 5 convolutional layers. We set\n3 basis 3D LUTs, with the dimension of each LUT set\nto3×173. We initialize the first 3D LUT as a identity\nmapping and the others as zero mappings. The sampling\ngrid is initialized to a uniform state. Consistent with [15],\nwe use L1 loss, L1, perceptual loss, Lperand SSIM loss,\nLssim to train the network, which is expressed as Ltotal=\nL1+β1Lper+β2Lssim, where the coefficients β1andβ1\nare empirically set to 0.1 and 0.5, respectively.\nDuring training, we use the ADAM [20] optimizer to\nminimize Ltotal in an end-to-end manner. The mini batch\nsize is set to 2. We set the initial learning rate to 4e−4and\nupdate it using the cosine annealing strategy. For adaptive\nsampling, the learning rate is decayed by 0.1 to stabilize\nthe training. We downsample the image to 256×256to\nfeed the network. For MSEC, SICE, and LCDP datasets,\nthe training process consists of 50, 200 and 200 epochs, re-\nspectively. Our models are implemented using Pytorch and\nrun on NVIDIA TITAN V GPUs.\n4.2. Comparison with State-of-the-Art Methods\nWe use PSNR, SSIM [37] and LPIPS [51] metrics for per-\nformance evaluation, as well as parameters, FLOPs and in-\nference times for efficiency evaluation.\nQuantitative Comparisons. Table 1 reports the quantita-\ntive results on the MSCE and SCIE datasets. We can see that\nour method has the best overall performance. On the MSEC\ndataset, our method has the best performance with 23.44dB\nPSNR, 0.8728 SSIM and 0.1232 LPIPS. On the SICE\n2988\nMethodsMSEC SICE\nUnder Over Average Under Over Average\nPSNR↑SSIM↑PSNR↑SSIM↑PSNR↑SSIM↑LPIPS↓PSNR↑SSIM↑PSNR↑SSIM↑PSNR↑SSIM↑LPIPS↓\nHE [29] 16.52 0.6918 16.53 0.6991 16.53 0.6959 0.2920 14.69 0.5651 12.87 0.4991 13.78 0.5376 0.3738\nCLAHE [56] 16.77 0.6211 14.45 0.5842 15.38 0.5990 0.4744 12.69 0.5037 10.21 0.4847 11.45 0.4942 0.4688\nLIME [13] 13.98 0.6630 9.88 0.5700 11.52 0.6070 0.2758 16.48 0.5832 6.67 0.4041 11.58 0.4937 0.3712\nWVM [10] 18.67 0.7280 12.75 0.645 15.12 0.6780 0.2284 15.16 0.5915 8.03 0.4485 11.60 0.5200 0.3432\nRetinexNet [38] 12.13 0.6209 10.47 0.5953 11.14 0.6048 0.3209 12.94 0.5171 12.87 0.5252 12.90 0.5212 0.4312\nURetinexNet [39] 13.85 0.7371 9.81 0.6733 11.42 0.6988 0.2858 17.39 0.6448 7.40 0.4543 12.40 0.5496 0.3549\nDRBN [44] 19.74 0.8290 19.37 0.8321 19.52 0.8309 0.2795 17.96 0.6767 17.33 0.6828 17.65 0.6798 0.3891\nSID [9] 19.37 0.8103 18.83 0.8055 19.04 0.8074 0.1862 19.51 0.6635 16.79 0.6444 18.15 0.6540 0.2417\nMSEC [1] 20.52 0.8129 19.79 0.8156 20.08 0.8145 0.1721 19.62 0.6512 17.59 0.6560 18.58 0.6536 0.2814\nZeroDCE [12] 14.55 0.5887 10.40 0.5142 12.06 0.5441 0.2923 16.92 0.6330 7.11 0.4292 12.02 0.5311 0.3532\nZero-DCE++ [22] 13.82 0.5887 9.74 0.5142 11.37 0.5583 0.3121 11.93 0.4755 6.88 0.4088 9.41 0.4422 0.3623\nRUAS [26] 13.43 0.6807 6.39 0.4655 9.20 0.5515 0.4819 16.63 0.5589 4.54 0.3196 10.59 0.4393 0.5122\nSCI [27] 9.97 0.6681 5.83 0.5190 7.49 0.5786 0.3116 17.86 0.6401 4.45 0.3629 12.49 0.5051 0.4239\nPairLIE [11] 11.78 0.6596 8.37 0.5887 9.73 0.6171 0.3605 16.67 0.5995 6.26 0.3846 11.47 0.4921 0.4138\nENC-SID [15] 22.59 0.8423 22.36 0.8519 22.45 0.8481 0.1827 21.30 0.6645 19.63 0.6941 20.47 0.6793 0.2797\nENC-DRBN [15] 22.72 0.8544 22.11 0.8521 22.35 0.8530 0.1724 21.89 0.7071 19.09 0.7229 20.49 0.7150 0.2318\nCLIP-LIT [24] 17.79 0.7611 12.02 0.6894 14.32 0.7181 0.2506 15.13 0.5847 7.52 0.4383 11.33 0.5115 0.3560\nFECNet [16] 22.96 0.8598 23.22 0.8748 23.12 0.8688 0.1419 22.01 0.6737 19.91 0.6961 20.96 0.6849 0.2656\nLCDPNet [30] 22.35 0.8650 22.17 0.8476 22.30 0.8552 0.1451 17.45 0.5622 17.04 0.6463 17.25 0.6043 0.2592\nFECNet+ERL [18] 23.10 0.8639 23.18 0.8759 23.15 0.8711 / 22.35 0.6671 20.10 0.6891 21.22 0.6781 /\nCoTF(Ours) 23.36 0.8630 23.49 0.8793 23.44 0.8728 0.1232 22.90 0.7029 20.13 0.7274 21.51 0.7151 0.1924\nTable 1. Quantitative comparisons on the MSEC and the SICE datasets. Some are absent (”/”) due to the unavailable source code. The best\nresults are highlighted in bold.\nInput\n MSECDRBN ZeroDCE URetinexNet SCI\nLCDPNet FECNet ENC -DRBN Ours GTCLIP -LIT\nFigure 6. Visual comparison with state-of-the-art methods on the MSEC dataset.\ndataset, our method has the highest PSNR and the second\nhighest SSIM score. Table 2 shows the quantitative results\non the LCDP dataset. As can be seen, our method improves\n0.65dB PSNR and 0.0161 SSIM compared to the second\nbest LCDPNet method. Overall, our method can achieve\ncomparable performance with state of the art methods.\nEfficiency Evaluation. We report the efficiency compar-\nisons of the different methods in Table 2. Our method\nsignificantly reduces the computational cost and meets the\nrequirements of real-time processing. For example, com-\npared to the pixel-wise transformation method FECNet,\nour method requires only 2% FLOPs and 8% runtime. Our\nmethod has 93% fewer FLOPs and is 80% faster compared\nto LCDPNet, which is partially run at low resolution.\nThis is because our method unifies pixel-wise and global\ntransformations in an efficient way that is insensitive to thenumber of pixels. These results demonstrate the efficiency\nand practicality of our method.\nQualitative Comparisons. We provide qualitative com-\nparisons in Figure 6, Figure 7 and Figure 8. As can be\nseen, other methods always suffer from over- or under-\nenhancement, color deviation and blurring effects. And our\nmethod succeeds in restoring proper global brightness and\nlocal contrast, consistent colors, and sharp details. These re-\nsults prove that our method produces more pleasing visual\neffects. More visual results can be found in the supplemen-\ntary material.\n4.3. Ablation Studies\nWe perform ablation studies on the LCDP dataset to verify\nthe effectiveness of each component of the our method.\nEffectiveness of each component. We set up different vari-\n2989\nInput\n MSECDRBN ZeroDCE URetinexNet SCI\nLCDPNet FECNet ENC -DRBN Ours GTCLIP -LIT\nFigure 7. Visual comparison with state-of-the-art methods on the SICE dataset.\nInput\n MSECDRBN ZeroDCE URetinexNet SCI\nLCDPNet FECNet ENC -DRBN Ours GTCLIP -LIT\nFigure 8. Visual comparison with state-of-the-art methods on the LCDP dataset.\nMethods PSNR↑SSIM↑LPIPS↓Param(M) ↓FLOPs(G) ↓Time(s) ↓\nHE [29] 15.98 0.6840 0.3871 - - -\nCLAHE [56] 16.33 0.6420 0.5054 - - -\nLIME [13] 17.34 0.6860 0.2759 - - -\nWVM [10] 18.16 0.7390 0.2123 - - -\nRetinexNet [38] 16.20 0.6304 0.2940 0.84 566.08 0.1529\nURetinexNet [39] 17.67 0.7369 0.2504 1.32 913.36 0.1877\nDRBN [44] 15.47 0.6979 0.3149 0.58 170.55 0.1226\nSID [9] 21.89 0.8082 0.1781 7.40 219.29 0.0387\nMSEC [1] 17.07 0.6428 0.3151 7.04 154.28 0.0468\nZeroDCE [12] 18.96 0.7743 0.2055 0.079 83.27 0.0229\nZero-DCE++ [22] 18.42 0.7669 0.2204 0.01 0.21 0.0024\nRUAS [26] 13.93 0.6340 0.3458 0.003 3.88 0.0281\nSCI [27] 15.96 0.6646 0.2913 0.0003 0.55 0.0021\nPairLIE [11] 16.51 0.6667 0.2945 0.34 358.37 0.0716\nENC-SID [15] 22.66 0.8195 0.1631 7.45 278.76 0.0647\nENC-DRBN [15] 23.08 0.8302 0.1536 0.58 227.73 0.1869\nCLIP-LIT [24] 19.24 0.7477 0.2262 0.28 292.56 0.0877\nFECNet [16] 22.34 0.8038 0.2334 0.15 94.61 0.1261\nLCDPNet [30] 23.24 0.8420 0.1368 0.96 27.12 0.0472\nFECNet+ERL [18] / / / 0.15 94.61 0.1261\nCoTF(Ours) 23.89 0.8581 0.1035 0.31 1.81 0.0095\nTable 2. Quantitative comparison on LCDP datasets. Some are\nabsent (”/”) due to the unavailable source code. We also report\nefficiency comparisons where FLOPs and runtimes are measured\nwith 1024×1024 images. Runtimes are averaged over 10 images\non the NVIDIA TITAN V GPU. The best results are highlighted\nin bold.\nants to validate the effectiveness of the proposed frame-\nwork. The results are listed in Table 3. Setting 1 has poor\nperformance using only low-resolution pixel-wise transfor-\nmations. Setting 2 is performed at high resolution, but the\nshallow network is still not impressive enough. SettingsSetting Pixel Trans Global Trans Feature Mod PSNR SSIM\n1 ✓ 20.08 0.5983\n2 ✓(HR) 22.34 0.8073\n3 w/o AdaSamp 23.07 0.8298\n4 ✓ 23.35 0.8343\n5 ✓ ✓ CAT 23.43 0.8372\n6 ✓ ✓ CA 23.62 0.8407\n7 ✓ ✓ RAM* 23.79 0.8550\n8 ✓ ✓ ✓ 23.89 0.8581\nTable 3. Ablation study on the key components of the CoTF. HR\ndenotes high resolution. AdaSamp denotes adaptive sampling.\nCAT, CA, and RAM* denote the use of concatenation, channel\nattention, or RAM module after upsampling.\n(a) 3D LUT (b) + AdaSamp (c) + Pixel Trans (d) + RAM (Full Model)\nFigure 9. Visual results of ablation study on the key components\nof the CoTF.\n3 and 4 show the effectiveness of 3D LUTs and proposed\nadaptive sampling strategy. We then verify the effective-\nness of the RAM module. Settings 5, 6, and 7 indicate fea-\nture modulation using concatenation, channel attention, or\nchannel self-attention, respectively, after upsampling fea-\n2990\n(a) (b)Figure 10. Ablation study of adaptive sampling with (a) 3D LUTs\nof different dimensions (N), and (b) different sampling resolutions.\n(a) UniSamp (b) AdaSamp (Ours) (c) UniSamp (d) AdaSamp (Ours)\nFigure 11. Visualization of our adaptive sampling.\ntures to the same resolution. As can be seen, our RAM mod-\nule provides better results, probably because direct cross-\nresolution interaction avoids ambiguity compared to naive\nupsampling, and self-attention can model correlations bet-\nter. As can be seen in Figure 9, with the help of adaptive\nsampling and collaborative transformations, our full model\nyields more visually pleasing results with better local con-\ntrast. These results consistently demonstrate the effective-\nness of our method.\nAnalysis of adaptive sampling. We perform ablation\nstudies to analysis the effect of adaptive sampling. For a\nfair comparison, we take the original 3D LUT with uniform\nsampling as a baseline. First , we evaluate the performance\nof adaptive sampling under different LUT sizes N. As\nshown in Figure 10(a), performance goes up as N increases,\nand our method consistently improves baseline under all\nN settings. Second , we analyze the effect at different\nsampling resolutions. From Figure 10(b), the performance\nimproves as the resolution increases, which shows the\nimportance of color information for LUT learning. While\nadaptive sampling boosts the performance at a given size,\nwhich proves that adaptive sampling retains more color in-\nformation. Finally , as illustrated in Figure 11, our method\ncan densely sample colorful regions and sparsely sample\nflat regions. Note that our adaptive sampling requires only\na slight increase in computation. For example, at a sample\nsize of 256×256, our method adds only 0.02G FLOPs,\nwhich is almost negligible.\n4.4. Extension and Discussion\nUltra-High-Definition (UHD) Images. We further extend\nour CoTF to UHD images, which is a more challenging.Method PSNR SSIM Time(s)\n3D LUT 18.11 0.6194 0.0023\nCoTF(Ours) 19.09 0.6390 0.0431\nTable 4. Quantitative results and runtime of our method on UHD\nimages ( 3840×2160 resolution).\nSetting Train Test PSNR SSIM\n1 × × 23.07 0.8298\n2 ✓ × 23.33 0.8324\n3 ✓ ✓ 23.35 0.8343\nTable 5. Investigation of adaptive sampling as a data augmentation\nstrategy ( i.e. Setting 2).\nWe use the original resolution version of SICE [2], which\ncontains 4K-5K resolution images. Most methods fail to\nprocess UHD images due to out-of-memory. In contrast,\nour method can still process UHD images efficiently on an\nNVIDIA TITAN V GPU, as shown in Table 4. Despite\nslower than 3D LUTs, our method has substantially im-\nproved performance and still meets real-time requirements.\nMore results are in the supplementary material.\nAdaptive sampling as data augmentation. We further\ninvestigate adaptive sampling as a data augmentation strat-\negy. As shown in Table 5, Setting 1 does not use adaptive\nsampling. Setting 2 utilizes adaptive sampling as a data\naugmentation strategy and deactivates it during testing. In\nsetting 3, adaptive sampling is a network module. It can be\nseen that using adaptive sampling to augment the samples\nalso improves the performance, suggesting that the higher\nquality data provided by adaptive sampling can facilitate\nthe 3D LUT learning.\n5. Conclusion\nIn this paper, we present a collaborative transformations\nframework (CoTF) for real-time exposure correction that\nefficiently integrates global and pixel-wise transformations.\nTo efficiently combine these two kinds of transformations,\nwe design a relation-aware modulation module (RAM)\nto complement the global transformation results with lo-\ncal context information. In addition, to further improve\nthe learning of 3D LUTs, we propose an adaptive sam-\npling strategy to preserve more color information and thus\nprovide higher quality input data. Extensive experiments\ndemonstrate the superiority of our method over the previ-\nous methods in terms of performance and efficiency.\nAcknowledgments. This work was partially sup-\nported by the National Key R&D Program of China\n2022YFC3301000 and Knowledge Innovation Program of\nWuhan-Shuguang Project under Grant 2023010201020226.\n2991\nReferences\n[1] Mahmoud Afifi, Konstantinos G Derpanis, Bjorn Ommer,\nand Michael S Brown. Learning multi-scale photo expo-\nsure correction. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition , pages 9157–\n9167, 2021. 1, 3, 5, 6, 7\n[2] Jianrui Cai, Shuhang Gu, and Lei Zhang. Learning a deep\nsingle image contrast enhancer from multi-exposure images.\nIEEE Transactions on Image Processing , 27(4):2049–2062,\n2018. 5, 8\n[3] Meng Cao, Long Chen, Mike Zheng Shou, Can Zhang, and\nYuexian Zou. On pursuit of designing multi-modal trans-\nformer for video grounding. In Proceedings of the 2021\nConference on Empirical Methods in Natural Language Pro-\ncessing , pages 9810–9823, 2021. 1\n[4] Meng Cao, Haozhi Huang, Hao Wang, Xuan Wang, Li Shen,\nSheng Wang, Linchao Bao, Zhifeng Li, and Jiebo Luo. Uni-\nfacegan: a unified framework for temporally consistent facial\nvideo editing. IEEE Transactions on Image Processing , 30:\n6107–6116, 2021.\n[5] Meng Cao, Ji Jiang, Long Chen, and Yuexian Zou. Cor-\nrespondence matters for video referring expression compre-\nhension. In Proceedings of the 30th ACM International Con-\nference on Multimedia , pages 4967–4976, 2022.\n[6] Meng Cao, Tianyu Yang, Junwu Weng, Can Zhang, Jue\nWang, and Yuexian Zou. Locvtp: Video-text pre-training for\ntemporal localization. In European Conference on Computer\nVision , pages 38–56. Springer, 2022.\n[7] Meng Cao, Can Zhang, Long Chen, Mike Zheng Shou, and\nYuexian Zou. Deep motion prior for weakly-supervised tem-\nporal action localization. IEEE Transactions on Image Pro-\ncessing , 31:5203–5213, 2022.\n[8] Meng Cao, Fangyun Wei, Can Xu, Xiubo Geng, Long\nChen, Can Zhang, Yuexian Zou, Tao Shen, and Daxin Jiang.\nIterative proposal refinement for weakly-supervised video\ngrounding. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , pages 6524–\n6534, 2023. 1\n[9] Chen Chen, Qifeng Chen, Jia Xu, and Vladlen Koltun.\nLearning to see in the dark. In Proceedings of the IEEE Con-\nference on Computer Vision and Pattern Recognition , pages\n3291–3300, 2018. 1, 3, 6, 7\n[10] Xueyang Fu, Delu Zeng, Yue Huang, Xiao-Ping Zhang, and\nXinghao Ding. A weighted variational model for simulta-\nneous reflectance and illumination estimation. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition , pages 2782–2790, 2016. 2, 6, 7\n[11] Zhenqi Fu, Yan Yang, Xiaotong Tu, Yue Huang, Xinghao\nDing, and Kai-Kuang Ma. Learning a simple low-light im-\nage enhancer from paired low-light instances. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition , pages 22252–22261, 2023. 6, 7\n[12] Chunle Guo, Chongyi Li, Jichang Guo, Chen Change Loy,\nJunhui Hou, Sam Kwong, and Runmin Cong. Zero-reference\ndeep curve estimation for low-light image enhancement. In\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition , pages 1777–1786, 2020. 3, 6, 7[13] Xiaojie Guo, Yu Li, and Haibin Ling. LIME: Low-light\nimage enhancement via illumination map estimation. IEEE\nTransactions on Image Processing , 26(2):982–993, 2017. 2,\n6, 7\n[14] Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima,\nXizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai\nWang, Lewei Lu, Xiaosong Jia, Qiang Liu, Jifeng Dai, Yu\nQiao, and Hongyang Li. Planning-oriented autonomous driv-\ning. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition , 2023. 1\n[15] Jie Huang, Yajing Liu, Xueyang Fu, Man Zhou, Yang Wang,\nFeng Zhao, and Zhiwei Xiong. Exposure normalization and\ncompensation for multiple-exposure correction. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition , pages 6043–6052, 2022. 1, 3, 4, 5, 6, 7\n[16] Jie Huang, Yajing Liu, Feng Zhao, Keyu Yan, Jinghao\nZhang, Yukun Huang, Man Zhou, and Zhiwei Xiong. Deep\nfourier-based exposure correction network with spatial-\nfrequency interaction. In Proceedings of the European Con-\nference on Computer Vision , pages 163–180. Springer, 2022.\n1, 3, 6, 7\n[17] Jie Huang, Man Zhou, Yajing Liu, Mingde Yao, Feng Zhao,\nand Zhiwei Xiong. Exposure-consistency representation\nlearning for exposure correction. In Proceedings of the 30th\nACM International Conference on Multimedia , pages 6309–\n6317, 2022. 3\n[18] Jie Huang, Feng Zhao, Man Zhou, Jie Xiao, Naishan Zheng,\nKaiwen Zheng, and Zhiwei Xiong. Learning sample rela-\ntionship for exposure correction. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition ,\npages 9904–9913, 2023. 1, 3, 6, 7\n[19] Yifan Jiang, Xinyu Gong, Ding Liu, Yu Cheng, Chen Fang,\nXiaohui Shen, Jianchao Yang, Pan Zhou, and Zhangyang\nWang. EnlightenGAN: Deep light enhancement without\npaired supervision. IEEE Transactions on Image Process-\ning, 30:2340–2349, 2021. 3\n[20] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. arXiv preprint arXiv:1412.6980 ,\n2014. 5\n[21] Chongyi Li, Chunle Guo, Ruicheng Feng, Shangchen Zhou,\nand Chen Change Loy. CuDi: Curve distillation for effi-\ncient and controllable exposure adjustment. arXiv preprint\narXiv:2207.14273 , 2022. 3\n[22] Chongyi Li, Chunle Guo, and Chen Change Loy. Learning to\nenhance low-light image via zero-reference deep curve esti-\nmation. IEEE Transactions on Pattern Analysis and Machine\nIntelligence , 44(8):4225–4238, 2022. 3, 6, 7\n[23] Mading Li, Jiaying Liu, Wenhan Yang, Xiaoyan Sun, and\nZongming Guo. Structure-revealing low-light image en-\nhancement via robust retinex model. IEEE Transactions on\nImage Processing , 27(6):2828–2841, 2018. 2\n[24] Zhexin Liang, Chongyi Li, Shangchen Zhou, Ruicheng\nFeng, and Chen Change Loy. Iterative prompt learning for\nunsupervised backlit image enhancement. In Proceedings\nof the IEEE International Conference on Computer Vision ,\npages 8094–8103, 2023. 3, 6, 7\n[25] Chengxu Liu, Huan Yang, Jianlong Fu, and Xueming Qian.\n4D LUT: learnable context-aware 4D lookup table for image\n2992\nenhancement. IEEE Transactions on Image Processing , 32:\n4742–4756, 2023. 3\n[26] Risheng Liu, Long Ma, Jiaao Zhang, Xin Fan, and Zhongx-\nuan Luo. Retinex-inspired unrolling with cooperative prior\narchitecture search for low-light image enhancement. In Pro-\nceedings of the IEEE Conference on Computer Vision and\nPattern Recognition , pages 10556–10565, 2021. 3, 6, 7\n[27] Long Ma, Tengyu Ma, Risheng Liu, Xin Fan, and Zhongx-\nuan Luo. Toward fast, flexible, and robust low-light im-\nage enhancement. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition , pages 5637–\n5646, 2022. 6, 7\n[28] Ntumba Elie Nsampi, Zhongyun Hu, and Qing Wang. Learn-\ning exposure correction via consistency modeling. In Pro-\nceedings of the British Machine Vision Conference , 2021. 3\n[29] Stephen M Pizer, E Philip Amburn, John D Austin,\nRobert Cromartie, Ari Geselowitz, Trey Greer, Bart ter\nHaar Romeny, John B Zimmerman, and Karel Zuiderveld.\nAdaptive histogram equalization and its variations. Com-\nputer vision, graphics, and image processing , 1987. 2, 6,\n7\n[30] Haoyuan Wang, Ke Xu, and Rynson WH Lau. Local color\ndistributions prior for image enhancement. In Proceedings\nof the European Conference on Computer Vision , pages 343–\n359. Springer, 2022. 3, 5, 6, 7\n[31] Ruixing Wang, Qing Zhang, Chi-Wing Fu, Xiaoyong Shen,\nWei-Shi Zheng, and Jiaya Jia. Underexposed photo enhance-\nment using deep illumination estimation. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition , pages 6842–6850, 2019. 3\n[32] Shuhang Wang, Jin Zheng, Hai-Miao Hu, and Bo Li. Nat-\nuralness preserved enhancement algorithm for non-uniform\nillumination images. IEEE Transactions on Image Process-\ning, 22(9):3538–3548, 2013. 2\n[33] Tao Wang, Yong Li, Jingyang Peng, Yipeng Ma, Xian Wang,\nFenglong Song, and Youliang Yan. Real-time image en-\nhancer via learnable spatial-aware 3D lookup tables. In Pro-\nceedings of the IEEE International Conference on Computer\nVision , pages 2471–2480, 2021. 2, 3\n[34] Tao Wang, Kaihao Zhang, Tianrun Shen, Wenhan Luo, Bjorn\nStenger, and Tong Lu. Ultra-high-definition low-light image\nenhancement: A benchmark and transformer-based method.\nInProceedings of the AAAI Conference on Artificial Intelli-\ngence , pages 2654–2662, 2023. 3\n[35] Yufei Wang, Renjie Wan, Wenhan Yang, Haoliang Li, Lap-\nPui Chau, and Alex Kot. Low-light image enhancement with\nnormalizing flow. In Proceedings of the AAAI Conference on\nArtificial Intelligence , pages 2604–2612, 2022. 3\n[36] Yang Wang, Long Peng, Liang Li, Yang Cao, and Zheng-\nJun Zha. Decoupling-and-aggregating for image exposure\ncorrection. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition , pages 18115–18124,\n2023. 1, 3\n[37] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Si-\nmoncelli. Image quality assessment: from error visibility to\nstructural similarity. IEEE Transactions on Image Process-\ning, 13(4):600–612, 2004. 5[38] Chen Wei, Wenjing Wang, Wenhan Yang, and Jiaying\nLiu. Deep retinex decomposition for low-light enhancement.\narXiv preprint arXiv:1808.04560 , 2018. 3, 6, 7\n[39] Wenhui Wu, Jian Weng, Pingping Zhang, Xu Wang, Wen-\nhan Yang, and Jianmin Jiang. Uretinex-net: Retinex-based\ndeep unfolding network for low-light image enhancement.\nInProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition , pages 5901–5910, 2022. 3, 6, 7\n[40] Ke Xu, Xin Yang, Baocai Yin, and Rynson W.H. Lau.\nLearning to restore low-light images via decomposition-\nand-enhancement. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition , pages 2278–\n2287, 2020. 3\n[41] Xiaogang Xu, Ruixing Wang, Chi-Wing Fu, and Jiaya Jia.\nSNR-aware low-light image enhancement. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition , pages 17693–17703, 2022. 3\n[42] Canqian Yang, Meiguang Jin, Xu Jia, Yi Xu, and Ying Chen.\nAdaInt: Learning adaptive intervals for 3D lookup tables on\nreal-time image enhancement. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition ,\npages 17522–17531, 2022. 2, 3\n[43] Canqian Yang, Meiguang Jin, Yi Xu, Rui Zhang, Ying Chen,\nand Huaida Liu. SepLUT: Separable image-adaptive lookup\ntables for real-time image enhancement. In Proceedings of\nthe European Conference on Computer Vision , pages 201–\n217. Springer, 2022. 3\n[44] Wenhan Yang, Shiqi Wang, Yuming Fang, Yue Wang, and\nJiaying Liu. From fidelity to perceptual quality: A semi-\nsupervised approach for low-light image enhancement. In\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition , pages 3060–3069, 2020. 3, 6, 7\n[45] Wenhan Yang, Shiqi Wang, Yuming Fang, Yue Wang, and Ji-\naying Liu. Band representation-based semi-supervised low-\nlight image enhancement: Bridging the gap between signal\nfidelity and perceptual quality. IEEE Transactions on Image\nProcessing , 30:3461–3473, 2021. 3\n[46] Wenhan Yang, Wenjing Wang, Haofeng Huang, Shiqi Wang,\nand Jiaying Liu. Sparse gradient regularized deep retinex\nnetwork for robust low-light image enhancement. IEEE\nTransactions on Image Processing , 30:2072–2086, 2021. 3\n[47] Lu Yuan and Jian Sun. Automatic exposure correction of\nconsumer photographs. In Proceedings of the European\nConference on Computer Vision , pages 771–785. Springer,\n2012. 2\n[48] Syed Waqas Zamir, Aditya Arora, Salman Khan, Mu-\nnawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang.\nRestormer: Efficient transformer for high-resolution image\nrestoration. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition , pages 5728–5739,\n2022. 5\n[49] Hui Zeng, Jianrui Cai, Lida Li, Zisheng Cao, and Lei Zhang.\nLearning image-adaptive 3D lookup tables for high perfor-\nmance photo enhancement in real-time. IEEE Transactions\non Pattern Analysis and Machine Intelligence , 44(4):2058–\n2073, 2022. 2, 3, 4, 5\n[50] Fengyi Zhang, Hui Zeng, Tianjun Zhang, and Lin Zhang.\nCLUT-Net: Learning adaptively compressed representations\n2993\nof 3DLUTs for lightweight image enhancement. In Proceed-\nings of the 30th ACM International Conference on Multime-\ndia, pages 6493–6501, 2022. 3\n[51] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-\nman, and Oliver Wang. The unreasonable effectiveness of\ndeep features as a perceptual metric. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 586–595, 2018. 5\n[52] Yonghua Zhang, Jiawan Zhang, and Xiaojie Guo. Kindling\nthe darkness: A practical low-light image enhancer. In Pro-\nceedings of the 27th ACM International Conference on Mul-\ntimedia , pages 1632–1640, 2019. 3\n[53] Yonghua Zhang, Xiaojie Guo, Jiayi Ma, Wei Liu, and Jiawan\nZhang. Beyond brightening low-light images. International\nJournal of Computer Vision , 129:1013–1037, 2021. 3\n[54] Zhao Zhang, Huan Zheng, Richang Hong, Mingliang Xu,\nShuicheng Yan, and Meng Wang. Deep color consistent\nnetwork for low-light image enhancement. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition , pages 1889–1898, 2022. 3\n[55] Anqi Zhu, Lin Zhang, Ying Shen, Yong Ma, Shengjie Zhao,\nand Yicong Zhou. Zero-shot restoration of underexposed im-\nages via robust retinex decomposition. In Proceedings of\nthe IEEE International Conference on Multimedia and Expo ,\npages 1–6, 2020. 3\n[56] Karel Zuiderveld. Contrast limited adaptive histogram equal-\nization. Graphics gems , 1994. 2, 6, 7\n2994'}, 'dist': 0.9286905527114868}
Result 2: {'text': 'data', 'metadata': {'created_at': 1736932570, 'modified_at': 1736932570, 'owner': 'sayandeep', 'path': 'RAG_CONTENT/conferences/cvpr_ocr/Chen_Beyond_Average_Individualized_Visual_Scanpath_Prediction_CVPR_2024_paper.txt', 'size': 56795, 'seen_at': 1737191136, 'data': 'Beyond Average: Individualized Visual Scanpath Prediction\r\nXianyu Chen Ming Jiang Qi Zhao\r\nUniversity of Minnesota, United States\r\n{chen6582, mjiang }@umn.edu, qzhao@cs.umn.edu\r\nAbstract\r\nUnderstanding how attention varies across individuals has\r\nsignificant scientific and societal impacts. However, exist-\r\ning visual scanpath models treat attention uniformly, ne-\r\nglecting individual differences. To bridge this gap, this pa-\r\nper focuses on individualized scanpath prediction (ISP), a\r\nnew attention modeling task that aims to accurately predict\r\nhow different individuals shift their attention in diverse vi-\r\nsual tasks. It proposes an ISP method featuring three novel\r\ntechnical components: (1) an observer encoder to charac-\r\nterize and integrate an observer’s unique attention traits,\r\n(2) an observer-centric feature integration approach that\r\nholistically combines visual features, task guidance, and\r\nobserver-specific characteristics, and (3) an adaptive fix-\r\nation prioritization mechanism that refines scanpath pre-\r\ndictions by dynamically prioritizing semantic feature maps\r\nbased on individual observers’ attention traits. These novel\r\ncomponents allow scanpath models to effectively address\r\nthe attention variations across different observers. Our\r\nmethod is generally applicable to different datasets, model\r\narchitectures, and visual tasks, offering a comprehensive\r\ntool for transforming general scanpath models into indi-\r\nvidualized ones. Comprehensive evaluations using value-\r\nbased and ranking-based metrics verify the method’s effec-\r\ntiveness and generalizability.\r\n1. Introduction\r\nSaccadic eye movements, such as fixations and saccades,\r\nenable individuals to shift their attention quickly and redi-\r\nrect their focus to different points in the visual field. Study-\r\ning various factors driving people’s eye movements is im-\r\nportant for understanding human attention and develop-\r\ning human-like attention systems. Computational models\r\npredicting eye movements have broad impacts across var-\r\nious domains, such as assessing image and video qual-\r\nity [8, 27, 47], developing intuitive human-computer inter-\r\naction systems [33, 40, 55, 64, 67], creating immersive vir-\r\ntual reality experiences [1, 57, 58], enhancing the safety and\r\nefficiency of autonomous vehicles [28, 77, 78], and diag-\r\n(a) (b) (c)\r\nFigure 1. Understanding and predicting the distinct eye move-\r\nments of each observer is the key objective of individualized scan-\r\npath prediction. These examples reveal the variations in the scan-\r\npaths of different observers, showing their distinct attention pref-\r\nerences in (a) faces, (b) objects, and (c) background. Each dot\r\nrepresents a fixation, with the number and radius indicating its or-\r\nder and duration, respectively. The blue and red dots indicate the\r\nbeginning and the end of the scanpath, respectively.\r\nnosing neurodevelopmental conditions [11, 22, 39].\r\nWhile existing models of saccadic eye movements pre-\r\ndominantly focus on modeling generic gaze patterns man-\r\nifested as observer-agnostic scanpaths ( i.e., a spatiotem-\r\nporal sequence of fixations), this work seeks to model\r\nthe individual variations in eye movements. As shown in\r\nFigure 1, there exists significant inter-observer variations\r\nin visual scanpaths. Such variations can be attributed to\r\na multitude of individual characteristics, such as gender,\r\nage, and neurodevelopmental conditions [56, 61]. For in-\r\nstance, females show more explorative gaze patterns than\r\nmales [53, 62, 63], older adults prefer faces [54] and ob-\r\njects with high color visibility [74], individuals with neu-\r\nrodevelopmental disorders, such as autism spectrum disor-\r\nder (ASD), may show a preference for repetitive patterns\r\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\r\nExcept for this watermark, it is identical to the accepted version;\r\nthe final published version of the proceedings is available on IEEE Xplore.\r\n25420\r\nwhile avoiding social cues [45, 66, 71]. Therefore, devel-\r\noping tailored models that cater to the uniqueness of each\r\nobserver is an essential step toward more precise and adap-\r\ntive attention modeling.\r\nExisting research efforts have failed to address the diver-\r\ngence between the personalized nature of human attention\r\nand the collective nature of current scanpath models. This\r\nis due to the lack of standardized methods for quantifying\r\nand representing individual attention traits, as well as the\r\nabsence of comprehensive frameworks that can accommo-\r\ndate the diverse range of observer characteristics. In this\r\npaper, we resolve this significant challenge with a novel\r\nindividualized scanpath prediction (ISP) method compris-\r\ning three novel components: (1) The observer encoder is\r\na key component for personalized scanpath modeling. It\r\nefficiently captures an observer’s unique attention traits by\r\nintroducing an observer-specific identifier as an additional\r\ninput, forming the basis for individualized scanpath pre-\r\ndictions. (2) The observer-centric feature integration mod-\r\nule adopts a comprehensive approach, fusing visual fea-\r\ntures, task guidance, and observer-specific attention traits\r\nspatially and channel-wise. This ensures consideration of\r\ndiverse bottom-up and top-down cues, simplifying subse-\r\nquent processing and enhancing the efficient prediction of\r\nindividualized scanpaths. (3) The adaptive fixation prior-\r\nitization module enhances scanpath precision by dynami-\r\ncally assigning priorities to the output features, generating\r\na probability map for each fixation. This adaptability en-\r\nsures refined predictions of individualized scanpaths.\r\nOur method has three distinctions from previous visual\r\nscanpath studies: (1) We go beyond prior work focusing\r\non general scanpath modeling and propose the first compre-\r\nhensive investigation of individualized scanpath prediction.\r\n(2) We emphasize the tight integration of observer features\r\ninto the scanpath prediction process, distinct from trivial in-\r\ndividualization techniques such as fine-tuning with single-\r\nobserver data. (3) Our method is generally applicable to\r\nvarious model architectures and visual tasks, broadening its\r\nusability in real-world applications.\r\nThe main contributions of this work are as follows:\r\n1. We study the underexplored task of individualized scan-\r\npath prediction, focusing on modeling how an observer’s\r\nunique attention traits affect their eye movements.\r\n2. We propose an individualization method featuring three\r\nnovel technical components: The observer encoder is an\r\nimportant addition to scanpath models, which enables\r\nobserver-centric feature integration and adaptive fixation\r\nprioritization. These components enable the model to\r\nadapt to individual observers, yielding accurate and in-\r\ndividualized predictions.\r\n3. We comprehensively evaluate scanpaths from individ-\r\nual observers’ perspectives, using both value-based and\r\nranking-based metrics. Experimental results on multipleeye-tracking datasets, with different model architectures\r\nand visual tasks, prove our method’s effectiveness and\r\ngeneralizability for predicting individualized scanpaths.\r\n2. Related Works\r\nOur work is related to prior studies on eye-tracking datasets\r\nand visual scanpath prediction methods.\r\n2.1. Eye-Tracking Datasets\r\nThe foundation for attention modeling relies on diverse,\r\nthoughtfully curated eye-tracking datasets spanning various\r\nstimuli, tasks, and observers [12, 22, 71, 79, 83]. These\r\ndatasets, from those dedicated to free-viewing [22, 71, 79]\r\nto those capturing goal-directed behaviors [12, 83], serve\r\nas invaluable resources for training and evaluating attention\r\nmodels. Specifically, several well-recognized eye-tracking\r\ndatasets have provided benchmarks to quantify the perfor-\r\nmance of saliency models [6, 37, 41, 79] and scanpath mod-\r\nels [22, 71, 79]. Subsequent studies have developed datasets\r\nof goal-directed behaviors to characterize how observers\r\nsearch for an object in an image [83] or answer image-\r\nrelated questions [12]. These efforts facilitate the develop-\r\nment of static saliency models [4, 9, 13, 15, 25, 31, 35, 43]\r\nas well as dynamic scanpath models [14, 19, 51, 59, 68,\r\n69, 83–85]. Our work sets itself apart from individualized\r\nsaliency models [13, 46, 49, 52, 80, 81] by predicting dy-\r\nnamic scanpaths rather than static saliency maps. It utilizes\r\ndatasets from various visual tasks and observer groups to\r\nexpand scanpath modeling, with emphasis on the distinct\r\nattention traits of each observer.\r\n2.2. Visual Scanpath Prediction\r\nScanpath prediction has been an underexplored topic in the\r\nfield of attention modeling. Early studies generate scan-\r\npaths by sampling fixations from saliency maps using the\r\ninhibition-of-return mechanism [34, 46, 50, 72, 73, 75]. Re-\r\ncent studies have developed computational models directly\r\npredicting the sequence of fixations and saccades [14, 19,\r\n51, 59, 69, 83–85]. Several scanpath models harness the\r\npower of deep neural networks [14, 19, 40, 44, 51, 59,\r\n69, 83–85], reinforcement learning techniques [14, 83, 84],\r\nand transformer-based models [51, 59], ultimately improv-\r\ning the accuracy of scanpath prediction to the human level.\r\nThese developments have significantly deepened our un-\r\nderstanding of the temporal dynamics of human attention.\r\nHowever, existing models focus on predicting general scan-\r\npaths rather than taking individual variations into account.\r\nDifferently, our method places particular emphasis on char-\r\nacterizing individual attention traits and integrating them\r\ninto a general scanpath model, thus enabling tailored pre-\r\ndictions that align with each observer’s gaze behavior. This\r\nunique approach extends the horizon of attention modeling,\r\n25421\r\nObserver-\r\nCentric\r\nFeature\r\nIntegrationV isual\r\nEncoderT ask\r\nEncoderFree-V iewing\r\nDecoder\r\n Adaptive\r\nFixation\r\nPrioritization\r\nA vgPools\r\nsoftmaxObserver-Centric Feature Integration Observer EncoderObserver\r\nEncoder\r\nsoftmax\r\nA vgPoolc\r\nA vgPoolsAdaptive Fixation PrioritizationSampleFigure 2. Our proposed method incorporates an observer encoder for characterizing individualized attention traits, followed by observer-\r\ncentric feature integration for holistic processing, and adaptive fixation prioritization for refined predictions.\r\nunderlining the importance of individual differences within\r\nthe broader context of human attention.\r\n3. Methodology\r\nThe core challenge in individualized scanpath modeling is\r\nthe need to predict unique gaze patterns for different ob-\r\nservers. This arises due to the inherent variations in atten-\r\ntion traits. Figure 2 presents an overview of our method.\r\nIt offers a threefold solution: (1) an observer encoder, (2)\r\nan observer-centric feature integration module, and (3) an\r\nadaptive fixation prioritization module. These components\r\nare designed to be flexible as they can be applied on a gen-\r\neral scanpath model based on the encoder-decoders, ( e.g.,\r\nwith a visual encoder and task encoder, and an LSTM [30]\r\nor Transformer [20] decoder) to provide robust and precise\r\npredictions tailored to each observer.\r\n3.1. Observer Encoding\r\nAt the core of our proposed method is an Observer En-\r\ncoder , a key component designed to enable the novel task\r\nof individualized scanpath prediction. It takes as input an\r\nobserver-specific identifier ˜u(e.g., a one-hot vector) and ef-\r\nficiently computes an observer feature u. This feature rep-\r\nresents the unique characteristics and preferences of each\r\nobserver. Our approach utilizes a linear embedding opera-\r\ntion to derive the observer feature:\r\nu=Wu˜u, (1)\r\nwhere Wuindicates learnable parameters. The linear em-\r\nbedding operation provides a straightforward mapping that\r\nretains important characteristics, offering a practical and\r\ncomputationally efficient solution for capturing unique at-\r\ntention traits.This observer encoder can be seamlessly integrated into\r\nan existing scanpath model. As shown in Figure 2, a typical\r\nVisual Encoder is used to transform the input image into\r\nmulti-channel feature maps Echaracterizing the bottom-\r\nup attention. To model the interaction between the visual\r\nfeature Eand the observer feature u, an observer guidance\r\nmap can be computed through a linear combination:\r\nmu= softmax\x00\r\nwT\r\neutanh(WeuE+Wmuu)\x01\r\n,(2)\r\nwhere weu,Weu,Wmuare learnable parameters. This ob-\r\nserver guidance localizes salient image regions of specific\r\ninterest to the observer.\r\nSome scanpath models use a Task Encoder to process\r\ntask-relevant information guiding the gaze behavior, such\r\nas a search target or a general question to answer. Such\r\ntop-down guidance can be represented as a spatial attention\r\nmapm0prioritizing task-relevant regions. These bottom-\r\nup and top-down features are typically processed with a de-\r\ncoder ( e.g., LSTM or Transformer) to predict a sequence\r\nof probability maps {mt|t= 1,2, . . . , T }and distribution\r\nparameters {(µt, σ2\r\nt)|t= 1,2, . . . , T }for sampling fixa-\r\ntion positions and durations, respectively, where Tis the\r\nnumber of fixations.\r\nIn Sections 3.2 and 3.3, we present specific modules that\r\nleverage the observer feature uto individualize the scan-\r\npath model. For clarity, our method description focuses on\r\nits integration with an LSTM model [14]. Please refer to\r\nSection 4.1 and Supplementary Material for details about\r\nits adaptation to a Transformer network [51].\r\n3.2. Observer-Centric Feature Integration\r\nWith the encoded observer features characterizing each ob-\r\nserver’s distinct attention traits, we design observer-centric\r\nfeature integration to address the critical need to fuse var-\r\nious inputs, including visual features, task relevance, and\r\n25422\r\nobserver-specific characteristics, into a unified representa-\r\ntion. The motivation behind this integration is to create\r\na comprehensive understanding of individualized attention\r\npatterns. This integration process results in a sequence\r\nof observer-centric feature maps {Rt|t= 1,2, . . . , T }\r\nrepresenting spatiotemporal fixation patterns, thus enabling\r\nthe model to track individualized attention dynamics over\r\ntime [12, 14, 38].\r\nSpecifically, to guide the prediction at each step, we\r\nleverage the predicted fixation distribution from the previ-\r\nous step ( i.e.,mt−1) as a soft attention map, applying it\r\nto the visual features to derive the previously fixated vi-\r\nsual features Xt=E◦mt−1, where the symbol ◦de-\r\nnotes the Hadamard product. It is noteworthy that the task\r\nguidance map m0is used initially to guide the first fixa-\r\ntion, mimicking the cognitive process that initially directs\r\neye movements based on the visual task. Similarly, the ob-\r\nserver guidance map muis used as the attention weights to\r\nobtain observer-centric visual features Xu=E◦mu.\r\nTo seamlessly integrate the fixated visual features and\r\nobserver-centric visual features, we concatenate the two\r\ntypes of feature maps\r\nXut=Xt∥Xu, (3)\r\nand perform spatial and channel-wise feature fusion, which\r\nare achieved by average-pooling the feature maps along the\r\nchannel (AvgPoolc) and spatial dimensions (AvgPools), re-\r\nspectively, followed by linear layer processing and the ad-\r\ndition of encoded observer features:\r\nus\r\nt=ReLU (WhsAvgPoolc(Xut) +bhs) +Wusu,(4)\r\nuc\r\nt=ReLU (WhcAvgPools(Xut) +bhc) +Wucu,(5)\r\nwhere Whs,Whc,Wus,Wuc,bhs, andbhcare learnable\r\nparameters. Ultimately, combining us\r\ntanduc\r\ntyields the\r\nfinal observer-centric feature maps\r\nRt=us\r\nt⊗uc\r\nt, (6)\r\nwhere⊗is the outer product. It represents the dynamic im-\r\nportance of individual attention traits in the prediction of the\r\ncurrent fixation, providing a more profound understanding\r\nof individualized visual behavior.\r\n3.3. Adaptive Fixation Prioritization\r\nWhile the observer-centric feature integration focuses on\r\nthe fusion of input features, the adaptive fixation prioriti-\r\nzation module addresses the variations of gaze behavior at\r\nthe output end of the decoder. To achieve this, instead of\r\ndirectly predicting fixation positions, our approach, aimed\r\nat individualizing fixation predictions, takes a distinct path.\r\nWe start by extracting semantic feature maps, denoted as\r\nAt, from the decoder. These feature maps are subsequentlyprioritized using attention weights specific to each observer,\r\nproviding a pragmatic means of refining fixation outputs\r\nbased on their unique attention traits.\r\nTo elaborate on the process, we begin by element-wise\r\nmultiplication of the semantic feature maps Atwith the in-\r\nput visual features E, and then perform average-pooling\r\nalong the spatial dimensions, resulting in a feature vec-\r\ntor that characterizes the observer’s attention distribution\r\nacross different semantic feature channels, defined as\r\nVt=AvgPools(E◦At). (7)\r\nConsidering that the visual preferences of various se-\r\nmantic features may vary for different observers, we in-\r\ntroduce normalized attention weights βthat prioritize the\r\ndifferent feature channels, taking into account the observer\r\nfeature:\r\nβt= softmax( wT\r\nbtanh(WbVt+Wumu)),(8)\r\nwhere Wb,Wumandwbare learnable parameters. Finally,\r\nthe attention weights βtare applied to the corresponding\r\nsemantic feature maps Atto compute the output\r\nmt=βT\r\ntAt. (9)\r\nThis mechanism reshapes the scanpath prediction pro-\r\ncess into a weighted combination of multi-channel feature\r\nmaps, allowing for the adaptive integration of these maps\r\ninto the output fixation map. This approach allows the mod-\r\nels to refine the fixation positions, providing a precise pre-\r\ndiction of an individual’s unique scanpath.\r\n4. Experiments\r\nThis section reports comprehensive experimental results\r\nand analyses, demonstrating the effectiveness and general-\r\nizability of our method across various datasets, model archi-\r\ntectures, and visual tasks. For further results, analyses, and\r\nimplementation details, please refer to the Supplementary\r\nMaterial.\r\n4.1. Experiment Settings\r\nTasks and Datasets. We conduct experiments on four eye-\r\ntracking datasets featuring a variety of visual tasks, includ-\r\ning free-viewing, visual search, and visual question answer-\r\ning: OSIE [79] comprising 700 images with free-viewing\r\ngaze data from 15 undergraduate and graduate students aged\r\n18–30, OSIE-ASD [71] with free-viewing gaze data from\r\n20 individuals with ASD and 19 controls, spanning ages 21\r\nto 60, including 33 males and 6 females, COCO-Search18\r\n[83] (target-present subset) featuring 6202 images with gaze\r\ndata from 6 males and 4 females aged 18 to 30, collected\r\nunder a visual search task, and AiR-D [12] offering im-\r\nages and questions from the GQA dataset [32] with gaze\r\n25423\r\nOSIE [79] OSIE-ASD [71] COCO-Search18 [83] AiR-D [12]\r\nMethod SM ↑ MM ↑ SED ↓ SM↑ MM ↑ SED ↓ SM↑ MM ↑ SED ↓ SM↑ MM ↑ SED ↓\r\nHuman 0.386 0.808 7.486 0.370 0.783 7.720 0.458 0.809 1.777 0.405 0.801 7.966\r\nSaltiNet [2] 0.151 0.739 8.790 0.137 0.735 8.688 0.127 0.712 3.821 0.116 0.747 10.661\r\nPathGAN [3] 0.056 0.744 9.393 0.042 0.732 9.342 0.231 0.714 2.454 0.072 0.739 9.888\r\nIOR-ROI [69] 0.294 0.791 7.966 0.301 0.788 7.655 0.197 0.787 7.087 0.239 0.791 8.584\r\nChenLSTM [14] 0.373 0.804 7.309 0.341 0.791 7.602 0.454 0.799 1.932 0.356 0.808 7.845\r\nGazeformer [51] 0.372 0.809 7.298 0.388 0.792 7.081 0.432 0.796 2.023 0.349 0.810 8.004\r\nChenLSTM-FT 0.378 0.808 7.344 0.394 0.796 7.067 0.454 0.804 1.936 0.341 0.806 8.282\r\nGazeformer-FT 0.373 0.810 7.319 0.387 0.795 7.083 0.432 0.796 2.026 0.350 0.812 8.068\r\nChenLSTM-ISP 0.377 0.810 7.284 0.401 0.798 6.599 0.480 0.811 1.862 0.371 0.813 7.651\r\nGazeformer-ISP 0.390 0.813 7.163 0.406 0.797 6.823 0.455 0.806 1.997 0.362 0.814 7.911\r\nTable 1. Comparison of value-based evaluation results for models’ ability to predict the scanpaths of individual observers.\r\nand question-answering data from 16 males and 4 females\r\naged 18 to 38. Dataset splits follow ChenLSTM [14] for the\r\nOSIE, OSIE-ASD, and AiR-D datasets, and the Gazeformer\r\n[51] for the COCO-Search18.\r\nEvaluation Metrics. We conduct individualized scan-\r\npath prediction evaluation using two complementary sets\r\nof metrics: value-based metrics and ranking-based metrics.\r\nThevalue-based metrics measure the similarity or dissimi-\r\nlarity between the prediction and ground-truth scanpaths of\r\nthe same observer. Different from existing studies [14] that\r\ncompare a generic prediction with all observers’ ground-\r\ntruth scanpaths, we evaluate each individualized prediction\r\nagainst the corresponding observer’s ground truth. Specif-\r\nically, ScanMatch (SM) [16, 65] measures the similarity of\r\nfixation position and duration using the Needleman-Wunsch\r\nalgorithm [5]; MultiMatch (MM) [21] measures scanpath\r\nsimilarity regarding shape, direction, length, position, and\r\nduration; String-Edit Distance (SED) [7, 23, 26] converts\r\nscanpaths into strings by associating each image region with\r\na character. To evaluate how well the model predicts dis-\r\ntinctly different scanpaths for different observers, we also\r\nemploy ranking-based metrics. For each predicted scan-\r\npath, we rank the ground-truth scanpaths based on their\r\nScanMatch similarity. Recall at K (R@K) [10, 76] quan-\r\ntifies whether the correct scanpath ( i.e., that from the same\r\nobserver) is within the top-K most similar scanpaths. Mean\r\nReciprocal Rank (MRR) [17, 18, 82] measures the quality\r\nof the ranking by calculating the reciprocal of the rank of\r\nthe correct scanpath. Thus, the combination of value-based\r\nmetrics focusing on the specific observer and ranking-based\r\nmetrics considering all observers offers a comprehensive\r\nand robust performance evaluation.\r\nCompared Models. We implement two individual-\r\nized scanpath prediction models representing typical au-\r\ntoregressive and non-autoregressive sequential processing\r\nparadigms, respectively: ChenLSTM-ISP adapts the ChenL-\r\nSTM [14] model, incorporating the observer encoder and\r\nthe observer-centric feature integration for input processing.The model’s LSTM decoder outputs are further modified\r\nfor the proposed adaptive fixation prioritization. Similarly,\r\nwe implement the Gazeformer-ISP model upon the Gaze-\r\nformer [51] architecture. It replaces the original visual-\r\nsemantic joint embedding with our observer-centric feature\r\nintegration and changes the Transformer decoder outputs\r\nfrom fixation coordinates to feature maps. We compare\r\nthese ISP models with their general counterparts and other\r\ngeneral scanpath prediction models, including SaltiNet [2],\r\nPathGAN [3], and IOR-ROI [69]. In addition, we fine-\r\ntune the general models on individual observer data ( i.e.,\r\nChenLSTM-FT, Gazeformer-FT) to provide a baseline for\r\nassessing the impact of explicitly incorporating observer-\r\nspecific characteristics.\r\nImplementation Details. We implement ChenL-\r\nSTM [14] and Gazeformer [51] following the original meth-\r\nods, such as using the same visual encoder ( i.e., ResNet-\r\n50 [29]) and task encoder ( i.e., RoBERTa [48] or AiR-\r\nM [12] or CenterNet [86] object detector). For both mod-\r\nels, the number of output feature channels for Atis empir-\r\nically set to 4. Specifically, for ChenLSTM [14] and Gaze-\r\nformer [51], we adopt supervised learning for 15 epochs\r\nand self-critical sequence training (SCST) [14, 60] for the\r\nremaining 10 epochs. In supervised learning, we train our\r\nmodel using the Adam [42] optimizer with learning rate\r\n10−4and weight decay 5×10−5, while in the SCST,\r\nwe linearly decayed learning rates starting at 10−5. To\r\nimprove the learning of discriminative features across ob-\r\nservers, each training batch includes different scanpaths for\r\nthe same image.\r\n4.2. Quantitative Results\r\nWe present value- and ranking-based evaluation results to\r\nassess the effectiveness of our ISP models in capturing the\r\nunique attention traits of individual observers.\r\nTable 1 presents the value-based evaluation results re-\r\nvealing how model predictions resemble the ground truth\r\nscanpath of each observer. While fine-tuning leads to minor\r\n25424\r\nOSIE [79] OSIE-ASD [71] COCO-Search18 [83] AiR-D [12]\r\nMethod MRR ↑ R@1 ↑ R@5 ↑ MRR ↑ R@1 ↑ R@5 ↑ MRR ↑ R@1 ↑ R@5 ↑ MRR ↑ R@1 ↑ R@5 ↑\r\nSaltiNet [2] 0.213 5.619 32.286 0.107 2.454 12.454 0.293 10.114 49.804 0.295 10.210 49.930\r\nPathGAN [3] 0.221 6.667 33.048 0.110 2.601 12.894 0.294 10.082 50.245 0.293 10.000 50.629\r\nIOR-ROI [69] 0.218 6.762 31.524 0.109 2.784 12.454 0.292 9.673 50.507 0.291 9.814 48.567\r\nChenLSTM [14] 0.222 7.048 32.952 0.108 2.418 13.114 0.296 10.199 50.719 0.297 9.957 51.433\r\nGazeformer [51] 0.223 7.048 32.476 0.107 2.564 11.758 0.292 9.873 50.114 0.299 10.459 51.361\r\nChenLSTM-FT 0.225 6.667 34.381 0.113 2.711 12.637 0.298 10.641 49.820 0.294 10.118 50.262\r\nGazeformer-FT 0.217 6.000 32.857 0.108 2.528 13.223 0.293 10.183 50.000 0.300 9.599 51.863\r\nChenLSTM-ISP 0.291 12.667 44.095 0.147 4.835 19.194 0.369 16.639 61.769 0.338 13.610 57.235\r\nGazeformer-ISP 0.268 10.095 41.905 0.141 4.286 18.571 0.353 15.299 60.020 0.334 13.539 57.450\r\nTable 2. Comparison of ranking-based evaluation results for models’ ability to distinguish different observers.\r\nModules ChenLSTM Gazeformer\r\nOE FI FP SM↑MM↑SED↓MRR↑R@1↑R@5↑SM↑MM↑SED↓MRR↑R@1↑R@5↑\r\n0.341 0.791 7.602 0.108 2.418 13.114 0.388 0.792 7.081 0.107 2.564 11.758\r\n✓ 0.377 0.791 7.112 0.110 2.601 13.000 0.397 0.796 7.079 0.122 3.017 15.092\r\n✓ ✓ 0.389 0.795 7.064 0.122 3.150 15.238 0.398 0.796 6.982 0.134 3.810 17.509\r\n✓ ✓ 0.389 0.795 7.063 0.112 2.784 13.150 0.397 0.797 7.073 0.120 3.077 15.165\r\n✓ ✓ ✓ 0.401 0.798 6.599 0.147 4.835 19.194 0.406 0.797 6.823 0.141 4.286 18.571\r\nTable 3. Ablation study for the proposed technical components: observer encoder (OE), observer-centric feature integration (FI), and\r\nadaptive fixation prioritization (FP).\r\nimprovements in some cases ( e.g., OSIE and OSIE-ASD),\r\nit struggles on datasets with less distinct inter-observer dif-\r\nferences ( e.g., COCO-Search18 and AiR-D). In contrast,\r\nthe ISP models consistently outperform the general meth-\r\nods and fine-tuning, indicating their ability to adapt to the\r\nunique attention traits of observers. This is particularly ev-\r\nident in the improved performance ( e.g., Gazeformer-ISP,\r\nSM=0.406) on the OSIE-ASD dataset with a diverse range\r\nof observer demographics. These results suggest that our\r\nmethod, by directly targeting the modeling of observer-\r\nspecific attention patterns, offers more robust and effective\r\nindividualization.\r\nTable 2 presents ranking-based evaluation comparing\r\nmodels’ ability to distinguish ground-truth scanpaths. Gen-\r\neral models, which are observer-agnostic, cannot differen-\r\ntiate the ground-truth scanpaths from similar ones ( e.g.,\r\nChenLSTM, R@1=2.4% on OSIE-ASD, lower than ran-\r\ndom). Even after fine-tuning with individual eye-tracking\r\ndata, their performance improvements are marginal ( e.g.,\r\nChenLSTM-FT, R@1=2.7% on OSIE-ASD), because inde-\r\npendently tuning parameters cannot effectively learn fea-\r\ntures that distinguish each observer from the others. Dif-\r\nferently, the individualized models achieve promising re-\r\nsults across all metrics and datasets. From ChenLSTM to\r\nChenLSTM-ISP, R@1 is significantly improved to 4.8% on\r\nthe OSIE-ASD dataset, doubling the probability of find-\r\ning the correct scanpath. It suggests that the ISP models\r\ncan predict scanpaths that align closely with an observer’s\r\nunique attention traits. Between network architectures,ChenLSTM-ISP consistently outperforms Gazeformer-ISP\r\nwhen ranking scanpaths. This performance gain may be\r\nattributed to LSTM’s autoregressive nature which is more\r\neffective than Transformer’s parallel approach in learning\r\nfine-grained spatiotemporal differences.\r\n4.3. Ablation Study\r\nTo evaluate the significance of the three technical compo-\r\nnents: observer encoder (OE), observer-centric feature in-\r\ntegration (FI), and adaptive fixation prioritization (FP), we\r\nconduct an ablation study on the OSIE-ASD dataset [79] by\r\napplying them incrementally to the ChenLSTM and Gaze-\r\nformer models. Table 3 shows that a fundamental module\r\nOE results in a significant improvement in the value-based\r\nevaluation and highlights its role of encoding attention traits\r\nof observers. Furthermore, based on OE, both FI and FP\r\nhave notable impacts on the model performance. First, both\r\ncomponents achieve similar performance improvements in\r\nSM, MM, and SED, demonstrating their ability to improve\r\nthe overall accuracy of scanpath predictions. Further, re-\r\ngarding the MRR, R@1, and R@5 metrics, FI results in\r\nmore significant improvements than FP, suggesting that the\r\nseamless integration of various input features is more sub-\r\nstantial than FP’s ability to prioritize where to look at the\r\noutput end. We also notice that combining both modules\r\nleads to the most significant overall performance improve-\r\nments, indicating that FI and FP offer complementary en-\r\nhancements. Ablation studies on the other datasets are re-\r\nported in the Supplementary Material.\r\n25425\r\nGround Truth ChenLSTM-ISP ChenLSTM-FT\r\n(a)\r\n(b)\r\n(c)\r\n(d)Figure 3. Qualitative examples of scanpaths predicted by\r\nChenLSTM-FT, ChenLSTM-ISP, and ground truth. Each row\r\ncompares the model predictions and the ground truth scanpath of\r\none observer. These observers show different gaze patterns, in-\r\ncluding (a) focusing on the image center, (b) exploring different\r\npeople and objects, (c) exploring broadly in the scene, and (d) fo-\r\ncusing on a particular region. The blue and red dots indicate the\r\nbeginning and the end of the scanpath, respectively.\r\n0.60.8CC\r\n0.70.80.9AUC\r\n2.02.53.0NSS\r\nBaseline\r\nFT\r\nISP\r\nChenLSTM Gazeformer0.700.750.80sAUC\r\nChenLSTM Gazeformer024KLD\r\nChenLSTM Gazeformer0.50.60.7SIM\r\nFigure 4. Saliency evaluation results of the baselines, fine-tuned\r\n(FT) models, and ISP models. Error bars indicate the standard\r\nerror of the mean.\r\n4.4. Qualitative Examples\r\nTo understand how the predicted scanpaths align with\r\nobserver-specific gaze patterns, we present a qualitative\r\ncomparison in Figure 3. Figure 3a and Figure 3b compare\r\nthe scanpaths between an observer with autistic traits and\r\na non-autistic observer. It can be seen that observer (a)\r\nfocused on the center of the image while avoiding direct\r\ngaze at people, while observer (b) looked at people more\r\nfrequently. Figure 3c and Figure 3d compare the scanpathsof two observers responding to the question ‘What is the de-\r\nvice on top of the nightstand made of wood?’ with different\r\nanswers. Observer (c) successfully found the correct answer\r\n‘phone’ by searching broadly within the image, but observer\r\n(d) responded with an incorrect answer ‘television’ because\r\nthe fixations were mostly distributed around the television.\r\nNotably, while the fine-tuning approach (column 1) falls\r\nshort in capturing observer-specific gaze patterns, the ISP\r\nmodels’ predictions (column 2) better align with the scan-\r\npaths of the human observers (column 3). This capability\r\nof ISP models opens up new avenues for understanding and\r\ninterpreting individual differences in visual perception and\r\ndecision-making processes.\r\n4.5. From Scanpaths to Saliency Maps\r\nTo further confirm the effectiveness of our ISP method, we\r\nassess the spatial accuracy of the predicted fixations using\r\nestablished saliency evaluation metrics [31, 37], including\r\nLinear Correlation Coefficient (CC), Area Under the ROC\r\ncurve (AUC), Normalized Scanpath Saliency (NSS), shuf-\r\nfled AUC (sAUC), Kullback-Leibler divergence (KLD), and\r\nsimilarity metric (SIM). Saliency maps are generated by ag-\r\ngregating predicted fixations from all observers and apply-\r\ning a Gaussian kernel smoothing to all fixation points. Fig-\r\nure 4 shows the substantial improvement of the ISP mod-\r\nels over the baselines and fine-tuned models when applied\r\nto the OSIE-ASD [71] dataset. This improvement shows\r\nthat our method not only accurately predicts individual ob-\r\nservers’ fixations but also enhances the overall prediction of\r\nfixation distributions for the population.\r\n4.6. Semantic Analyses\r\nMoving forward, we conduct statistical analyses on the\r\nOSIE-ASD dataset to test ISP models’ ability to learn the at-\r\ntention differences across observers and populations. While\r\nthe evaluations above focus on fixation positions and du-\r\nrations, this analysis considers how the predicted fixations\r\nalign with the ground truth regarding their semantic-level\r\nstatistics. Specifically, we group fixations into three cate-\r\ngories based on the region of interest (ROI) annotations pro-\r\nvided by OSIE [79], which are social regions (directly relat-\r\ning to humans, including faces, emotion, touched, gazed),\r\nnonsocial regions ( e.g., implied motion, relating to nonvi-\r\nsual senses, designed to attract attention, and other objects),\r\nand background. Each observer has a unique fixation distri-\r\nbution over the three categories ( i.e., social, nonsocial, and\r\nbackground), which enables the following individual-level\r\nand population-level analyses.\r\nIndividual Level. To evaluate how the predicted scan-\r\npaths resemble human fixation statistics, we rank observers\r\nby their proportion of fixations in each category. The fix-\r\nations can be obtained from the model predictions or the\r\nground truth. Table 4 presents Spearman’s rank correlation\r\n25426\r\nMethod Social Nonsocial Background\r\nChenLSTM [14] 0.181 -0.159 0.067\r\nGazeformer [51] -0.141 -0.253 -0.211\r\nChenLSTM-FT 0.137 0.040 -0.166\r\nGazeformer-FT 0.045 0.164 0.051\r\nChenLSTM-ISP 0.621 0.655 0.720\r\nGazeformer-ISP 0.692 0.572 0.699\r\nTable 4. Spearmans’ correlation coefficients of fixation propor-\r\ntions in 3 semantic ROIs ( i.e., social, nonsocial, and background)\r\nbetween the ground truth and predictions. Bold numbers indicate\r\nsignificant positive correlations ( p <0.05).\r\n0.00.51.0Fixation Proportion**\r\n*ChenLSTM-ISP\r\n**\r\n*Gazeformer-ISP\r\n**\r\n*Ground-Truth\r\nASD Controls\r\n012Fixation Latency (s)**\r\n*\r\n**\r\n*\r\n**\r\nSocial Non-\r\n SocialBackground012Fixation Duration (s)**\r\n*\r\nSocial Non-\r\n SocialBackground**\r\n*\r\nSocial Non-\r\n SocialBackground*\r\n*\r\nFigure 5. Statistical comparison between the predicted fixations\r\nfor the ASD and Control groups [71]. Error bars indicate the stan-\r\ndard error of the mean. Asterisks indicate significant differences\r\n(unpaired t-test, p <0.05).\r\n 1\r\n 2  3 4\r\n 5 6\r\n 7 8\r\n 910111213\r\n14\r\n151617\r\n1819\r\n20\r\n212223\r\n24 25262728 29\r\n303132\r\n333435\r\n3637\r\n38\r\n39ChenLSTM-ISP\r\nASD Controls\r\n 1\r\n 2\r\n 3 4 5 6 7\r\n 8 910 1112\r\n1314\r\n151617\r\n1819\r\n20\r\n21\r\n2223\r\n24252627\r\n2829 3031\r\n32333435\r\n363738\r\n39Gazeformer-ISP\r\nChenLSTM-ISP Gazeformer-ISP0.60.81.0\r\nAccuracy\r\nAUC\r\nFigure 6. Visualization of features extracted from ISP models\r\n(numbers indicate observer identities) and results of ASD classifi-\r\ncation using the features.\r\ncoefficient [70] to compare the observer rankings between\r\nthe predictions and the ground-truth fixations. While fine-\r\ntuning is less effective, showing low correlations across all\r\ncategories, ISP models consistently achieve significant and\r\nhigh positive correlations, suggesting their ability to resem-\r\nble each human observer’s unique fixation patterns.\r\nPopulation Level. Beyond individual characterization,\r\nISP models also effectively capture and reproduce distinc-tive attention traits observed at the population level. For\r\nexample, individuals with ASD exhibit lower proportions,\r\nhigher latency, and shorter duration of fixations to both so-\r\ncial and nonsocial cues [45, 66, 71]. Figure 5 shows that fix-\r\nations predicted by the ISP models achieve similar statistics.\r\nThe statistical agreement between the model predictions\r\nand the ground-truth scanpaths demonstrates our method’s\r\nability to generalize and represent population-level charac-\r\nteristics, reinforcing its potential utility in a variety of ap-\r\nplications.\r\n4.7. Application\r\nTo showcase the potential applicability of ISP models in\r\nthe diagnosis of neurodevelopmental conditions, we visu-\r\nalize ISP model features and use these features to clas-\r\nsify people with ASD. First, the individualization ability of\r\nour method is highlighted through t-distributed stochastic\r\nneighbor embedding (t-SNE) visualization. By concatenat-\r\ning all observer-specific features from Equations (2), (4),\r\n(5), and (8), into v= [Wmuu∥Wusu∥Wucu∥Wumu],\r\nwhere∥represents the vector concatenation, Figure 6 shows\r\nthat the ISP model features can clearly distinguish people\r\nwith ASD from the controls. It is noteworthy that such fea-\r\ntures are learned in an unsupervised manner without know-\r\ning each observer’s class label, suggesting the strong learn-\r\ning power of the ISP models. Further, based on a leave-\r\none-out cross-validation, we train a two-layer perceptron\r\nto classify people with ASD using the extracted feature v.\r\nChenLSTM-ISP and Gazeformer-IPS achieve 82.1% and\r\n71.8% classification accuracy, respectively, similar to clini-\r\ncal gold standards [24, 36]. These results demonstrate ISP\r\nmodels’ potential in real-world healthcare applications.\r\n5. Conclusion\r\nWe have introduced a novel approach to predicting indi-\r\nvidualized human visual scanpaths. Our approach features\r\nthree novel components: observer encoder, observer-centric\r\nfeature integration, and adaptive fixation prioritization.\r\nThrough extensive experiments across multiple datasets,\r\nnetwork architectures, and visual tasks, our method con-\r\nsistently outperforms state-of-the-art scanpath prediction\r\nmethods and individualization based on observer-specific\r\nfine-tuning. The results demonstrate the method’s ability to\r\ngenerate human-like scanpaths and account for individual\r\nobservers’ gaze patterns. By providing a better understand-\r\ning of how individuals process visual information, our study\r\nhas significant implications for tailored, user-centric solu-\r\ntions, such as improving the design of interfaces, products,\r\nand services across a wide range of application domains.\r\nAcknowledgments\r\nThis work is supported by NSF Grant 2143197.\r\n25427\r\nReferences\r\n[1] Isayas Berhe Adhanom, Paul MacNeilage, and Eelke Folmer.\r\nEye gaze techniques for human computer interaction: A re-\r\nsearch survey. Virtual Reality , 2023. 1\r\n[2] Marc Assens, Kevin McGuinness, Xavier Giro-i-Nieto, and\r\nNoel E. O’Connor. SaltiNet: Scan-path prediction on 360 de-\r\ngree images using saliency volumes. In Proceedings of the\r\nIEEE International Conference on Computer Vision Work-\r\nshop (ICCVW) , 2017. 5, 6\r\n[3] Marc Assens, Xavier Giro-i-Nieto, Kevin McGuinness, and\r\nNoel E. O’Connor. PathGAN: Visual scanpath prediction\r\nwith generative adversarial networks. In Proceedings of the\r\nEuropean Conference on Computer Vision Workshop (EC-\r\nCVW) , 2018. 5, 6\r\n[4] Bahar Aydemir, Ludo Hoffstetter, Tong Zhang, Mathieu\r\nSalzmann, and Sabine Susstrunk. TempSAL - uncovering\r\ntemporal information for deep saliency prediction. In Pro-\r\nceedings of the IEEE Conference on Computer Vision and\r\nPattern Recognition (CVPR) , 2023. 2\r\n[5] Saul B.Needleman and Christian D.Wunsch. A general\r\nmethod applicable to the search for similarities in the amino\r\nacid sequence of two proteins. Journal of Molecular Biology\r\n(JMB) , 1970. 5\r\n[6] Ali Borji and Laurent Itti. CAT2000: A large scale fixa-\r\ntion dataset for boosting saliency research. arXiv preprint\r\narXiv:1505.03581v1 , 2015. 2\r\n[7] Stephan A. Brandt and Lawrence W. Stark. Spontaneous\r\neye movements during visual imagery reflect the content of\r\nthe visual scene. Journal of Cognitive Neuroscience (JCN) ,\r\n1997. 5\r\n[8] Patrick Le Callet and Ernst Niebur. Visual attention and ap-\r\nplications in multimedia technologies. Proceedings of the\r\nInstitution of Electrical Engineers , 2013. 1\r\n[9] Souradeep Chakraborty, Zijun Wei, Conor Kelton, Seoy-\r\noung Ahn, Aruna Balasubramanian, Gregory J. Zelinsky, and\r\nDimitris Samaras. Predicting visual attention in graphic de-\r\nsign documents. IEEE Transactions on Multimedia (TMM) ,\r\n2022. 2\r\n[10] Jiacheng Chen, Hexiang Hu, Hao Wu, Yuning Jiang, and\r\nChanghu Wang. Learning the best pooling strategy for visual\r\nsemantic embedding. In Proceedings of the IEEE Confer-\r\nence on Computer Vision and Pattern Recognition (CVPR) ,\r\n2021. 5\r\n[11] Shi Chen and Qi Zhao. Attention-based autism spectrum\r\ndisorder screening with privileged modality. In Proceedings\r\nof the IEEE International Conference on Computer Vision\r\n(ICCV) , 2019. 1\r\n[12] Shi Chen, Ming Jiang, Jinhui Yang, and Qi Zhao. AiR: At-\r\ntention with reasoning capability. In Proceedings of the Eu-\r\nropean Conference on Computer Vision (ECCV) , 2020. 2, 4,\r\n5, 6\r\n[13] Shi Chen, Nachiappan Valliappan, Shaolei Shen, Xinyu Ye,\r\nKai Kohlhoff, and Junfeng He. Learning from unique per-\r\nspectives: User-aware saliency modeling. In Proceedings\r\nof the IEEE Conference on Computer Vision and Pattern\r\nRecognition (CVPR) , 2023. 2[14] Xianyu Chen, Ming Jiang, and Qi Zhao. Predicting hu-\r\nman scanpaths in visual question answering. In Proceed-\r\nings of the IEEE Conference on Computer Vision and Pattern\r\nRecognition (CVPR) , 2021. 2, 3, 4, 5, 6, 8\r\n[15] Marcella Cornia, Lorenzo Baraldi, Giuseppe Serra, and Rita\r\nCucchiara. Predicting human eye fixations via an lstm-based\r\nsaliency attentive model. IEEE Transactions on Image Pro-\r\ncessing (IEEE TIP) , 2018. 2\r\n[16] Filipe Cristino, Sebastiaan Math ˆot, Jan Theeuwes, and\r\nIain D Gilchrist. ScanMatch: A novel method for compar-\r\ning fixation sequences. Behavior Research Methods (BRM) ,\r\n2010. 5\r\n[17] Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh,\r\nDeshraj Yadav, Jos ´e M. F. Moura, Devi Parikh, and Dhruv\r\nBatra. Visual dialog. In Proceedings of the IEEE Confer-\r\nence on Computer Vision and Pattern Recognition (CVPR) ,\r\n2017. 5\r\n[18] Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh,\r\nDeshraj Yadav, Stefan Lee, Jos ´e M. F. Moura, Devi Parikh,\r\nand Dhruv Batra. Visual dialog. IEEE Transactions on\r\nPattern Analysis and Machine Intelligence (IEEE TPAMI) ,\r\n2019. 5\r\n[19] Ryan Anthony Jalova de Belen, Tomasz Bednarz, and Arcot\r\nSowmya. Scanpathnet: A recurrent mixture density network\r\nfor scanpath prediction. In Proceedings of the IEEE Confer-\r\nence on Computer Vision and Pattern Recognition Workshop\r\n(CVPRW) , 2022. 2\r\n[20] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\r\nToutanova. BERT: Pre-training of deep bidirectional\r\ntransformers for language understanding. arXiv preprint\r\narXiv:1810.04805 , 2018. 3\r\n[21] Richard Dewhurst, Marcus Nystr ¨om, Halszka Jarodzka, Tom\r\nFoulsham, Roger Johansson, and Kenneth Holmqvist. It de-\r\npends on how you look at it: Scanpath comparison in mul-\r\ntiple dimensions with MultiMatch, a vector-based approach.\r\nBehavior Research Methods (BRM) , 2012. 5\r\n[22] Huiyu Duan, Guangtao Zhai, Xiongkuo Min, Zhaohui Che,\r\nYi Fang, Xiaokang Yang, Jes ´us Guti ´errez, and Patrick Le\r\nCallet. A dataset of eye movements for the children with\r\nautism spectrum disorder. In ACM Multimedia Systems Con-\r\nference (MMSys) , 2019. 1, 2\r\n[23] Lapo Faggi, Alessandro Betti, Dario Zanca, Stefano Melacci,\r\nand Marco Gori. Wave propagation of visual stimuli in focus\r\nof attention. arXiv preprint arXiv:2006.11035 , 2020. 5\r\n[24] Torbj ¨orn Falkmer, Katie Anderson, Marita Falkmer, and\r\nChiara Horlin. Diagnostic procedures in autism spectrum\r\ndisorders: a systematic literature review. European Child &\r\nAdolescent Psychiatry , 2013. 8\r\n[25] Camilo Fosco, Vincent Casser, Amish Kumar Bedi, Peter\r\nO’Donovan, Aaron Hertzmann, and Zoya Bylinskii. Pre-\r\ndicting visual importance across graphic design types. In\r\nACM Symposium on User Interface Software and Technol-\r\nogy, 2020. 2\r\n[26] Tom Foulsham and Geoffrey Underwood. What can saliency\r\nmodels predict about eye movements? Spatial and sequential\r\naspects of fixations during encoding and recognition. Jour-\r\nnal of Vision (JoV) , 2008. 5\r\n25428\r\n[27] Ke Gu, Shiqi Wang, Huan Yang, Weisi Lin, Guangtao Zhai,\r\nXiaokang Yang, and Wenjun Zhang. Saliency-guided quality\r\nassessment of screen content images. IEEE Transactions on\r\nMultimedia (TMM) , 2016. 1\r\n[28] Xinyue Gui, Koki Toda, Stela Hanbyeol Seo, Chia-Ming\r\nChang, and Takeo Igarashi. “I am going this way”: Gazing\r\neyes on self-driving car show multiple driving directions. In\r\nInternational Conference on Automotive User Interfaces and\r\nInteractive Vehicular Applications , 2022. 1\r\n[29] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\r\nDeep residual learning for image recognition. In Proceed-\r\nings of the IEEE Conference on Computer Vision and Pattern\r\nRecognition (CVPR) , 2016. 5\r\n[30] Sepp Hochreiter and J ¨urgen Schmidhuber. Long short-term\r\nmemory. Neural Computation , 1997. 3\r\n[31] Xun Huang, Chengyao Shen, Xavier Boix, and Qi Zhao.\r\nSALICON: Reducing the semantic gap in saliency predic-\r\ntion by adapting deep neural networks. In Proceedings of the\r\nIEEE International Conference on Computer Vision (ICCV) ,\r\n2015. 2, 7\r\n[32] Drew A. Hudson and Christopher D. Manning. GQA: A new\r\ndataset for real-world visual reasoning and compositional\r\nquestion answering. In Proceedings of the IEEE Conference\r\non Computer Vision and Pattern Recognition (CVPR) , 2019.\r\n4\r\n[33] Thomas E. Hutchinson, K. Preston White, Worthy N. Martin,\r\nKelly C. Reichert, and Lisa A. Frey. Human-computer inter-\r\naction using eye-gaze input. IEEE Transactions on Systems,\r\nMan, and Cybernetics (TSMC) , 1989. 1\r\n[34] Laurent Itti, Christof Koch, and Ernst Niebur. A model\r\nof saliency-based visual attention for rapid scene analysis.\r\nIEEE Transactions on Pattern Analysis and Machine Intelli-\r\ngence (IEEE TPAMI) , 1998. 2\r\n[35] Sen Jia and Neil D. B. Bruce. EML-NET:an expandable\r\nmulti-layer network for saliency prediction. Image and Vi-\r\nsion Computing , 2020. 2\r\n[36] Ming Jiang and Qi Zhao. Learning visual attention to iden-\r\ntify people with autism spectrum disorder. In Proceedings\r\nof the IEEE International Conference on Computer Vision\r\n(ICCV) , 2017. 8\r\n[37] Ming Jiang, Shengsheng Huang, Juanyong Duan, and Qi\r\nZhao. SALICON: Saliency in context. In Proceedings of the\r\nIEEE International Conference on Computer Vision (ICCV) ,\r\n2015. 2, 7\r\n[38] Ming Jiang, Shi Chen, Jinhui Yang, and Qi Zhao. Fantas-\r\ntic answers and where to find them: Immersive question-\r\ndirected visual attention. In Proceedings of the IEEE Confer-\r\nence on Computer Vision and Pattern Recognition (CVPR) ,\r\n2020. 4\r\n[39] Ming Jiang, Sunday M Francis, Angela Tseng, Diksha Sr-\r\nishyla, Megan DuBois, Katie Beard, Christine Conelea, Qi\r\nZhao, and Suma Jacob. Predicting core characteristics of\r\nasd through facial emotion recognition and eye tracking in\r\nyouth. In International Conference of the IEEE Engineering\r\nin Medicine & Biology Society (EMBC) , 2020. 1\r\n[40] Yue Jiang, Luis A. Leiva, Hamed R. Tavakoli, Paul R. B.\r\nHoussel, Julia Kylm ¨al¨a, and Antti Oulasvirta. UEyes: Un-\r\nderstanding visual saliency across user interface types. InACM CHI Conference on Human Factors in Computing Sys-\r\ntems (CHI) , 2023. 1, 2\r\n[41] Tilke Judd, Krista Ehinger, Fr ´edo Durand, and Antonio Tor-\r\nralba. Learning to predict where humans look. In Proceed-\r\nings of the IEEE International Conference on Computer Vi-\r\nsion (ICCV) , 2013. 2\r\n[42] Diederik P. Kingma and Jimmy Ba. Adam: A method for\r\nstochastic optimization. In Proceedings of the International\r\nConference on Learning Representations (ICLR) , 2015. 5\r\n[43] Matthias K ¨ummerer, Thomas S. A. Wallis, and Matthias\r\nBethge. DeepGaze II: Reading fixations from deep\r\nfeatures trained on object recognition. arXiv preprint\r\narXiv:1610.01563 , 2016. 2\r\n[44] Matthias K ¨ummerer, Matthias Bethge, and Thomas S. A.\r\nWallis. DeepGaze III: Modeling free-viewing human scan-\r\npaths with deep learning. Journal of Vision (JoV) , 2022. 2\r\n[45] Mark H. Lewis and James W. Bodfish. Repetitive behav-\r\nior disorders in autism. Developmental Disabilities Research\r\nReviews , 1998. 2, 8\r\n[46] Aoqi Li and Zhenzhong Chen. Individual trait oriented scan-\r\npath prediction for visual attention analysis. In IEEE Inter-\r\nnational Conference on Image Processing (ICIP) , 2017. 2\r\n[47] Leida Li, Yu Zhou, Weisi Lin, Jinjian Wu, Xinfeng Zhang,\r\nand Beijing Chen. No-reference quality assessment of de-\r\nblocked images. Neurocomputing , 2016. 1\r\n[48] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\r\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\r\nZettlemoyer, and Veselin Stoyanov. RoBERTa: A ro-\r\nbustly optimized bert pretraining approach. arXiv preprint\r\narXiv:1907.11692 , 2019. 5\r\n[49] Xinhui Luo, Zhi Liu, Weijie Wei, Linwei Ye, Tianhong\r\nZhang, Lihua Xu, and Jijun Wang. Few-shot personalized\r\nsaliency prediction using meta-learning. Image and Vision\r\nComputing , 2022. 2\r\n[50] Olivier Le Meur and Zhi Liu. Saccadic model of eye move-\r\nments for free-viewing condition. Vision Research (VR) ,\r\n2015. 2\r\n[51] Sounak Mondal, Zhibo Yang, Seoyoung Ahn, Gregory\r\nZelinsky, Dimitris Samaras, and Minh Hoai. Gazeformer:\r\nScalable, effective and fast prediction of goal-directed hu-\r\nman attention. In Proceedings of the IEEE Conference on\r\nComputer Vision and Pattern Recognition (CVPR) , 2023. 2,\r\n3, 5, 6, 8\r\n[52] Yuya Moroto, Keisuke Maeda, Takahiro Ogawa, and Miki\r\nHaseyama. Few-shot personalized saliency prediction based\r\non adaptive image selection considering object and visual at-\r\ntention. IEEE International Conference on Consumer Elec-\r\ntronics , 2020. 2\r\n[53] Felix Joseph Mercer Moss, Roland Baddeley, and Nishan\r\nCanagarajah. Eye movements to natural images as a func-\r\ntion of sex and personality. PLoS One , 2012. 1\r\n[54] Young Hoon Oh and Da Young Ju. Age-related differences\r\nin fixation pattern on a companion robot. Sensors , 2020. 1\r\n[55] Uchenna Chinyere Onyemauche, Samuel Makuochi Nkwo,\r\nand Charity Elochukwu Mbanusi. Towards the use of eye\r\ngaze tracking technology: Human computer interaction (hci)\r\nresearch. In African Human-Computer Interaction Confer-\r\nence: Inclusiveness and Empowerment , 2021. 1\r\n25429\r\n[56] Matthew F. Peterson and Miguel P. Eckstein. Individual dif-\r\nferences in eye movements during face identification reflect\r\nobserver-specific optimal points of fixation. Psychological\r\nScience , 2013. 1\r\n[57] Thammathip Piumsomboon, Gun Lee, Robert W. Lindeman,\r\nand Mark Billinghurst. Exploring natural eye-gaze-based in-\r\nteraction for immersive virtual reality. In IEEE Symposium\r\non 3D User Interfaces (3DUI) , 2017. 1\r\n[58] Kun Qian, Tomoki Arichi, Anthony Price, Sofia Dall’Orso,\r\nJonathan Eden, Yohan Noh, Kawal Rhode, Etienne Burdet,\r\nMark Neil, A. David Edwards, and Joseph V . Hajnal. An eye\r\ntracking based virtual reality system for use inside magnetic\r\nresonance imaging systems. Scientific Reports , 2021. 1\r\n[59] Mengyu Qiu, Yi Guo, Mingguang Zhang, Jingwei Zhang,\r\nTian Lan, and Zhilin Liu. Simulating human visual sys-\r\ntem based on vision transformer. In Proceedings of the 2023\r\nACM Symposium on Spatial User Interaction , 2023. 2\r\n[60] Steven J. Rennie, Etienne Marcheret, Youssef Mroueh, Jarret\r\nRoss, and Vaibhava Goel. Self-critical sequence training for\r\nimage captioning. In Proceedings of the IEEE Conference\r\non Computer Vision and Pattern Recognition (CVPR) , 2017.\r\n5\r\n[61] Evan F. Risko, Nicola C. Anderson, Sophie Lanthier, and\r\nAlan Kingstone. Curious eyes: Individual differences in per-\r\nsonality predict eye movement behavior in scene-viewing.\r\nCognition , 2012. 1\r\n[62] Negar Sammaknejad, Hamidreza Pouretemad, Changiz Es-\r\nlahchi, Alireza Salahirad, and Ashkan Alinejad. Gender clas-\r\nsification based on eye movements: A processing effect dur-\r\ning passive face viewing. Advances in Cognitive Psychology ,\r\n2017. 1\r\n[63] Bahman Abdi Sargezeh, Niloofar Tavakoli, and ohammad\r\nReza Daliri. Gender-based eye movement differences in pas-\r\nsive indoor picture viewing: An eye-tracking study. Physiol-\r\nogy & Behavior , 2019. 1\r\n[64] Anjana Sharma and Pawanesh Abrol. Eye gaze techniques\r\nfor human computer interaction: A research survey. Interna-\r\ntional Journal of Computer Applications , 2013. 1\r\n[65] Hiroyuki Sogo. Gazeparser: an open-source and multiplat-\r\nform library for low-cost eye tracking and analysis. Behavior\r\nReserch Methods (BRM) , 2013. 5\r\n[66] Mikle South, Sally Ozonoff, and William M. McMahon.\r\nRepetitive behavior profiles in asperger syndrome and high-\r\nfunctioning autism. Journal of Autism and Developmental\r\nDisorders , 2005. 2, 8\r\n[67] Tommy Strandvall. Eye tracking in human-computer inter-\r\naction and usability research. In IFIP Conference on Human-\r\nComputer Interaction , 2009. 1\r\n[68] Xiangjie Sui, Yuming Fang, Hanwei Zhu, Shiqi Wang, and\r\nZhou Wang. ScanDMM: A deep markov model of scan-\r\npath prediction for 360° images. In Proceedings of the\r\nIEEE Conference on Computer Vision and Pattern Recog-\r\nnition (CVPR) , 2023. 2\r\n[69] Wanjie Sun, Zhenzhong Chen, and Feng Wu. Visual scan-\r\npath prediction using IOR-ROI recurrent mixture density\r\nnetwork. IEEE Transactions on Pattern Analysis and Ma-\r\nchine Intelligence (IEEE TPAMI) , 2019. 2, 5, 6[70] Alexander Toet. Computational versus psychophysical\r\nbottom-up image saliency: A comparative evaluation study.\r\nIEEE Transactions on Pattern Analysis and Machine Intelli-\r\ngence (IEEE TPAMI) , 2011. 8\r\n[71] Shuo Wang, Ming Jiang, Xavier Morin, Duchesne, Eliza-\r\nbeth A. Laugeson, Daniel P. Kennedy, Ralph Adolphs, and\r\nQi Zhao. Atypical visual saliency in autism spectrum dis-\r\norder quantified through model-based eye tracking. Neuron ,\r\n2015. 2, 4, 5, 6, 7, 8\r\n[72] Wei Wang, Cheng Chen, Yizhou Wang, Tingting Jiang, Fang\r\nFang, and Yuan Yao. Simulating human saccadic scanpaths\r\non natural images. In Proceedings of the IEEE Conference\r\non Computer Vision and Pattern Recognition (CVPR) , 2011.\r\n2\r\n[73] Yixiu Wang, Bin Wang, Xiaofeng Wu, and Liming Zhang.\r\nScanpath estimation based on foveated image saliency. Cog-\r\nnitive Processing (CP) , 2017. 2\r\n[74] Ze-Yu Wang and Ji Young Cho. Older adults’ response to\r\ncolor visibility in indoor residential environment using eye-\r\ntracking technology. Sensors , 2022. 1\r\n[75] Calden Wloka, Iuliia Kotseruba, and John K. Tsotsos. Ac-\r\ntive fixation control to predict saccade sequences. In Pro-\r\nceedings of the IEEE Conference on Computer Vision and\r\nPattern Recognition (CVPR) , 2018. 2\r\n[76] Hao Wu, Jiayuan Mao, Yufeng Zhang, Yuning Jiang, Lei Li,\r\nWeiwei Sun, and Wei-Ying Ma. Unified visual-semantic\r\nembeddings: Bridging vision and language with struc-\r\ntured meaning representations. In Proceedings of the IEEE\r\nConference on Computer Vision and Pattern Recognition\r\n(CVPR) , 2019. 5\r\n[77] Ye Xia, Danqing Zhang, Jinkyu Kim, Ken Nakayama, Karl\r\nZipser, and David Whitney. Predicting driver attention in\r\ncritical situations. In Asian Conference on Computer Vision\r\n(ACCV) , 2018. 1\r\n[78] Ye Xia, Jinkyu Kim, John Canny, Karl Zipser, Teresa Canas-\r\nBajo, and David Whitney. Periphery-fovea multi-resolution\r\ndriving model guided by human attention. In Proceedings\r\nof the IEEE Winter Conference on Applications of Computer\r\nVision (WACV) , 2019. 1\r\n[79] Juan Xu, Ming Jiang, Shuo Wang, Mohan S. Kankanhalli,\r\nand Qi Zhao. Predicting human gaze beyond pixels. Journal\r\nof Vision (JoV) , 2014. 2, 4, 5, 6, 7\r\n[80] Yanyu Xu, Nianyi Li, Junru Wu, Jingyi Yu, and Shenghua\r\nGao. Beyond universal saliency: Personalized saliency pre-\r\ndiction with multi-task cnn. In Proceedings of the Twenty-\r\nSixth International Joint Conference on Artificial Intelli-\r\ngence (IJCAI) , 2017. 2\r\n[81] Yanyu Xu, Shenghua Gao, Junru Wu, Nianyi Li, and Jingyi\r\nYu. Personalized saliency and its prediction. IEEE Trans-\r\nactions on Pattern Analysis and Machine Intelligence (IEEE\r\nTPAMI) , 2018. 2\r\n[82] Jinhui Yang, Xianyu Chen, Ming Jiang, Shi Chen, Louis\r\nWang, and Qi Zhao. VisualHow: Multimodal problem solv-\r\ning. In Proceedings of the IEEE Conference on Computer\r\nVision and Pattern Recognition (CVPR) , 2022. 5\r\n[83] Zhibo Yang, Lihan Huang, Yupei Chen, Zijun Wei, Seoy-\r\noung Ahn, Gregory Zelinsky, Dimitris Samaras, and Minh\r\n25430\r\nHoai. Predicting goal-directed human attention using inverse\r\nreinforcement learning. In Proceedings of the IEEE Confer-\r\nence on Computer Vision and Pattern Recognition (CVPR) ,\r\n2020. 2, 4, 5, 6\r\n[84] Zhibo Yang, Sounak Mondal, Seoyoung Ahn, Gregory\r\nZelinsky, Minh Hoai, and Dimitris Samaras. Target-absent\r\nhuman attention. In Proceedings of the European Confer-\r\nence on Computer Vision (ECCV) , 2022. 2\r\n[85] Zhibo Yang, Sounak Mondal, Seoyoung Ahn, Gregory\r\nZelinsky, Minh Hoai, and Dimitris Samaras. Predicting hu-\r\nman attention using computational attention. arXiv preprint\r\narXiv:2303.09383 , 2023. 2\r\n[86] Xingyi Zhou, Dequan Wang, and Philipp Kr ¨ahenb ¨uhl. Ob-\r\njects as points. In Proceedings of the IEEE Conference on\r\nComputer Vision and Pattern Recognition (CVPR) , 2019. 5\r\n25431'}, 'dist': 0.9286905527114868}
Result 3: {'text': 'data', 'metadata': {'created_at': 1736932570, 'modified_at': 1736932570, 'owner': 'sayandeep', 'path': 'RAG_CONTENT/conferences/cvpr_ocr/Li_Learning_Transferable_Negative_Prompts_for_Out-of-Distribution_Detection_CVPR_2024_paper.txt', 'size': 71952, 'seen_at': 1737191136, 'data': 'Learning Transferable Negative Prompts for Out-of-Distribution Detection\nTianqi Li1, Guansong Pang*2, Xiao Bai*1, Wenjun Miao1, and Jin Zheng1\n1School of Computer Science and Engineering, State Key Laboratory of Complex & Critical Software\nEnvironment, Jiangxi Research Institute, Beihang University, China\n2School of Computing and Information Systems, Singapore Management University\nAbstract\nExisting prompt learning methods have shown certain\ncapabilities in Out-of-Distribution (OOD) detection, but the\nlack of OOD images in the target dataset in their train-\ning can lead to mismatches between OOD images and In-\nDistribution (ID) categories, resulting in a high false posi-\ntive rate. To address this issue, we introduce a novel OOD\ndetection method, named ‘NegPrompt’, to learn a set of\nnegative prompts, each representing a negative connotation\nof a given class label, for delineating the boundaries be-\ntween ID and OOD images. It learns such negative prompts\nwith ID data only, without any reliance on external out-\nlier data. Further, current methods assume the availabil-\nity of samples of all ID classes, rendering them ineffective\nin open-vocabulary learning scenarios where the inference\nstage can contain novel ID classes not present during train-\ning. In contrast, our learned negative prompts are trans-\nferable to novel class labels. Experiments on various Im-\nageNet benchmarks show that NegPrompt surpasses state-\nof-the-art prompt-learning-based OOD detection methods\nand maintains a consistent lead in hard OOD detection in\nclosed- and open-vocabulary classification scenarios. Code\nis available at https://github.com/mala-lab/negprompt.\n1. Introduction\nSince the advent of deep learning, numerous image recog-\nnition models [5, 9, 29] have relied solely on image features\nfor classification. However, in recent years large pre-trained\nvision-language models (VLMs), such as CLIP [37], inte-\ngrated natural language processing into computer vision,\nenhancing the semantic understanding capabilities of com-\nputer vision models. It has been observed that these VLMs\nexcel in image classification, particularly in zero-shot sce-\nnarios. This is attributed to their extensive self-supervised\npre-training on web-scale image-text data, which has en-\ndowed them with robust semantic transfer abilities.\n*Corresponding author: G. Pang (gspang@smu.edu.sg) and X. Bai\n(baixiao@buaa.edu.cn)\nelephantbirdbeehorsePrompt Embedding Space\ndogcatcat\ncat\ncat\ndog\ndog\ndog\n<latexit sha1_base64="1mgy36mbSmkRdxUbNESh1s8PNiI=">AAACQXicbZA7T8MwFIUd3pRXgZElokJiQFWCEDAiWBhBoqVSEyrHdahVx47sG0QV+a+x8A/Y2FkYQIiVBactUChXsvX5nHtl+0QpZxo879GZmJyanpmdmy8tLC4tr5RX1+paZorQGpFcqkaENeVM0Bow4LSRKoqTiNPLqHtS+Jc3VGkmxQX0Uhom+FqwmBEMVmqVGwHQW4jivGmCBEPH0k3LNzs/h10TkLaEke3bEuZrOjRX+RenUhtTapUrXtXrlzsO/hAqaFhnrfJD0JYkS6gAwrHWTd9LIcyxAkY4NaUg0zTFpIuvadOiwAnVYd5PwLhbVmm7sVR2CXD76uhEjhOte0lkO4vX679eIf7nNTOID8OciTQDKsjgojjjLki3iNNtM0UJ8J4FTBSzb3VJBytMwIZehOD//fI41Her/n7VP9+rHB0P45hDG2gTbSMfHaAjdIrOUA0RdIee0At6de6dZ+fNeR+0TjjDmXX0q5yPT+FMtA4=</latexit>[v1,v2···vn]pos\n<latexit sha1_base64="1mgy36mbSmkRdxUbNESh1s8PNiI=">AAACQXicbZA7T8MwFIUd3pRXgZElokJiQFWCEDAiWBhBoqVSEyrHdahVx47sG0QV+a+x8A/Y2FkYQIiVBactUChXsvX5nHtl+0QpZxo879GZmJyanpmdmy8tLC4tr5RX1+paZorQGpFcqkaENeVM0Bow4LSRKoqTiNPLqHtS+Jc3VGkmxQX0Uhom+FqwmBEMVmqVGwHQW4jivGmCBEPH0k3LNzs/h10TkLaEke3bEuZrOjRX+RenUhtTapUrXtXrlzsO/hAqaFhnrfJD0JYkS6gAwrHWTd9LIcyxAkY4NaUg0zTFpIuvadOiwAnVYd5PwLhbVmm7sVR2CXD76uhEjhOte0lkO4vX679eIf7nNTOID8OciTQDKsjgojjjLki3iNNtM0UJ8J4FTBSzb3VJBytMwIZehOD//fI41Her/n7VP9+rHB0P45hDG2gTbSMfHaAjdIrOUA0RdIee0At6de6dZ+fNeR+0TjjDmXX0q5yPT+FMtA4=</latexit>[v1,v2···vn]pos\n<latexit sha1_base64="KQIhiNxs31FpfmzY3tDFoT2BguA=">AAACQXicbZA7T8MwFIUd3pRXgZElokJiQFWCEDAiWBhBolCpCZXj3hQLx4nsm4oq8l9j4R+wsbMwgBArC04pb65k6/M598r2iTLBNXrenTMyOjY+MTk1XZmZnZtfqC4uneo0VwwaLBWpakZUg+ASGshRQDNTQJNIwFl0eVD6Zz1QmqfyBPsZhAntSh5zRtFK7WozQLjCKC5aJkgoXljqtX2z8XXYNAHrpPht+7Sk+ZgOzXnxwRK6xlTa1ZpX9wbl/gV/CDUyrKN29TbopCxPQCITVOuW72UYFlQhZwJMJcg1ZJRd0i60LEqagA6LQQLGXbNKx41TZZdEd6B+nyhoonU/iWxn+Xr92yvF/7xWjvFuWHCZ5QiSvV8U58LF1C3jdDtcAUPRt0CZ4vatLrugijK0oZch+L+//BdON+v+dt0/3qrt7Q/jmCIrZJWsE5/skD1ySI5IgzByTe7JI3lybpwH59l5eW8dcYYzy+RHOa9vvJaz9g==</latexit>[v1,v2···vn]neg<latexit sha1_base64="KQIhiNxs31FpfmzY3tDFoT2BguA=">AAACQXicbZA7T8MwFIUd3pRXgZElokJiQFWCEDAiWBhBolCpCZXj3hQLx4nsm4oq8l9j4R+wsbMwgBArC04pb65k6/M598r2iTLBNXrenTMyOjY+MTk1XZmZnZtfqC4uneo0VwwaLBWpakZUg+ASGshRQDNTQJNIwFl0eVD6Zz1QmqfyBPsZhAntSh5zRtFK7WozQLjCKC5aJkgoXljqtX2z8XXYNAHrpPht+7Sk+ZgOzXnxwRK6xlTa1ZpX9wbl/gV/CDUyrKN29TbopCxPQCITVOuW72UYFlQhZwJMJcg1ZJRd0i60LEqagA6LQQLGXbNKx41TZZdEd6B+nyhoonU/iWxn+Xr92yvF/7xWjvFuWHCZ5QiSvV8U58LF1C3jdDtcAUPRt0CZ4vatLrugijK0oZch+L+//BdON+v+dt0/3qrt7Q/jmCIrZJWsE5/skD1ySI5IgzByTe7JI3lybpwH59l5eW8dcYYzy+RHOa9vvJaz9g==</latexit>[v1,v2···vn]neg\n<latexit sha1_base64="KQIhiNxs31FpfmzY3tDFoT2BguA=">AAACQXicbZA7T8MwFIUd3pRXgZElokJiQFWCEDAiWBhBolCpCZXj3hQLx4nsm4oq8l9j4R+wsbMwgBArC04pb65k6/M598r2iTLBNXrenTMyOjY+MTk1XZmZnZtfqC4uneo0VwwaLBWpakZUg+ASGshRQDNTQJNIwFl0eVD6Zz1QmqfyBPsZhAntSh5zRtFK7WozQLjCKC5aJkgoXljqtX2z8XXYNAHrpPht+7Sk+ZgOzXnxwRK6xlTa1ZpX9wbl/gV/CDUyrKN29TbopCxPQCITVOuW72UYFlQhZwJMJcg1ZJRd0i60LEqagA6LQQLGXbNKx41TZZdEd6B+nyhoonU/iWxn+Xr92yvF/7xWjvFuWHCZ5QiSvV8U58LF1C3jdDtcAUPRt0CZ4vatLrugijK0oZch+L+//BdON+v+dt0/3qrt7Q/jmCIrZJWsE5/skD1ySI5IgzByTe7JI3lybpwH59l5eW8dcYYzy+RHOa9vvJaz9g==</latexit>[v1,v2···vn]neg<latexit sha1_base64="KQIhiNxs31FpfmzY3tDFoT2BguA=">AAACQXicbZA7T8MwFIUd3pRXgZElokJiQFWCEDAiWBhBolCpCZXj3hQLx4nsm4oq8l9j4R+wsbMwgBArC04pb65k6/M598r2iTLBNXrenTMyOjY+MTk1XZmZnZtfqC4uneo0VwwaLBWpakZUg+ASGshRQDNTQJNIwFl0eVD6Zz1QmqfyBPsZhAntSh5zRtFK7WozQLjCKC5aJkgoXljqtX2z8XXYNAHrpPht+7Sk+ZgOzXnxwRK6xlTa1ZpX9wbl/gV/CDUyrKN29TbopCxPQCITVOuW72UYFlQhZwJMJcg1ZJRd0i60LEqagA6LQQLGXbNKx41TZZdEd6B+nyhoonU/iWxn+Xr92yvF/7xWjvFuWHCZ5QiSvV8U58LF1C3jdDtcAUPRt0CZ4vatLrugijK0oZch+L+//BdON+v+dt0/3qrt7Q/jmCIrZJWsE5/skD1ySI5IgzByTe7JI3lybpwH59l5eW8dcYYzy+RHOa9vvJaz9g==</latexit>[v1,v2···vn]neg\n<latexit sha1_base64="KQIhiNxs31FpfmzY3tDFoT2BguA=">AAACQXicbZA7T8MwFIUd3pRXgZElokJiQFWCEDAiWBhBolCpCZXj3hQLx4nsm4oq8l9j4R+wsbMwgBArC04pb65k6/M598r2iTLBNXrenTMyOjY+MTk1XZmZnZtfqC4uneo0VwwaLBWpakZUg+ASGshRQDNTQJNIwFl0eVD6Zz1QmqfyBPsZhAntSh5zRtFK7WozQLjCKC5aJkgoXljqtX2z8XXYNAHrpPht+7Sk+ZgOzXnxwRK6xlTa1ZpX9wbl/gV/CDUyrKN29TbopCxPQCITVOuW72UYFlQhZwJMJcg1ZJRd0i60LEqagA6LQQLGXbNKx41TZZdEd6B+nyhoonU/iWxn+Xr92yvF/7xWjvFuWHCZ5QiSvV8U58LF1C3jdDtcAUPRt0CZ4vatLrugijK0oZch+L+//BdON+v+dt0/3qrt7Q/jmCIrZJWsE5/skD1ySI5IgzByTe7JI3lybpwH59l5eW8dcYYzy+RHOa9vvJaz9g==</latexit>[v1,v2···vn]neg<latexit sha1_base64="KQIhiNxs31FpfmzY3tDFoT2BguA=">AAACQXicbZA7T8MwFIUd3pRXgZElokJiQFWCEDAiWBhBolCpCZXj3hQLx4nsm4oq8l9j4R+wsbMwgBArC04pb65k6/M598r2iTLBNXrenTMyOjY+MTk1XZmZnZtfqC4uneo0VwwaLBWpakZUg+ASGshRQDNTQJNIwFl0eVD6Zz1QmqfyBPsZhAntSh5zRtFK7WozQLjCKC5aJkgoXljqtX2z8XXYNAHrpPht+7Sk+ZgOzXnxwRK6xlTa1ZpX9wbl/gV/CDUyrKN29TbopCxPQCITVOuW72UYFlQhZwJMJcg1ZJRd0i60LEqagA6LQQLGXbNKx41TZZdEd6B+nyhoonU/iWxn+Xr92yvF/7xWjvFuWHCZ5QiSvV8U58LF1C3jdDtcAUPRt0CZ4vatLrugijK0oZch+L+//BdON+v+dt0/3qrt7Q/jmCIrZJWsE5/skD1ySI5IgzByTe7JI3lybpwH59l5eW8dcYYzy+RHOa9vvJaz9g==</latexit>[v1,v2···vn]negFigure 1. Illustration of the key intuition of NegPrompt. For each\nID class, NegPrompt trains a small set of learnable prompts that\nhave negative semantics to the learned positive prompt of the given\nclass. As a result, OOD samples exhibit higher similarity to the\nnegative prompts than the positive prompts.\nDespite the strong zero-shot classification capabilities of\nVLMs, numerous research efforts are put to unleash their\npotential, e.g., by investigating whether models like CLIP\ncan achieve enhanced performance with training on down-\nstream target datasets. This has led to the development of\nvarious techniques to fine-tune CLIP [8, 18], among which\nprompt learning has sparked widespread interest. Prompt\nlearning (or prompt tuning) [19, 39, 50], a methodology\noriginating from natural language processing, focuses on\nlearning the prompt inputs into a large-scale pre-trained net-\nwork, rather than learning or fine-tuning the parameters of\nthe network. In CLIP, a common prompt is ‘ a photo of\na [class name] ’. The aim of prompt learning, e.g., in\napproaches like CoOp [54], is to learn a soft/differentiable\ncontext vector to replace the fixed text prompt like ‘ a\nphoto of a ’, thereby leveraging CLIP’s powerful gen-\neralization in semantic understanding while also fine-tuning\nfor specific target datasets [53, 54].\nHowever, even though prompt learning enhances the tar-\nget dataset perception capabilities of VLMs, it struggles\nwith Out-Of-Distribution (OOD) detection [10, 26]. Un-\nder the OOD detection task, the test set comprises images\nfrom both the training classes – in-distribution (ID) data –\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n17584\nand images from other unknown categories (OOD images).\nVLM-based classification is typically performed by first re-\nplacing the ‘ [class name] ’ with the class label of each\ncategory in a prompt, which is then processed by the text\nencoder of CLIP to obtain the class embedding and assign\nthe class label that has the highest cosine similarity with the\nembedding of a test image from the image encoder of CLIP.\nHowever in OOD detection tasks, the models do not have\naccess to the class names of OOD images, thereby lacking\nthe knowledge about OOD data.\nThis issue, exacerbated by the models’ tendency towards\noverconfidence that often predicts OOD data as ID images\nwith high confidence [34], undermines their ability to effec-\ntively detect OOD images.\nThere have been a number of CLIP-based methods de-\nsigned specifically for VLM-driven OOD detection meth-\nods, but most of them [6, 32, 46] focused on a zero-shot set-\nting where no training data of the target dataset is available.\nDue to the lack of adaptation to the target dataset, they tend\nto detect unusual ID images as OOD data. Among them,\nCLIPN [46], which trains an additional ‘no’ text encoder to\nprovide prompt embeddings of not having specific classes,\nis the best detector, but it relies on a large-scale auxiliary\ndataset to train such a text encoder and it is computationally\nexpensive.\nThe most related work is a very recent approach Lo-\nCoOp [33] that utilizes the training ID data to tune CLIP\nto capture local features of the ID classes in the prompt.\nIt shows substantially improved performance compared to\nzero-shot methods [32], but it may compromise the ID\nclassification accuracy due to an overemphasis on model-\ning local features. LoCoOp also lacks knowledge about\nOOD samples, making it difficult to differentiate boundary\nID/OOD images.\nIn this work, we propose a novel CLIP-based OOD de-\ntection method, named NegPrompt . Inspired by [25], Neg-\nprompt is designed to learn a set of negative prompts , each\nrepresenting a negative connotation of a given ID class la-\nbel, to delineate the boundaries between ID and OOD im-\nages, as shown in Fig. 1. The negative prompts represent\nspecialized ID-class-dependent concepts, guiding the mod-\nels to pay attention to the characteristics that are contrary to\nor disjoint from the ID classes. NegPrompt aims to utilize\nID training data and positive prompts (i.e., the text prompt\nembeddings of ID classes) to learn such negative prompts in\na way to which OOD images exhibits higher similarity than\nID images.\nEssentially, the learned negative prompts have similar se-\nmantics to the text prompts generated from the ‘no’ text en-\ncoder in CLIPN, but NegPrompt presents a fundamentally\ndifferent approach: it capitalizes on the generalization abil-\nity of the CLIP model and learns the negative prompts with\ntraining ID data only, eliminating the reliance on externaldata and the extensive computation overhead as in CLIPN.\nFurthermore, benefiting from the superior generalization\nability of CLIP, our method does not require the exposure\nto all ID classes during training. In other words, the model\nlearns transferable negative prompts by using only a small\nsubset of the ID classes, after which we can obtain the nega-\ntive prompts for the other ID classes by simply replacing the\n‘[class name] ’ in the prompts with the name of those\nunexposed classes. This allows our model to work in open-\nvocabulary [7, 31] learning settings, where the models are\nrequired to classify images of novel classes that are not seen\nduring training, in addition to a set of training base classes.\nOur main contributions can be summarized as follows:\n• We propose a prompt learning-based OOD detection ap-\nproach NegPrompt, which is able to learn negative se-\nmantics relative to specific ID classes, thereby enhanc-\ning the VLMs’ sensitivity to unknown samples. It is a\nlightweight method that does not require training extra\nencoders on external data as in related methods [46].\n• NegPrompt possesses an open-vocabulary capability due\nto the transferability of its negative prompts. This means\nthat with training images from just a small subset of ID\nclasses and the class names of all IDs, we can achieve\nOOD detection on test data with all these ID classes. To\nthe best of our knowledge, there have been no previous\nfine-tuning methods exploring such a capability.\n• Extensive experiments on multiple ImageNet-based\nbenchmarks show that NegPrompt consistently outper-\nforms current state-of-the-art methods in both conven-\ntional and hard OOD Detection settings.\n2. Related Work\n2.1. Pre-trained Vision-Language Models\nUnderstanding the semantic information of images remains\na significant challenge in the field of computer vision. With\nthe advent of Transformer [44] in computer vision tasks [5],\nCLIP [37] has been introduced as one of the most advanced\npre-trained VLMs. Utilizing contrastive learning [21],\nlarge-scale models and datasets [38], CLIP employs image-\ntext pairs as the training data for self-supervised learning.\nThis approach has successfully trained the model to align\nvisual and text signals in a latent space. Concurrently, other\nresearchers [1, 24, 49] have also shown the remarkable gen-\neralization capabilities of CLIP and similar vision-language\nmodels [17] in various downstream tasks [7, 20, 56].\n2.2. Prompt Learning\nThe concept of prompt learning was initially focused on au-\ntomating the creation of templates/prompts for extracting\nknowledge from Bert [4] or GPT [36]. To bypass man-\nual creation, prompt learning advocates the use of super-\nvised learning to automate the prompt development. [39]\nproposed a gradient-based approach for identifying the\n17585\nbest prompt, establishing the foundation of prompt learn-\ning. Later, CoOp [54] integrated prompt learning into\ncomputer vision. CoOp learns a segment of the context\nbefore it is fed into the text encoder of CLIP, thus tai-\nloring the learned prompt to the specific target dataset.\nMany other prompt learning methods for different vision\ntasks [13, 18, 20, 40, 47, 53, 55] are subsequently intro-\nduced. However, they are not designed for OOD detection,\nso they struggle with dealing with the unknown OOD sam-\nples during inference.\n2.3. Out-of-Distribution Detection\nOOD detection is committed to identifying images in im-\nage classification tasks that belong to categories not present\nin the training dataset, typically originating from a differ-\nent distribution. While traditional OOD detection meth-\nods often tackle the problem by either exploiting the pre-\ndiction logits to define OOD scores [10, 12, 15, 26, 27] or\nfocusing on the class-agnostic information in feature space\nthat is not recoverable from logits [41, 45], recent meth-\nods [11, 14, 22, 28, 30, 42, 52] introduce extra or synthetic\nOOD data, employing fine-tuning to elevate their model’s\nsensitivity towards unknown classes.\nWith the introduction of large pre-trained VLMs, OOD\ndetection has embarked on a new trajectory driven by\nVLMs. MCM [32] aims to integrate the idea of maxi-\nmum softmax probability [10] into the inference process\nof CLIP, while ZOC [6] enhances OOD detection in a\nzero-shot setting by learning an additional image inter-\npreter and guessing the category of images. CLIPN [46]\nand LoCoOp [33], the most related methods to ours, are\nbased on text prompts. However, CLIPN, during its pre-\ntraining phase, trains an additional negative text encoder us-\ning a large external dataset to improve its negative seman-\ntic prompt, which increases network parameters and devi-\nates from prompt learning that is focused on tuning the tar-\nget data. LoCoOp, on the other hand, uses prompt learn-\ning for matching text and image local features, which can\ncompromise the global perception capability of CLIP and\nreduce classification accuracy for in-distribution (ID) sam-\nples. Also, its model lacks knowledge about OOD samples,\nwhich can often lead to high detection errors.\n3. Method\nIn this paper, we propose an approach named NegPropmt\nthat leverages pre-trained VLMs, specifically CLIP, to learn\nnegative prompts relative to ID classes for the purpose of\nOOD detection. The negative prompts are learned in the\nCLIP’s text-image-aligned embedding space with the sup-\nport of training ID data and their positive prompts ( i.e.,\nprompt embeddings of ID classes); no external outlier data\nis required. Due to its general effectiveness, the popular\nprompt learning method CoOp [54] is used by default to\nprovide the positive prompts for training NegPrompt.3.1. Preliminaries\nProblem Statement. Formally, we assume that we have\ntwo datasets, namely ID dataset denoted as Dinand OOD\ndataset denoted as Dout. The ID dataset consists of image-\nlabel pairs (xin, yin), where yin∈Yin={0,1,2,3...k}\nbelong to the ID class set. Similarly, the OOD dataset con-\ntains image-label pairs (xout, yout), but all yout∈Yout=\n{k+ 1, k+ 2, ...}belong to the OOD class set. It is im-\nportant to note that these two sets do not intersect, meaning\nthatYin∩Yout=∅. As we have a test set Xtestconsists\nof images from ID and OOD, the goal of OOD detection\nis to train a classifier ϕ(x)that takes an image xas input\nand returns whether the image belongs to OOD. Unlike ex-\nisting studies using full-/zero-shot ID training samples, we\nonly use a few samples (16 samples per class) for Din\ntrain\nlike [33] does. In the open-vocabulary detection setting, we\nonly use a small part (10%) but not all the classes of Din\ntrain .\nCLIP and CoOp. CLIP [37] is currently one of the most\npopular image-text models. During the pre-training phase,\nit uses large-scale image-text pairs for self-supervised\ncontrastive learning, aligning images and texts into the\nsame latent space. The main components of CLIP are\nan image encoder Encoderimage(I)and a text encoder\nEncodertext(T), which respectively accept image and text\ninputs. In zero-shot image classification tasks, assume\nwe have kclass labels for classification, such as “cat”,\n“dog”, etc., CLIP first incorporates the class labels into\npre-designed hard/unlearnable text prompts, such as “ a\nphoto of a [class name] ”, forming a prompt in-\nput set of “ a photo of a cat ”, “a photo of a\ndog” and so on. These prompts are then individually fed\ninto the text encoder Encodertext(T)to obtain ktext fea-\ntures Tf. The testing image is then input into the image\nencoder to obtain an image feature If. The cosine similar-\nity is calculated between the normalized image feature and\nall text features, formally, Sim(Tf, If) =Tf·If, and the\ntext feature Tfwith the highest similarity to Ifis consid-\nered to be the category to which the image belongs.\nBesides zero-shot classification, many have explored\nways to improve CLIP’s performance when the target data\nis accessible. CoOp [54] introduces prompt learning with\nCLIP by freezing its encoders and using backpropagation\nto learn dataset-specific soft/learnable prompts. The text\nprompts are represented as ti={ωpos\n1, ωpos\n2, ..., ωpos\nn, ci},\nwhere ciis the word embedding of the class name and ωposs\nare learnable vectors that have positive semantics w.r.t. the\nclass i. The goal is to optimize these ωpositives. Specifi-\ncally, tiis processed by Encodertextto yield Tf,pos\ni as a\npositive prompt embedding of the class i, and then the pre-\ndiction probability is computed as:\np(y=i|x) =exp(sim(Tf,pos\ni, If)/τ)\nPk\nj=1exp(sim(Tf,pos\nj, If)/τ),(1)\n17586\nText\nEncoder[Class Label]dogcar<latexit sha1_base64="krSO5P2xGYK11gnet8qGuAcGMv0=">AAAB+nicbVDLSgMxFL1TX7W+prp0EyyCqzIjoi6LblxWsA9oh5LJZNrQTDIkGaXUfoobF4q49Uvc+Tem7Sxq64F7OZxzL7k5YcqZNp734xTW1jc2t4rbpZ3dvf0Dt3zY1DJThDaI5FK1Q6wpZ4I2DDOctlNFcRJy2gqHt1O/9UiVZlI8mFFKgwT3BYsZwcZKPbfcJZE0aKH33IpX9WZAq8TPSQVy1HvudzeSJEuoMIRjrTu+l5pgjJVhhNNJqZtpmmIyxH3asVTghOpgPDt9gk6tEqFYKlvCoJm6uDHGidajJLSTCTYDvexNxf+8Tmbi62DMRJoZKsj8oTjjyEg0zQFFTFFi+MgSTBSztyIywAoTY9Mq2RD85S+vkuZ51b+s+vcXldpNHkcRjuEEzsCHK6jBHdShAQSe4AXe4N15dl6dD+dzPlpw8p0j+APn6xfATJOt</latexit>···busTokenizer<latexit sha1_base64="l9yZhi4EFkgrrt1ORlYy+R3Qlhw=">AAAB6HicbVBNS8NAEJ34WetX1aOXxSIIQklE1GPRi8cW7Ae0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+Oyura+sbm4Wt4vbO7t5+6eCwqeNUMWywWMSqHVCNgktsGG4EthOFNAoEtoLR3dRvPaHSPJYPZpygH9GB5CFn1Fipft4rld2KOwNZJl5OypCj1it9dfsxSyOUhgmqdcdzE+NnVBnOBE6K3VRjQtmIDrBjqaQRaj+bHTohp1bpkzBWtqQhM/X3REYjrcdRYDsjaoZ60ZuK/3md1IQ3fsZlkhqUbL4oTAUxMZl+TfpcITNibAllittbCRtSRZmx2RRtCN7iy8ukeVHxripe/bJcvc3jKMAxnMAZeHANVbiHGjSAAcIzvMKb8+i8OO/Ox7x1xclnjuAPnM8fc32MtQ==</latexit>+\n<latexit sha1_base64="98GFsDGNARcto84kAzgcKolUg7s=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi94q2g9oQ9lsN+3SzSbsToQS+hO8eFDEq7/Im//GbZuDtj4YeLw3w8y8IJHCoOt+O4WV1bX1jeJmaWt7Z3evvH/QNHGqGW+wWMa6HVDDpVC8gQIlbyea0yiQvBWMbqZ+64lrI2L1iOOE+xEdKBEKRtFKD3e9sFeuuFV3BrJMvJxUIEe9V/7q9mOWRlwhk9SYjucm6GdUo2CST0rd1PCEshEd8I6likbc+Nns1Ak5sUqfhLG2pZDM1N8TGY2MGUeB7YwoDs2iNxX/8zophld+JlSSIldsvihMJcGYTP8mfaE5Qzm2hDIt7K2EDammDG06JRuCt/jyMmmeVb2Lqnd/Xqld53EU4QiO4RQ8uIQa3EIdGsBgAM/wCm+OdF6cd+dj3lpw8plD+APn8wcWYo2s</latexit>If<latexit sha1_base64="krSO5P2xGYK11gnet8qGuAcGMv0=">AAAB+nicbVDLSgMxFL1TX7W+prp0EyyCqzIjoi6LblxWsA9oh5LJZNrQTDIkGaXUfoobF4q49Uvc+Tem7Sxq64F7OZxzL7k5YcqZNp734xTW1jc2t4rbpZ3dvf0Dt3zY1DJThDaI5FK1Q6wpZ4I2DDOctlNFcRJy2gqHt1O/9UiVZlI8mFFKgwT3BYsZwcZKPbfcJZE0aKH33IpX9WZAq8TPSQVy1HvudzeSJEuoMIRjrTu+l5pgjJVhhNNJqZtpmmIyxH3asVTghOpgPDt9gk6tEqFYKlvCoJm6uDHGidajJLSTCTYDvexNxf+8Tmbi62DMRJoZKsj8oTjjyEg0zQFFTFFi+MgSTBSztyIywAoTY9Mq2RD85S+vkuZ51b+s+vcXldpNHkcRjuEEzsCHK6jBHdShAQSe4AXe4N15dl6dD+dzPlpw8p0j+APn6xfATJOt</latexit>···\nimage\nfeatureslearned positive prompt\n\nEnforce positive and negative\ntext features not too far<latexit sha1_base64="os1LA9GpKIWQzKWa3ZUY5OmdQUI=">AAAB+3icbVDLSsNAFL2pr1pfsS7dDBbBVUlE1GVRFy5EKtgHtCFMptN26GQSZiZiCfkVNy4UceuPuPNvnLRZaPXAwOGce7lnThBzprTjfFmlpeWV1bXyemVjc2t7x96ttlWUSEJbJOKR7AZYUc4EbWmmOe3GkuIw4LQTTC5zv/NApWKRuNfTmHohHgk2ZARrI/l2FfVDrMcE8/Qm89Pb5lXm2zWn7syA/hK3IDUo0PTtz/4gIklIhSYcK9VznVh7KZaaEU6zSj9RNMZkgke0Z6jAIVVeOsueoUOjDNAwkuYJjWbqz40Uh0pNw8BM5kHVopeL/3m9RA/PvZSJONFUkPmhYcKRjlBeBBowSYnmU0MwkcxkRWSMJSba1FUxJbiLX/5L2sd197Tu3p3UGhdFHWXYhwM4AhfOoAHX0IQWEHiEJ3iBVyuznq03630+WrKKnT34BevjG6/BlDg=</latexit>LNPD\nDistribute the probability\ndistribution evenly<latexit sha1_base64="OHOj4AtWE0KT+4t9maEsWkaj27s=">AAAB+3icbVDLSsNAFL2pr1pfsS7dDBbBVUlE1GXRjYJIRfuANoTJdNIOnTyYmYgl5FfcuFDErT/izr9x0mahrQcGDufcyz1zvJgzqSzr2ygtLa+srpXXKxubW9s75m61LaNEENoiEY9E18OSchbSlmKK024sKA48Tjve+DL3O49USBaFD2oSUyfAw5D5jGClJdeson6A1Yhgnt5kbnp7fZ+5Zs2qW1OgRWIXpAYFmq751R9EJAloqAjHUvZsK1ZOioVihNOs0k8kjTEZ4yHtaRrigEonnWbP0KFWBsiPhH6hQlP190aKAykngacn86By3svF/7xeovxzJ2VhnCgaktkhP+FIRSgvAg2YoETxiSaYCKazIjLCAhOl66roEuz5Ly+S9nHdPq3bdye1xkVRRxn24QCOwIYzaMAVNKEFBJ7gGV7hzciMF+Pd+JiNloxiZw/+wPj8AbvilEA=</latexit>LNIS\n❄ Frozen Parameters\n🔥 Learnable Parameters\n<latexit sha1_base64="krSO5P2xGYK11gnet8qGuAcGMv0=">AAAB+nicbVDLSgMxFL1TX7W+prp0EyyCqzIjoi6LblxWsA9oh5LJZNrQTDIkGaXUfoobF4q49Uvc+Tem7Sxq64F7OZxzL7k5YcqZNp734xTW1jc2t4rbpZ3dvf0Dt3zY1DJThDaI5FK1Q6wpZ4I2DDOctlNFcRJy2gqHt1O/9UiVZlI8mFFKgwT3BYsZwcZKPbfcJZE0aKH33IpX9WZAq8TPSQVy1HvudzeSJEuoMIRjrTu+l5pgjJVhhNNJqZtpmmIyxH3asVTghOpgPDt9gk6tEqFYKlvCoJm6uDHGidajJLSTCTYDvexNxf+8Tmbi62DMRJoZKsj8oTjjyEg0zQFFTFFi+MgSTBSztyIywAoTY9Mq2RD85S+vkuZ51b+s+vcXldpNHkcRjuEEzsCHK6jBHdShAQSe4AXe4N15dl6dD+dzPlpw8p0j+APn6xfATJOt</latexit>···\n🔥<latexit sha1_base64="krSO5P2xGYK11gnet8qGuAcGMv0=">AAAB+nicbVDLSgMxFL1TX7W+prp0EyyCqzIjoi6LblxWsA9oh5LJZNrQTDIkGaXUfoobF4q49Uvc+Tem7Sxq64F7OZxzL7k5YcqZNp734xTW1jc2t4rbpZ3dvf0Dt3zY1DJThDaI5FK1Q6wpZ4I2DDOctlNFcRJy2gqHt1O/9UiVZlI8mFFKgwT3BYsZwcZKPbfcJZE0aKH33IpX9WZAq8TPSQVy1HvudzeSJEuoMIRjrTu+l5pgjJVhhNNJqZtpmmIyxH3asVTghOpgPDt9gk6tEqFYKlvCoJm6uDHGidajJLSTCTYDvexNxf+8Tmbi62DMRJoZKsj8oTjjyEg0zQFFTFFi+MgSTBSztyIywAoTY9Mq2RD85S+vkuZ51b+s+vcXldpNHkcRjuEEzsCHK6jBHdShAQSe4AXe4N15dl6dD+dzPlpw8p0j+APn6xfATJOt</latexit>···\n<latexit sha1_base64="S9thO0d7WpslZZKs2eluMwHd3Yk=">AAAB9XicbVDLSgNBEOz1GeMr6tHLYBA8hV0R9Rj04jGCeUCyCbOT2WTIPJaZWSUs+Q8vHhTx6r9482+cJHvQxIKGoqqb7q4o4cxY3//2VlbX1jc2C1vF7Z3dvf3SwWHDqFQTWieKK92KsKGcSVq3zHLaSjTFIuK0GY1up37zkWrDlHyw44SGAg8kixnB1kndjhJ0gHtBN0uUmfRKZb/iz4CWSZCTMuSo9Upfnb4iqaDSEo6NaQd+YsMMa8sIp5NiJzU0wWSEB7TtqMSCmjCbXT1Bp07po1hpV9Kimfp7IsPCmLGIXKfAdmgWvan4n9dObXwdZkwmqaWSzBfFKUdWoWkEqM80JZaPHcFEM3crIkOsMbEuqKILIVh8eZk0zivBZSW4vyhXb/I4CnAMJ3AGAVxBFe6gBnUgoOEZXuHNe/JevHfvY9664uUzR/AH3ucPuFWSqQ==</latexit>!pos1<latexit sha1_base64="dDvjCW8HYg8XZ3hJRhyid2vpWHw=">AAAB9XicbVDLSgNBEOyNrxhfUY9eBoPgKewGUY9BLx4jmAckmzA7mSRD5rHMzCphyX948aCIV//Fm3/jJNmDJhY0FFXddHdFMWfG+v63l1tb39jcym8Xdnb39g+Kh0cNoxJNaJ0ornQrwoZyJmndMstpK9YUi4jTZjS+nfnNR6oNU/LBTmIaCjyUbMAItk7qdpSgQ9yrdNNYmWmvWPLL/hxolQQZKUGGWq/41ekrkggqLeHYmHbgxzZMsbaMcDotdBJDY0zGeEjbjkosqAnT+dVTdOaUPhoo7UpaNFd/T6RYGDMRkesU2I7MsjcT//PaiR1chymTcWKpJItFg4Qjq9AsAtRnmhLLJ45gopm7FZER1phYF1TBhRAsv7xKGpVycFkO7i9K1ZssjjycwCmcQwBXUIU7qEEdCGh4hld48568F+/d+1i05rxs5hj+wPv8Abnfkqo=</latexit>!pos2<latexit sha1_base64="Y13Ng0C5G5XLuKm3sVrm6UzW8uo=">AAAB9XicbVDLSgNBEOz1GeMr6tHLYBA8hV0R9Rj04jGCeUCyCbOT2WTIPJaZWSUs+Q8vHhTx6r9482+cJHvQxIKGoqqb7q4o4cxY3//2VlbX1jc2C1vF7Z3dvf3SwWHDqFQTWieKK92KsKGcSVq3zHLaSjTFIuK0GY1up37zkWrDlHyw44SGAg8kixnB1kndjhJ0gHuymyXKTHqlsl/xZ0DLJMhJGXLUeqWvTl+RVFBpCcfGtAM/sWGGtWWE00mxkxqaYDLCA9p2VGJBTZjNrp6gU6f0Uay0K2nRTP09kWFhzFhErlNgOzSL3lT8z2unNr4OMyaT1FJJ5ovilCOr0DQC1GeaEsvHjmCimbsVkSHWmFgXVNGFECy+vEwa55XgshLcX5SrN3kcBTiGEziDAK6gCndQgzoQ0PAMr/DmPXkv3rv3MW9d8fKZI/gD7/MHFkaS5g==</latexit>!posn\n❄\nImage\nEncoder\n❄\n❄<latexit sha1_base64="krSO5P2xGYK11gnet8qGuAcGMv0=">AAAB+nicbVDLSgMxFL1TX7W+prp0EyyCqzIjoi6LblxWsA9oh5LJZNrQTDIkGaXUfoobF4q49Uvc+Tem7Sxq64F7OZxzL7k5YcqZNp734xTW1jc2t4rbpZ3dvf0Dt3zY1DJThDaI5FK1Q6wpZ4I2DDOctlNFcRJy2gqHt1O/9UiVZlI8mFFKgwT3BYsZwcZKPbfcJZE0aKH33IpX9WZAq8TPSQVy1HvudzeSJEuoMIRjrTu+l5pgjJVhhNNJqZtpmmIyxH3asVTghOpgPDt9gk6tEqFYKlvCoJm6uDHGidajJLSTCTYDvexNxf+8Tmbi62DMRJoZKsj8oTjjyEg0zQFFTFFi+MgSTBSztyIywAoTY9Mq2RD85S+vkuZ51b+s+vcXldpNHkcRjuEEzsCHK6jBHdShAQSe4AXe4N15dl6dD+dzPlpw8p0j+APn6xfATJOt</latexit>···\n❄\n🔥\n<latexit sha1_base64="krSO5P2xGYK11gnet8qGuAcGMv0=">AAAB+nicbVDLSgMxFL1TX7W+prp0EyyCqzIjoi6LblxWsA9oh5LJZNrQTDIkGaXUfoobF4q49Uvc+Tem7Sxq64F7OZxzL7k5YcqZNp734xTW1jc2t4rbpZ3dvf0Dt3zY1DJThDaI5FK1Q6wpZ4I2DDOctlNFcRJy2gqHt1O/9UiVZlI8mFFKgwT3BYsZwcZKPbfcJZE0aKH33IpX9WZAq8TPSQVy1HvudzeSJEuoMIRjrTu+l5pgjJVhhNNJqZtpmmIyxH3asVTghOpgPDt9gk6tEqFYKlvCoJm6uDHGidajJLSTCTYDvexNxf+8Tmbi62DMRJoZKsj8oTjjyEg0zQFFTFFi+MgSTBSztyIywAoTY9Mq2RD85S+vkuZ51b+s+vcXldpNHkcRjuEEzsCHK6jBHdShAQSe4AXe4N15dl6dD+dzPlpw8p0j+APn6xfATJOt</latexit>···\n<latexit sha1_base64="X8WVPY0U3GQk+wDeysSOXdw59Fc=">AAAB+3icbVBNS8NAEN3Ur1q/Yj16CRbBg5RERD0WvXisYD+gjWGznbRLN7thdyOWkL/ixYMiXv0j3vw3btsctPXBwOO9GWbmhQmjSrvut1VaWV1b3yhvVra2d3b37P1qW4lUEmgRwYTshlgBoxxammoG3UQCjkMGnXB8M/U7jyAVFfxeTxLwYzzkNKIEayMFdrUvYhjih4zDMA8y79TLA7vm1t0ZnGXiFaSGCjQD+6s/ECSNgWvCsFI9z020n2GpKWGQV/qpggSTMR5Cz1COY1B+Nrs9d46NMnAiIU1x7czU3xMZjpWaxKHpjLEeqUVvKv7n9VIdXfkZ5UmqgZP5oihljhbONAhnQCUQzSaGYCKpudUhIywx0SauignBW3x5mbTP6t5F3bs7rzWuizjK6BAdoRPkoUvUQLeoiVqIoCf0jF7Rm5VbL9a79TFvLVnFzAH6A+vzB77+lD8=</latexit>!neg1,1<latexit sha1_base64="xb5lE+MSr79TVSD4bs90JnwXGgk=">AAAB+3icbVBNS8NAEN34WetXrEcvi0XwICUpoh6LXjxWsB/QxrDZTtqlm03Y3Ygl5K948aCIV/+IN/+N2zYHbX0w8Hhvhpl5QcKZ0o7zba2srq1vbJa2yts7u3v79kGlreJUUmjRmMeyGxAFnAloaaY5dBMJJAo4dILxzdTvPIJULBb3epKAF5GhYCGjRBvJtyv9OIIhecgEDHM/c8/quW9XnZozA14mbkGqqEDTt7/6g5imEQhNOVGq5zqJ9jIiNaMc8nI/VZAQOiZD6BkqSATKy2a35/jEKAMcxtKU0Him/p7ISKTUJApMZ0T0SC16U/E/r5fq8MrLmEhSDYLOF4UpxzrG0yDwgEmgmk8MIVQycyumIyIJ1SausgnBXXx5mbTrNfei5t6dVxvXRRwldISO0Sly0SVqoFvURC1E0RN6Rq/ozcqtF+vd+pi3rljFzCH6A+vzB8CDlEA=</latexit>!neg1,2<latexit sha1_base64="B8mfKLODnnqvOd24K9+UBBPLyLw=">AAAB+3icbVBNS8NAEN3Ur1q/Yj16CRbBg5RERD0WvXisYD+gjWGznbRLN7thdyOWkL/ixYMiXv0j3vw3btsctPXBwOO9GWbmhQmjSrvut1VaWV1b3yhvVra2d3b37P1qW4lUEmgRwYTshlgBoxxammoG3UQCjkMGnXB8M/U7jyAVFfxeTxLwYzzkNKIEayMFdrUvYhjih4zDMA8y75TngV1z6+4MzjLxClJDBZqB/dUfCJLGwDVhWKme5ybaz7DUlDDIK/1UQYLJGA+hZyjHMSg/m92eO8dGGTiRkKa4dmbq74kMx0pN4tB0xliP1KI3Ff/zeqmOrvyM8iTVwMl8UZQyRwtnGoQzoBKIZhNDMJHU3OqQEZaYaBNXxYTgLb68TNpnde+i7t2d1xrXRRxldIiO0Any0CVqoFvURC1E0BN6Rq/ozcqtF+vd+pi3lqxi5gD9gfX5Axu+lHw=</latexit>!neg1,n\n<latexit sha1_base64="cYoVl5MAHDFkxuzHA3zwhWWbNzY=">AAAB+3icbVBNS8NAEN34WetXrEcvi0XwICUpoh6LXjxWsB/QxrDZTtqlm03Y3Ygl5K948aCIV/+IN/+N2zYHbX0w8Hhvhpl5QcKZ0o7zba2srq1vbJa2yts7u3v79kGlreJUUmjRmMeyGxAFnAloaaY5dBMJJAo4dILxzdTvPIJULBb3epKAF5GhYCGjRBvJtyv9OIIhecgEDHM/q5+J3LerTs2ZAS8TtyBVVKDp21/9QUzTCISmnCjVc51EexmRmlEOebmfKkgIHZMh9AwVJALlZbPbc3xilAEOY2lKaDxTf09kJFJqEgWmMyJ6pBa9qfif10t1eOVlTCSpBkHni8KUYx3jaRB4wCRQzSeGECqZuRXTEZGEahNX2YTgLr68TNr1mntRc+/Oq43rIo4SOkLH6BS56BI10C1qohai6Ak9o1f0ZuXWi/VufcxbV6xi5hD9gfX5Ax1FlH0=</latexit>!neg2,n\n<latexit sha1_base64="12fowCcFz2rwzN1Qw0nTG4VFIZc=">AAAB+3icbVBNS8NAEN34WetXrEcvwSJ4kJIUUY9FLx4r2A9oY9hsJ+3S3U3Y3Ygl5K948aCIV/+IN/+N2zYHbX0w8Hhvhpl5YcKo0q77ba2srq1vbJa2yts7u3v79kGlreJUEmiRmMWyG2IFjApoaaoZdBMJmIcMOuH4Zup3HkEqGot7PUnA53goaEQJ1kYK7Eo/5jDED5mAYR5k9bN6HthVt+bO4CwTryBVVKAZ2F/9QUxSDkIThpXqeW6i/QxLTQmDvNxPFSSYjPEQeoYKzEH52ez23DkxysCJYmlKaGem/p7IMFdqwkPTybEeqUVvKv7n9VIdXfkZFUmqQZD5oihljo6daRDOgEogmk0MwURSc6tDRlhiok1cZROCt/jyMmnXa95Fzbs7rzauizhK6Agdo1PkoUvUQLeoiVqIoCf0jF7Rm5VbL9a79TFvXbGKmUP0B9bnD8IKlEE=</latexit>!neg2,2\n<latexit sha1_base64="edBl8JdrPKYHCiQEEACUNRXYh7A=">AAAB+3icbVBNS8NAEN34WetXrEcvi0XwICUpoh6LXjxWsB/QxrDZTtqlm03Y3Ygl5K948aCIV/+IN/+N2zYHbX0w8Hhvhpl5QcKZ0o7zba2srq1vbJa2yts7u3v79kGlreJUUmjRmMeyGxAFnAloaaY5dBMJJAo4dILxzdTvPIJULBb3epKAF5GhYCGjRBvJtyv9OIIhecgEDHM/q5+5uW9XnZozA14mbkGqqEDTt7/6g5imEQhNOVGq5zqJ9jIiNaMc8nI/VZAQOiZD6BkqSATKy2a35/jEKAMcxtKU0Him/p7ISKTUJApMZ0T0SC16U/E/r5fq8MrLmEhSDYLOF4UpxzrG0yDwgEmgmk8MIVQycyumIyIJ1SausgnBXXx5mbTrNfei5t6dVxvXRRwldISO0Sly0SVqoFvURC1E0RN6Rq/ozcqtF+vd+pi3rljFzCH6A+vzB8CFlEA=</latexit>!neg2,1<latexit sha1_base64="krSO5P2xGYK11gnet8qGuAcGMv0=">AAAB+nicbVDLSgMxFL1TX7W+prp0EyyCqzIjoi6LblxWsA9oh5LJZNrQTDIkGaXUfoobF4q49Uvc+Tem7Sxq64F7OZxzL7k5YcqZNp734xTW1jc2t4rbpZ3dvf0Dt3zY1DJThDaI5FK1Q6wpZ4I2DDOctlNFcRJy2gqHt1O/9UiVZlI8mFFKgwT3BYsZwcZKPbfcJZE0aKH33IpX9WZAq8TPSQVy1HvudzeSJEuoMIRjrTu+l5pgjJVhhNNJqZtpmmIyxH3asVTghOpgPDt9gk6tEqFYKlvCoJm6uDHGidajJLSTCTYDvexNxf+8Tmbi62DMRJoZKsj8oTjjyEg0zQFFTFFi+MgSTBSztyIywAoTY9Mq2RD85S+vkuZ51b+s+vcXldpNHkcRjuEEzsCHK6jBHdShAQSe4AXe4N15dl6dD+dzPlpw8p0j+APn6xfATJOt</latexit>···learnable negative prompts\n\nLearn diverse, non-overlapping\n negative prompts<latexit sha1_base64="ysNe6n4E+he89hOLYgCD5XfHE3k=">AAAB+3icbVDLSsNAFL2pr1pfsS7dDBbBVUlE1GVRFy6kVLAPaEOYTCft0MmDmYlYQn7FjQtF3Poj7vwbJ20W2npg4HDOvdwzx4s5k8qyvo3Syura+kZ5s7K1vbO7Z+5XOzJKBKFtEvFI9DwsKWchbSumOO3FguLA47TrTa5zv/tIhWRR+KCmMXUCPAqZzwhWWnLNKhoEWI0J5uld5qbN5k3mmjWrbs2AloldkBoUaLnm12AYkSSgoSIcS9m3rVg5KRaKEU6zyiCRNMZkgke0r2mIAyqddJY9Q8daGSI/EvqFCs3U3xspDqScBp6ezIPKRS8X//P6ifIvnZSFcaJoSOaH/IQjFaG8CDRkghLFp5pgIpjOisgYC0yUrquiS7AXv7xMOqd1+7xu35/VGldFHWU4hCM4ARsuoAG30II2EHiCZ3iFNyMzXox342M+WjKKnQP4A+PzB6y1lDY=</latexit>LNND\n<latexit sha1_base64="qnaZBL5VQEk10m0NMw/pplbC56o=">AAAB/XicbVDLSsNAFJ3UV62v+Ni5CRbBVUlE1GXRjcsKfUEbw2QyaYdOMmHmRqwh+CtuXCji1v9w5984bbPQ1gMXDufcy733+AlnCmz72ygtLa+srpXXKxubW9s75u5eW4lUEtoiggvZ9bGinMW0BQw47SaS4sjntOOPrid+555KxUTchHFC3QgPYhYygkFLnnnQvMsSoXIv6wN9gCwQgzz3zKpds6ewFolTkCoq0PDMr34gSBrRGAjHSvUcOwE3wxIY4TSv9FNFE0xGeEB7msY4osrNptfn1rFWAisUUlcM1lT9PZHhSKlx5OvOCMNQzXsT8T+vl0J46WYsTlKgMZktClNugbAmUVgBk5QAH2uCiWT6VosMscQEdGAVHYIz//IiaZ/WnPOac3tWrV8VcZTRITpCJ8hBF6iOblADtRBBj+gZvaI348l4Md6Nj1lryShm9tEfGJ8/1bKWHw==</latexit>Tposdog<latexit sha1_base64="+BAXpZgUU4Ay0ChMV5OLc/2Xf84=">AAAB/XicbVDLSsNAFJ3UV62v+Ni5CRbBVUlE1GXRjcsKfUEbw2Q6aYdOZsLMjVhD8FfcuFDErf/hzr9x+lho64ELh3Pu5d57woQzDa77bRWWlldW14rrpY3Nre0de3evqWWqCG0QyaVqh1hTzgRtAANO24miOA45bYXD67HfuqdKMynqMEqoH+O+YBEjGIwU2Af1uyyROg+yLtAHyAhWeR7YZbfiTuAsEm9GymiGWmB/dXuSpDEVQDjWuuO5CfgZVsAIp3mpm2qaYDLEfdoxVOCYaj+bXJ87x0bpOZFUpgQ4E/X3RIZjrUdxaDpjDAM9743F/7xOCtGlnzGRpEAFmS6KUu6AdMZROD2mKAE+MgQTxcytDhlghQmYwEomBG/+5UXSPK145xXv9qxcvZrFUUSH6AidIA9doCq6QTXUQAQ9omf0it6sJ+vFerc+pq0Fazazj/7A+vwBz4qWGw==</latexit>Tposcar<latexit sha1_base64="XNXsmM2l/TFdEfo3KuB3ZeYxu0Q=">AAAB/XicbVDLSsNAFJ3UV62v+Ni5CRbBVUlE1GXRjcsKfUEbw2Q6aYdOZsLMjVhD8FfcuFDErf/hzr9x+lho64ELh3Pu5d57woQzDa77bRWWlldW14rrpY3Nre0de3evqWWqCG0QyaVqh1hTzgRtAANO24miOA45bYXD67HfuqdKMynqMEqoH+O+YBEjGIwU2Af1uyyROg+yLtAHyMJU53lgl92KO4GzSLwZKaMZaoH91e1JksZUAOFY647nJuBnWAEjnOalbqppgskQ92nHUIFjqv1scn3uHBul50RSmRLgTNTfExmOtR7FoemMMQz0vDcW//M6KUSXfsZEkgIVZLooSrkD0hlH4fSYogT4yBBMFDO3OmSAFSZgAiuZELz5lxdJ87TinVe827Ny9WoWRxEdoiN0gjx0garoBtVQAxH0iJ7RK3qznqwX6936mLYWrNnMPvoD6/MH7hSWLw==</latexit>Tposbus<latexit sha1_base64="PU0xpDgPs8E6XXLNiizLTma4m5Y=">AAAB/3icbVBNS8NAEN34WetXVPDiJVgED1ISEfVY9OKxQr+gjWWznbZLN5uwOxFLzMG/4sWDIl79G978N24/Dtr6YODx3gwz84JYcI2u+20tLC4tr6zm1vLrG5tb2/bObk1HiWJQZZGIVCOgGgSXUEWOAhqxAhoGAurB4Hrk1+9BaR7JCg5j8EPak7zLGUUjte39yl0qoZe1U++khfCAaZDoLGvbBbfojuHME29KCmSKctv+anUiloQgkQmqddNzY/RTqpAzAVm+lWiIKRvQHjQNlTQE7afj+zPnyCgdpxspUxKdsfp7IqWh1sMwMJ0hxb6e9Ubif14zwe6ln3IZJwiSTRZ1E+Fg5IzCcDpcAUMxNIQyxc2tDutTRRmayPImBG/25XlSOy1650Xv9qxQuprGkSMH5JAcE49ckBK5IWVSJYw8kmfySt6sJ+vFerc+Jq0L1nRmj/yB9fkDq1mWiA==</latexit>Tneg1,bus<latexit sha1_base64="z3VHxIiqj/VlehU5OHxjEBknDCM=">AAAB/3icbVDLSgNBEJyNrxhfq4IXL4tB8CBhN4h6DHrxGCEvSGKYnXSSIbOzy0yvGNY9+CtePCji1d/w5t84eRw0saChqOqmu8uPBNfout9WZml5ZXUtu57b2Nza3rF392o6jBWDKgtFqBo+1SC4hCpyFNCIFNDAF1D3h9djv34PSvNQVnAUQTugfcl7nFE0Usc+qNwlEvppJymethAeMPFjnaYdO+8W3AmcReLNSJ7MUO7YX61uyOIAJDJBtW56boTthCrkTECaa8UaIsqGtA9NQyUNQLeTyf2pc2yUrtMLlSmJzkT9PZHQQOtR4JvOgOJAz3tj8T+vGWPvsp1wGcUIkk0X9WLhYOiMw3C6XAFDMTKEMsXNrQ4bUEUZmshyJgRv/uVFUisWvPOCd3uWL13N4siSQ3JETohHLkiJ3JAyqRJGHskzeSVv1pP1Yr1bH9PWjDWb2Sd/YH3+AKzplok=</latexit>Tneg2,bus\n<latexit sha1_base64="wm81vzrjK1KIGltNcM8PQa7VGUw=">AAAB/3icbVBNS8NAEN34WetXVfDiJVgED1ISEfVY9OKxQr+grWWznaRLN5uwOxFLzMG/4sWDIl79G978N24/Dtr6YODx3gwz87xYcI2O820tLC4tr6zm1vLrG5tb24Wd3bqOEsWgxiIRqaZHNQguoYYcBTRjBTT0BDS8wfXIb9yD0jySVRzG0AlpILnPGUUjdQv71btUQpB1U/ekjfCAaS8KsqxbKDolZwx7nrhTUiRTVLqFr3YvYkkIEpmgWrdcJ8ZOShVyJiDLtxMNMWUDGkDLUElD0J10fH9mHxmlZ/uRMiXRHqu/J1Iaaj0MPdMZUuzrWW8k/ue1EvQvOymXcYIg2WSRnwgbI3sUht3jChiKoSGUKW5utVmfKsrQRJY3IbizL8+T+mnJPS+5t2fF8tU0jhw5IIfkmLjkgpTJDamQGmHkkTyTV/JmPVkv1rv1MWldsKYze+QPrM8fkveWeA==</latexit>Tneg1,dog<latexit sha1_base64="XEVpJ/rTdyOrL6/Y7b7QH9syl2E=">AAAB/3icbVDLSgNBEJyNrxhfq4IXL4tB8CBhN4h6DHrxGCEvSGKYnXSSIbOzy0yvGNY9+CtePCji1d/w5t84eRw0saChqOqmu8uPBNfout9WZml5ZXUtu57b2Nza3rF392o6jBWDKgtFqBo+1SC4hCpyFNCIFNDAF1D3h9djv34PSvNQVnAUQTugfcl7nFE0Usc+qNwlEvppJymethAeMOmG/TTt2Hm34E7gLBJvRvJkhnLH/mp1QxYHIJEJqnXTcyNsJ1QhZwLSXCvWEFE2pH1oGippALqdTO5PnWOjdJ1eqExJdCbq74mEBlqPAt90BhQHet4bi/95zRh7l+2EyyhGkGy6qBcLB0NnHIbT5QoYipEhlClubnXYgCrK0ESWMyF48y8vklqx4J0XvNuzfOlqFkeWHJIjckI8ckFK5IaUSZUw8kieySt5s56sF+vd+pi2ZqzZzD75A+vzB5SHlnk=</latexit>Tneg2,dog\n<latexit sha1_base64="MAcQeGQh5g2HfnovlVtkaZrGk3o=">AAAB/3icbVBNS8NAEN34WetXVPDiJVgED1ISEfVY9OKxQr+gjWWznbZLN5uwOxFLzMG/4sWDIl79G978N24/Dtr6YODx3gwz84JYcI2u+20tLC4tr6zm1vLrG5tb2/bObk1HiWJQZZGIVCOgGgSXUEWOAhqxAhoGAurB4Hrk1+9BaR7JCg5j8EPak7zLGUUjte39yl0qoZe1U++khfCAKaMqy9p2wS26YzjzxJuSApmi3La/Wp2IJSFIZIJq3fTcGP2UKuRMQJZvJRpiyga0B01DJQ1B++n4/sw5MkrH6UbKlERnrP6eSGmo9TAMTGdIsa9nvZH4n9dMsHvpp1zGCYJkk0XdRDgYOaMwnA5XwFAMDaFMcXOrw/pUUYYmsrwJwZt9eZ7UToveedG7PSuUrqZx5MgBOSTHxCMXpERuSJlUCSOP5Jm8kjfryXqx3q2PSeuCNZ3ZI39gff4AjM+WdA==</latexit>Tneg1,car<latexit sha1_base64="lAJtD1qwdRUyzWCHnZmwdEOs+Nk=">AAAB/3icbVDLSgNBEJyNrxhfq4IXL4tB8CBhN4h6DHrxGCEvSGKYnXSSIbOzy0yvGNY9+CtePCji1d/w5t84eRw0saChqOqmu8uPBNfout9WZml5ZXUtu57b2Nza3rF392o6jBWDKgtFqBo+1SC4hCpyFNCIFNDAF1D3h9djv34PSvNQVnAUQTugfcl7nFE0Usc+qNwlEvppJymethAeMGFUpWnHzrsFdwJnkXgzkiczlDv2V6sbsjgAiUxQrZueG2E7oQo5E5DmWrGGiLIh7UPTUEkD0O1kcn/qHBul6/RCZUqiM1F/TyQ00HoU+KYzoDjQ895Y/M9rxti7bCdcRjGCZNNFvVg4GDrjMJwuV8BQjAyhTHFzq8MGVFGGJrKcCcGbf3mR1IoF77zg3Z7lS1ezOLLkkByRE+KRC1IiN6RMqoSRR/JMXsmb9WS9WO/Wx7Q1Y81m9skfWJ8/jl+WdQ==</latexit>Tneg2,carpositive\ntext featuresnegative\ntext featuresFigure 2. Overview of NegPropmt. Given the CLIP model and positive prompts learned by existing prompt learning methods such as CoOp\n[54], NegPrompt learns a set of negative prompts relative to different ID class labels via three loss functions that enforce the separation\nbetween negative prompts and ID images, and between negative and positive prompts, as well as the diversity of the negative prompts.\nwhere If=Encoderimage(x)andτis a temperature pa-\nrameter. Next, cross-entropy loss is used to maximize simi-\nlarity between ID class text embeddings and images.\nLpositive =Exin∼Din\ntrain[−log(p(y=i|x)]. (2)\nThis method largely improves CLIP’s classification perfor-\nmance by adapting the learnable prompts to the target data.\nThe resulting prompt embeddings of ID classes are used as\npositive prompts to support the accurate learning of nega-\ntive prompts in our method below.\n3.2. Proposed Approach\nNegPrompt aims to learn a set of negative prompts, each\nrepresenting a negative connotation of an ID class. As\nshown in Fig. 2, it involves generating a series of prompts\nthat resemble the positive prompts obtained through CoOp,\nbut with a negative class semantic, such as “A photo of not\na [class label]”. These negative prompts, combined with\nclass labels, can generate a range of negative text features\naround the positive prompts, so that OOD images exhibit\nhigher similarity to the negative prompts than ID images.\n3.2.1 Learning Negative Prompts\nTo acquire accurate negative prompts representing nega-\ntive class semantics, we first utilize CoOp to learn positive\nprompts ωposs, after which we consider positive prompts\nto accurately capture the class semantics of samples in the\nID dataset. Thereafter, we freeze the positive prompts and\nfocus solely on learning the negative prompts. A negative\nprompt is denoted as {ωneg\n1, ωneg\n2, ..., ωneg\nn}, where nrep-\nresents the number of context vectors we aim to learn. Byutilizing the frozen CLIP text encoder, we can obtain neg-\native prompt embeddings Tf,neg\nl,i=Encodertext(tneg\nl,i),\nwhere tneg\nl,i={ωneg\nl,1, ωneg\nl,2, ..., ωneg\nl,n, ci}is the l-th negative\nprompt relative the ID class i. Thus, due to the presence\nof the negative prompts, we turn the prediction probability\ninto the following form:\np(y=i|x) =exp(Sf,pos\ni)\nPk\nj=1exp(Sf,pos\nj) +Pp\nl=1Pk\nj=1exp(Sf,neg\nl,j)\n(3)\nwhere Sf,pos\nj = sim(Tf,pos\nj, If)/τ,Sf,neg\nl,j=\nsim(Tf,neg\nl,j, If)/τandpis the number of negative\nprompts we aim to learn for each ID class. The objective\nis to learn negative text prompt embeddings that can effec-\ntively separate ID and OOD samples around the positive\ntext features. To achieve this, we introduce the following\nthree loss functions:\nNegative-Image Separation Loss. One of our objectives\nis to have the negative text prompt embeddings serve as the\nclosest text features to OOD images. However, we only\nhave images from the ID dataset; no OOD images are avail-\nable. To remedy this problem, we take an alternative ap-\nproach that aims to push the negative text features away\nfrom the ID images. In this regard, we draw inspiration\nfrom OE [11]. Contrary to the approach in OE, where\nthe probability distribution of outlier images is evenly dis-\ntributed among all ID labels, we distribute the probability\ndistribution obtained from ID images evenly among all neg-\native prompts. Since the network parameters are frozen, this\ndrives the negative text features to move away from the ID\n17587\nimages, resulting in the learning of prompts with a negative\nconnotation. The formulation is as follows:\nLNIS=Exin∼Din\ntrain[H(u;F(x))], (4)\nwhere F(x)is the probability vector computed as\nSoftmax (Sf,neg), anduis a uniform distribution and H\nis the cross entropy loss.\nNegative-Positive Distance Loss. To avoid learning trivial\nnegative prompts that are distant from both ID and OOD\nimages, we need to control the negative text feature within\na certain range between the ID and OOD images. To this\nend, we devise a constraint, enforcing that the negative text\nfeature does not deviate too far from the positive text fea-\nture. Therefore, we introduce the following loss function to\nguarantee a certain level of similarity between the negative\nand positive text feature in the latent space:\nLNPD =−1\nk∗pkX\nj=1pX\ni=1sim(Tf,neg\ni,j, Tf,pos\nj).(5)\nNegative-Negative Distance Loss. Furthermore, to ensure\nthat we are effectively learning diverse, non-overlapping\nnegative prompts, we extend the distances between differ-\nent negative text features within the same label via the fol-\nlowing loss function:\nLNND =1\nk∗p∗(p−1)kX\nj=1pX\ni=1X\nl̸=isim(Tf,neg\ni,j, Tf,neg\nl,j).\n(6)\nOverall, our NegPrompt objective consists of the above\nthree losses. During training, we are able to obtain a diverse\nset of non-trivial prompts that effectively convey negative\nmeanings relative to the ID class labels by minimizing the\nfollowing overall loss:\nLNegativePrompts =LNIS+β∗LNPD +γ∗LNND,(7)\nwhere βandγare hyper-parameters to balance the losses.\n3.2.2 Open-Vocabulary Capability\nSince the negative prompts we learn do not depend on spe-\ncific class labels but are instead generic templates represent-\ning negative semantics of any given class labels, it is pos-\nsible to utilize the generalization ability of CLIP to learn a\nset of transferable negative prompts. Specifically, instead of\nutilizing all of the ID classes Din\ntrain , NegPrompt may only\nemploy its small subset Din,sub\ntrain for training the negative\nprompts ωneg, where Din,sub\ntrain ={(x, yin\nsub)|yin\nsub⊂yin}.\nAfter obtaining the trained negative prompts, we combine\nthem with the remaining ID class names, i.e., replace ciin\ntneg\nl,iwith the unseen ID class names, to obtain the corre-\nsponding negative prompts for the novel ID classes that areID OOD\nSplit-1 All dog classes Non-animal classes\nSplit-2 Half of hunting dog classes Other 4-legged animal classes\nSplit-3 Mix of common classes Mix of common classes\nTable 1. Three ImageNet-1K splits for hard OOD detection.\nunseen during training. This approach, which achieves out-\nof-distribution detection by only exposing with a small por-\ntion of ID images, is unprecedented in prior research. We\nrefer this as to be open-vocabulary OOD detection.\n3.2.3 Inference\nDuring inference, we employ the MCM [32] scoring ap-\nproach for OOD detection, but with the addition of our neg-\native prompts into the softmax function. Particularly, MCM\nuses the inverse of the maximum softmax score in Eq. 1 as\nthe OOD score. Our OOD scoring extends MCM and de-\nfines it as: s(x) = max( p(y=i|x)), where p(y=i|x)\nis defined in Eq. 3 that also includes the similarities of the\ntest image to the negative prompts, in addition to the sim-\nilarities to the positive prompts. The rationale behind this\nis that for ID images, they will be matched to one of the\npositive text features, leading to a higher Sf,posbut lower\nSf,neg, and thereby a higher maximum softmax score ( i.e.,\na lower OOD score). Conversely, for OOD data, it will be\nmatched to one of the negative text features, resulting in a\nlower maximum softmax score ( i.e., a higher OOD score).\n4. Experiment\n4.1. Experimental Details\nDatasets. For conventional OOD detection, we use a popu-\nlar benchmark in which ImageNet-1K [3] with 1,000 classes\nis used as the ID dataset, and the same OOD datasets as in\n[32] are used, including subsets of Texture [2], iNatural-\nist [43], Places [51] and SUN [48]. In addressing the more\nchallenging OOD scenarios, we partitioned the ImageNet1k\ndataset into two segments: one segment of the data serves\nas the ID, while the other serves as OOD. As shown in Ta-\nble 1, three different splits are derived, following from [35].\nWe further create another ImageNet split, Split-4, in which\nthe first 100 classes are used as ID data and the subsequent\n900 classes are used as OOD samples. Following CoOp and\nLoCoOp [33, 54], during our training process, we utilized\nonly few-shot training data for each category. Particularly,\nwe only train the model with 16 images per ID class and\nwithout any exposure to OOD images. During testing, we\nemploy the entire ID and OOD test set for evaluation.\nImplementation Details. Following existing studies [46],\nwe use CLIP based on CLIP-B/16 which is pre-trained from\nOpenCLIP [16]. NegPrompt is trained using 16-shot im-\nages of all ID classes under the normal OOD detection set-\nting. For open-vocabulary OOD detection, we train our\nmodel using only the images of the first 10% classes from\nthe ID dataset, withholding 90% ID classes that only appear\n17588\nMethodTexture iNaturalist Places SUN Avg\nAUC↑FPR95 ↓AUC↑FPR95 ↓AUC↑FPR95 ↓AUC↑FPR95 ↓AUC↑FPR95 ↓\nZero-shot methods\nMCM [32]†86.11 57.77 94.61 30.91 89.77 44.69 92.57 34.59 90.76 42.74\nCLIPN [46]†90.93 40.83 95.27 23.94 92.28 33.45 93.92 26.17 93.10 31.10\nCLIP-based posthoc methods\nMSP [10]†74.84 73.66 77.74 74.57 72.18 79.12 73.97 76.95 74.98 76.22\nMaxLogit [12]†88.63 48.72 88.03 60.88 87.45 55.54 91.16 44.83 88.82 52.49\nEnergy [27]†88.22 50.39 87.18 64.98 87.33 57.40 91.17 46.42 88.48 54.80\nReAct [41]†88.13 49.88 86.87 65.57 87.42 56.85 91.04 46.17 88.37 54.62\nODIN [26]†87.85 51.67 94.65 30.22 85.54 55.06 87.17 54.04 88.80 47.75\nPrompt learning methods\nCoOp [54] 89.47 45.00 93.77 29.81 90.58 40.11 93.29 40.83 91.78 51.68\nLoCoOp [33]†90.19 42.28 96.86 16.05 91.98 32.87 95.07 23.44 93.52 28.66\nNegPrompt (Ours) 91.60 35.21 98.73 6.32 93.34 27.60 95.55 22.89 94.81 23.01\nOpen-vocabulary OOD detection\nCoOp (10%) 87.58 50.55 91.08 42.53 89.56 46.12 91.52 41.92 89.94 45.28\nLoCoOp (10%) 88.21 47.32 94.47 34.90 91.64 39.85 92.54 26.30 90.15 37.09\nNegPrompt (Ours) (10%) 90.30 39.31 98.39 7.48 92.68 29.75 93.70 26.92 93.76 25.86\nTable 2. Conventional OOD detection results. We trained using ImageNet1k as the ID and CLIP-B/16 as the CLIP backbone. The boldfaced\nresults indicate the best performance. Results marked with †are taken from [46] and [33]. ‘METHOD’ (10%) in open-vocabulary OOD\ndetection is to evaluate the performance of the ‘METHOD’ when only images from 10% ID classes are accessible during training.\ntogether with OOD data during inference. We train a shared\npositive prompt and two shared negative prompts w.r.t. each\ntraining ID class. The hyperparameters βandγare set to\n0.1 and 0.05, respectively (see Appendix A for detail). In\nthe first stage, CoOp is trained for 100 epochs to obtain the\npositive prompts. In the second stage, the positive prompts\nare frozen, and our model is trained for 10 epochs to learn\nthe negative prompts. For all experiments, we report the av-\neraged results over three runs with different random seeds.\nComparison Methods. To substantiate the effectiveness\nof NegPrompts, we conduct an empirical analysis of three\ndistinct categories of methodologies employed for OOD de-\ntection utilizing Vision-language models. These categories\nencompass zero-shot pretraining approaches, the methods\nthat combine the CLIP image encoder with classical ap-\nproaches, and the methods grounded in prompt learning.\nIn the context of zero-shot methods, we opted for the two\nrecent methods, MCM [32] and CLIPN [46]. MCM em-\nploys the original CLIP, utilizing the maximum softmax\nprobability operation on the similarities for detection, and\nCLIPN involves an additional training phase during pre-\ntraining, specifically training a negative text encoder us-\ning large external data. For the second group of methods,\nwe adapt previous logits-based methodologies to the use of\nthe CLIP image encoder, including MSP [10], Energy [27],\nMaxLogit [12], ReAct [41] and ODIN [26], to serve as the\nCLIP-adapted methods. For the prompt learning methods,\nNegPrompt is compared with CoOp [54] and LoCoOp [33].\nEvaluation Metrics. Two OOD detection metrics are used.\nThe first metric is the False Positive Rate at a 95% True\nNegative Rate (FPR95), which denotes the rate of falsely\nidentified OOD instances when the true negative rate is\nmaintained at 95%. The second metric is the Area Under\nthe Receiver Operating Characteristic curve (AUROC), rep-resenting the measure of OOD ranking across various clas-\nsification thresholds. We also check the classification accu-\nracy of the ID data to evaluate how the OOD detectors affect\nthe ID classification.\n4.2. Comparison to State-of-the-art Models\nConventional OOD Detection. The results of conven-\ntional OOD detection are reported in Table 2. It is clear\nthat our proposed NegPrompt achieves consistently supe-\nrior performance in both individual OOD datasets and the\naveraged results. When compared with the zero-shot meth-\nods, on average, our approach surpasses the best competing\nmethod CLIPN by more than 1.5% in AUC and around 8%\nin FPR95, despite the fact that CLIPN requires the use of an\nadditional large external dataset to train an additional neg-\native text encoder. In other words, although NegPrompt is\nsignificantly more lightweight than CLIPN in model size,\nit can substantially and consistently outperform CLIPN in\nboth metrics across all OOD datasets. The adapted post-hoc\nmethods generally do not leverage the CLIP’s capabilities\nwell and thus perform less effectively.\nNegPrompt also substantially surpasses both prompt\nlearning-based methods, reducing the FPR95 by about 28%\n(CoOp) and 5% (LoCoOp). This indicates that the learned\nnegative prompts provide informed knowledge about OOD\ndata, which is lacking in the competing methods, helping\nlargely reduce detection errors.\nHard OOD Detection. Hard OOD detection presents\nunique challenges as the OOD samples often exhibit some\nsimilar features as the ID samples. The results on the four\nhard OOD datasets derived from ImageNet-1K are shown\nin Table 3. Similar empirical observations can be derived.\nOur method NegPropmt is consistently the best performer\nin the average performance, showcasing its general effec-\n17589\nMethodSplit-1 Split-2 Split-3 Split-4 Avg\nAUC↑FPR95 ↓AUC↑FPR95 ↓AUC↑FPR95 ↓AUC↑FPR95 ↓AUC↑FPR95 ↓\nZero-shot methods\nMCM 97.93 9.17 88.10 56.40 90.34 33.05 98.72 4.73 93.77 25.83\nCLIPN 99.38 2.07 97.77 10.55 90.03 36.85 98.83 4.68 96.50 13.53\nCLIP-based posthoc methods\nMSP 77.85 63.60 68.73 83.63 79.10 70.55 82.40 65.52 77.02 70.83\nMaxLogit 99.87 0.49 98.06 8.69 90.96 34.34 99.35 2.66 97.06 11.55\nEnergy 99.88 0.46 98.18 8.40 90.65 35.02 99.36 2.83 97.02 11.68\nReAct 99.34 0.72 97.91 9.33 90.72 35.65 99.12 2.94 96.77 12.16\nODIN 98.78 1.12 98.23 8.18 89.92 37.20 98.76 13.20 96.42 14.92\nPrompt learning methods\nCoOp 98.53 6.78 88.25 50.76 90.64 33.89 98.54 5.11 93.99 24.14\nLoCoOp 98.64 6.29 84.63 61.09 91.30 27.79 98.83 41.44 93.35 34.15\nNegPrompt (Ours) 99.85 0.62 98.54 7.60 93.89 22.89 99.57 1.60 97.96 8.18\nOpen-vocabulary OOD detection\nCoOp (10%) 97.97 12.217 80.11 74.62 87.92 46.00 96.59 16.60 90.65 37.36\nLoCoOp (10%) 98.00 9.23 87.02 52.18 80.51 59.93 82.41 48.72 86.99 42.52\nNegPrompt (Ours) (10%) 99.66 1.36 96.30 19.89 91.75 26.92 98.14 5.24 96.46 13.36\nTable 3. Hard OOD detection results. We use the same notations here as those used in Table 2.\ntiveness across different dataset splits. The superiority of\nNegPrompt over the zero-shot and prompt learning-based\nmethods is similar to that in Table 2. Although it is slightly\nless effective than Energy under the Split-1 setting, it out-\nperforms Energy in all other metrics, achieving maximally\nover 3% AUC and 12% FPR95 improvement among the\nperformance on the other OOD datasets.\nNote that the results in Table 3 are generally more\npromising than that in Table 2. This is mainly because the\nOOD detection difficulty in all four ImageNet-1K splits is\nlargely reduced since the number of their ID classes is sig-\nnificantly less than that in the full ImageNet-1K data.\nOpen-Vocabulary OOD Detection. The open-vocabulary\nOOD detection results are reported in both Tables 2 and 3.\nImpressively, even when training on only 10% ID classes,\nour approach can still perform better than the competing\nmethods using the full ID classes, e.g., the average AUC\nand FPR95 in Table 2. In this open-vocabulary setting, in\ngeneral, both LoCoOp and CoOp exhibit a much larger per-\nformance decline than NegPrompt, especially on the results\nin Table 3, in which CoOp has over 3% AUC drop and Lo-\nCoOp has over 6% AUC drop while our method has only\nabout 1.5% AUC drop. These results demonstrate that the\nnegative prompts in NegPrompt have much better transfer-\nability than those in the two competing methods.\n4.2.1 Classification Accuracy on ID Data\nWe also evaluate the classification accuracy on the ID data\nwhen using NegPrompt for OOD detection, with CoOp, Lo-\nCoOp, MCM and CLIPN as the baselines. The classifica-\ntion accuracy results on the full ImageNet-1K test data are\nshown in Table 4. When using the full ImageNet-1K train-\ning ID class data, our method NegPrompt can maintain the\nsame classification accuracy as CoOp.\nOur accuracy is slightly compromised when using only\n10% ID classes in our training. On the other hand, the OODMethod Top-1 Accuracy\nCoOp 72.1\nLoCoOp†71.7\nCLIPN & MCM 67.0\nNegPrompt(Ours)(10%) 71.9\nNegPrompt(Ours)(Full) 72.1\nTable 4. Top-1 Accuracy. Results with †are taken from [33].\nID-Pos ID-Neg OOD-Pos OOD-Neg0.200.220.240.260.28Similarity0.281\n0.241\n0.2190.253T exture\nID-Pos ID-Neg OOD-Pos OOD-Neg0.200.220.240.260.28Similarity0.281\n0.241\n0.2340.274iNaturalist\nID-Pos ID-Neg OOD-Pos OOD-Neg0.200.220.240.260.28Similarity0.281\n0.241\n0.2240.265Places365\nID-Pos ID-Neg OOD-Pos OOD-Neg0.200.220.240.260.28Similarity0.281\n0.241\n0.2230.247SUN\nFigure 3. Similarity of ID/OOD and Positive/Negative Prompts.\ndetection in LoCoOp compromises the ID classification ac-\ncuracy, dropped from 72.1% in CoOp to 71.7%. This may\nbe attributed to its focus on the localized regions within the\nbackground rather than the primary object of interest, re-\nsulting in the missing of some discriminative features for\nID data classification. CLIPN and MCM, being zero-shot\nmethods, have not been exposed to the target ID data, lead-\ning to a much lower accuracy than the other methods.\n17590\nAblation StudyTexture iNaturalist Places SUN Avg\nAUC↑FPR95 ↓AUC↑FPR95 ↓AUC↑FPR95 ↓AUC↑FPR95 ↓AUC↑FPR95 ↓ACC↑\nBackbones\nResNet-50 79.44 72.73 90.97 44.10 84.35 61.37 88.04 50.55 85.70 57.19 68.2\nResNet-101 82.97 70.83 93.96 31.3 86.41 51.66 88.81 47.61 88.04 50.35 70.3\nViT-B-32 90.43 38.79 97.40 12.64 92.83 32.79 92.82 26.03 93.37 27.56 72.6\n# Negative Prompts\n1 90.04 40.67 98.24 9.23 90.37 32.62 92.26 27.33 92.73 27.46 72.1\nTraining Process\nOne-stage 80.25 72.87 77.02 95.74 82.53 95.94 81.13 95.72 80.23 90.07 62.8\nViT-B-16 & 2 & Two-stage 90.30 39.31 98.39 7.48 92.68 29.75 93.70 26.92 93.76 25.86 72.1\nTable 5. Results of our ablation experiments.\nPositive text featuresNegative text featuresOOD imagesID images\nFigure 4. T-SNE visualization of NegPrompt, utilizing a subset of\nImageNet - TinyImageNet as the dataset.\n4.3. Analysis of NegPrompt\n4.3.1 Why Does NegPrompt Work?\nTo better understand the effectiveness of NegPrompt,\nwe summarize the average maximum similarity between\nID/OOD images and positive/negative prompts, as shown\nin Fig. 3. Across all four OOD datasets, ID images have the\nhighest similarity with positive prompts, and OOD images\nwith negative prompts. This suggests positive prompts are\ncloser to ID images and negative prompts to OOD images in\nlatent space, ensuring OOD images receive lower softmax\nscores ( i.e., higher OOD scores) than ID images in Eq. 3.\nUsing TinyImageNet [23], a subset of ImageNet, we fur-\nther visualized the learned negative text features with its\ntest data [6]. Learning three negative prompts per ID class,\nthe results in Fig. 4 demonstrate that positive text features\nfrom positive prompts align closely with ID images in latent\nspace. Conversely, the negative text features encoded from\nnegative prompts lie outside of the ID data, with OOD im-\nages interspersed among them. This suggests negative text\nfeatures effectively act as a fence aligning much better to the\nOOD images than the ID images, well supporting the OOD\ndetection while preserving the ID classification accuracy.\n4.3.2 Ablation Study\nTable 5 shows our ablation study results on the backbone,\nthe number of negative prompts, and the training process.\nBackbones. We experiment with diverse CLIP back-\nbones. The results reveal that for CNN-based backboneslike ResNet50 and ResNet101, the OOF detection perfor-\nmance is not as proficient as the ViT-based backbones. Re-\ngarding ViT-B-32 and ViT-B-16, their performance is found\nto be uneven. Overall, the performance of OOD detection\ntends to increase with more advanced backbones.\nThe Number of Negative Prompts. The number of nega-\ntive prompts also influences the results. It is observed that\nthe OOD detection performance improves when increasing\nthe number of negative prompts from one to two. There-\nfore, it is suggested to further increase the number of neg-\native prompts for better detection accuracy. However, note\nthat with an increase in the number of negative prompts, the\ncomputational cost also rises rapidly. A good balance be-\ntween computational cost and detection accuracy is needed\nwhen determining the number of negative prompts.\nTraining Process. The training process is also important.\nAs discussed before, due to the necessity of anchoring the\npositive prompts, a two-stage training process is used in our\nmodel. This process involves training the positive prompts\nin the first stage and freezing them before proceeding to\ntrain the negative prompts in the second stage. When si-\nmultaneously training both positive and negative prompts in\na unified step, the model’s ability to effectively learn posi-\ntive prompts is significantly undermined, and consequently\nwe obtain unstable negative prompts, leading to the largely\ndecreased OOD detection performance.\n5. Conclusion\nWe present NegPrompt, a novel approach for prompt\nlearning-based OOD detection. It utilizes VLMs to learn\na small set of negative prompts for conveying negative se-\nmantics relative to ID classes. Our empirical results reveal\nthat NegPrompt 1) achieves superior OOD detection perfor-\nmance compared to the SOTA models across various OOD\ndatasets in both conventional and hard OOD detection sce-\nnarios, and 2) learns transfer negative prompts that enable\nexcellent open-vocabulary OOD detection performance.\n6. Acknowledgement\nThis work was supported by the National Natural Science\nFoundation of China 62276016, 62372029.\n17591\nReferences\n[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine\nMiech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,\nKatherine Millican, Malcolm Reynolds, et al. Flamingo: a\nvisual language model for few-shot learning. Advances in\nNeural Information Processing Systems , 35:23716–23736,\n2022. 2\n[2] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy\nMohamed, and Andrea Vedaldi. Describing textures in the\nwild. In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pages 3606–3613, 2014. 5\n[3] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE conference on computer vision and\npattern recognition , pages 248–255. Ieee, 2009. 5\n[4] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint\narXiv:1810.04805 , 2018. 2\n[5] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. arXiv preprint\narXiv:2010.11929 , 2020. 1, 2\n[6] Sepideh Esmaeilpour, Bing Liu, Eric Robertson, and Lei\nShu. Zero-shot out-of-distribution detection based on the\npre-trained model clip. In Proceedings of the AAAI confer-\nence on artificial intelligence , pages 6568–6576, 2022. 2, 3,\n8\n[7] Ruohuan Fang, Guansong Pang, and Xiao Bai. Simple\nimage-level classification improves open-vocabulary object\ndetection. arXiv preprint arXiv:2312.10439 , 2023. 2\n[8] Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao\nFang, Yongfeng Zhang, Hongsheng Li, and Yu Qiao.\nClip-adapter: Better vision-language models with feature\nadapters. International Journal of Computer Vision , pages\n1–15, 2023. 1\n[9] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition , pages 770–778, 2016. 1\n[10] Dan Hendrycks and Kevin Gimpel. A baseline for detect-\ning misclassified and out-of-distribution examples in neural\nnetworks. In International Conference on Learning Repre-\nsentations , 2017. 1, 3, 6\n[11] Dan Hendrycks, Mantas Mazeika, and Thomas Dietterich.\nDeep anomaly detection with outlier exposure. arXiv\npreprint arXiv:1812.04606 , 2018. 3, 4\n[12] Dan Hendrycks, Steven Basart, Mantas Mazeika, Andy Zou,\nJoe Kwon, Mohammadreza Mostajabi, Jacob Steinhardt, and\nDawn Song. Scaling out-of-distribution detection for real-\nworld settings. arXiv preprint arXiv:1911.11132 , 2019. 3,\n6\n[13] Ping Hu, Ximeng Sun, Stan Sclaroff, and Kate Saenko. Du-\nalcoop++: Fast and effective adaptation to multi-label recog-nition with limited annotations. IEEE Transactions on Pat-\ntern Analysis and Machine Intelligence , 2023. 3\n[14] Rui Huang and Yixuan Li. Mos: Towards scaling out-of-\ndistribution detection for large semantic space. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition , pages 8710–8719, 2021. 3\n[15] Rui Huang, Andrew Geng, and Yixuan Li. On the impor-\ntance of gradients for detecting distributional shifts in the\nwild. Advances in Neural Information Processing Systems ,\n34:677–689, 2021. 3\n[16] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade\nGordon, Nicholas Carlini, Rohan Taori, Achal Dave,\nVaishaal Shankar, Hongseok Namkoong, John Miller, Han-\nnaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Open-\nclip, 2021. 5\n[17] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,\nHieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom\nDuerig. Scaling up visual and vision-language representa-\ntion learning with noisy text supervision. In International\nconference on machine learning , pages 4904–4916, 2021. 2\n[18] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie,\nSerge Belongie, Bharath Hariharan, and Ser-Nam Lim. Vi-\nsual prompt tuning. In European Conference on Computer\nVision , pages 709–727. Springer, 2022. 1, 3\n[19] Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neu-\nbig. How can we know what language models know? Trans-\nactions of the Association for Computational Linguistics , 8:\n423–438, 2020. 1\n[20] Muhammad Uzair Khattak, Hanoona Rasheed, Muhammad\nMaaz, Salman Khan, and Fahad Shahbaz Khan. Maple:\nMulti-modal prompt learning. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 19113–19122, 2023. 2, 3\n[21] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna,\nYonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and\nDilip Krishnan. Supervised contrastive learning. Advances\nin neural information processing systems , 33:18661–18673,\n2020. 2\n[22] Shu Kong and Deva Ramanan. Opengan: Open-set recog-\nnition via open data generation. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision ,\npages 813–822, 2021. 3\n[23] Ya Le and Xuan Yang. Tiny imagenet visual recognition\nchallenge. CS 231N , 7(7):3, 2015. 8\n[24] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.\nBlip: Bootstrapping language-image pre-training for uni-\nfied vision-language understanding and generation. In In-\nternational Conference on Machine Learning , pages 12888–\n12900. PMLR, 2022. 2\n[25] Tianqi Li, Guansong Pang, Xiao Bai, Jin Zheng, Lei Zhou,\nand Xin Ning. Learning adversarial semantic embeddings for\nzero-shot recognition in open worlds. Pattern Recognition ,\n149:110258, 2024. 2\n[26] Shiyu Liang, Yixuan Li, and R. Srikant. Enhancing the re-\nliability of out-of-distribution image detection in neural net-\nworks. In International Conference on Learning Represen-\ntations , 2018. 1, 3, 6\n17592\n[27] Weitang Liu, Xiaoyun Wang, John Owens, and Yixuan\nLi. Energy-based out-of-distribution detection. Advances\nin neural information processing systems , 33:21464–21475,\n2020. 3, 6\n[28] Yuyuan Liu, Choubo Ding, Yu Tian, Guansong Pang,\nVasileios Belagiannis, Ian Reid, and Gustavo Carneiro.\nResidual pattern learning for pixel-wise out-of-distribution\ndetection in semantic segmentation. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision ,\npages 1151–1161, 2023. 3\n[29] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\nHierarchical vision transformer using shifted windows. In\nProceedings of the IEEE/CVF international conference on\ncomputer vision , pages 10012–10022, 2021. 1\n[30] Wenjun Miao, Guansong Pang, Tianqi Li, Xiao Bai, and Jin\nZheng. Out-of-distribution detection in long-tailed recog-\nnition with calibrated outlier class learning. arXiv preprint\narXiv:2312.10686 , 2023. 3\n[31] Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim\nNeumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh\nMahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran\nShen, et al. Simple open-vocabulary object detection. In\nEuropean Conference on Computer Vision , pages 728–755.\nSpringer, 2022. 2\n[32] Yifei Ming, Ziyang Cai, Jiuxiang Gu, Yiyou Sun, Wei Li,\nand Yixuan Li. Delving into out-of-distribution detection\nwith vision-language representations. Advances in Neural\nInformation Processing Systems , 35:35087–35102, 2022. 2,\n3, 5, 6\n[33] Atsuyuki Miyai, Qing Yu, Go Irie, and Kiyoharu Aizawa.\nLocoop: Few-shot out-of-distribution detection via prompt\nlearning. In Thirty-Seventh Conference on Neural Informa-\ntion Processing Systems , 2023. 2, 3, 5, 6, 7\n[34] Anh Nguyen, Jason Yosinski, and Jeff Clune. Deep neural\nnetworks are easily fooled: High confidence predictions for\nunrecognizable images. In CVPR , pages 427–436, 2015. 2\n[35] Andres Palechor, Annesha Bhoumik, and Manuel G ¨unther.\nLarge-scale open-set classification protocols for imagenet. In\nProceedings of the IEEE/CVF Winter Conference on Appli-\ncations of Computer Vision , pages 42–51, 2023. 5\n[36] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya\nSutskever, et al. Improving language understanding by gen-\nerative pre-training. 2018. 2\n[37] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever. Learning transferable visual\nmodels from natural language supervision, 2021. 1, 2, 3\n[38] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon, Ross Wightman, Mehdi Cherti, Theo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, et al. Laion-5b: An open large-scale dataset for training\nnext generation image-text models. Advances in Neural In-\nformation Processing Systems , 35:25278–25294, 2022. 2\n[39] Taylor Shin, Yasaman Razeghi, Robert L Logan IV , Eric\nWallace, and Sameer Singh. Autoprompt: Eliciting knowl-edge from language models with automatically generated\nprompts. arXiv preprint arXiv:2010.15980 , 2020. 1, 2\n[40] Ximeng Sun, Ping Hu, and Kate Saenko. Dualcoop: Fast\nadaptation to multi-label recognition with limited annota-\ntions. Advances in Neural Information Processing Systems ,\n35:30569–30582, 2022. 3\n[41] Yiyou Sun, Chuan Guo, and Yixuan Li. React: Out-of-\ndistribution detection with rectified activations. Advances in\nNeural Information Processing Systems , 34:144–157, 2021.\n3, 6\n[42] Yu Tian, Yuyuan Liu, Guansong Pang, Fengbei Liu, Yuan-\nhong Chen, and Gustavo Carneiro. Pixel-wise energy-biased\nabstention learning for anomaly segmentation on complex\nurban driving scenes. In European Conference on Computer\nVision , pages 246–263. Springer, 2022. 3\n[43] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui,\nChen Sun, Alex Shepard, Hartwig Adam, Pietro Perona, and\nSerge Belongie. The inaturalist species classification and de-\ntection dataset. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition , pages 8769–8778,\n2018. 5\n[44] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. Advances in neural\ninformation processing systems , 30, 2017. 2\n[45] Haoqi Wang, Zhizhong Li, Litong Feng, and Wayne Zhang.\nVim: Out-of-distribution with virtual-logit matching. In\n2022 IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR) , pages 4911–4920, 2022. 3\n[46] Hualiang Wang, Yi Li, Huifeng Yao, and Xiaomeng Li.\nClipn for zero-shot ood detection: Teaching clip to say no.\nInProceedings of the IEEE/CVF International Conference\non Computer Vision , pages 1802–1812, 2023. 2, 3, 5, 6\n[47] Peng Wu, Xuerong Zhou, Guansong Pang, Lingru Zhou,\nQingsen Yan, Peng Wang, and Yanning Zhang. Vadclip:\nAdapting vision-language models for weakly supervised\nvideo anomaly detection. arXiv preprint arXiv:2308.11681 ,\n2023. 3\n[48] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva,\nand Antonio Torralba. Sun database: Large-scale scene\nrecognition from abbey to zoo. In 2010 IEEE computer so-\nciety conference on computer vision and pattern recognition ,\npages 3485–3492. IEEE, 2010. 5\n[49] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision , pages 3836–3847, 2023. 2\n[50] Zexuan Zhong, Dan Friedman, and Danqi Chen. Factual\nprobing is [mask]: Learning vs. learning to recall. arXiv\npreprint arXiv:2104.05240 , 2021. 1\n[51] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva,\nand Antonio Torralba. Places: A 10 million image database\nfor scene recognition. IEEE transactions on pattern analysis\nand machine intelligence , 40(6):1452–1464, 2017. 5\n[52] Da-Wei Zhou, Han-Jia Ye, and De-Chuan Zhan. Learning\nplaceholders for open-set recognition. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition , pages 4401–4410, 2021. 3\n17593\n[53] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei\nLiu. Conditional prompt learning for vision-language mod-\nels. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition , pages 16816–16825,\n2022. 1, 3\n[54] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei\nLiu. Learning to prompt for vision-language models. In-\nternational Journal of Computer Vision , 130(9):2337–2348,\n2022. 1, 3, 4, 5, 6\n[55] Qihang Zhou, Guansong Pang, Yu Tian, Shibo He, and\nJiming Chen. Anomalyclip: Object-agnostic prompt learn-\ning for zero-shot anomaly detection. arXiv preprint\narXiv:2310.18961 , 2023. 3\n[56] Jiawen Zhu and Guansong Pang. Toward generalist anomaly\ndetection via in-context residual learning with few-shot sam-\nple prompts. arXiv preprint arXiv:2403.06495 , 2024. 2\n17594'}, 'dist': 0.9286905527114868}
Result 4: {'text': 'data', 'metadata': {'created_at': 1736932570, 'modified_at': 1736932570, 'owner': 'sayandeep', 'path': 'RAG_CONTENT/conferences/cvpr_ocr/Chen_Unsupervised_Blind_Image_Deblurring_Based_on_Self-Enhancement_CVPR_2024_paper.txt', 'size': 44191, 'seen_at': 1737191136, 'data': 'Unsupervised Blind Image Deblurring Based on Self-Enhancement\nLufei Chen Xiangpeng Tian Shuhua Xiong Yinjie Lei Chao Ren*\nCollege of Electronics and Information Engineering, Sichuan University, China\n{chenlufei, tianxp }@stu.scu.edu.cn, {xiongsh, yinjie, chaoren }@scu.edu.cn\nAbstract\nSignificant progress in image deblurring has been\nachieved by deep learning methods, especially the remark-\nable performance of supervised models on paired synthetic\ndata. However, real-world quality degradation is more com-\nplex than synthetic datasets, and acquiring paired data in\nreal-world scenarios poses significant challenges. To ad-\ndress these challenges, we propose a novel unsupervised\nimage deblurring framework based on self-enhancement.\nThe framework progressively generates improved pseudo-\nsharp and blurry image pairs without the need for real paired\ndatasets, and the generated image pairs with higher qualities\ncan be used to enhance the performance of the reconstruc-\ntor. To ensure the generated blurry images are closer to\nthe real blurry images, we propose a novel re-degradation\nprincipal component consistency loss, which enforces the\nprincipal components of the generated low-quality images to\nbe similar to those of re-degraded images from the original\nsharp ones. Furthermore, we introduce the self-enhancement\nstrategy that significantly improves deblurring performance\nwithout increasing the computational complexity of network\nduring inference. Through extensive experiments on multiple\nreal-world blurry datasets, we demonstrate the superiority\nof our approach over other state-of-the-art unsupervised\nmethods.\n1. Introduction\nImage deblurring is a classical problem in the field of com-\nputer vision, which aims to recover a clear image from its\nblurred version. The image deblurring tasks can be divided\ninto blind and non-blind deblurring, where the blind image\ndeblurring with unknown degradation is more challenging in\ngeneral. Conventional model-based methods for blind image\ndeblurring typically involve two main steps: estimating the\nblur kernel, and then reconstructing the sharp image from\nthe blurred input [ 10,26,40,42]. These methods’ perfor-\nmance is largely constrained by the accuracy of blur kernal\n*Corresponding author\nCycleGAN\nUAUDUSR -DAUIDGAN\nUSDFFCLGAN\nSEMGUD \n(Ours)GoPro\nHIDE\nRealBlur -J\nRealBlur -RFigure 1. Performance comparison of our proposed SEMGUD with\nother unsupervised methods [ 12,23,36,39,48,50] on different\ndatasets.\nestimation. For instance, in [ 27], the dark channel prior is\nused to estimate the blur kernel and reconstruct the sharp\nimage. However, the blurry characteristics in real-world sce-\nnarios are quite complex, making it challenging to accurately\nestimate the optimal blur kernel. In addition, these meth-\nods often require complex iterative optimisation processes,\nwhich may lead to long inference time.\nIn recent years, with the rapid development of deep learn-\ning technology, convolutional neural networks (CNNs) have\nbeen widely used in deblurring tasks, achieving significant\nsuccess. The supervised methods [ 4–6,15,16,19,29,34,44–\n46] focus on training deep neural network models using a\nlarge number of paired sharp and blurry images. This en-\nables the network to learn the mapping from blurry images\nto sharp images without the need for blur kernel estima-\ntion, achieving end-to-end reconstruction of the blurry and\nsharp images. For example, DeepDeblur [ 24] proposes a\nmulti-scale CNN to implement a coarse-to-fine processing\npipeline and directly restores sharp images. However, in\nthe real world cases, for the supervised learning methods,\ncollecting paired datasets from the real world is challenging,\nand manually synthesized datasets are difficult to simulate\nthe complex real image degradation processes.\nCompared to supervised deep learning methods, unsuper-\nvised deep learning methods [ 3,25,49,50] for real world\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n25691\nimages typically achieve end-to-end image reconstruction\nwithout requiring real world paired sharp and blurry images\nduring training. This allows unsupervised methods to more\neffectively handle complex real-world scenarios, especially\nwhen data collection cost is high, or the blur degradation is\ncomplex and challenging to model. Unfortunately, due to\nthe high difficulty and challenges of unsupervised deblur-\nring methods in training, there is still very little relevant\nresearch [ 12,23,36,48] in this field. Specifically, because\nunsupervised methods lack the strong constraints provided\nby paired datasets, researchers tend to design more extensive\ndeep models and complex inference processes to establish\nconnections between inputs and outputs. This often leads to\nlonger model inference times. Additionally, deploying large\nmodels in practical applications presents significant chal-\nlenges. Therefore, how to synthesize high-quality pseudo-\npaired datasets and how to improve the performance of unsu-\npervised methods without increasing the complexity of the\nmodel have become key research issues.\nIn this paper, we employ Generative Adversarial Net-\nwork (GAN) [ 7] to learn the real-world image degradation\ndistribution, addressing the issue of the lack of real-world\npaired data. We also introduce a novel re-degradation princi-\npal component consistency loss to more accurately synthe-\nsize blurred images. Considering that progressively update\npseudo paired data can lead to higher performance [ 21], we\npropose a self-enhancement deblurring strategy within our\nunsupervised framework to further enhance the network’s\nperformance. This strategy significantly improves network\ndeblurring performance without altering the existing network\nstructure and without increasing inference computational\ncomplexity. The main contributions of this paper are as\nfollows:\n•We propose a novel self-enhancement based unsupervised\ndeblurring framework. This framework can progressively\nimprove the generated pseudo-paired data and reconstruc-\ntor without the need for real paired datasets, addressing\nthe issues of insufficient paired data in real deblurring\ntasks, as well as the substantial complexity increase in the\nconventional geometric augmentation inference.\n•We design a novel loss function called re-degradation\nprincipal component consistency ( RPC2) loss. By in-\ntroducing blur kernel prediction in principal component\nconstraint, the RPC2loss enforces the principal compo-\nnents of the synthesized low-quality images to be similar\nto those of re-degraded images from the original sharp\nones. It can decrease the effect of noise interfere in blur-\nring images, and also make the synthetic image has more\nsimilar blurring degradation to the real data.\n•Extensive experiments are conducted on typical real blur-\nring datasets and the results verify the superior perfor-\nmance and generalization of our method over other exist-\ning unsupervised deblurring methods.2. Related Work\n2.1. Deep Supervised Image Deblurring\nRecently, deep learning methods have achieved significant\nsuccess in the field of image restoration [ 20–22,24,25,28,\n30,34,41]. For image deblurring, DL-MRF [ 35] proposes\na CNN-based model to estimate the blur kernel and elim-\ninate motion blur. In [ 2], a CNN is employed to compute\nan estimate of a clear image blurred by an unknown motion\nkernel. Thanks to the availability of some paired datasets,\nlearning-based methods have gradually shifted their focus\ntowards learning the mapping from given blurry images to\nthe original sharp images, without explicitly estimating the\nblur kernel [ 11,15,16,19,29,31,44,45]. MPRNet [ 45]\nintroduces a novel multi-level progressive architecture to\ngenerate context-rich and spatially accurate output. DBGAN\n[47] designs an effective GAN-based model for simulating\nthe synthesis of real-world blurry images. Stripformer [ 38]\ndevelops a transformer-based architecture by constructing\nhorizontal and vertical labels to reweight image features.\nNAFNet [ 5] introduces a simple baseline network for image\ndeblurring and denoising tasks. MRLPFNet [ 8] proposes\na simple and effective multi-scale residual low-pass filter\nnetwork that can better model both low and high frequency\ninformation. UFPNet [ 9] represents the motion blur kernel\nspace in the latent space using normalized flows and designs\na CNN to predict latent codes instead of motion blur kernels.\nOverall, supervised real image deblurring methods are de-\npendent on real paired datasets, yet the scarcity of such data\nin real-world scenarios frequently limits their applicability.\n2.2. Deep Unsupervised Image Deblurring\nCompared to supervised methods, unsupervised real image\ndeblurring methods [ 25,39,49,50] do not involve real paired\ndata during the training process. Consequently, unsupervised\nmethods have weaker constraints between input and output,\nmaking it challenging to accurately learn the blur-to-clear\nmapping. Building on the basic GAN, UID-GAN [ 23] en-\ntangles the content and blur features of blurry images on\na domain-specific dataset and then removes the blur from\nthe blurry images. FCLGAN [ 48] proposes a lightweight\nand real-time unsupervised single-image blind deblurring\nbaseline. USDF [ 12] introduces a multi-step self-supervised\ndeblurring framework that iteratively decomposes and re-\nassembles input images, exploiting the uncertainty of blur\nartifacts to generate a variety of pseudo-blurred and sharp\nimage pairs. UADU [ 36] proposes an unsupervised semi-\nblind deblurring model that can effectively recover uniformly\nblurry image. However, unsupervised real deblurring meth-\nods have only received limited research attention in recent\nyears. Existing methods overlook the potential in perfor-\nmance enhancement, and traditional model augmentation\ntends to increase inference computational complexity. Ad-\n25692\nConvReLUConvReLU\nResidual blockRecconcat\n⚫⚫⚫⚫⚫⚫ConvConv\nConv\nGenerator\nSharp feature Extraction\nY: Real -world blur image    \nX：Sharp image \n𝒚𝝐𝒀\n𝒙𝝐𝑿𝒚𝒓𝒆𝒄𝝐𝑫𝒚𝒃𝟏𝝐𝑺𝒚𝒃𝟐𝝐𝑺\n𝒙𝒃𝟏𝝐𝑺\n𝒙𝒃𝟐𝝐𝑺S：Synthetic blur  image \nD: Deblur image\n𝒙𝒓𝒆𝒄𝝐𝑫+\nDGIG Module+ G\nG\nconvconv\nGblockdownblock\nblock++\nReconstructor𝒚𝒓𝒆𝒄𝝐𝑫\nconv\nspectral norm\nDDDD 𝒚𝝐𝒀\n𝒚𝝐𝒀\n𝒚𝝐𝒀𝑳𝒂𝒅𝒗𝟏𝑳𝒂𝒅𝒗𝟐𝑳𝒂𝒅𝒗𝟒𝑳𝒂𝒅𝒗𝟑\nupblock\nblock⚫⚫⚫⚫⚫⚫ ⚫⚫⚫⚫⚫⚫DGIG Module\nBackbone GECM -1GECM -3 GECM -2\nFigure 2. Multi-Generator Unsupervised Deblurring (MGUD) framework. The whole framework employs four generators and discriminators\nand uses NAFNet [ 5] as reconstructor. The red arrows represent the backbone of MGUD, and the blue arrows ,purple arrows , and\ngreen arrows respectively represent the different generator complementary constraint modules GECM-1 ,GECM-2 , and GECM-3 . DGIG\nModule denotes the degradation guidance information generation module.\nditionally, methods based on synthetic pseudo-real paired\nimages don’t address how to continuously improve the qual-\nity of synthetic images.\n3. Methodology\n3.1. Unsupervised Deblurring Framework\nWe provide a detailed explanation of our proposed unsuper-\nvised framework in this subsection. Our approach aims to\naddress the issue of insufficient paired data in real-world\ndeblurring applications, and significantly improve network\nperformance without altering the existing network structure\nand without increasing inference computational complexity.\n3.1.1 Overall Framework of Proposed Method\nIn response to the lack of paired datasets in the real world,\nwe propose the Multi-Generator Unsupervised Deblurring\n(MGUD) framework, as illustrated in Fig. 2. The framework\nemploys ResNet-based generator with 6 residual blocks,\nPatchGAN [ 50] discriminator, and NAFNet [ 5] reconstructor.\nThe detailed structures are presented in the supplementary\nmaterials.\nIn generator, we effectively utilize unpaired sharp and\nblurry images to synthesize pseudo-paired datasets. Inspired\nby CycleGAN [ 50], we design a GAN structure that transi-\ntions from “original sharp →synthetic blur →reconstructedsharp→secondary synthetic blur”, as shown by the red and\nblue arrows in Fig. 2. The process (x,F(y))→xb1trans-\nforms original sharp image xto synthetic blurry image xb1\naccording to degradation of blurry image y, where “ F(·)” is\nthe degradation guide information generation (DGIG) mod-\nule, providing ample pseudo-paired data for deblurring. The\nprocess xb1→xrecachieves deblurring by transforming xb1\nto reconstructed sharp image xrec. We incorporate a genera-\ntor complementary constraint module (GECM) following the\nCycleGAN concept, (xrec,F(xb1))→xb2, facilitating the\ntransition from reconstructed sharp image xrecto secondary\nsynthetic blur image xb2, adding additional constraints and\nenhancing training stability.\nConsidering that the unsupervised deblurring frame-\nwork’s performance mainly relies on the blur image genera-\ntor’s capability, and also considering the challenges in GAN\ntraining, we further introduce two GECMs: (yrec,F(y))→\nyb1and(yrec,F(xb1))→yb2, which substantially enhance\nthe qualities of the synthetic images.\n3.1.2 Generation of Pseudo Paired Datasets and Recon-\nstructor for Deblurring\nCompared to supervised methods, the training process of un-\nsupervised methods struggle to accurately learn the mapping\nfrom blurry to sharp due to the lack of strong constraints\nfrom paired data. GAN is often preferred for unsupervised\n25693\n𝒚𝒚𝝐𝝐𝒀𝒀\n𝒙𝒙𝝐𝝐𝑿𝑿𝒚𝒚𝒃𝒃𝒃𝒃𝝐𝝐𝑺𝑺\n𝒙𝒙𝒃𝒃𝒃𝒃𝝐𝝐𝑺𝑺C\nG\nG\n𝑹𝑹𝑹𝑹𝑹𝑹Rec\n𝒌𝒌\n�𝒙𝒙𝝐𝝐𝑺𝑺𝐿𝐿𝑃𝑃𝑃𝑃𝑃𝑃2\nkernel estimation network\ndegradation networkconv fixedDGIG Module\nFigure 3. The implementation principles of the proposed self-\nenhancement strategy and re-degradation principal component con-\nsistency loss function. kdenotes the blur kernel estimated from\nthe real blurry images. ¯Rec denotes using the fixed reconstructor\ntrained in the last round at the input of DGIG to synthesize better\npseudo-paired data.\nmethods due to its superior data synthesis capability. Cur-\nrent mainstream unsupervised methods focus on developing\nmore capable generators to simulate the image blurring pro-\ncess and generate synthetic images that closely resemble\nreal blur. Theoretically, the closer the synthetic blur images\nare to actual blur, the better the reconstructor is expected\nto perform. The structure of our generator is depicted in\nFig. 2, incorporating a sharp feature extraction module and\n6 residual blocks.\nThe synthesis of high-quality realistic blurred images is\nchallenging due to varying image content influences. Con-\nsidering many potential factors that could lead to undesired\nblurring artifacts, we initially process the blurred images y\nusing a DGIG module to obtain blurred features. The DGIG\nmodule employs a U-Net architecture, comprising a down-\nsampling layer followed by an upsampling layer. The G\nlearns the blurring characteristics of blurry images to guide\nthe synthesis of sharp images towards realistic blurring. To\nensure the generation of more realistic blurred images, we\nalso train a discriminator Dto distinguish between synthe-\nsized and real-world blurred images, where the generator\nand discriminator learn collaboratively in an adversarial man-\nner. In the MGUD framework, the adversarial loss Ladv1\nof the backbone is constrained between the yϵY andxb1ϵS,\nwhere YandSdenote the real blurred image and synthetic\nblurred image respectively. Considering that the pseudo-\npaired images generated by the generator are crucial to the\nreconstructor’s performance in this unsupervised framework,\nand accurate degradation is difficult to obtain, additional\nthree adversarial losses are further introduced to enhance\nconstraints as shown in Fig. 2:\nLGAN =Ladv1+Ladv2+Ladv3+Ladv4 (1)For the synthesized blurry image xb1, the deblurred image\nxrecis obtained through the reconstructor Rec. In our entire\nframework, we optimize the following loss function to train\nthe reconstructor:\nLRec=LPSNR (x, xrec) +LSSIM (x, xrec) (2)\nwhereLPSNR (·)represents the PSNR loss used to constrain\nthe peak signal-to-noise ratio of the image, and LSSIM (·)\nrepresents the SSIM loss used to constrain the structural\ninformation of the image.\n3.1.3 Re-Degradation Principal Component Consis-\ntency Loss\nAlthough adversarial losses can effectively improve the per-\nformance of the generator and discriminator, leading to more\naccurately synthesized realistic blurred images, the data syn-\nthesis process is highly susceptible to interference from the\ninherent information in blurred images, such as content and\ncolor. To mitigate this, we draw inspiration from [ 13] and\ndesign a novel loss function called re-degradation princi-\npal component consistency ( RPC2) loss. By introducing\nblur kernel prediction in principal component constraint, this\nRPC2loss ensures the principal components of the synthe-\nsized low-quality images align with those of re-degraded\nimages from the original sharp ones, which can decrease the\nimpact of noise interfere in the blurring process. As shown\nin Fig. 3, we use the kernel estimation network to estimate\nthe blur kernel of the blurry image y, and then utilize this\nkernel to further re-blur the sharp image x. Subsequently,\nwe maintain the principal component consistency between x\nand the synthesized blurred image xb1by introducing the L1\nnorm loss. The process is defined as follows:\nLRPC2=X\nϕ=3,5,7ωϕ||Gϕ(x⊗K(y))− Gϕ(xb1)||1(3)\nwhere K(·)denotes the blur kernel estimation process, ⊗\ndenotes the blur operation, Gϕ(·)denotes a Gaussian filter\noperator with a kernel size of ϕ, andωϕrepresents the weight\nfor level ϕ.\nSpecifically, the RPC2loss’s role is to guide Gin isolat-\ning the main degradation component (e.g., motion blur) from\ny, derived from K, with Gaussian filter acting to prevent G\nfrom incorporating residual texture interference (like content\nor color from y) onto x. Our test results in Section 4.3 show\nthat without using the kernel estimation, the performance\nwill significant decrease, verifying the necessity of the kernel\nestimation. In addition, we test three well-performed ker-\nnel estimation methods [ 1,9,20] and find that they achieve\nsimilar final performance. This is due to the introduction of\nmulti-scale Gaussian low-pass filtering, the impact of noise\nand kernel estimation biases on the image is mitigated to\n25694\ncompare\nTraining \nExpenses\nTest \nExpenses\nPerformance \nImprovementNet1\n2\nknon-increase\nnon-increaseincrease\nk-fold \ngrowth\nslight increase\n(within 0.3dB )significantly \nimprove\n(more than 1dB )\n3𝑹𝑹𝑹𝑹𝑹𝑹𝟏𝟏\n𝑹𝑹𝑹𝑹𝑹𝑹𝟐𝟐\n𝑹𝑹𝑹𝑹𝑹𝑹𝒏𝒏−𝟏𝟏\n𝑹𝑹𝑹𝑹𝑹𝑹𝒏𝒏……\nDGIG Module\n𝑹𝑹𝑹𝑹𝑹𝑹Feedback\nEnhance\n……\nFeedback\nEnhance𝑹𝑹𝑹𝑹𝑹𝑹𝒏𝒏\n…… ……fusion\n(c) The progressively process for self -\nenhancement strategy.(a) Existing typical deblurring network\ngeometric augmentation  strategy in testing. (b) Comparison of the two strategies.bestFigure 4. Comparison of our proposed self-enhancement strategy with the conventional geometric augmentation strategy in terms of training\nexpenses, testing expenses, and performance improvement respectively.\nsome extent, ensuring good consistency of the main content\nin the image.\n3.2. Inference Complexity Invariant Self-\nEnhancement Strategy\nIn order to enable the reconstructor to self-correct and en-\nhance its performance without altering its structure or in-\ncreasing the complexity of the original network , we propose\nthe Self-Enhancement (SE) strategy. Fig. 3 illustrates the im-\nplementation scheme of one of the DGIGs integrating the SE\nstrategy, and the other DGIG with the SE strategy is similar.\nBy employing the SE strategy, a better reconstructor ( Rec)\ncan be obtained, thereby learning more accurate degrada-\ntion. This results in the generation of more realistic synthetic\nblurred-sharp image pairs and progressively improves the\nperformance of the updated Rec with higher quality syn-\nthetic samples. It is exciting to note that the performance of\ntheRec using the SE strategy shows significant improvement\nover the one without the SE strategy. The implementation of\nthe SE strategy involves several steps. First, we need to train\nan initial reconstructor Rec 1. Then, as shown in Fig. 3, keep\ntheRec 1parameters fixed to guide the generator to learn\ndegradation information more accurately, generating more\nrealistic pseudo-blurry images. Subsequently, retrain G,D,\nandRec untilRec reaches convergence. Finally, repeat the\nabove process, progressively replacing and enhancing the\nreconstructor until obtain the best-performing Recn. There-\nfore, the complete Rec’s loss function is as follows:\nLSE−Rec=LPSNR (¯Rec(xb1), xrec)\n+LSSIM (¯Rec(xb1), xrec)\n+LPSNR (¯Rec(y), yrec)\n+LSSIM (¯Rec(y), yrec) +LRec(4)\nFinally, we obtain the total loss function:\nL=min\nGmax\nDLGAN + ΦRPC2LRPC2+LSE−Rec (5)Strategies Original Geometric Augmentation Self-Enhancement\nPSNR 27.68 27.79 29.06\nPSNR Gains — 0.11 1.38\nTable 1. Comparing the performance enhancement of the self-\nenhancement strategy and the geometric augmentation strategy on\nthe GoPro dataset.\nwhere ΦRPC2represents the hyperparameter for the re-\ndegradation principal component consistency loss.\nThe basic idea of the SE strategy is to use the results of the\nprevious phase as feedback information to guide and improve\nthe training of subsequent stages. A comparative analysis\nwith existing typical geometric augmentation methods, as\npresented in Fig. 4, reveals that the SE strategy markedly\nboosts performance with slightly additional training cost,\nwithout imposing extra complexity or testing overhead. As\nTable 1 indicates, our method improves the PSNR by more\nthan 1dB after incorporating the SE strategy, which demon-\nstrates the effectiveness of SE strategy in image deblurring.\n4. Experiments\n4.1. Datasets and Implementation Details\nDatasets. Following the state-of-the-art image deblurring\nmethods [ 5,9], we first divide the GoPro dataset [ 24] (con-\nsists of 2103 pairs of blurred and sharp images) into separate\nsharp and blurred image parts and further constitute unpaired\nblurred datasets to train the algorithm proposed in this paper.\nWe evaluate our method on the GoPro dataset [ 24], RealBlur\ndataset [ 32], and HIDE dataset [ 33]. The GoPro dataset\nconsists of 1111 test images, the RealBlur dataset includes\n980 images for testing, and the HIDE dataset provides 2025\ntest images.\nImplementation Details. We follow the experimental set-\ntings described in [ 5]. We adopt the Adam optimizer [ 14]\n(β1= 0.9, β2= 0.999), the initial learning rate is set to 10−4,\n25695\nConference/Journal MethodsGoPro HIDE RealBlur-R RealBlur-J\nPSNR↑SSIM↑PSNR↑SSIM↑PSNR↑SSIM↑PSNR↑SSIM↑\nDeep supervisedCVPR 2018 DeepDeblur [24] 29.23 0.916 25.73 0.874 32.51 0.841 27.87 0.827\nCVPR 2018 SRN [37] 30.26 0.934 28.36 0.915 35.66 0.947 28.56 0.867\nCVPR 2021 HINet [4] 32.71 0.959 30.32 0.932 35.75 0.949 28.17 0.849\nECCV 2022 Stripfromer [38] 33.08 0.962 31.03 0.940 36.07 0.952 28.82 0.876\nECCV 2022 MSDI-Net [17] 33.28 0.964 31.02 0.940 35.88 0.952 28.59 0.869\nECCV 2022 NAFNet [5] 33.69 0.967 31.32 0.943 35.50 0.953 28.32 0.857\nICCV 2023 icDPM [31] 33.20 0.963 30.96 0.938 N/A N/A 28.81 0.872\nCVPR 2023 UFPNet [9] 34.06 0.968 31.74 0.947 36.25 0.953 29.87 0.884\nDeep unsupervisedICCV 2017 CycleGAN [50] 22.54 0.720 21.81 0.690 12.38 0.242 19.79 0.633\nICCV 2017 DualGAN [43] 22.86 0.722 N/A N/A N/A N/A N/A N/A\nCVPR 2019 UIDGAN [23] 23.56 0.738 22.70 0.715 16.64 0.323 22.87 0.671\nACM MM 2022 FCLGAN [48] 24.84 0.771 23.43 0.732 28.37 0.663 25.35 0.736\nICCV 2021 USR-DA [39] 25.49 0.787 23.91 0.756 32.32 0.821 26.39 0.784\nACM MM 2023 USDF [12] 25.58 0.857 23.93 0.829 32.57 0.923 26.59 0.881\nCVPR 2023 UAUD [36] 26.12 0.869 24.37 0.837 32.91 0.885 26.84 0.792\n— SEMGUD (Ours) 29.06 0.927 27.64 0.892 35.51 0.946 28.01 0.844\nTable 2. The comparison results on the benchmark datasets. All the models are trained on the GoPro dataset.\nBlurry image from GoPro testset\nBlurry\n CycleGAN\n UIDGAN\n USR-DA\nFCLGAN\n UAUD\n Ours\n GT\nFigure 5. Visual comparisons on the GoPro dataset. From left to right: blurry image, results from CycleGAN [ 50], UIDGAN [ 23], USR-DA\n[39], FCLGAN [48], UAUD [36], SEMGUD (ours), and ground-truth.\nand the training patch size is 128 ×128. For the RPC2loss\nhyperparameter ( ΦRPC2) in the loss function of Eq. 5 is\nset to 3. More details are presented in the supplementary\nmaterials.\n4.2. Experimental Results\nWe compare our method with the most recent unsuper-\nvised methods from recent years, including [ 12,23,36,\n39,43,48,50]. Additionally, we also list the state-of-the-\nart supervised methods based on paired images, including\n[4,5,9,17,24,31,37,38]. Note that the related models for\ncomparison are limited, since only a few unsupervised de-\nblurring models have been proposed in the field. We evaluate\nthe effectiveness of each method using performance metrics\n(PSNR and SSIM). The results are directly cited from theoriginal papers or generated using the official models pro-\nvided by the authors.\nQuantitative comparison. Table 2 presents the PSNR and\nSSIM results for various single-image deblurring test meth-\nods on the GoPro, HIDE, and RealBlur test datasets. It\nis evident that our proposed method outperforms other un-\nsupervised methods. On the GoPro dataset, our method\nachieves 2.94dB improvement in PSNR over the state-of-\nthe-art unsupervised methods. To validate the effectiveness\nand generalization of our method, we compare the results\non various real-world blurry datasets. Fig. 1 intuitively\ndemonstrates the performance improvement of our method\ncompared to other unsupervised methods. It is well known\nthat supervised methods usually outperform unsupervised\nmethods. But on the RealBlur-R dataset, the performance of\n25696\nBlurry image from HIDE testset\nBlurry\n CycleGAN\n UIDGAN\n USR-DA\nFCLGAN\n UAUD\n Ours\n GT\nFigure 6. Visual comparisons on the HIDE dataset. From left to right: blurry image, results from CycleGAN [ 50], UIDGAN [ 23], USR-DA\n[39], FCLGAN [48], UAUD [36], SEMGUD (ours), and ground-truth.\nBlurry image from RealBlur testset\nBlurry\n CycleGAN\n UIDGAN\n USR-DA\nFCLGAN\n UAUD\n Ours\n GT\nFigure 7. Visual comparisons on the RealBlur dataset. From left to right: blurry image, results from CycleGAN [ 50], UIDGAN [ 23],\nUSR-DA [39], FCLGAN [48], UAUD [36], SEMGUD (ours), and ground-truth.\nUFPNet (NAFNet)+ NAFNet\n (UFPNet)+\nFigure 8. The “mode collapse” in NAFNet [ 5] and UFPNet [ 9]:\ntrained on the GoPro dataset may output anomalous pixel re-\ngions during testing on the RealBlur-J dataset. “(UFPNet)+” and\n“(NAFNet)+” denote the results obtained through training with our\nSEMGUD framework.\nour method is even higher than NAFNet [ 5]. This suggests\nthat our method’s effectiveness is improving and it’s even\nbecoming competitive with certain supervised methods. It’sKEϕ ωϕ GoPro\n3,5,7 3,7,9 3,9,15 1,0.2,0.04 1,0.1,0.01 PSNR↑SSIM↑\n%" % % % " 28.54 0.919\n"% % " " % 28.72 0.921\n"% % " % " 28.83 0.923\n"% " % " % 28.89 0.922\n"% " % % " 28.97 0.924\n"" % % " % 29.00 0.925\n"" % % % " 29.06 0.927\nTable 3. Ablation study on the hyperparameters ϕandωϕof the\nRPC2loss. The first column indicates whether the kernel estima-\ntion network is used.\nworth noting that all the mentioned models are trained on\nthe GoPro dataset, which further demonstrates the excellent\ngeneralization performance of our method from GoPro to\nother real-world datasets.\n25697\nBlurry Sharp NAFNet Ours\n24.69dB 32.54 dBFigure 9. Crop the blurred region from the ReloBlur dataset for\nvisual comparison.\nVisual comparison. As shown in Figs. 5, 6, and 7, we\ncompare the visual deblurring results of different unsuper-\nvised methods on the GoPro, HIDE, and RealBlur datasets,\nrespectively. Note that although UAUD [ 36] effectively re-\nstores images with uniform blur, such performance may not\nextend to images with non-uniform blur. It can be seen that\nour proposed method achieves good results in removing mo-\ntion blur from real blurry images. Additionally, we note\nthat existing unsupervised methods often perform poorly\nin dealing with more severe blurring as shown in Fig. 6.\nFurthermore, we observe that while some existing advanced\nsupervised methods like NAFNet and UFPNet achieve the\nbest performance when trained on the GoPro paired datasets,\nthey exhibit “mode collapse” when directly applied to other\ndatasets, as illustrated in Fig. 8. In contrast, unsupervised\nmodels trained with our framework do not exhibit this phe-\nnomenon. Additionally, we directly use our GoPro-trained\nmodel to test the ReloBlur [ 18] dataset, which contains Real\nWorld Partly-blurred images, and find it to be effective for\nsome of the images, as shown in Fig. 9.\n0.411.041.38\n0.300.761.40\n0.380.571.42\n0.260.480.85\nGoPro HIDE RealBlur -J RealBlur -RNAFNet UFPNet FCLGAN\nFigure 10. Performance improvement (PSNR gain) of different\nmethods on various datasets driven by the self-enhancement strat-\negy.\n4.3. Ablation Study\nThe Re-Degradation Principal Component Consistency\nLoss. In fact, we find that if we remove the RPC2loss, the\ngenerator always struggles to generate satisfactory blurry\nimages. Due to the minimal impact of different kernel es-\ntimation networks on the final deblurring performance as\nillustrated in Section 3.1.3, we can use any well-performedMethods SEGoPro HIDE\nPSNR↑SSIM↑PSNR↑SSIM↑\nFCLGAN [48]% 24.70 0.768 23.49 0.733\n" 25.11 0.772 23.79 0.741\nUFPNet [9]% 27.37 0.903 25.91 0.862\n" 28.41 0.916 26.67 0.875\nNAFNet [5] (Ours)% 27.68 0.911 26.24 0.859\n" 29.06 0.927 27.64 0.892\nTable 4. Ablation study on the superiority and generalization of the\nself-enhancement strategy for different reconstructors.\nkernel estimation networks, e.g., [ 1] can be used for testing.\nTherefore, our ablation study on the RPC2loss primarily\nfocuses on the blur kernel estimation network and the set-\ntings of hyperparameters ϕandωin Eq. 3. As shown in\nTable 3, we observe that the blur kernel estimation network\nsignificantly improves the performance of our method.\nThe Self-Enhancement Strategy. As shown in Table 4, the\neffect is improved by 1.38dB on the GoPro dataset with the\nSE strategy. Furthermore, to demonstrate the superiority\nand versatility of our proposed self-enhancement strategy,\nwe also conduct experiments incorporating SE strategy on\nseveral deblurring methods, including NAFNet [ 5], FCL-\nGAN [ 48], UFPNet [ 9]. Fig. 10 shows in detail the different\nnetwork performance improvements before and after using\nthe self-enhancement strategy. Note that due to FCLGAN do\nnot provide the complete network weights, the experimental\nresults in the table are obtained from our retraining. This\nstrongly validates the superiority and significant potential of\nthe self-enhancement strategy.\n5. Conclusion\nIn this paper, we introduce a novel unsupervised frame-\nwork for image deblurring. Our approach incorporates a\nre-degradation principal component consistency loss, ensur-\ning that the principal components of the synthetically blurred\nimages align closely with those from the re-degraded ver-\nsions of the original sharp images. We also put forward a\nself-enhancement strategy that substantially improves perfor-\nmance without modifying the model’s architecture or adding\nto the computational cost in inference. Comprehensive tests\non standard datasets reveal that our method surpasses the\nexisting state-of-the-art unsupervised methods with strong\ngeneralization capabilities across various real-world blurry\nimage datasets.\nAcknowledgement. This work was supported by the Na-\ntional Natural Science Foundation of China under Grant\n62171304 and the Cooperation Science and Technology\nProject of Sichuan University and Dazhou City under Grant\n2022CDDZ-09.\n25698\nReferences\n[1]Guillermo Carbajal, Patricia Vitoria, Mauricio Delbracio,\nPablo Mus ´e, and Jos ´e Lezama. Non-uniform motion blur\nkernel estimation via adaptive decomposition. arXiv e-prints ,\npages arXiv–2102, 2021. 4, 8\n[2]Ayan Chakrabarti. A neural approach to blind motion deblur-\nring. In European Conference on Computer Vision (ECCV) ,\npages 221–235, 2016. 2\n[3]Huaijin Chen, Jinwei Gu, Orazio Gallo, Ming-Yu Liu, Ashok\nVeeraraghavan, and Jan Kautz. Reblur2deblur: Deblurring\nvideos via self-supervised learning. In IEEE International\nConference on Computational Photography (ICCP) , pages\n1–9, 2018. 1\n[4]Liangyu Chen, Xin Lu, Jie Zhang, Xiaojie Chu, and Cheng-\npeng Chen. Hinet: Half instance normalization network for\nimage restoration. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition (CVPR)\nWorkshops , pages 182–192, 2021. 1, 6\n[5]Liangyu Chen, Xiaojie Chu, Xiangyu Zhang, and Jian Sun.\nSimple baselines for image restoration. In European Confer-\nence on Computer Vision (ECCV) , pages 17–33, 2022. 2, 3,\n5, 6, 7, 8\n[6]Sung-Jin Cho, Seo-Won Ji, Jun-Pyo Hong, Seung-Won Jung,\nand Sung-Jea Ko. Rethinking coarse-to-fine approach in\nsingle image deblurring. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision (ICCV) , pages\n4641–4650, 2021. 1\n[7]Antonia Creswell, Tom White, Vincent Dumoulin, Kai\nArulkumaran, Biswa Sengupta, and Anil A Bharath. Gen-\nerative adversarial networks: An overview. IEEE signal\nprocessing magazine , pages 53–65, 2018. 2\n[8]Jiangxin Dong, Jinshan Pan, Zhongbao Yang, and Jinhui\nTang. Multi-scale residual low-pass filter network for image\ndeblurring. In Proceedings of the IEEE/CVF International\nConference on Computer Vision (ICCV) , pages 12345–12354,\n2023. 2\n[9]Zhenxuan Fang, Fangfang Wu, Weisheng Dong, Xin Li, Jin-\njian Wu, and Guangming Shi. Self-supervised non-uniform\nkernel estimation with flow-based motion prior for blind im-\nage deblurring. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR) , pages\n18105–18114, 2023. 2, 4, 5, 6, 7, 8\n[10] Amit Goldstein and Raanan Fattal. Blur-kernel estimation\nfrom spectral irregularities. In European Conference on Com-\nputer Vision (ECCV) , pages 622–635, 2012. 1\n[11] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent\nDumoulin, and Aaron C Courville. Improved training of\nwasserstein gans. Advances in neural information processing\nsystems , 30, 2017. 2\n[12] Runhua Jiang and Yahong Han. Uncertainty-aware variate\ndecomposition for self-supervised blind image deblurring. In\nProceedings of the ACM International Conference on Multi-\nmedia (ACM MM) , pages 252–260, 2023. 1, 2, 6\n[13] Xin Jin, Zhibo Chen, Jianxin Lin, Zhikai Chen, and Wei Zhou.\nUnsupervised single image deraining with self-supervised\nconstraints. In IEEE International Conference on Image\nProcessing (ICIP) , pages 2761–2765, 2019. 4[14] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. arXiv preprint arXiv:1412.6980 ,\n2014. 5\n[15] Orest Kupyn, V olodymyr Budzan, Mykola Mykhailych,\nDmytro Mishkin, and Ji ˇr´ı Matas. Deblurgan: Blind motion\ndeblurring using conditional adversarial networks. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR) , pages 8183–8192, 2018. 1,\n2\n[16] Orest Kupyn, Tetiana Martyniuk, Junru Wu, and Zhangyang\nWang. Deblurgan-v2: Deblurring (orders-of-magnitude)\nfaster and better. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision (ICCV) , pages 8878–\n8887, 2019. 1, 2\n[17] Dasong Li, Yi Zhang, Ka Chun Cheung, Xiaogang Wang,\nHongwei Qin, and Hongsheng Li. Learning degradation\nrepresentations for image deblurring. In European Conference\non Computer Vision (ECCV) , pages 736–753, 2022. 6\n[18] Haoying Li, Ziran Zhang, Tingting Jiang, Peng Luo, Huajun\nFeng, and Zhihai Xu. Real-world deep local motion deblur-\nring. In Proceedings of the AAAI Conference on Artificial\nIntelligence , pages 1314–1322, 2023. 8\n[19] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc\nVan Gool, and Radu Timofte. Swinir: Image restoration using\nswin transformer. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision (ICCV) Workshops ,\npages 1833–1844, 2021. 1, 2\n[20] Jingyun Liang, Guolei Sun, Kai Zhang, Luc Van Gool, and\nRadu Timofte. Mutual affine network for spatially variant\nkernel estimation in blind image super-resolution. In Proceed-\nings of the IEEE/CVF International Conference on Computer\nVision (ICCV) , pages 4096–4105, 2021. 2, 4\n[21] Xin Lin, Chao Ren, Xiao Liu, Jie Huang, and Yinjie Lei.\nUnsupervised image denoising in real-world scenarios via\nself-collaboration parallel generative adversarial branches. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision (ICCV) , pages 12642–12652, 2023. 2\n[22] Xiao Liu, Xiangyu Liao, Xiuya Shi, Linbo Qing, and Chao\nRen. Efficient Information Modulation Network for Image\nSuper-Resolution . 2023. 2\n[23] Boyu Lu, Jun-Cheng Chen, and Rama Chellappa. Unsu-\npervised domain-specific deblurring via disentangled repre-\nsentations. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR) , pages\n10225–10234, 2019. 1, 2, 6, 7\n[24] Seungjun Nah, Tae Hyun Kim, and Kyoung Mu Lee. Deep\nmulti-scale convolutional neural network for dynamic scene\ndeblurring. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR) , pages\n3883–3891, 2017. 1, 2, 5, 6\n[25] Thekke Madam Nimisha, Kumar Sunil, and AN Rajagopalan.\nUnsupervised class-specific deblurring. In European Confer-\nence on Computer Vision (ECCV) , pages 353–369, 2018. 1,\n2\n[26] Jinshan Pan, Zhe Hu, Zhixun Su, and Ming-Hsuan Yang.\nDeblurring text images via l0-regularized intensity and gra-\ndient prior. In Proceedings of the IEEE/CVF Conference\n25699\non Computer Vision and Pattern Recognition (CVPR) , pages\n2901–2908, 2014. 1\n[27] Jinshan Pan, Deqing Sun, Hanspeter Pfister, and Ming-Hsuan\nYang. Blind image deblurring using dark channel prior. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR) , pages 1628–1636, 2016. 1\n[28] Yizhong Pan, Xiao Liu, Xiangyu Liao, Yuanzhouhan Cao,\nand Chao Ren. Random sub-samples generation for self-\nsupervised real image denoising. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision\n(ICCV) , pages 12150–12159, 2023. 2\n[29] Dongwon Park, Dong Un Kang, Jisoo Kim, and Se Young\nChun. Multi-temporal recurrent neural networks for progres-\nsive non-uniform single image deblurring with incremental\ntemporal training. In European Conference on Computer\nVision (ECCV) , pages 327–343, 2020. 1, 2\n[30] Chao Ren, Xiaohai He, and Truong Q Nguyen. Adjusted\nnon-local regression and directional smoothness for image\nrestoration. IEEE Transactions on Multimedia , pages 731–\n745, 2018. 2\n[31] Mengwei Ren, Mauricio Delbracio, Hossein Talebi, Guido\nGerig, and Peyman Milanfar. Multiscale structure guided dif-\nfusion for image deblurring. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision (ICCV) , pages\n10721–10733, 2023. 2, 6\n[32] Jaesung Rim, Haeyun Lee, Jucheol Won, and Sunghyun Cho.\nReal-world blur dataset for learning and benchmarking de-\nblurring algorithms. In European Conference on Computer\nVision (ECCV) , pages 184–201, 2020. 5\n[33] Ziyi Shen, Wenguan Wang, Xiankai Lu, Jianbing Shen,\nHaibin Ling, Tingfa Xu, and Ling Shao. Human-aware mo-\ntion deblurring. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision (ICCV) , pages 5572–\n5581, 2019. 5\n[34] Maitreya Suin, Kuldeep Purohit, and AN Rajagopalan.\nSpatially-attentive patch-hierarchical network for adaptive\nmotion deblurring. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition (CVPR) ,\npages 3606–3615, 2020. 1, 2\n[35] Jian Sun, Wenfei Cao, Zongben Xu, and Jean Ponce. Learning\na convolutional neural network for non-uniform motion blur\nremoval. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR) , pages\n769–777, 2015. 2\n[36] Xiaole Tang, Xile Zhao, Jun Liu, Jianli Wang, Yuchun Miao,\nand Tieyong Zeng. Uncertainty-aware unsupervised im-\nage deblurring with deep residual prior. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR) , pages 9883–9892, 2023. 1, 2, 6, 7, 8\n[37] Xin Tao, Hongyun Gao, Xiaoyong Shen, Jue Wang, and\nJiaya Jia. Scale-recurrent network for deep image deblurring.\nInProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR) , pages 8174–8182,\n2018. 6\n[38] Fu-Jen Tsai, Yan-Tsung Peng, Yen-Yu Lin, Chung-Chi Tsai,\nand Chia-Wen Lin. Stripformer: Strip transformer for fast\nimage deblurring. In European Conference on Computer\nVision (ECCV) , pages 146–162, 2022. 2, 6[39] Wei Wang, Haochen Zhang, Zehuan Yuan, and Changhu\nWang. Unsupervised real-world super-resolution: A domain\nadaptation perspective. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision (ICCV) , pages\n4318–4327, 2021. 1, 2, 6, 7\n[40] Oliver Whyte, Josef Sivic, Andrew Zisserman, and Jean\nPonce. Non-uniform deblurring for shaken images. Inter-\nnational journal of computer vision , pages 168–186, 2012.\n1\n[41] Lin X, Yue J, and Ren C. Unlocking low-light-rainy image\nrestoration by pairwise degradation feature vector guidance.\nInarXiv , 2023. 2\n[42] Li Xu, Shicheng Zheng, and Jiaya Jia. Unnatural l0 sparse\nrepresentation for natural image deblurring. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR) , pages 1107–1114, 2013. 1\n[43] Zili Yi, Hao Zhang, Ping Tan, and Minglun Gong. Dualgan:\nUnsupervised dual learning for image-to-image translation.\nInProceedings of the IEEE/CVF International Conference\non Computer Vision (ICCV) , pages 2849–2857, 2017. 6\n[44] Yuan Yuan, Wei Su, and Dandan Ma. Efficient dynamic\nscene deblurring using spatially variant deconvolution net-\nwork with optical flow guided training. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR) , pages 3555–3564, 2020. 1, 2\n[45] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar\nHayat, Fahad Shahbaz Khan, Ming-Hsuan Yang, and Ling\nShao. Multi-stage progressive image restoration. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR) , pages 14821–14831, 2021. 2\n[46] Hongguang Zhang, Yuchao Dai, Hongdong Li, and Piotr Ko-\nniusz. Deep stacked hierarchical multi-patch network for\nimage deblurring. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition (CVPR) ,\npages 5978–5986, 2019. 1\n[47] Kaihao Zhang, Wenhan Luo, Yiran Zhong, Lin Ma, Bjorn\nStenger, Wei Liu, and Hongdong Li. Deblurring by realistic\nblurring. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR) , pages\n2737–2746, 2020. 2\n[48] Suiyi Zhao, Zhao Zhang, Richang Hong, Mingliang Xu, Yi\nYang, and Meng Wang. Fcl-gan: A lightweight and real-time\nbaseline for unsupervised blind image deblurring. In Pro-\nceedings of the ACM International Conference on Multimedia\n(ACM MM) , pages 6220–6229, 2022. 1, 2, 6, 7, 8\n[49] Suiyi Zhao, Zhao Zhang, Richang Hong, Mingliang Xu, Hai-\njun Zhang, Meng Wang, and Shuicheng Yan. Crnet: Unsuper-\nvised color retention network for blind motion deblurring. In\nProceedings of the ACM International Conference on Multi-\nmedia (ACM MM) , pages 6193–6201, 2022. 1, 2\n[50] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros.\nUnpaired image-to-image translation using cycle-consistent\nadversarial networks. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision (ICCV) , pages\n2223–2232, 2017. 1, 2, 3, 6, 7\n25700'}, 'dist': 0.9286905527114868}
Result 5: {'text': 'data', 'metadata': {'created_at': 1736932570, 'modified_at': 1736932570, 'owner': 'sayandeep', 'path': 'RAG_CONTENT/conferences/cvpr_ocr/Buettner_Incorporating_Geo-Diverse_Knowledge_into_Prompting_for_Increased_Geographical_Robustness_in_CVPR_2024_paper.txt', 'size': 51033, 'seen_at': 1737191136, 'data': 'Incorporating Geo-Diverse Knowledge into Prompting for Increased\nGeographical Robustness in Object Recognition\nKyle Buettner1, Sina Malakouti2, Xiang Lorraine Li1,2, Adriana Kovashka1,2\n1Intelligent Systems Program,2Department of Computer Science, University of Pittsburgh, PA, USA\n{buettnerk, sem238 }@pitt.edu, {xianglli, kovashka }@cs.pitt.edu\nhttps://krbuettner.github.io/GeoKnowledgePrompting\nAbstract\nExisting object recognition models have been shown to\nlack robustness in diverse geographical scenarios due to\ndomain shifts in design and context. Class representations\nneed to be adapted to more accurately reflect an object con-\ncept under these shifts. In the absence of training data\nfrom target geographies, we hypothesize that geographi-\ncally diverse descriptive knowledge of categories can en-\nhance robustness. For this purpose, we explore the feasibil-\nity of probing a large language model for geography-based\nobject knowledge, and we examine the effects of integrat-\ning knowledge into zero-shot and learnable soft prompt-\ning with CLIP . Within this exploration, we propose geog-\nraphy knowledge regularization to ensure that soft prompts\ntrained on a source set of geographies generalize to an un-\nseen target set. Accuracy gains over prompting baselines on\nDollarStreet while training only on Europe data are up to\n+2.8/1.2/1.6 on target data from Africa/Asia/Americas, and\n+4.6 overall on the hardest classes. Competitive perfor-\nmance is shown vs. few-shot target training, and analysis is\nprovided to direct future study of geographical robustness.\n1. Introduction\nThe performance of object recognition models degrades\nwhen tested in new geographies (e.g., cities, countries, con-\ntinents) [7, 21, 33, 39, 43]. Numerous factors contribute to\nthe challenging problem of geographical domain shift , such\nas cross-geography changes in object design/parts, materi-\nals, and context. These changes in turn may be due to cul-\ntural, climate, or economic differences around the world.\nRecent work has shown standard adaptation techniques fail\nwhen used for geographical domain shifts [21, 33], but there\nhas yet to be significant progress in the creation of tech-\nniques that improve geographical robustness. Such progress\nis necessary to ensure equitable use of AI in the future.\nFigure 1. Descriptive knowledge can address concept shifts\nacross geographies. Observe the wide range of object designs and\ncontexts in the DollarStreet [11] category tools around the world.\nOur work’s premise is that textual representations for classes in\nvision-language models can be enhanced to better suit diverse ob-\nject representations across geographies. Map made with [16].\nOverall, models need representations that adequately\ncapture a category’s various forms around the world. A\nnatural solution is to collect training data of objects from\ndifferent regions. However, this approach is expensive,\ntakes significant effort, and is difficult for regions with lim-\nited Internet access. Fortunately, geographical shifts have a\nunique property compared to other common domain shifts\n(e.g. ones due to artistic style or weather changes)—they\ncan be addressed with descriptive knowledge about con-\ncept changes. In other words, it is possible to describe the\nfeatures of an object in a region and use this information\nto adapt a model’s default representation. For instance, as\nshown in Fig. 1, for rural areas in Papua New Guinea, tools\ncan be described as being used for “cooking, hunting, and\nfishing”, and for rural areas in Malawi, tools may often be\n“made of metal and wood, for farming”. Models should ac-\ncount for diverse presentations and contexts of a category\nand not be limited to biased presentations ( e.g. if the model\nlearns tools as just being “metallic with logos”).\nWe examine the effects of probing geo-diverse knowl-\nedge in two ways. First, we analyze whether a vision-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n13515\nlanguage model (VLM, i.e. CLIP [36]) has encoded cat-\negories in a geo-specific manner, such that adding a coun-\ntry’s name to a prompt ( e.g. “A photo of a house in China”)\nelicits knowledge that improves recognition. Second, we\nprobe a large language model (LLM, i.e. GPT-3 davinci-\n003) for geography-specific knowledge to obtain visual fea-\nture descriptors for an object in different locations. We an-\nalyze results in zero-shot inference on geographically and\nsocioeconomically diverse data (DollarStreet [11]), finding\nthe combination of knowledge to often be complementary.\nWe further consider a practical scenario where CLIP is\noptimized with soft prompting, using only a “source” ge-\nography with easy-to-access data ( e.g. Europe), while the\nmodel is applied downstream on “target” data from other\nparts of the world ( e.g. Africa, Asia, Americas). We propose\ngeography knowledge regularization , which uses knowl-\nedge ensembled over countries to enable soft prompts to\nachieve geographically generalizable class representations.\nWe test our method on the recent DollarStreet and GeoNet\n[21] datasets. Our regularization boosts performance over\nbaseline soft prompting methods, and has benefits with\nrespect to few-shot target-specific training (a 16-shot-per-\nclass regularized model without any target data outperforms\na 12-shot-per-class target-trained model on DollarStreet).\nOur method is the first to effectively address geo shifts in\nobject recognition. It outperforms zero-shot CLIP (assumed\nto have some robustness) by 10.3% on Africa, CoOp [52] by\n3.3%, and the best baseline by 4.6% on the hardest classes.\nTo summarize, we answer the following questions: (1)\nDoes adding geographical context (i.e. country names)\nto CLIP prompts improve recognition across geographies?\n(2) Can an LLM provide useful geographical descriptive\nknowledge to improve recognition? (3) How can we opti-\nmize soft prompts for CLIP using an accessible data source\nwith consideration of target geographies not represented in\nthe training set? (4) Where can soft prompts enhanced with\ngeographical knowledge provide the most benefits?\n2. Related Work\nGeographical domain shifts occur when the target set-\nting is in a different geography ( e.g. continent, country,\ncity) than where the source data was acquired. Shifts in-\nvolve changes in object design ( e.g. differences in house\narchitecture) and context (i.e. background/co-occurring ob-\njects vary). Datasets tailored to cross-country/continent ob-\nject recognition have recently been proposed, e.g. Dol-\nlarStreet [11], GeoNet [21], GeoDE [37], GeoYFCC [9],\nand OpenImages-Extended [5]. Interestingly, [21, 33]\ndemonstrate that traditional methods in unsupervised do-\nmain adaptation [10, 19, 20, 27, 28, 38, 44, 45, 50] which\nseek to bridge gaps based on visual features alone, do not\neffectively address geographical domain shift . They achieve\nnegligible gains (e.g. 0.14 for [10] in [33]) or often dropsin performance (e.g. all methods tested in [21]), compared\nto just using the source model. Attempts to specifically ad-\ndress geographical robustness are limited: [43] corrects for\ndifferences in the sizes of cars, [9] proposes a discriminative\ndomain embedding from target data, and GiVL [48] pre-\ntrains with knowledge from Wikipedia. In contrast, our de-\nscriptive knowledge regularization works for different cate-\ngories (not just cars); we do not require target domain data\nto achieve gains cross-geography; we explore the strong ca-\npabilities of LLMs to gather relevant knowledge; and we\npropose lightweight adaptation through soft prompting (un-\nlike GiVL’s expensive pretraining).\nVision-language (VL) models [17, 25, 26, 36, 49] excel on\na variety of tasks. CLIP [36] shows impressive zero-shot\nobject recognition across different settings. Yet its perfor-\nmance given geographical shift is less apparent . GeoNet\n[21] only shows finetuned performance, which is expensive\ngiven CLIP’s large scale. GeoDE [37] only shows zero-shot\ninference with CLIP’s default prompts. Neither work eval-\nuates descriptive knowledge or soft prompting.\nLearning soft textual prompts. Several recent works to\nadapt CLIP have focused on parameter and data efficiency\nusing linear probing [36] and prompting [18, 23, 52]. Soft\ntextual prompting ( e.g. CoOp [52]) is notable as it optimizes\nclass text embeddings (without manual tuning), which we\nhypothesize is critical to adequately adapt for geographical\nrobustness. As CoOp overfits on base (seen) classes, Co-\nCoOp [51] proposes to condition prompts on the image for\nbetter generalizability. KgCoOp [47] alternatively guides\nlearned prompt embeddings towards CLIP’s manual prompt\nembeddings through a distance constraint to avoid degrada-\ntion on unseen classes. Our approach also uses a distance\nconstraint, but it differs from [47] with the purpose of regu-\nlarizing learned prompt representations for cross- geography\ngeneralization instead of the base-to-new- class setting. We\nalso show novel benefits of regularization when used with\nanensemble of CLIP’s internal geographical knowledge\nandexternal geographical descriptive knowledge. Our ap-\nproach notably outperforms each of CoOp, CoCoOp, and\nKgCoOp by at least +2.8 accuracy on target countries in\nAfrica in DollarStreet. External knowledge aids unseen\nclasses in KAPT [22], but not with respect to geographical\nknowledge . Prompt tuning for adaptation has been tested in\n[12, 40], but not with descriptive knowledge.\nKnowledge probed from large language models like\n[4, 6, 31, 32] has been used for visual reasoning [46],\nembodied agent planning [15, 41], and to generate addi-\ntional context for VLM class prompts in object recognition\n[30, 34]. We uniquely probe LLMs for distinguishable vi-\nsual descriptions for the same object class across different\ngeographical regions . We are also the first to incorporate\ngeographical knowledge from LLMs into soft prompting.\n13516\nFigure 2. Geography knowledge regularization. To ensure robustness in soft prompt learning, we (1) incorporate knowledge internal to\nCLIP and externally obtained from an LLM. (2) This descriptive knowledge regularizes class representations when training on a specific\nsource geography ( e.g. Europe), thus (3) increasing robustness when generalizing to target geographies ( e.g. Vietnam).\n3. Approach\nWe investigate geographical shift in object recognition with\nVLMs. We posit that the manner in which classes are de-\nscribed is critical due to cross-geography shifts in design\nand context. We also hypothesize that CLIP’s default class\nrepresentations elicited through “a photo of a/an <object >”\nprompts may not adequately represent classes around the\nworld. Instead, they may be more aligned to high-resource\ngeographies due to Internet-based training data. Optimiz-\ning representations (with soft prompts) on a specific geog-\nraphy ( e.g. Europe) may exacerbate a lack of robustness.\nOur main idea (Fig. 2) is to incorporate object-related ge-\nographical knowledge into prompting to ensure model ro-\nbustness in different regions. We outline our mechanism to\nobtain geography-specific context by probing CLIP’s inter-\nnal knowledge and an external LLM’s descriptive knowl-\nedge. We further propose geography knowledge regulariza-\ntion to ensure soft prompts do not overfit when training data\nis limited to certain geographies.\nPreliminaries. We consider object recognition on a dataset\nScontaining a class set C(sizeNc) over a set of geographies\nG. We consider a geography gto be either a country or\ncontinent. Our VLM is CLIP [36], with an image encoder f\nand language encoder h. We incorporate knowledge of each\ngeography ginto prompting using (1) zero-shot inference or\n(2) soft textual prompting. Prompts are defined as t(each\nis a set of tokens), and class embeddings ware calculated\nash(t). We refer to CLIP’s default prompt “a photo of a/an\n<object >” for a class castdefault\nc .\n3.1. Geographical Knowledge Probing\nProbing CLIP’s internal geographical knowledge. Our\nfirst strategy of investigation is to augment CLIP’s manual\nprompts to include country names, as we surmise that someof the resulting class representations may be better aligned\nto how categories present in different regions. [3] inspires\nthis hypothesis, showing that adding country names to im-\nage generation prompts can achieve gains in geographical\nrepresentativeness. However, it is an open question whether\nadding country names in prompts improves recognition.\nWe define the setting CountryInPrompt , using the prompt\ntCountryInPrompt\nc with template “a photo of a/an <object >in\n<country >”,e.g. “a photo of a stove in Burundi.”\nProbing external LLM geographical knowledge. As\nCLIP may not have sufficient knowledge of objects in some\nregions, we consider further augmenting prompts with ex-\nternal knowledge. Motivated by probing LLMs for gen-\neral attribute-based object descriptions [30, 34] ( e.g. atiger\nwith “stripes and sharp teeth”), we probe GPT-3 ( davinci-\n003) for geography-specific descriptions of object styles,\ncontexts, and materials.1We reason that since LLMs are\ntrained on large information sources ( e.g. CommonCrawl\n[1], WebText [35], Wikipedia [2]), they may have knowl-\nedge about how an object presents in a region due to cli-\nmate, economics, and/or cultural factors. For instance, roofs\nmay sometimes be “thatched” in tropical and temperate cli-\nmates, and cutlery may sometimes be made of “bamboo” in\nareas with bamboo forests. Our goal is unique vs. [30, 34]\nin that we explore descriptive knowledge differences for the\nsame class to address domain shifts across regions.\nAcquiring knowledge. We follow [30], but instead of gath-\nering one set of feature descriptors D(c)for each c, we col-\nlect sets per country . For each class cand geography g, we\nprompt the LLM to generate descriptor lists Dg(c), using\na template consisting of an example question, answer, and\nformat. We use 1-shot prompting to show how to capture\ngeographically representative object designs and contexts.\n1We found ChatGPT to perform worse than GPT-3, also found in [34].\n13517\nOur prompt exemplifies this below, using the descriptors for\nJapanese ofuro (お風呂,bathtub ):\nQ:What are useful features for distinguishing a\nbathtub in a photo that I took in Japan ?\nA:There are several useful visual features to tell there\nis a bathtub in a photo that I took in Japan :\n- short in length and deep\n- square shape\n- wooden, plastic, or steel material\n- white or brown color\n- benches on side\n- next to shower\nQ: What are useful features for distinguishing\n<category >in a photo that I took in <country >?\nA:There are several useful visual features to tell there\nis/are <category >in a photo that I took in <country >:\nUsing knowledge. To convert LLM outputs to CLIP\nprompts, each descriptor dinDg(c)serves in a prompt\ntc,d. The format of tc,dis “a photo of a/an <object >which\n(is/has/etc.) <descriptor >”. The setting where geography-\nspecific LLM descriptors are used in prompting is referred\nto as CountryLLM (prompts tCountryLLM\nc,d), while [30] is\nGeneralLLM (prompts tGeneralLLM\nc,d ). To perform zero-shot\ninference on an image I, each class score s(c, I)is com-\nputed using the average of CLIP logits ϕ(I, d)over each d\nin the set D. For GeneralLLM, the score is calculated as:\ns(c, I) =1\n|D(c)|X\nd∈D(c)ϕ(I, d) (1)\nFor CountryLLM, we use the geo-specific set:\ns(c, I, g ) =1\n|Dg(c)|X\nd∈Dg(c)ϕ(I, d) (2)\nThe argmax of swith respect to cis taken as the prediction.\nDue to averaging over descriptor scores, not every descrip-\ntor needs to strongly activate in a correct prediction. The\nmodel therefore can account for diverse features of objects\nwithin a geography. These descriptors effectively serve as\ncomplements to CLIP’s default knowledge of class names.\nCombining knowledge. Our third method of exploration,\nCountryInPrompt+LLM , combines both CLIP’s internal\nknowledge and LLM external knowledge. The prompt tem-\nplate tCountryInPrompt+LLM\nc,dis “a photo of a/an <object >in\n<country >which (is/has/etc.) <descriptor >”.\n3.2. Regularizing Soft Prompts via Geo Knowledge\nAdaptation scenario. In practice, one may want to fur-\nther optimize a VLM for a downstream task. To update\na model effectively, one promising strategy is soft textual\nprompting. It is parameter-efficient [52] and avoids feature\ndistortion unlike finetuning [24]. Its mechanism is to learncontext parameters that directly change the class text em-\nbeddings used in inference. We posit that learning context\non a dataset with limited diversity ( e.g. just Europe) may\ntailor these class representations to the region and overfit.\nTo investigate cross-geography generalization when using\nsoft prompting, we pose a domain generalization scenario\nwhere we aim to learn only from a high-resource source set\nof countries and generalize to a target set of countries at in-\nference time. A method that performs well in this setting\ncould provide a viable alternative to few-shot target training\nwhen acquiring target data for training is not feasible.\nSoft prompts. Our idea is to learn soft prompts while con-\nstraining the class text embeddings to be close to geograph-\nical knowledge of objects outside of source geographies. In\nthis way, we hope to learn class representations that are\nmore applicable to the rest of the world. Building from\nCoOp [52], we assume there is a text prompt tcfor each\nclassc. All prompts share Mcontext vectors (each denoted\n[V]m), which are the same size as the word embeddings (i.e.\n512-D) and precede a class name token [CLASS c]:\ntc= [V]1[V]2...[V]M[CLASS c] (3)\nThe respective class text embedding wcis produced as\nh(tc), forwarding the prompt through the text encoder.\nLearning proceeds by minimization of cross-entropy, for\nimage kwith features fk, using ground-truth source labels\nyk,cand temperature τ:\nLce=−NcX\nc=1yk,clogexp(cos ( wc,fk)/τ)PNc\nj=1exp(cos ( wj,fk)/τ)(4)\nGeography knowledge regularization (gkr). We mini-\nmize the cosine distance of normalized class embedding wc\nand overall target class knowledge ktgt\nc, over all c:\nLgkr= 1−1\nNcNcX\nc=1cos(wc,ktgt\nc) (5)\nGeo knowledge ensemble. To define ktgt\nc, we identify that\na model may be deployed in various locations. Therefore,\nwe define a target geography set Gt, which can practically\nbe thought of as the countries that a model may be deployed\nin that are not in the training set D(e.g. Africa, Asia, Amer-\nicas in Gtif only Europe in D). Then for each geography g\ninGt, we define the corresponding class knowledge kg\ncas:\nkg\nc=1\n|Dg(c)|X\nd∈Dg(c)wCountryInPrompt+LLM\nc,d(6)\nThis is defined analogously for CountryInPrompt and\nCountryLLM. The final regularization target ktgt\ncfor class\ncaggregates the set’s geographical knowledge:\nktgt\nc=1\n|Gt|X\ng∈Gtkg\nc (7)\n13518\nWhile the loss formulation includes cosine distance like\nKgCoOp [47], it serves a different purpose: we regu-\nlarize for cross-geography domain generalization , while\nKgCoOp regularizes for base-class-to-new-class inference.\nOur method outperforms KgCoOp in cross-geography gen-\neralization due to its use of geo-specific knowledge.\nOverall loss. The final loss Lfor learning soft prompts,\nwhere λcontrols the strength of regularization, is:\nL=Lce+λLgkr (8)\n4. Experimental Setup\nDatasets. We use DollarStreet [11], which has 38,479\nimages of household objects across regions (Africa,\nSouth/Central/North America, Asia, Europe) and incomes.\nThe classes may represent abstract concepts ( e.g.most loved\ntoys), so we narrow focus to 95 object classes. We merge\nespecially close categories ( light sources by bed/in living\nroom ) and ignore multi-label examples, resulting in 23,114\ntotal images. For zero-shot inference, the entire set is used.\nFor training, the source is Europe, and the target is Ameri-\ncas, Asia, and Africa. 20% of source data, stratified based\non class proportions, is heldout for testing; target evalua-\ntion is on all data from target continents. To set up ktgt\nc,\nthe 49 target countries in DollarStreet make up Gt. We also\nuse the GeoImNet benchmark of GeoNet [21], comprised of\n171,692 images across 600 objects from the USA (source)\nand 78,358 images across the same number in Asia (target).\nWe use existing train-test splits for soft prompt training. For\nGeoNet, given the relatively large number of categories and\ninference costs of davinci-003 ,ktgt\ncandGtuse the top 10\nmost frequent countries in the GeoNet set.\nBaselines. We evaluate geography knowledge regulariza-\ntion vs. CoOp [52], CoCoOp [51] and KgCoOp [47]. For\nzero-shot inference, we evaluate CLIP with default prompts\nand the classification via description method of [30].\nMetrics. We report balanced accuracy, which is the average\nof per-class recall scores. We use this metric to account for\nclass imbalance in both DollarStreet and GeoNet. For zero-\nshot inference, we also show top-3 accuracy as some similar\ncategories exist ( e.g.cooking utensils ,cutlery ).\nExperimental details. For all soft prompting experiments,\nmodels are trained with 16 shots, context length M= 4,\nand for 100 epochs, unless otherwise stated. The class token\nposition follows the soft prompts, and class-shared context\nis used. Our method uses a batch size of 128 (same as Kg-\nCoOp), while the batch sizes for CoOp and CoCoOp follow\n[47] (i.e. 32, and 1 for CoCoOp due to memory limita-\ntions). The encoders used for training include ViT-B/16 [8]\nand ResNet-50 (RN50) [14] as reported in [47]. Both our\nmethod and KgCoOp use a regularization weight λ. We set\nλ= 4 for DollarStreet, and compare to KgCoOp at λ= 4(which performs better than KgCoOp’s default λ= 8). For\nGeoNet, we use λ= 8. Training is performed on 1 NVIDIA\nQuadro RTX A5000 GPU with 24 GB of memory. All re-\nported soft prompt results are averages over 3 runs. For\nexperiments in the zero-shot setting, results are shown over\nViT-B/16, ViT-B/32, and RN50 encoders. LLM descriptors\nfor all experiments are generated from the davinci-003 ver-\nsion of GPT-3, with max tokens 100 and temperature 0.7.\n5. Results\n5.1. Zero-shot CLIP Inference with Geo Knowledge\nWe gauge the effectiveness of three zero-shot strategies: (1)\nCountryInPrompt (including countries in prompts to probe\nCLIP’s knowledge), (2) CountryLLM (gathering descrip-\ntive knowledge of objects with davinci-003 ), and (3) Coun-\ntryInPrompt+LLM (using country names and LLM knowl-\nedge). We compare to [30] (GeneralLLM) and CLIP with\nmanual prompts (i.e. “a photo of a/an <object >”). Results\non DollarStreet are shown in Table 1.\nIncluding country names in prompts can improve ob-\nject recognition, especially in Africa and Asia . This ob-\nservation is supported by gains for CountryInPrompt vs.\nZero-Shot CLIP, especially in Africa and Asia (up to +5.4\nand +2.6 top-1 gains for RN50, resp.). Such differences\nmay occur as country-specific context can align represen-\ntations closer to these regions, while default prompts do\nnot adequately capture objects around the world (esp. from\nnon-Western regions). In Americas/Europe, adding country\nnames leads to gains with RN50, but slight drops with ViT-\nB/16 and ViT-B/32. We reason that CLIP’s default prompts\nmay be already well-aligned to countries in these regions\nfor those encoders due to overrepresentation in training.\nPrompting with country-specific descriptive knowledge\nfrom LLMs outperforms general object knowledge . We\nobserve this from CountryLLM’s larger gains over default\nCLIP than GeneralLLM’s for almost all encoders, regions,\nand metrics. The largest top-1 difference is with ViT-\nB/32 (in Total, 52.6% for CountryLLM vs. 51.4% for\nGeneralLLM). In top-3 accuracy, the differences for Coun-\ntryLLM/GeneralLLM in Total are 74.6/73.0 for ViT-B/32,\n78.8/77.9 for ViT-B/16, 70.0/68.6 for RN50. These sug-\ngest that default non-country-specific knowledge is less ad-\nequate for various countries. The gains of CountryLLM vs.\nZero-Shot CLIP are generally largest on Africa and Asia,\nas countries in these regions may have greater shifts vs.\nthe default prompts, but CountryLLM also performs well\non Europe. LLM description in general is less effective in\nAmericas, though Americas has a large proportion of USA\nimages, for which default CLIP may be well-aligned.\nThere are complementary effects when using CLIP’s in-\nternal and external LLM geo knowledge. This observa-\ntion is supported by CountryInPrompt+LLM, the combina-\n13519\nTop-1 Accuracy Top-3 Accuracy\nEncoder Prompting Method Europe Africa Asia Americas Total Europe Africa Asia Americas Total\nAcc∆Acc∆Acc∆Acc∆Acc∆Acc∆Acc∆Acc∆Acc∆Acc∆\nViT-B/32 Zero-Shot CLIP [36] 59.1 -43.7 -50.8 -55.3 -51.7 -81.1 -64.8 -72.3 -77.4 -73.7 -\nGeneralLLM [30] 57.3 -1.844.3 +0.650.9 +0.154.6 -0.7 51.4 -0.378.8 -2.364.5 -0.372.1 -0.275.7 -1.7 73.0 -0.7\nCountryInPrompt 57.5 -1.645.2 +1.551.9 +1.155.0 -0.3 52.1 +0.4 80.2 -0.965.5 +0.773.3 +1.076.9 -0.5 73.9 +0.2\nCountryLLM 59.4 +0.345.2 +1.552.1 +1.355.3 0.052.6 +0.9 80.9 -0.266.4 +1.673.6 +1.377.4 0.074.6 +0.9\nCountryInPrompt+LLM 60.8 +1.745.3 +1.652.2 +1.455.0 -0.3 52.8 +1.1 81.5 +0.467.4 +2.673.6 +1.376.7 -0.7 74.7 +1.0\nViT-B/16 Zero-Shot CLIP [36] 64.3 -46.9 -53.9 -60.1 -55.5 -84.3 -69.3 -75.9 -81.1 -77.2 -\nGeneralLLM [30] 64.2 -0.148.8 +1.956.0 +2.158.5 -1.6 56.8 +1.3 83.9 -0.471.1 +1.876.3 +0.480.4 -0.7 77.9 +0.7\nCountryInPrompt 63.9 -0.449.6 +2.755.7 +1.859.3 -0.8 56.6 +1.1 84.0 -0.371.3 +2.076.5 +0.680.0 -1.1 77.7 +0.5\nCountryLLM 65.2 +0.949.6 +2.755.6 +1.759.7 -0.4 57.0 +1.5 84.3 0.071.8 +2.577.5 +1.681.5 +0.4 78.8 +1.6\nCountryInPrompt+LLM 65.5 +1.250.8 +3.956.0 +2.159.7 -0.4 57.4 +1.9 85.5 +1.272.5 +3.277.0 +1.180.9 -0.2 78.7 +1.5\nRN50 Zero-Shot CLIP [36] 53.0 -38.0 -44.4 -49.8 -45.7 -76.5 -60.2 -66.4 -72.7 -68.1 -\nGeneralLLM [30] 55.5 +2.540.9 +2.946.9 +2.550.3 +0.5 47.9 +2.2 76.0 -0.561.2 +1.067.7 +1.371.1 -1.6 68.6 +0.5\nCountryInPrompt 54.5 +1.543.4 +5.447.0 +2.650.8 +1.0 48.4 +2.7 76.0 -0.564.0 +3.868.7 +2.372.7 0.070.0 +1.9\nCountryLLM 56.2 +3.241.1 +3.147.3 +2.950.4 +0.6 48.3 +2.6 77.2 +0.762.5 +2.368.8 +2.472.4 -0.3 70.0 +1.9\nCountryInPrompt+LLM 56.4 +3.443.0 +5.048.0 +3.650.9 +1.1 49.1 +3.4 76.7 +0.263.1 +2.968.3 +1.971.1 -1.6 69.4 +1.3\nTable 1. Zero-shot CLIP inference with descriptive knowledge prompts, top-1/3 balanced accuracy (Acc) on DollarStreet. Strategies\nto capture CLIP’s internal country knowledge (CountryInPrompt), external LLM country knowledge (CountryLLM), and their combination\n(CountryInPrompt+LLM), often improve vs. the zero-shot CLIP baseline (prompt “a photo of a/an <object >”), especially on Africa and\nAsia; gains in green, drops in red. CountryLLM notably outperforms the GeneralLLM [30] baseline.\nSource Target\nEncoder Prompting Method Europe Africa Asia Americas Total\nAcc ∆ Acc ∆ Acc ∆ Acc ∆ Acc ∆\nViT-B/16 CoOp [52] 72.2 -53.9 -61.5 -68.6 -61.7 -\nCoCoOp [51] 73.2 -54.3 -61.2 -68.3 -61.4 -\nKgCoOp [47] 73.1 -54.4 -62.6 -68.7 -62.4 -\nCountryInPrompt Reg 71.8 -1.4 56.8 +2.4 63.0 +0.4 69.8 +1.1 63.5 +1.1\nCountryLLM Reg 73.2 0.0 55.6 +1.2 63.0 +0.4 70.0 +1.3 63.2 +0.8\nCountryInPrompt+LLM Reg 73.6 +0.4 57.2 +2.8 63.8 +1.2 70.3 +1.6 64.0 +1.6\nRN50 CoOp [52] 64.6 -45.2 -51.6 -59.5 -52.2 -\nCoCoOp [51] 62.9 -44.5 -51.0 -58.3 -51.4 -\nKgCoOp [47] 63.5 -46.3 -53.9 -60.5 -53.9 -\nCountryInPrompt Reg 63.5 -1.1 48.0 +1.7 53.9 0.0 60.3 -0.2 54.3 +0.4\nCountryLLM Reg 64.5 -0.1 47.4 +1.1 54.2 +0.3 59.9 -0.6 54.3 +0.4\nCountryInPrompt+LLM Reg 65.5 +0.9 48.1 +1.8 54.5 +0.6 60.4 -0.1 54.8 +0.9\nTable 2. Regularizing soft prompts with geographical knowledge, top-1 bal. acc. on DollarStreet. We emphasize that our regulariza-\ntion aims to improve target performance, rather than source (gray, italicized ). Gains/drops are shown vs. the bestof soft prompt baselines\n(shaded). CountryInPrompt+LLM Reg achieves notable gains in target, especially on Africa. Methods use 16 shots per class.\ntion of CountryInPrompt and CountryLLM, achieving the\nbest Total top-1 performance for every encoder. The Total\ngains vs. default CLIP are as large as +3.4 (RN50). While\nCLIP has internal knowledge of country-specific categories,\nit may be incomplete and imprecise due to limited repre-\nsentation in the image-text training corpus. Adding LLM\nknowledge, trained on a purely textual corpus, may address\nsome gaps. CountryInPrompt+LLM is notably the top set-\nting in 3/4 regions for each encoder in top-1 accuracy.\n5.2. Soft Prompting\nWe next evaluate geography knowledge regularization\n(Sec. 3.2), our method to improve target performance by\nensuring that soft prompts do not overfit class text represen-\ntations to a source dataset with limited geographical rep-resentativeness ( e.g. only data from Europe). We compare\nregularization with ensembles of CountryInPrompt, Coun-\ntryLLM, and CountryInPrompt+LLM prompts vs. state-of-\nthe-art soft prompting methods in Tables 2/3.\nRegularizing soft prompts with target geographical\nknowledge reduces overfitting to source geographies.\nOur method effectively improves the ability of CLIP, with\nprompts trained only on images from Europe , to general-\nize to target countries. This observation is supported by\nTotal Target gains for CountryInPrompt, CountryLLM, and\nCountryInPrompt+LLM Reg on DollarStreet (+1.1/0.8/1.6\nover the best soft prompt baseline for ViT-B/16). Im-\nprovements are notable in Africa: CountryInPrompt+LLM\nachieves +2.8 for ViT-B/16 and +1.8 for RN50. The effec-\ntiveness extends to GeoNet in Table 3: target gains are +1.3\n13520\nSource Target\nEncoder Method USA Asia\nAcc∆ Acc∆\nViT-B/16 CoOp [52] 58.7 -51.2 -\nCoCoOp [51] 57.7 -52.6 -\nKgCoOp [47] 58.2 -52.6 -\nCIP Reg 57.5 -1.2 53.5 +0.9\nLLM Reg 58.5 -0.2 53.1 +0.5\nCIP+LLM Reg 57.6 -1.1 53.9 +1.3\nRN50 CoOp [52] 51.4 -45.6 -\nCoCoOp [51] 51.1 -46.3 -\nKgCoOp [47] 51.8 -46.9 -\nCIPReg 50.6 -1.2 47.6 +0.7\nLLMReg 51.8 0.0 47.4 +0.5\nCIP+LLMReg 51.1 -0.7 48.3 +1.4\nTable 3. Regularizing soft prompts with geographical knowl-\nedge, top-1 bal. accuracy on GeoNet. The regularization method\naccomplishes our goal to increase target performance in GeoNet’s\nUSA-to-Asia transfer setting. CIP = CountryInPrompt, LLM =\nCountryLLM, CIP+LLM = CountryInPrompt+LLM.\nThreshold t(# Classes)\nMethod <40% <60% <80% ≤100%\n(13)∆(45)∆(77)∆(95)∆\nCoOp [52] 31.2 -45.6 -55.6 -61.7 -\nCoCoOp [51] 32.8 +1.6 45.4 -0.255.2 -0.461.4 -0.3\nKgCoOp [47] 35.3 +4.1 47.9 +2.3 56.7 +1.1 62.4 +0.7\nCIPReg 37.5 +6.3 48.9 +3.3 57.8 +2.2 63.5 +1.8\nLLMReg 36.8 +5.6 48.1 +2.5 57.2 +1.6 63.2 +1.5\nCIP+LLMReg 39.9 +8.7 49.5 +3.9 58.2 +2.6 64.0 +2.3\nTable 4. Performance on DollarStreet classes with less than t%\nrecall in CoOp , with ViT-B/16. Gains w.r.t. CoOp of our ge-\nography knowledge regularization are especially large for CoOp’s\ndifficult classes (+8.7 in <40%), compared to KgCoOp’s (+4.1 in\n<40%, i.e. a 4.6difference from ours). CIP = CountryInPrompt,\nLLM = CountryLLM, CIP+LLM = CountryInPrompt+LLM.\nfor ViT-B/16 and +1.4 for RN50. The combined strategy\nworks best on target, showing the value of incorporating\ndescriptive knowledge. Since regularization prevents over-\nfitting and potentially optimal source performance, we natu-\nrally observe source drops for CountryInPrompt and Coun-\ntryLLM in Tables 2/3. However, CountryInPrompt+LLM\nin Table 2 even offers source gains. It is also notable that\nthere are small drops in Americas for RN50, but upon in-\nspection, countries in North America overall have a -0.9\ndrop, while ones in Central/South America have a +0.8\ngain. These results concur with our hypothesis that CLIP is\nalready aligned to countries like the USA. More is in supp.,\nalong with experiments varying the source and ensemble.\nRegularization helps significantly on difficult classes. As\ncertain objects may be especially sensitive to geographi-\ncal domain shift, we break down classwise performance on\nDollarStreet in Table 4, using a stratification of class diffi-\nculty based on default soft prompting performance (CoOp).\nThe CountryInPrompt+LLM strategy achieves significant\nFigure 3. Geography knowledge-regularized soft prompts\ntrained on source data (ours, green line) vs. few-shot soft\nprompts trained on target data (blue curve) . (a) Src=Europe,\nTgt=Africa,Asia,Amer.; (b) Src=USA,Tgt=Asia. Our 16-shot\nmodel trained on only source data (green) outperforms a model\nwith prompts trained on 12 or 4 shots per class of target data (on\nDollarStreet&GeoNet, resp.), which is 1140&2400 images total.\ngains on the classes most difficult with respect to the CoOp\nbaseline . In particular, gains of +8.7% in balanced accu-\nracy are achieved for classes with <40% baseline recall,\nwhile the highest achieved by KgCoOp is 4.1%. Exam-\nple classes in this subset are snacks ,clothes , and makeup .\nThe DollarStreet classes with greatest improvement, inde-\npendent of original CoOp accuracy are: piercings ,clothes ,\nhomes ,medication , and refrigerators (all at least +14% over\nCoOp). In GeoNet, dome ,goby (fish), eland (antelope), and\ngloriosa (flower) have >20 samples and >30% improve-\nment. A total of 64/95 classes in DollarStreet and 209/344\nin GeoNet improve vs. CoOp, showing broad coverage.\nRegularized source-only prompts outperform few-shot\ntarget-trained prompts. Given that soft prompts can show\nstrong performance in few-shot settings, a potential alter-\nnative to regularizing soft prompts on source data is to di-\nrectly acquire a few examples of target data for training. We\nevaluate this setting by splitting target data into train/test,\nand training CoOp at varying # of shots of target data for\nGeoNet and DollarStreet, shown in Fig. 3. Notably, training\non 16 shots per class of source data with our regularization\nmethod outperforms using 12 & 4 shots per class of target\ndata on DollarStreet & GeoNet. This is a vast amount of\ntarget data overall ( e.g. 12 shots x 95 classes = 1140 target\nsamples in DollarStreet, 4 shots x 600 classes = 2400 sam-\nples in GeoNet). The baseline CoOp trained on 16 shots of\nsource data only outperforms an 8-shot/2-shot target-trained\nCoOp model (DollarStreet/GeoNet). Our strategy is thus\nmore compelling in the absence of a lot of target data.\nPerformance by income. DollarStreet provides esti-\nmated monthly income of the household in which an im-\nage was captured. We evaluate with the delineation of\nlow, medium, and high-income buckets from [13]. Com-\npared to CoOp/KgCoOp, CountryInPrompt+LLM gains are\n+2.5/+3.4 in low, +2.4/+1.5 in medium, and +2.1/+0.7 in\nhigh. Thus our method especially improves in low-income\nareas, though it helps across levels. The table is in supp.\n13521\nStatistic CIP CountryLLM CIP+LLM\nGDP Per Capita (US $) 0.219 0.063 0.217\nHuman Devel. Index (HDI) 0.439 0.385 0.451\nLand Area (km2) -0.072 0.050* -0.046*\nPopulation (#) -0.131 0.077 -0.123\nPopulation Density (#/km2)0.103 0.158 0.081\n% Agricultural Land 0.139 0.070 0.122\n% Forest Area 0.191 0.087 0.201\nAvg. Yearly Temp. (◦C) 0.380 0.256 0.391\nAvg. Yearly Precip. (mm/year) 0.236 0.124 0.230\nTable 5. Correlation (Pearson’s ρ) of avg. CLIP class text em-\nbedding distance and country statistic difference ,e.g. economic\n(GDP per capita, HDI) and climate factors (temperature, precipi-\ntation, forest area). We use the 63 countries in DollarStreet (1,953\npairs). Bold values have ρ>0.2, * means not significant ( α=0.01).\nFigure 4. Qualitative analysis. We show examples where\ngeography-specific descriptors improve/hurt vs. general descrip-\ntors in zero-shot inference. We highlight the prediction’s descrip-\ntors, bolding the highest activating one. Encoder=RN50.\n5.3. Further Analysis\nAre descriptions correlated with key country statistics?\nFor a pair of countries, we compute two values. We measure\nthe distance between each class embedding and take the av-\nerage overall distance as one value. We also take the abso-\nlute difference between statistics for those countries (from\n[2, 42], e.g. difference in avg. yearly temperature) as the\nother value. We compute the correlation between these two\nvalues over every unique country pair in DollarStreet, show-\ning results in Table 5. We find that the strongest correlation\nacross each prompt type is with HDI, which summarizes\nhuman development. It is notable that factors like yearly\ntemperature and precipitation also show moderate correla-\ntions, indicating a potential role of climate. Future work\nmay further explore how object differences present with re-\nspect to these factors. It will also be critical to ensure that\ndifferences between countries are representative and not ex-\naggerated in embeddings.\nDescriptor topics. We show a UMAP [29] visualization\ncomparing CountryLLM text embeddings for the category\nhomes across geographies in Fig. 5. Countries tend to group\nFigure 5. UMAP [29] plot for CountryLLM and the category\nhomes in DollarStreet. Country-specific descriptors are often\nclose to those of other countries intra-continent, likely due to sim-\nilar weather, environment, and/or economic conditions.\nby continent, showing the representations may capture sim-\nilarity in features like climate and/or economics. We exam-\nine a few topics mentioned in the CountryLLM descriptors\nforhomes . While “stone” is described across continents,\n“bright colors” and “mud” are mentioned mostly in Africa,\nand “balcony” in Europe and Asia. We show more in supp.\nSuccess and failure examples. We provide examples of\nCountryLLM vs. GeneralLLM in Fig. 4 on DollarStreet.\nThe model captures geographical descriptive knowledge\nlike “sandy colored stucco walls” for homes in Jordan, a fea-\nture which may be less common for Western homes. Some-\ntimes the model may be too attentive to attributes, leading\nto confusion ( e.g. choosing rugover bed). Future work that\nenhances alignment in VLMs can likely improve results.\n6. Conclusion\nIn this work, we bring attention to how various strategies\nto prompt CLIP affect recognition performance across ge-\nographies. In addition, through soft prompting with de-\nscriptive knowledge, we provide a mechanism to achieve\na more geo-generalizable set of class representations across\nregions. Our work is only a first step in this important area.\nLimitations and ethical considerations. While our\nmethod’s proof of concept is demonstrated in a positive ef-\nfort to debias CLIP’s default representations through diver-\nsity, due to the biased worldview of the Internet, CLIP’s\nrepresentations are likely inadequate, exaggerated, and/or\nnot fully representative for some countries. While we ex-\npect quality LLM knowledge to guide better representa-\ntions, LLM knowledge can also be incorrect (e.g., through\nhallucination), imprecise, or biased.\nFuture work. For the above reasons, our future efforts aim\nto ensure more representative VLM/LLM knowledge. We\nstrongly advocate for the community to seek communica-\ntion with diverse groups within all countries (i.e. to capture\nareas that range from low to high income) to ensure better\nrepresentation and fairness in AI technology use. There are\nnotable continent-level disparities to still improve upon.\nAcknowledgement. This work was supported by National\nScience Foundation Grants No. 2006885 and 2329992.\n13522\nReferences\n[1] Common Crawl, 2023. https://commoncrawl.org/. 3\n[2] Wikipedia, 2023. https://en.wikipedia.org/. 3, 8\n[3] Abhipsa Basu, R. Venkatesh Babu, and Danish Pruthi. In-\nspecting the geographical representativeness of images from\ntext-to-image models. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision (ICCV) , pages\n5136–5147, 2023. 3\n[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-\nguage models are few-shot learners. Advances in Neural In-\nformation Processing Systems , 33:1877–1901, 2020. 2\n[5] Pei-Yu Peggy Chi, Matthew Long, Akshay Gaur, Abhi-\nmanyu Deora, Anurag Batra, and Daphne Luong. Crowd-\nsourcing images for global diversity. In Proceedings of the\n21st International Conference on Human-Computer Interac-\ntion with Mobile Devices and Services , pages 1–10, 2019.\n2\n[6] Jonathan H Choi, Kristin E Hickman, Amy Monahan, and\nDaniel Schwarcz. ChatGPT goes to law school. Available at\nSSRN , 2023. 2\n[7] Terrance De Vries, Ishan Misra, Changhan Wang, and Lau-\nrens Van der Maaten. Does object recognition work for ev-\neryone? In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition Workshops , pages\n52–59, 2019. 1\n[8] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. International Con-\nference on Learning Representations (ICLR) , 2021. 5\n[9] Abhimanyu Dubey, Vignesh Ramanathan, Alex Pentland,\nand Dhruv Mahajan. Adaptive methods for real-world do-\nmain generalization. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition , pages\n14340–14349, 2021. 2\n[10] Yaroslav Ganin and Victor Lempitsky. Unsupervised domain\nadaptation by backpropagation. In International Conference\non Machine Learning , pages 1180–1189. PMLR, 2015. 2\n[11] William Gaviria Rojas, Sudnya Diamos, Keertan Kini, David\nKanter, Vijay Janapa Reddi, and Cody Coleman. The Dollar\nStreet dataset: Images representing the geographic and so-\ncioeconomic diversity of the world. In Advances in Neural\nInformation Processing Systems , pages 12979–12990, 2022.\n1, 2, 5\n[12] Chunjiang Ge, Rui Huang, Mixue Xie, Zihang Lai, Shiji\nSong, Shuang Li, and Gao Huang. Domain adaptation via\nprompt learning. arXiv preprint arXiv:2202.06687 , 2022. 2\n[13] Priya Goyal, Adriana Romero Soriano, Caner Hazirbas, Lev-\nent Sagun, and Nicolas Usunier. Fairness indicators for sys-\ntematic assessments of visual feature extractors. In Proceed-\nings of the 2022 ACM Conference on Fairness, Accountabil-\nity, and Transparency , pages 70–88, 2022. 7\n[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-ings of the IEEE Conference on Computer Vision and Pattern\nRecognition , pages 770–778, 2016. 5\n[15] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor\nMordatch. Language models as zero-shot planners: Extract-\ning actionable knowledge for embodied agents. In Interna-\ntional Conference on Machine Learning , pages 9118–9147.\nPMLR, 2022. 2\n[16] Plotly Technologies Inc. Plotly: The python graphing library,\n2023. https://plotly.com/python/. 1\n[17] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,\nHieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom\nDuerig. Scaling up visual and vision-language representa-\ntion learning with noisy text supervision. In International\nConference on Machine Learning , pages 4904–4916. PMLR,\n2021. 2\n[18] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie,\nSerge Belongie, Bharath Hariharan, and Ser-Nam Lim. Vi-\nsual prompt tuning. In European Conference on Computer\nVision , pages 709–727. Springer, 2022. 2\n[19] Ying Jin, Ximei Wang, Mingsheng Long, and Jianmin Wang.\nMinimum class confusion for versatile domain adaptation.\nInEuropean Conference on Computer Vision (ECCV) , pages\n464–480. Springer, 2020. 2\n[20] Tarun Kalluri, Astuti Sharma, and Manmohan Chandraker.\nMemsac: Memory augmented sample consistency for large\nscale domain adaptation. In European Conference on Com-\nputer Vision , pages 550–568. Springer, 2022. 2\n[21] Tarun Kalluri, Wangdong Xu, and Manmohan Chandraker.\nGeonet: Benchmarking unsupervised adaptation across ge-\nographies. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , pages 15368–\n15379, 2023. 1, 2, 5\n[22] Baoshuo Kan, Teng Wang, Wenpeng Lu, Xiantong Zhen,\nWeili Guan, and Feng Zheng. Knowledge-aware prompt tun-\ning for generalizable vision-language models. In Proceed-\nings of the IEEE/CVF International Conference on Com-\nputer Vision , pages 15670–15680, 2023. 2\n[23] Muhammad Uzair Khattak, Hanoona Rasheed, Muhammad\nMaaz, Salman Khan, and Fahad Shahbaz Khan. Maple:\nMulti-modal prompt learning. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 19113–19122, 2023. 2\n[24] Ananya Kumar, Aditi Raghunathan, Robbie Jones, Tengyu\nMa, and Percy Liang. Fine-tuning can distort pretrained\nfeatures and underperform out-of-distribution. International\nConference on Learning Representations (ICLR) , 2022. 4\n[25] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare,\nShafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi.\nAlign before fuse: Vision and language representation learn-\ning with momentum distillation. Advances in Neural Infor-\nmation Processing Systems , 34:9694–9705, 2021. 2\n[26] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2: Bootstrapping language-image pre-training with\nfrozen image encoders and large language models. Inter-\nnational Conference on Machine Learning , 2023. 2\n[27] Mingsheng Long, Yue Cao, Jianmin Wang, and Michael Jor-\ndan. Learning transferable features with deep adaptation net-\n13523\nworks. In International Conference on Machine Learning ,\npages 97–105. PMLR, 2015. 2\n[28] Mingsheng Long, Zhangjie Cao, Jianmin Wang, and\nMichael I Jordan. Conditional adversarial domain adapta-\ntion. Advances in Neural Information Processing Systems ,\n31, 2018. 2\n[29] Leland McInnes, John Healy, Nathaniel Saul, and Lukas\nGrossberger. UMAP: Uniform manifold approximation and\nprojection. The Journal of Open Source Software , 3(29):861,\n2018. 8\n[30] Sachit Menon and Carl V ondrick. Visual classification via\ndescription from large language models. International Con-\nference on Learning Representations, ICLR , 2023. 2, 3, 4, 5,\n6\n[31] OpenAI. ChatGPT: Optimizing language models for dia-\nlogue, 2022. 2\n[32] OpenAI. GPT-4 technical report, 2023. 2\n[33] Viraj Prabhu, Ramprasaath R Selvaraju, Judy Hoffman, and\nNikhil Naik. Can domain adaptation make object recognition\nwork for everyone? In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition , pages\n3981–3988, 2022. 1, 2\n[34] Sarah Pratt, Ian Covert, Rosanne Liu, and Ali Farhadi. What\ndoes a platypus look like? Generating customized prompts\nfor zero-shot image classification. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision ,\npages 15691–15701, 2023. 2, 3\n[35] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario\nAmodei, Ilya Sutskever, et al. Language models are unsu-\npervised multitask learners. OpenAI blog , 1(8):9, 2019. 3\n[36] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In International Conference on Machine Learning ,\npages 8748–8763. PMLR, 2021. 2, 3, 6\n[37] Vikram V Ramaswamy, Sing Yu Lin, Dora Zhao, Aaron B\nAdcock, Laurens van der Maaten, Deepti Ghadiyaram, and\nOlga Russakovsky. GeoDE: a geographically diverse eval-\nuation dataset for object recognition. Advances in Neural\nInformation Processing Systems , 2023. 2\n[38] Kuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, and Tat-\nsuya Harada. Maximum classifier discrepancy for unsuper-\nvised domain adaptation. In Proceedings of the IEEE Con-\nference on Computer Vision and Pattern Recognition , pages\n3723–3732, 2018. 2\n[39] Shreya Shankar, Yoni Halpern, Eric Breck, James Atwood,\nJimbo Wilson, and D. Sculley. No classification without rep-\nresentation: Assessing geodiversity issues in open data sets\nfor the developing world. In NeurIPS 2017 Workshop: Ma-\nchine Learning for the Developing World , 2017. 1\n[40] Manli Shu, Weili Nie, De-An Huang, Zhiding Yu, Tom\nGoldstein, Anima Anandkumar, and Chaowei Xiao. Test-\ntime prompt tuning for zero-shot generalization in vision-\nlanguage models. Advances in Neural Information Process-\ning Systems , 35:14274–14289, 2022. 2\n[41] Chan Hee Song, Jiaman Wu, Clayton Washington, Brian M.\nSadler, Wei-Lun Chao, and Yu Su. LLM-planner: Few-shotgrounded planning for embodied agents with large language\nmodels. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision (ICCV) , 2023. 2\n[42] The World Bank. World Bank - Indicators, 2023.\nhttps://data.worldbank.org/indicator/. 8\n[43] Yan Wang, Xiangyu Chen, Yurong You, Li Erran Li, Bharath\nHariharan, Mark Campbell, Kilian Q Weinberger, and Wei-\nLun Chao. Train in Germany, test in the USA: Making 3D\nobject detectors generalize. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\npages 11713–11723, 2020. 1, 2\n[44] Guoqiang Wei, Cuiling Lan, Wenjun Zeng, Zhizheng Zhang,\nand Zhibo Chen. Toalign: Task-oriented alignment for un-\nsupervised domain adaptation. Advances in Neural Informa-\ntion Processing Systems , 34:13834–13846, 2021. 2\n[45] Ruijia Xu, Guanbin Li, Jihan Yang, and Liang Lin. Larger\nnorm more transferable: An adaptive feature norm approach\nfor unsupervised domain adaptation. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision ,\npages 1426–1435, 2019. 2\n[46] Zhengyuan Yang*, Linjie Li*, Jianfeng Wang*, Kevin Lin*,\nEhsan Azarnasab*, Faisal Ahmed*, Zicheng Liu, Ce Liu,\nMichael Zeng, and Lijuan Wang. MM-ReAct: Prompt-\ning ChatGPT for multimodal reasoning and action. arXiv\npreprint arXiv:2303.11381 , 2023. 2\n[47] Hantao Yao, Rui Zhang, and Changsheng Xu. Visual-\nlanguage prompt tuning with knowledge-guided context op-\ntimization. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , pages 6757–\n6767, 2023. 2, 5, 6, 7\n[48] Da Yin, Feng Gao, Govind Thattai, Michael Johnston, and\nKai-Wei Chang. Givl: Improving geographical inclusivity of\nvision-language models with pre-training methods. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition , pages 10951–10961, 2023. 2\n[49] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mo-\njtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive\ncaptioners are image-text foundation models. Transactions\non Machine Learning Research , 2022. 2\n[50] Yuchen Zhang, Tianle Liu, Mingsheng Long, and Michael\nJordan. Bridging theory and algorithm for domain adap-\ntation. In International Conference on Machine Learning ,\npages 7404–7413. PMLR, 2019. 2\n[51] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei\nLiu. Conditional prompt learning for vision-language mod-\nels. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR) , pages 16816–\n16825, 2022. 2, 5, 6, 7\n[52] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei\nLiu. Learning to prompt for vision-language models. In-\nternational Journal of Computer Vision , 130(9):2337–2348,\n2022. 2, 4, 5, 6, 7\n13524'}, 'dist': 0.9286905527114868}
Result 6: {'text': 'data', 'metadata': {'created_at': 1736932570, 'modified_at': 1736932570, 'owner': 'sayandeep', 'path': 'RAG_CONTENT/conferences/cvpr_ocr/Chen_MoCha-Stereo_Motif_Channel_Attention_Network_for_Stereo_Matching_CVPR_2024_paper.txt', 'size': 42452, 'seen_at': 1737191136, 'data': 'MoCha-Stereo: Motif Channel Attention Network for Stereo Matching\r\nZiyang Chen1*Wei Long1*He Yao1*Yongjun Zhang1†\r\nBingshu Wang2Yongbin Qin1Jia Wu1\r\n1†College of Computer Science and Technology, The State Key Laboratory of Public Big Data,\r\nInstitute of Artificial Intelligence, Guizhou University\r\n2College of Software, Northwest Polytechnical University\r\nziyangchen2000@gmail.com; zyj6667@126.com†\r\nFigure 1. Motivation. Addressing the issue of geometric structure loss in feature channels arising from deep learning. (From left to\r\nright: Input image, visualization of a Normal Channel, visualization of a Motif [39] Channel.) Due to the fuzziness of geometric edges in\r\ncertain channels, achieving accurate matching of stereo image edges is a challenging problem. MoCha-Stereo guides ordinary channels to\r\nfocus on edge features through motif channels, achieving more accurate detail matching. Motif Channel refers to channel that composed\r\nof repeatedly occurring geometric contours. The regions delineated by the yellow border represent the magnified details.\r\nAbstract\r\nLearning-based stereo matching techniques have made\r\nsignificant progress. However, existing methods inevitably\r\nlose geometrical structure information during the feature\r\nchannel generation process, resulting in edge detail mis-\r\nmatches. In this paper, the MotifChannel Attention Stereo\r\nMatching Network ( MoCha-Stereo ) is designed to address\r\nthis problem. We provide the Motif Channel Correlation\r\nVolume (MCCV) to determine more accurate edge match-\r\ning costs. MCCV is achieved by projecting motif chan-\r\nnels, which capture common geometric structures in fea-\r\nture channels, onto feature maps and cost volumes. In ad-\r\ndition, edge variations in the reconstruction error map also\r\naffect details matching, we propose the Reconstruction Er-\r\nror Motif Penalty (REMP) module to further refine the full-\r\nresolution disparity estimation. REMP integrates the fre-\r\nquency information of typical channel features from the re-\r\nconstruction error. MoCha-Stereo ranks 1ston the KITTI-\r\n2015 and KITTI-2012 Reflective leaderboards. Our struc-\r\nture also shows excellent performance in Multi-View Stereo.\r\n*Co-first author.\r\n†Corresponding author.Code is avaliable at MoCha-Stereo.\r\n1. Introduction\r\nStereo matching remains a foundational challenge in\r\ncomputer vision, bearing significant relevance to au-\r\ntonomous driving, virtualization, rendering, and related sec-\r\ntors [40]. The primary goal of the assignment is to establish\r\na pixel-wise displacement map, or disparity, which can be\r\nused to identify the depth of the pixels in the scene. Edge\r\nperformance of disparity maps is particularly vital in tech-\r\nniques requiring pixel-level rendering, such as virtual real-\r\nity and augmented reality, where precise fitting between the\r\nscene model and image mapping is essential [23]. This un-\r\nderscores the need for a close alignment between the edges\r\nof the disparity map and the original RGB image.\r\nTraditional stereo matching relies on global [8], semi-\r\nglobal [13], or local [2] grayscale relationships between left\r\nand right view pixels. These methods struggle to fully lever-\r\nage scene-specific prior knowledge. Achieving optimal re-\r\nsults often involves human observation, this tuning process\r\ncan be resource-intensive in scenes with complex images\r\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\r\nExcept for this watermark, it is identical to the accepted version;\r\nthe final published version of the proceedings is available on IEEE Xplore.\r\n27768\r\n[41]. With the advancement of deep learning, learning-\r\nbased methods [15, 30, 40] have generally achieved better\r\nresults. A case in point is RAFT-Stereo [17], which in-\r\ntroduced a coarse-to-fine scheme by computing All-Pairs\r\nCorrelation (APC). To comprehensively learn channel fea-\r\ntures, GwcNet [12] proposes a method of computing corre-\r\nlations by grouping left and right features along the channel\r\ndimension, called Group-Wise Correlation (GWC). IGEV-\r\nStereo [36] introduces Combined Geometry Encoding V ol-\r\nume (CGEV) , a cost calculation method that combines\r\nGWC [12] and APC [17], this cost calculation approach has\r\nachieved state-of-the-art results. There is also a body of re-\r\nsearch focused on obtaining more accurate matching results\r\nthrough post-processing of disparities. They [3, 16, 47] uti-\r\nlize CNN structures directly applied to the additional error\r\nmaps in the hope of achieving better results.\r\nLearning-based methods have achieved impressive re-\r\nsults. However, numerous channels experience loss of ge-\r\nometric details during the generation of feature channels.\r\nThis phenomenon leads to a mismatch in the representation\r\nof object edges. Loss of geometric edges in the channel\r\nis a challenging problem because each block of the neural\r\nnetwork performs non-linear transformations [31], exces-\r\nsive nonlinearity can saturate activations in some channels,\r\nand insufficient nonlinearity leads to inadequate values. It\r\nis difficult for deep learning to directly recover geometric\r\ndetails. As shown in the middle picture of Fig. 1, certain\r\nnormal channels suffer from severe blurring. Details loss in\r\nchannels naturally complicates the matching of edges.\r\nTo address the above problems, we propose Motif\r\nChannel Attention Stereo Matching Network (MoCha-\r\nStereo). The core idea of MoCha-Stereo is to restore the\r\nlost detailed features in feature channels by utilizing the re-\r\npeated geometric contours within normal channels. Chan-\r\nnels that preserve the common features are referred to as\r\nmotif channels . The following improvements are pre-\r\nsented:\r\n1) We introduce a novel stereo matching framework that in-\r\ncorporates repeated geometric contours. This architecture\r\nenables more accurate cost computation and disparity esti-\r\nmation through detail restoration of feature channels.\r\n2) We propose Motif Channel Attention (MCA) to mitigate\r\nimbalanced nonlinear transformations in network training.\r\nMCA optimize feature channels through motif channel pro-\r\njection instead of direct network optimization. Inspired by\r\ntime-series motif mining, we capture motif channel using\r\nsliding windows.\r\n3) To achieve more precise matching cost computation for\r\nedge matching, we construct the Channel Affinity Matrix\r\nProfile (CAMP)-guided correlation volume. This volume\r\nis derived from the correlation matrix between normal and\r\nmotif channels, then mapped onto the base correlation vol-\r\nume to produce a more rational cost volume called Motif\r\n(a)\r\n (b)\r\nFigure 2. (a) Comparison with SOTA methods [15, 17, 28, 36] on\r\nKITTI 2012 [9] and 2015 leaderboards [22] (lower is better). (b)\r\nPerformance evaluation of the Scene Flow test set [21] in compar-\r\nison to IGEV-Stereo [36] and DLNR [47] as the number of itera-\r\ntions changes (lower EPE means better).\r\nChannel Correlation V olume (MCCV).\r\n4) To leverage the geometric information of the potential\r\nchannels in the reconstruction error map, we develop Re-\r\nconstruction Error Motif Penalty (REMP) to extract the mo-\r\ntif channels from the error map, optimising the disparity\r\nbased on the high and low-frequency signals.\r\nWe validated the performance of MoCha-Stereo on sev-\r\neral leaderboards. As shown in Fig. 2 (a), MoCha-Stereo\r\nranks 1ston KITTI 2015 [22] and KITTI 2012 Reflective\r\n[9], achieves SOTA result in Scene Flow [21], zero-shot\r\nperformance [24, 25], and MVS domain [14]. Our designs\r\nalso make iteration more efficient. As illustrated in Fig. 2\r\n(b), MoCha-Stereo achieves superior results with fewer it-\r\nerations, allowing users to choose between efficient or high-\r\nprecision settings based on their preferences.\r\n2. Related Work\r\n2.1. Motif Mining for Time series analysis\r\nThe concept of motif originates from time series analysis.\r\nMotif mining has become one of the most commonly used\r\nprimitives in time series data mining [6, 39]. In a time series\r\nT, there exists a subsequence Ti,L, which starts from the i-\r\nth position in the time series Tand is a continuous subset of\r\nvalues with a length of L. Motif is the pair of subsequences\r\nTa,LandTb,Lin a time series that are the most similar. In\r\nmathematical notation, For case ∀i, j∈[1,2, ..., n−L+ 1]\r\nanda̸=b , i̸=j, the motif [1] satisfies as Equ. 1.\r\ndist(Ta,L, Tb,L)≤dist(Ti,L, Tj,L) (1)\r\nwhere dist means a distance measure. The distances be-\r\ntween all subsequences and their nearest neighbors are\r\n27769\r\nFigure 3. Overview of MoCha-Stereo. MoCha-Stereo initially constructs the Motif Channel Correlation V olume ( MCCV ) by projecting\r\nthe relationship between motif channels and normal channels into the basic group correlation volume. Subsequently, based on this cost\r\nvolume, we employ an iterative way to build the disparity map. Finally, the Reconstruction Error Motif Penalty ( REMP ) module is applied\r\nto penalize the generation of the full-resolution disparity map. In REMP, LFE refers to the Low-frequency Error branch, LMC denotes\r\nto the Latent Motif Channel branch, and HFE means the High-frequency Error branch. The star symbol means our primary innovations.\r\nstored as Matrix Profile ( MP) [39], representing the repet-\r\nitive pattern in the time series. This numerical relational\r\ninformation is utilized to identify recurring patterns, detect\r\nanomalies, and perform pattern matching in time series.\r\nFor the channel features, the geometric structure of\r\ngraphics theoretically repeats as well. Differing from time\r\nseries, the repetitive patterns we aim to uncover represent\r\nthe geometric structures in images, necessitating consider-\r\nation of two-dimensional contextual information. Further-\r\nmore, the computation of MP requires multiple samplings\r\nof subsequences from the series. Selecting sub-patches\r\nfrom multi-channel image features for computing similar-\r\nity is computationally expensive. Therefore, we develop a\r\ncollection of adaptable sliding windows that are arranged\r\ninto two-dimensional vectors. These windows are used for\r\ncapturing repeating geometrical patterns. These repeated\r\npatterns are stored as motif channels for MoCha-Stereo.\r\n2.2. Learning-based Stereo Matching\r\nThe field of stereo matching has witnessed considerable\r\nadvancements owing to learning-based techniques in recent\r\nyears. Gwc-Net [12] introduced the novel concept of GWC\r\nvolume, which served as a major inspiration for future land-\r\nmark achievements in the field [18, 27]. RAFT-Stereo [17]\r\nadvances the construction of the cost volume by deploying\r\nan APC pyramid V olume. Building upon the foundation\r\nset by Gwc-Net [12] and RAFT-Stereo [17], IGEV-Stereo\r\n[36] proposed a volume that combines APC [17] and GWC\r\n[12], utilizing an iterative indexing method for the update\r\nof the disparity map. Another portion of learning-basedmethods are focuse on obtaining more accurate matching\r\nresults through post-processing of disparities. iResNet [16]\r\nand DLNR [47] utilize UNet and Hourglass, respectively,\r\nemploying convolutional operations to directly output the\r\nconvolutional results of the reconstruction errors. DRNet\r\n[3] opts not to attach additional reconstruction error maps\r\nbut instead appends Geometric and Photometric Error.\r\nLearning-based methods have made significant progress.\r\nHowever, state-of-the-art algorithms [36, 47] inevitably lose\r\ngeometric details in certain channels. Few studies have con-\r\nsidered using repeated geometric profiles in multiple feature\r\nmaps to restore the edge texture of channel feature maps.\r\nAdditionally, there is limited recognition regarding the sig-\r\nnificance of the frequency information in the potential motif\r\nchannels from error maps for shaping the edges.\r\n3. Method\r\nTo address the aforementioned issues, we respectively in-\r\ntroduce MCCV and REMP as depicted in Fig. 3. These\r\ncomponents utilise the projection of Motif Channels to re-\r\ninstate the geometric structure of channel features.\r\n3.1. Context Extractor & Feature Extractor\r\nFollowing [17, 36], the context extractor consisting of\r\na series of residual blocks and downsampling layers, gen-\r\nerating context at 1/4,1/8, and 1/16scales. The fea-\r\nture extractor, which also follows [36], uses a backbone\r\npretrained on ImageNet [7] as the frozen layer [29]. The\r\nupsampling blocks utilizes skip connections from out-\r\nputs of downsampling blocks to obtain multi-scale outputs\r\n27770\r\nfl,i(fr,i)∈RCi×H\r\ni×H\r\ni(i= 4,8,16,32),Cirepresents the\r\nfeature channels, and fl(fr)denotes the left (right) view\r\nfeatures here.\r\n3.2. Motif Channel Correlation Volume\r\nAlthough multi-channel extractors contribute to the\r\nlearning of intricate features, an excess of nonlinearity may\r\nsaturate activation values in specific channels, while insuf-\r\nficient nonlinearity can yield suboptimal activation values.\r\nMCCV is proposed to address the imbalanced learning of\r\ngeometric structures in feature channels.\r\nMotif Channel Attention (MCA) for feature maps. Fea-\r\nture channels exhibit varying degrees of geometric struc-\r\nture loss, but the fundamental geometric structure of feature\r\nchannels is theoretically invariant. Inspired by motif min-\r\ning for time series [6], we use Nssets of sliding windows\r\nSW (Ns= 4), each with a length of 9, and organized the\r\nwindows into 3×3, to mine repetitive patterns. Unlike the\r\nwindow used for mining temporal motifs, we designed a\r\nset of adaptive-weight windows, rather than extracting val-\r\nues directly from the feature map. The initial values of the\r\nsliding window are set as random parameters. Based on\r\nthe gradient loss, we backpropagate to adjust the values of\r\nthe window weights. Following Equ. 2, we obtain the s-\r\nth (1≤s≤Ns) motif channel feature map fmc\r\nfrein the\r\nfrequency domain.\r\nfmc\r\nfre(s, h, w ) =NcX\r\nc=13X\r\ni=13X\r\nj=1\r\n(SW(s, h+i, w+j)×ffre(c, h, w )) (2)\r\nwhere (h, w)are the coordinates of the pixel, cdenotes the\r\nc-th feature channel, Ncis the number of normal feature\r\nchannels. The frequency domain feature ffre=F(f−\r\nG(f)).Fis the Fourier transform, Gdenotes Gaussian\r\nhigh-pass filter with 3×3kernels. This approach aims\r\nto capture repeatedly occurring frequency-domain features.\r\nThe rationale behind this is that edge textures often exhibit\r\nhigh-frequency expressions. We then transformed the re-\r\nsults back to the spatial domain, accumulating and normal-\r\nizing them to derive motif channels fmc. This aggrega-\r\ntion method takes into account the surrounding pixel infor-\r\nmation and strengthens attention to the geometric structure\r\nrepeated across channels through accumulation, enhancing\r\nthe reliability of matching for edge textures.\r\nChannel Affinity Matrix Profile (CAMP) guided Corre-\r\nlation Volume. In order to enhance the accuracy of match-\r\ning cost computation for edges with the assistance of mo-\r\ntif channels, we propose the Correlation V olume guided by\r\nCAMP, as shown in Fig. 3. The foundational cost volume\r\nstill employs the extraction of feature maps fl(r),4using\r\nGWC [12] according to Equ. 3.\r\nCg(d, h, w, g ) =1\r\nNc/Ng⟨fg\r\nl,4(h, w), fg\r\nr,4(h, w+d)⟩(3)where dis the disparity level, ⟨···,···⟩ is the inner prod-\r\nuct,Ncis the number of channels, Ngis the group num-\r\nber (Ng= 8). Caculating Correlation V olume solely from\r\nfeature maps makes it challenging to accurately match de-\r\ntails. This difficulty arises because certain channels lose\r\ngeometric structure. MoCha-Stereo exploits the motif chan-\r\nnelsfmcobtained by MCA and the normal channels fl,4\r\nobtained by feature extractor. Using their affinity, MoCha-\r\nStereo constructs CAMP, which is a matrix that stores the\r\nrelationship between motif channels and normal channels,\r\nas shown in Equ. 4.\r\nCAMP (s, c, h, w ) =fmc(s, h, w )×fl,4(c, h, w ),\r\nwhere 1≤s≤Ns,1≤c≤Nc (4)\r\nwhere sdenotes the s-th motif channel, and cdenotes the\r\nc-th normal channel. CAMP allows the projection of motif\r\nchannels onto normal channels, serving as a coefficient to\r\nmodulate the spatial domain information of normal chan-\r\nnels. This enhances attention to the geometric structure\r\nof channels. We can construct a Channel Correlation ( Cc)\r\nbased on CAMP. As shown in Equ. 5, Ccperforms spatial\r\ninteraction across channels, theoretically reinforcing fre-\r\nquent patterns in space, which are the desired geometric\r\nstructure.\r\nCc(d, h, w ) =NsX\r\ns=1NcX\r\nc=1⟨3DConv (CAMP (s, c, h, w )),\r\n3DConv (CAMP (s, c, h, w +d))⟩ (5)\r\nwher 3DConv means 3D convolution operator.\r\nTo obtain the final cost volume C, MoCha-Stereo uses\r\nCcas a weight-adjusted basis for the basic Correlation V ol-\r\nume. Since geometric structures are theoretically invariant,\r\nthere is no need to learn new Ccby adding extra groups.\r\nBroadcasting is sufficient to achieve the interaction between\r\nCcand GWC Cg, as illustrated in Equ. 6, enabling different\r\ngroups to learn the same set of geometric structure features.\r\nC(d, h, w ) =NgX\r\ng=1(Cg(d, h, w, g )×Cc(d, h, w )) (6)\r\n3.3. Iterative Update Operator\r\nFollowing [36, 46, 47], MoCha-Stereo utilizes the it-\r\nerative update operator [47] to obtain the disparity map\r\ndk=dk−1+△dkat 1/4 resolution for the k-th iteration.\r\n3.4. Reconstruction Error Motif Penalty\r\nThe disparity map output by the iteration is at a resolution\r\nof 1/4 of the original image. There is still room for opti-\r\nmization after upsampling the disparity map. Several works\r\n[16, 46, 47] have been dedicated to refinement networks\r\nbased on reconstruction error. However, none of these\r\nworks have addressed the separation of the low-frequency\r\n27771\r\nFigure 4. REMP module for Full-Resolution Refine. The upper\r\nbranch (LFE) obtains low-frequency information through pooling,\r\nthe lower branch (HFE) retains the original high-resolution im-\r\nage as high-frequency detailed information, and the middle branch\r\n(LMC) learns motif features through CNN.\r\nand high-frequency components of the reconstruction error,\r\nmaking the refinement process challenging to achieve more\r\neffective results. We propose the Reconstruction Error Mo-\r\ntif Penalty (REMP) module for Full-Resolution Refine. The\r\ndisparity map is a single-channel image. To obtain multi-\r\nchannel information, we draw inspiration from [3], using\r\nthe error as the input to the network and outputting multi-\r\nchannel features. In contrast to [3], we use reconstruction\r\nerror Eobtained by Equ. 7 and the disparity map dnfrom\r\nthe last iteration as inputs.\r\nE=Kl(R−TNT\r\nD)K−1\r\nrIr−Il (7)\r\nKl(r)represents the intrinsic matrix of the left (right) cam-\r\nera in the stereo system, Ris the rotation matrix from the\r\nright view coordinate system to the left view coordinate sys-\r\ntem,Tis the translation matrix from the right view coordi-\r\nnate system to the left view coordinate system, Nis the nor-\r\nmal vector of the object plane in the right view coordinate\r\nsystem, Dis the perpendicular distance between the object\r\nplane and the camera light source (this distance is obtained\r\nfrom the computed disparity), Il(r)is the left (right) image.\r\nAs illustrated in Fig. 4, the UNet in REMP is solely\r\ndesigned to obtain multi-channel features related to the\r\nreconstruction error Eand disparity map dn. The core\r\nidea of REMP is to optimize both high-frequency and low-\r\nfrequency errors in the disparity map using representative\r\nmotif information. REMP divides the features output by\r\nthe UNet into three branches. The pooling operation in the\r\nupper branch (LFE) effectively acts as a low-pass filter, pre-\r\nserving low-frequency information in the image while at-\r\ntenuating high-frequency information to some extent. The\r\nmiddle branch (LMC) guides the network in learning typ-\r\nical motif information, and the lower branch (HFE) un-\r\ndergoes no transformation, aiming to preserve the orig-inal high-frequency details of the high-resolution image.\r\nThrough the mappings of these three branches, we learn the\r\nfeature errors as penalties to refine the disparity map dn, as\r\nshown in Equ. 8.\r\no=UNet (Concat (d′\r\nn, E))\r\nHFE (o) =o⊙LMC (o)\r\ndn=d′\r\nn−Conv (LFE (o)⊙(1−LMC (o)) +HEF (o))\r\n(8)\r\nwhere ⊙means Hadamard product, d′\r\nnrepresents the dis-\r\nparity map before refinement, lfedenotes the computation\r\nprocess of low-frequency error, and lmc refers to the com-\r\nputation in the LMC branch.\r\n3.5. Loss Function\r\nThe computation of the loss function requires the dispar-\r\nity maps outputted at each iteration as well as the initial\r\ndisparity map d0. The initial disparity d0is obtained from\r\nthe V olume Vg(g∈ {1,2, . . . , N g})projected by Ccand\r\nGWCCg. For each group g(g≤Ng), the cost calculation\r\nmethod Cgis uniquely associated with a corresponding vol-\r\numeVg. We generate d0through Nggroups of correlation\r\nvolumes, expressed by Equ. 9.\r\nd0=SoftMax\x00\r\n3DConv\x00\r\nV1⊕V2⊕. . .⊕VNg\x01\x01\r\n(9)\r\nwhere ⊕refers to the concatenation operation performed\r\nalong the group dimension. The initial disparity d0serves as\r\nthe starting point for the iteration and is input to the update\r\nmodule. Following [36], the total loss is defined as Equ. 10.\r\nL=Smooth L1(d0−dgt) +nX\r\ni=1γn−i∥di−dgt∥1(10)\r\nwhere Smooth L1is defined by [10], γ= 0.9,dgtis ground\r\ntruth disparity.\r\n4. Experiment\r\n4.1. Implementation Details\r\nMoCha-Stereo is implemented using the PyTorch frame-\r\nwork, with the AdamW [19] optimizer employed during\r\ntraining. For training and ablation experiments, our model\r\nwas trained on the Scene Flow [21] for 200k epochs, with a\r\nbatch size of 8, which is equipped with 2NVIDIA A6000\r\nGPUs. To evaluate the performance of our model, we con-\r\nducted assessments using the KITTI-2012 [9], KITTI-2015\r\n[22], Scene Flow [21], ETH3D [25], and Middlebury [24].\r\nThe training and testing settings are consistent with [17, 36].\r\n27772\r\nMethod Lac-GwcNet[18] UPFNet [5] ACVNet [35] DLNR [47] IGEV-Stereo [36] MoCha-Stereo (Ours)\r\nEPE (px) ↓ 0.75 0.71 0.48 0.48 0.47 0.41 (−12.77%)\r\nTime (s) ↓ 0.65 0.27 0.48 0.30 0.37 0.34\r\nTable 1. Quantitative evaluation on Scene Flow test set. The best result is bolded, and the second-best result is underscored. The variations\r\nin the performance of our method compared to the optimal results of other methods are indicated in red font.\r\nAll Reflective\r\nOut-Noc Out-All Avg-Noc Avg-All Out-Noc Out-All Avg-Noc Avg-All Method\r\n(%)↓ (%)↓ (px)↓ (px)↓ (%)↓ (%)↓ (px)↓ (px)↓\r\nGwcNet[12] 1.32 1.70 0.5 0.5 7.80 9.28 1.3 1.4\r\nAcfNet[45] 1.17 1.54 0.5 0.5 6.93 8.52 1.8 1.9\r\nRAFT-Stereo [17] 1.30 1.66 0.4 0.5 5.40 6.48 1.3 1.3\r\nHITNet[30] 1.41 1.89 0.4 0.5 5.91 7.54 1.0 1.2\r\nCREStereo[15] 1.14 1.46 0.4 0.5 6.27 7.27 1.4 1.4\r\nLac-GwcNet [18] 1.13 1.49 0.5 0.5 6.26 8.02 1.5 1.7\r\nIGEV-Stereo [36] 1.12 1.44 0.4 0.4 4.35 5.00 1.0 1.1\r\nMoCha-Stereo(Ours) 1.06 (−5.36%) 1.36 0.4 0.4 3.83 (−11.95%) 4.50 0.8 0.9\r\nTable 2. Results on the KITTI-2012 leaderboard. Out-Noc represents the percentage of erroneous pixels in non-occluded areas, Out-All\r\ndenotes the percentage of erroneous pixels in the entire image. Avg-Noc refers to the end-point error in non-occluded areas, Avg-All\r\nindicates the average disparity error across the entire image. Error threshold is 3 px.\r\nAll pixels (%) ↓ Noc pixels (%) ↓\r\nMethodbg fg all bg fg all\r\nGwcNet[12] 1.74 3.93 2.11 1.61 3.49 1.92\r\nRAFT-Stereo[17] 1.58 3.05 1.82 1.45 2.94 1.69\r\nCREStereo[15] 1.45 2.86 1.69 1.33 2.60 1.54\r\nLac-GwcNet[18] 1.43 3.44 1.77 1.30 3.29 1.63\r\nCFNet[26] 1.54 3.56 1.81 1.43 3.25 1.73\r\nUPFNet[5] 1.38 2.85 1.62 1.26 2.70 1.50\r\nCroCo-Stereo[34] 1.38 2.65 1.59 1.30 2.56 1.51\r\nIGEV-Stereo[36] 1.38 2.67 1.59 1.27 2.62 1.49\r\nDLNR[47] 1.60 2.59 1.76 1.45 2.39 1.61\r\nMoCha-Stereo1.36 2.431.531.24 2.421.44\r\n(Ours) −3.77% −3.36%\r\nTable 3. Results on the KITTI-2015 leaderboard. Error threshold\r\nis 3 px. Background error is indicated by bg, and front-ground\r\nerror by fg.\r\n4.2. Comparisons with State-of-the-art\r\nWe contrast the SOTA techniques on KITTI-2015 [22],\r\nKITTI-2012 [9], and Scene Flow [21]. MoCha-Stereo\r\nachieves excellent performance on each of the aforemen-\r\ntioned datasets. On the Scene Flow [21] dataset, MoCha-\r\nStereo achieves a new SOTA EPE of 0.41, which surpasses\r\nIGEV-Stereo [36] by a margin of 12.77% . The quantitative\r\ncomparisons are presented in Tab. 1.\r\nIn order to validate the performance of MoCha-Stereo\r\nin real-world scenarios, we conducted experiments on\r\nthe KITTI-2012 [9] and KITTI-2015 [22] benchmarks.\r\nMoCha-Stereo ranks 1stamong all the methods submitted\r\nto these online benchmarks. Evaluation details are shown\r\nin Tab. 2 and Tab. 3. We also provide visualizations ofMoCha-Stereo and compare it with existing SOTA algo-\r\nrithms [17, 18, 36] in Fig. 5. Moreover, MoCha-Stereo\r\nachieves 1stresult in reflective regions, where determin-\r\ning geometric edges is often more challenging, as shown\r\nin Tab. 2. MoCha-Stereo is the first algorithm to control\r\nAvg-Noc to within 0.8px under 5px error threshold and to\r\ncontrol Out-Noc to less than 4%for an error threshold of 3\r\npx among all published methods.\r\n4.3. Zero-shot Generalization\r\nDue to the difficulty in obtaining a large amount of\r\nground truth for real-world scenes, generalization ability\r\nis also crucial. We evaluate the generalization ability of\r\nMoCha-Stereo by testing it on the Middlebury [24] and\r\nETH3D [25] datasets without fine-tune. As illustrated in\r\nFig. 6 and Tab. 4, our method exhibits SOTA performance\r\nin the zero-shot scenarios.\r\n4.4. Extension to MVS\r\nMoCha-Stereo has been extended as MoCha-MVS for\r\napplication in the field of MVS. Compared to recent\r\nlearning-based MVS methods, MoCha-MVS achieves ex-\r\ncellent performance by balancing accuracy and complete-\r\nness. As shown in Tab. 5, our method outperforms SOTA\r\nmethods specifically designed for MVS, indicating the ex-\r\ncellent scalability of our approach.\r\n4.5. Ablations\r\nTo validate and comprehensively understand the archi-\r\ntecture of our model, we conducted certain ablation exper-\r\n27773\r\nFigure 5. Visualisation on the KITTI dataset. We conducted comparisons with existing SOTA methods [17, 18, 36]. Our method accurately\r\ncaptures the edges of the left car and right roof in the first scene. In the second scene, it avoids confusion in the positions of the left two\r\ncars, and achieving a complete match of the edges of the right car doors. It is evident that our method excels in matching edge details.\r\nFigure 6. Visualisation on the Middlebury dataset. All results presented in this section demonstrate zero-shot generalization on the Scene\r\nFlow dataset. The odd-numbered columns show the original images, the even-numbered columns present zoomed-in details.\r\nMiddleburyETH↓MethodFull↓ Half↓ Quarter ↓\r\nPSMNet[4] 39.5 15.8 9.8 10.2\r\nGANet[42] 32.2 13.5 8.5 6.5\r\nDSMNet[43] 21.8 13.8 8.1 6.2\r\nCFNet[26] 28.2 15.3 9.8 5.8\r\nDLNR [47] 14.5 9.5 7.6 23.1\r\nIGEV-Stereo [36] 15.2 7.1 6.2 3.6\r\nMoCha-Stereo 12.4 6.2 4.9 3.2\r\n(Ours) −14.5%−12.7% −21.0% −11.1%\r\nTable 4. Zero-shot evaluation on [24, 25]. Every model under-\r\ngoes scene flow training without fine-tuning on Middlebury and\r\nETH3D dataset. The 2-pixel error rate is employed for Middle-\r\nbury, and 1-pixel error rate for ETH3D.\r\niments. Following [17, 36], all hyperparameter settings re-\r\nmained consistent with the pretraining phase for the Scene\r\nFlow dataset.\r\nMotif Channel Correlation Attention (MCCV). For all\r\nmodels in the ablation studies, we perform 16 iterations of\r\nupdating at inference. As shown in Tab. 6, MCCV con-\r\ntributes to improved prediction accuracy. The decomposi-\r\ntion of MCCV from coarse to fine stages into the actions onMethod Ove.↓ Acc.↓Comp. ↓\r\nMVSNet[38] 0.462 0.396 0.527\r\nCasMVSNet[11] 0.355 0.325 0.385\r\nPatchmatchNet[32] 0.352 0.427 0.277\r\nIterMVS[33] 0.363 0.373 0.354\r\nCER-MVS[20] 0.332 0.359 0.305\r\nVis-MVSNet[44] 0.365 0.369 0.361\r\nMiper-MVS[48] 0.345 0.364 0.327\r\nDispMVS[37] 0.339 0.354 0.324\r\nMoCha-MVS 0.319 (−5.90%) 0.314 0.325\r\nTable 5. Quantitative evaluation on DTU [14] dataset expanded in\r\nMVS domain. Acc. means an indicator of accuracy, Comp. means\r\nan indicator of completeness, and Ove. means an indicator of the\r\noverall consideration of Acc. and Comp. (lower means better).\r\nthe feature map by MCA and on the Correlation V olume by\r\nCAMP results in a synergistic effect, leading to a reduction\r\nof 7.6% in EPE (from 0.458 to 0.423). The effective im-\r\nprovement achieved by MCCV is attributed to its ability to\r\naddress the bottleneck of existing feature extractors, which\r\nseverely lose geometric edge information in some channels.\r\nThis results in a more reasonable computation of the match-\r\ning cost for geometric edges. As shown in Fig. 7, directing\r\n27774\r\nFigure 7. An example of one of the feature channels in visual form. The first picture shows the initial normal channel, and the last picture\r\nshows the visualization after paying attention to Motif Channels. The middle picture visualize a motif channel. It can be observed that the\r\nedge texture details are emphasized in the new feature channels.\r\nModelMCA for Correlation V olumeREMP EPE (px)D1-ErrorTime (s) Params.(M)Feature Maps guided by CAMP >3px(%)\r\nBaseline 0.458 2.536 0.33 18.31\r\n+MCA ✓ 0.449 2.492 0.33 18.32\r\n+REMP ✓ 0.445 2.469 0.33 20.70\r\n+MCA+REMP ✓ ✓ 0.438 2.434 0.33 20.71\r\n+MCCV ✓ ✓ 0.423 2.358 0.34 18.35\r\nFull model ✓ ✓ ✓ 0.412 2.302 0.34 20.74\r\nTable 6. Ablation study for MoCha-Stereo. The baseline employed in these experiments utilized EfficientNet [29] as the backbone for\r\nIGEV-Stereo [36] with 16 iterations. The Time denotes the inference time on single NVIDIA A6000.\r\nNumber of Iterations\r\n1 2 3 4 8 16\r\nEPE (px) 0.56 0.52 0.48 0.46 0.42 0.41\r\nTime (s) 0.19 0.20 0.21 0.22 0.26 0.34\r\nTable 7. Ablation study for number of iterations.\r\nthe attention of normal channels to motif channels summa-\r\nrizing repeated geometric textures in channel features en-\r\nhances the clarity of edge textures in normal channels.\r\nReconstruction Error Motif Penalty (REMP). As shown\r\nin Tab. 6, REMP, by incorporating reconstruction error,\r\nlearns motif information on the channels to understand high\r\nand low-frequency errors. The learned errors are then uti-\r\nlized as a penalty term to adjust the disparity map, resulting\r\nin a reduction of EPE by 2.8% (from 0.458 to 0.445). This\r\nexperimental result validates the effectiveness of REMP.\r\nNumber of Iterations. MoCha-Stereo enhances the ef-\r\nficiency of iterations. As shown in Tab. 7, MoCha-\r\nStereo, with the information recovered by the Motif Chan-\r\nnel, achieves SOTA results without the need for a large\r\nnumber of iterations. For instance, in a comparable infer-\r\nence time, we achieve a 40.8% reduction in EPE compared\r\nto UPFNet [5] (EPE 0.71 px, time 0.27 s) with 8iterations.\r\nWith only 4iterations, MoCha-Stereo outperforms IGEV-\r\nStereo [36] (EPE 0.47 px, time 0.37 s) by over 2.1% in ac-\r\ncuracy and saves 40.5% of the inference time. Information\r\nabout [5, 36] can be obtained in Tab. 1. Overall, MoCha-Stereo achieves SOTA performance even with a small num-\r\nber of iterations, allowing users to balance time efficiency\r\nand performance based on their specific needs.\r\n5. Conclusion and Future Work\r\nWe propose MoCha-Stereo, a novel stereo matching\r\nframework. MoCha-Stereo aims to alleviate edge mismatch\r\ncaused by the geometric structure blurring of channel\r\nfeatures. MCCV utilizes the geometric structure of\r\nrepeated patterns in channel features to restore missing\r\nedge details and reconstructs the cost volume based on\r\nthis novel channel feature structure. REMP penalizes the\r\ngeneration of the full-resolution disparity map based on\r\nthe high and low-frequency information of the potential\r\nmotif channel in the reconstruction error. MoCha-Stereo\r\nshowcases robust cross-dataset generalization capabilities.\r\nIt ranks 1ston the KITTI-2015 and KITTI-2012 Reflective\r\nonline benchmarks and demonstrates SOTA performance\r\non ETH3D, Middlebury, Scene Flow datasets and MVS\r\ndomain. In the future, we plan to extend the motif channel\r\nattention mechanism to more processes in stereo matching,\r\nfurther enhancing the capability of algorithm for edge\r\nmatching.\r\nAcknowledgement. This research is supported by Science\r\nand Technology Planning Project of Guizhou Province, De-\r\npartment of Science and Technology of Guizhou Province,\r\nChina (Project No. [2023]159). Natural Science Research\r\nProject of Guizhou Provincial Department of Education,\r\nChina (QianJiaoJi[2022]029, QianJiaoHeKY[2021]022).\r\n27775\r\nReferences\r\n[1] Sara Alaee, Kaveh Kamgar, and Eamonn Keogh. Matrix pro-\r\nfile xxii: exact discovery of time series motifs under dtw. In\r\nInt. Conf. Data Mining , pages 900–905. IEEE, 2020. 2\r\n[2] Michael Bleyer and Margrit Gelautz. Simple but effec-\r\ntive tree structures for dynamic programming-based stereo\r\nmatching. In VISAPP (2) , pages 415–422, 2008. 1\r\n[3] Rohan Chabra, Julian Straub, Christopher Sweeney, Richard\r\nNewcombe, and Henry Fuchs. Stereodrnet: Dilated residual\r\nstereonet. In IEEE Conf. Comput. Vis. Pattern Recog. , pages\r\n11786–11795, 2019. 2, 3, 5\r\n[4] Jia-Ren Chang and Yong-Sheng Chen. Pyramid stereo\r\nmatching network. In IEEE Conf. Comput. Vis. Pattern\r\nRecog. , pages 5410–5418, 2018. 7\r\n[5] Qibo Chen, Baozhen Ge, and Jianing Quan. Unambigu-\r\nous pyramid cost volumes fusion for stereo matching. IEEE\r\nTrans. Circuit Syst. Video Technol. , 2023. 6, 8\r\n[6] Hoang Anh Dau and Eamonn Keogh. Matrix profile v: A\r\ngeneric technique to incorporate domain knowledge into mo-\r\ntif discovery. In ACM Special Interest Group Knowl. Discov-\r\nery Data Mining , pages 125–134, 2017. 2, 4\r\n[7] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\r\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\r\ndatabase. In IEEE Conf. Comput. Vis. Pattern Recog. , pages\r\n248–255. Ieee, 2009. 3\r\n[8] Pedro F Felzenszwalb and Daniel P Huttenlocher. Efficient\r\nbelief propagation for early vision. Int. J. Comput. Vis. , 70:\r\n41–54, 2006. 1\r\n[9] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we\r\nready for autonomous driving? the kitti vision benchmark\r\nsuite. In IEEE Conf. Comput. Vis. Pattern Recog. , 2012. 2,\r\n5, 6\r\n[10] Ross Girshick. Fast r-cnn. In Int. Conf. Comput. Vis. , pages\r\n1440–1448, 2015. 5\r\n[11] Xiaodong Gu, Zhiwen Fan, Siyu Zhu, Zuozhuo Dai, Feitong\r\nTan, and Ping Tan. Cascade cost volume for high-resolution\r\nmulti-view stereo and stereo matching. In IEEE Conf. Com-\r\nput. Vis. Pattern Recog. , pages 2495–2504, 2020. 7\r\n[12] Xiaoyang Guo, Kai Yang, Wukui Yang, Xiaogang Wang, and\r\nHongsheng Li. Group-wise correlation stereo network. In\r\nIEEE Conf. Comput. Vis. Pattern Recog. , pages 3273–3282,\r\n2019. 2, 3, 4, 6\r\n[13] Heiko Hirschmuller. Stereo processing by semiglobal match-\r\ning and mutual information. IEEE Trans. Pattern Anal.\r\nMach. Intell. , 30(2):328–341, 2007. 1\r\n[14] Rasmus Jensen, Anders Dahl, George V ogiatzis, Engin Tola,\r\nand Henrik Aanæs. Large scale multi-view stereopsis eval-\r\nuation. In IEEE Conf. Comput. Vis. Pattern Recog. , pages\r\n406–413, 2014. 2, 7\r\n[15] Jiankun Li, Peisen Wang, Pengfei Xiong, Tao Cai, Ziwei\r\nYan, Lei Yang, Jiangyu Liu, Haoqiang Fan, and Shuaicheng\r\nLiu. Practical stereo matching via cascaded recurrent net-\r\nwork with adaptive correlation. In IEEE Conf. Comput. Vis.\r\nPattern Recog. , pages 16263–16272, 2022. 2, 6[16] Zhengfa Liang, Yiliu Feng, Yulan Guo, Hengzhu Liu, Wei\r\nChen, Linbo Qiao, Li Zhou, and Jianfeng Zhang. Learning\r\nfor disparity estimation through feature constancy. In IEEE\r\nConf. Comput. Vis. Pattern Recog. , pages 2811–2820, 2018.\r\n2, 3, 4\r\n[17] Lahav Lipson, Zachary Teed, and Jia Deng. Raft-stereo:\r\nMultilevel recurrent field transforms for stereo matching. In\r\nInt. Conf. 3D Vision , pages 218–227. IEEE, 2021. 2, 3, 5, 6,\r\n7\r\n[18] Biyang Liu, Huimin Yu, and Yangqi Long. Local similarity\r\npattern and cost self-reassembling for deep stereo matching\r\nnetworks. In Assoc. Advancement Artif. Intell. , pages 1647–\r\n1655, 2022. 3, 6, 7\r\n[19] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\r\nregularization. In Int. Conf. Learn. Represent. , 2018. 5\r\n[20] Zeyu Ma, Zachary Teed, and Jia Deng. Multiview stereo with\r\ncascaded epipolar raft. In Eur. Conf. Comput. Vis. , pages\r\n734–750. Springer, 2022. 7\r\n[21] Nikolaus Mayer, Eddy Ilg, Philip Hausser, Philipp Fischer,\r\nDaniel Cremers, Alexey Dosovitskiy, and Thomas Brox. A\r\nlarge dataset to train convolutional networks for disparity,\r\noptical flow, and scene flow estimation. In IEEE Conf. Com-\r\nput. Vis. Pattern Recog. , pages 4040–4048, 2016. 2, 5, 6\r\n[22] Moritz Menze, Christian Heipke, and Andreas Geiger. Joint\r\n3d estimation of vehicles and scene flow. ISPRS annals of\r\nthe photogrammetry, remote sensing and spatial information\r\nsciences , 2:427, 2015. 2, 5, 6\r\n[23] Daniel Scharstein and Richard Szeliski. A taxonomy and\r\nevaluation of dense two-frame stereo correspondence algo-\r\nrithms. Int. J. Comput. Vis. , 47:7–42, 2002. 1\r\n[24] Daniel Scharstein, Heiko Hirschm ¨uller, York Kitajima,\r\nGreg Krathwohl, Nera Ne ˇsi´c, Xi Wang, and Porter West-\r\nling. High-resolution stereo datasets with subpixel-accurate\r\nground truth. In Pattern Recog. German Conf. , pages 31–42.\r\nSpringer, 2014. 2, 5, 6, 7\r\n[25] Thomas Schops, Johannes L Schonberger, Silvano Galliani,\r\nTorsten Sattler, Konrad Schindler, Marc Pollefeys, and An-\r\ndreas Geiger. A multi-view stereo benchmark with high-\r\nresolution images and multi-camera videos. In IEEE Conf.\r\nComput. Vis. Pattern Recog. , pages 3260–3269, 2017. 2, 5,\r\n6, 7\r\n[26] Zhelun Shen, Yuchao Dai, and Zhibo Rao. Cfnet: Cascade\r\nand fused cost volume for robust stereo matching. In IEEE\r\nConf. Comput. Vis. Pattern Recog. , pages 13906–13915,\r\n2021. 6, 7\r\n[27] Zhelun Shen, Yuchao Dai, Xibin Song, Zhibo Rao, Dingfu\r\nZhou, and Liangjun Zhang. Pcw-net: Pyramid combination\r\nand warping cost volume for stereo matching. In Eur. Conf.\r\nComput. Vis. , pages 280–297. Springer, 2022. 3\r\n[28] Zhelun Shen, Xibin Song, Yuchao Dai, Dingfu Zhou, Zhibo\r\nRao, and Liangjun Zhang. Digging into uncertainty-based\r\npseudo-label for robust stereo matching. IEEE Trans. Pattern\r\nAnal. Mach. Intell. , 2023. 2\r\n[29] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model\r\nscaling for convolutional neural networks. In Int. Conf.\r\nMach. Learn. , pages 6105–6114. PMLR, 2019. 3, 8\r\n27776\r\n[30] Vladimir Tankovich, Christian Hane, Yinda Zhang, Adarsh\r\nKowdle, Sean Fanello, and Sofien Bouaziz. Hitnet: Hierar-\r\nchical iterative tile refinement network for real-time stereo\r\nmatching. In IEEE Conf. Comput. Vis. Pattern Recog. , pages\r\n14362–14372, 2021. 2, 6\r\n[31] Jan L van Hemmen and Reimer K ¨uhn. Nonlinear neural net-\r\nworks. Physical Rev. Lett. , 57(7):913, 1986. 2\r\n[32] Fangjinhua Wang, Silvano Galliani, Christoph V ogel, Pablo\r\nSpeciale, and Marc Pollefeys. Patchmatchnet: Learned\r\nmulti-view patchmatch stereo. In IEEE Conf. Comput. Vis.\r\nPattern Recog. , pages 14194–14203, 2021. 7\r\n[33] Fangjinhua Wang, Silvano Galliani, Christoph V ogel, and\r\nMarc Pollefeys. Itermvs: iterative probability estimation for\r\nefficient multi-view stereo. In IEEE Conf. Comput. Vis. Pat-\r\ntern Recog. , pages 8606–8615, 2022. 7\r\n[34] Philippe Weinzaepfel, Thomas Lucas, Vincent Leroy,\r\nYohann Cabon, Vaibhav Arora, Romain Br ´egier, Gabriela\r\nCsurka, Leonid Antsfeld, Boris Chidlovskii, and Jerome\r\nRevaud. Croco v2: Improved cross-view completion pre-\r\ntraining for stereo matching and optical flow. In Int. Conf.\r\nComput. Vis. , pages 17969–17980, 2023. 6\r\n[35] Gangwei Xu, Junda Cheng, Peng Guo, and Xin Yang. Atten-\r\ntion concatenation volume for accurate and efficient stereo\r\nmatching. In IEEE Conf. Comput. Vis. Pattern Recog. , pages\r\n12981–12990, 2022. 6\r\n[36] Gangwei Xu, Xianqi Wang, Xiaohuan Ding, and Xin Yang.\r\nIterative geometry encoding volume for stereo matching.\r\nInIEEE Conf. Comput. Vis. Pattern Recog. , pages 21919–\r\n21928, 2023. 2, 3, 4, 5, 6, 7, 8\r\n[37] Qingsong Yan, Qiang Wang, Kaiyong Zhao, Bo Li, Xiaowen\r\nChu, and Fei Deng. Rethinking disparity: a depth range free\r\nmulti-view stereo based on disparity. In Assoc. Advancement\r\nArtif. Intell. , pages 3091–3099, 2023. 7\r\n[38] Yao Yao, Zixin Luo, Shiwei Li, Tian Fang, and Long Quan.\r\nMvsnet: Depth inference for unstructured multi-view stereo.\r\nInEur. Conf. Comput. Vis. , pages 767–783, 2018. 7\r\n[39] Chin-Chia Michael Yeh, Yan Zhu, Liudmila Ulanova, Nur-\r\njahan Begum, Yifei Ding, Hoang Anh Dau, Diego Furtado\r\nSilva, Abdullah Mueen, and Eamonn Keogh. Matrix profile\r\ni: all pairs similarity joins for time series: a unifying view\r\nthat includes motifs, discords and shapelets. In Int. Conf.\r\nData Mining , pages 1317–1322. Ieee, 2016. 1, 2, 3\r\n[40] Jure Zbontar and Yann LeCun. Computing the stereo match-\r\ning cost with a convolutional neural network. In IEEE Conf.\r\nComput. Vis. Pattern Recog. , pages 1592–1599, 2015. 1, 2\r\n[41] Jure Zbontar, Yann LeCun, et al. Stereo matching by training\r\na convolutional neural network to compare image patches. J.\r\nMach. Learn. Res. , 17(1):2287–2318, 2016. 2\r\n[42] Feihu Zhang, Victor Prisacariu, Ruigang Yang, and\r\nPhilip HS Torr. Ga-net: Guided aggregation net for end-\r\nto-end stereo matching. In IEEE Conf. Comput. Vis. Pattern\r\nRecog. , pages 185–194, 2019. 7\r\n[43] Feihu Zhang, Xiaojuan Qi, Ruigang Yang, Victor Prisacariu,\r\nBenjamin Wah, and Philip Torr. Domain-invariant stereo\r\nmatching networks. In Eur. Conf. Comput. Vis. , pages 420–\r\n439. Springer, 2020. 7[44] Jingyang Zhang, Shiwei Li, Zixin Luo, Tian Fang, and Yao\r\nYao. Vis-mvsnet: Visibility-aware multi-view stereo net-\r\nwork. Int. J. Comput. Vis. , 131(1):199–214, 2023. 7\r\n[45] Youmin Zhang, Yimin Chen, Xiao Bai, Suihanjin Yu, Kun\r\nYu, Zhiwei Li, and Kuiyuan Yang. Adaptive unimodal cost\r\nvolume filtering for deep stereo matching. In Assoc. Ad-\r\nvancement Artif. Intell. , pages 12926–12934, 2020. 6\r\n[46] Haoliang Zhao, Huizhou Zhou, Yongjun Zhang, Yong Zhao,\r\nYitong Yang, and Ting Ouyang. Eai-stereo: Error aware it-\r\nerative network for stereo matching. In Asian Conf. Comput.\r\nVision , pages 315–332, 2022. 4\r\n[47] Haoliang Zhao, Huizhou Zhou, Yongjun Zhang, Jie Chen,\r\nYitong Yang, and Yong Zhao. High-frequency stereo match-\r\ning network. In IEEE Conf. Comput. Vis. Pattern Recog. ,\r\npages 1327–1336, 2023. 2, 3, 4, 6, 7\r\n[48] Huizhou Zhou, Haoliang Zhao, Qi Wang, Gefei Hao, and\r\nLiang Lei. Miper-mvs: Multi-scale iterative probability esti-\r\nmation with refinement for efficient multi-view stereo. Neu-\r\nral Netw. , 162:502–515, 2023. 7\r\n27777'}, 'dist': 0.9286905527114868}
Result 7: {'text': 'data', 'metadata': {'created_at': 1736932570, 'modified_at': 1736932570, 'owner': 'sayandeep', 'path': 'RAG_CONTENT/conferences/cvpr_ocr/Bao_From_Feature_to_Gaze_A_Generalizable_Replacement_of_Linear_Layer_CVPR_2024_paper.txt', 'size': 46130, 'seen_at': 1737191136, 'data': 'From Feature to Gaze: A Generalizable Replacement\nof Linear Layer for Gaze Estimation\nYiwei Bao Feng Lu*\nState Key Laboratory of VR Technology and Systems, School of CSE, Beihang University\n{baoyiwei, lufeng }@buaa.edu.cn\nAbstract\nDeep-learning-based gaze estimation approaches often\nsuffer from notable performance degradation in unseen tar-\nget domains. One of the primary reasons is that the Fully\nConnected layer is highly prone to overfitting when map-\nping the high-dimensional image feature to 3D gaze. In this\npaper, we propose Analytical Gaze Generalization frame-\nwork (AGG) to improve the generalization ability of gaze\nestimation models without touching target domain data.\nThe AGG consists of two modules, the Geodesic Projection\nModule (GPM) and the Sphere-Oriented Training (SOT).\nGPM is a generalizable replacement of FC layer, which\nprojects high-dimensional image features to 3D space ana-\nlytically to extract the principle components of gaze. Then,\nwe propose Sphere-Oriented Training (SOT) to incorpo-\nrate the GPM into the training process and further improve\ncross-domain performances. Experimental results demon-\nstrate that the AGG effectively alleviate the overfitting prob-\nlem and consistently improves the cross-domain gaze esti-\nmation accuracy in 12 cross-domain settings, without re-\nquiring any target domain data. The insight from the Ana-\nlytical Gaze Generalization framework has the potential to\nbenefit other regression tasks with physical meanings.\n1. Introduction\nEye gaze reveals where human attention lands, which has\nbeen widely applied in a variety of territories, such as\nVR/AR systems [3, 15, 26], medical analysis [4, 13, 14] and\nhuman-computer interaction[17, 27, 34]. Gaze estimation\nmethods can be classified into two categories: model-based\napproaches and appearance-based approaches. Both ap-\nproaches have their own strengths and weaknesses. Model-\nbased approaches estimate gaze by modeling the anatomical\nstructure of the eyeball. These methods achieve remarkable\naccuracy in controlled environment. But they typically re-\nquire dedicated hardware such as infrared cameras and light\nsources. Appearance-based approaches use cost-effective\n*Corresponding Author. This work was supported by the National Nat-\nural Science Foundation of China (NSFC) under Grant 62372019.\nGeneralization\nFC Layer\nFace Images\nGaze1500+ params\nFeaturesConventional\n512D 2D\nGeodesic Projection Module ( GPM ) —10 params\nIsometric Sphere\nMapping Alignment\n𝜓𝜓\n𝜃𝜃Less Overfitting\nUnseen\nDomainsSource\nDomainGeneralize\n512D 3D 2D\nReplaced by→ Overfitted\nFigure 1. Overview of the proposed AGG framework for general-\nizing gaze estimation models to unseen target domains.\nweb cameras. They typically train Convolutional Neural\nNetworks (CNNs) in an end-to-end way, enabling them to\npredict gaze direction from user face/eye images directly. In\nrecent years, appearance-based approaches have garnered\ngreat interest due to their simplified hardware requirements\nand the potential for widespread applications.\nHowever, appearance-based methods suffer from severe\nperformance degradation in cross-domain settings. To im-\nprove the cross-domain performance, various domain adap-\ntation approaches have been proposed, i.e., adversarial\nlearning [12, 32], contrastive learning [33] and collaborative\nlearning [20]. However, these approaches require a num-\nber of target domain samples for adaptation, which is not\nalways attainable in real-world scenarios. More recently,\nCheng et al. proposed to generalize gaze estimation model\nby purifying gaze feature during source domain training\n[8]. The gaze generalization task is more practical yet more\nchallenging, because it does not have access to any target\ndomain data.\nOne of the significant reasons for the poor cross-domain\nperformances is the overfitting problem. Gaze estimation\nCNNs are trained to extract high-dimensional image fea-\ntures ( e.g. 512D) from input face images and map these\nfeatures to gaze (3D unit vector) by a Fully Connected layer\nwith thousands of parameters. The numerous parameters\nof the FC layer easily overfit to gaze irrelevant factors\nwithin the high-dimensional image features during the\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n1409\nend-to-end training process. One possible solution is to\nextract gaze-related information from the image features,\ni.e. the Principle Component of Gaze (PCG), while exclud-\ning other irrelevant information.\nIn this paper, we introduce the Analytical Gaze Gener-\nalization framework(AGG), a novel gaze generalization ap-\nproach that connects the high-dimensional image features\nto gaze analytically. The AGG consists of two modules,\nthe Geodesic Projection Module (GPM) and the Sphere-\nOriented Training (SOT) module. Given a pretrained gaze\nestimation model, the GPM serves as a replacement of\nthe last FC layer, alleviating the overfitting issue by an-\nalytical projection and alignment. Based on the observa-\ntion that the geodesic distance between the image features\nis proportional to the angular gaze difference between sam-\nples, the GPM projects the high-dimensional image fea-\ntures to 3D space by the geodesic distance to extract the\nPrinciple Component of Gaze. Then, we estimate gaze\nfrom the projected features by the proposed Sphere Align-\nment algorithm using physical rotation and scaling with\nonly 10 learnable parameters. Next, we propose the Sphere-\nOriented Training to incorporate the GPM into the training\nprocess to improve the generalization ability of the whole\nnetwork. Given source domain labels, the Sphere-Oriented\nTraining optimizes the gaze estimation network based on\nthe reverse process of the GPM.\nExperiments show that, by only replacing the last FC\nlayer of the baseline model with GPM, both the accu-\nracy and the stability in cross-domain testing have been\nimproved. After optimizing the model using the Sphere-\nOriented Training in the source domain, the performance\nis further improved and outperforms SOTA gaze estimation\nmethods in multiple different cross-domain settings. The\nprimary contributions of this work are as follow:\n• We propose the Geodesic Projection Module (GPM), a\nnovel method that predicts gaze from the geodesic dis-\ntance between the image features analytically. As a novel\nand explainable approach for gaze estimation, the insight\nGPM presents may also inspire other regression tasks like\npose estimation.\n• We propose the AGG framework for generalizable gaze\nestimation. The AGG framework utilizes the Sphere-\nOriented Training module to optimize the gaze estima-\ntion model based on the reverse process of the GPM for\nbetter generalization ability.\n• Experimental results illustrate that the proposed\nAGG achieves consistent improvements in 12 different\ncross-dataset settings. The AGG improves the gener-\nalization ability of the baseline model up to 35.79%\nwithout touching target domain data.\nThe subsequent sections are organized as follow: in\nSec. 3, we present the motivation and design principles of\nthe AGG through several validation experiments. In Sec. 4,we provide a detailed introduction to the AGG framework.\nIn Sec. 5, we assess the proposed method both quantita-\ntively and qualitatively.\n2. Related Work\n2.1. Gaze Estimation\nThere are two mainstream gaze estimation approaches,\nthe model-based approaches and the appearance-based ap-\nproaches. Model-based approaches estimate gaze by recon-\nstructing the anatomy structure of the eyeball [11]. These\nmethods achieve remarkable accuracy but also require per-\nsonal calibration and dedicated devices such as depth sen-\nsors [28, 35], infrared cameras [10, 29] and lights [10, 19].\nAppearance-based approaches usually estimate gaze\nfrom user images captured by a single web camera. Early\nmethods estimate gaze from eye images by traditional ma-\nchine learning algorithms like manifold embedding [25] and\nadaptive linear regression [21]. Lu et al. propose to esti-\nmate eye rotation by measuring the geodesic distance be-\ntween eye images [22]. Wang et al. propose to combine the\neye appearance with eye geometry by a Hierarchical Gen-\nerative Model []. More recently, a number of gaze estima-\ntion datasets have been collected [9, 12, 16, 38, 39]. These\ndatasets provide hundreds of thousands of user images with\ngaze labels, which makes deep-learning-based gaze estima-\ntion possible. Representative studies include gaze estima-\ntion using convolutional neural networks (CNNs) [37] with\neye images [7, 37] or face images [1, 5, 6, 16, 38]. Some\nprevious studies also represent gaze features as low dimen-\nsional manifolds for personalization [23] and unsupervised\nlearning [36]. But these methods still construct manifolds\nby data-driven learning approach with supervision like gaze\nredirection. Our method analytically connects the high-\ndimensional image feature to gaze.\n2.2. Cross-domain Gaze Estimation\nOne of the major problem of the deep-learning-based ap-\nproaches is that the performance degrades severely when\ntesting on a different domain. To improve the cross-domain\nperformance, a number of unsupervised domain adaptation\nmethods have been proposed. Liu et al. propose to adapt\nthe model to target domain with the guidance of outliers\nby collaborative learning [20]. Wang et al . utilize con-\ntrastive learning to pull features with close gaze labels to-\ngether [33]. Bao et al. propose to improve the cross-dataset\naccuracy by the rotation consistency of gaze [2]. Neverthe-\nless, above methods require target domain images to train\ndomain specific models, which is infeasible in real world\nsettings, as target domain data is often inaccessible. Re-\ncently, Cheng et al. propose to improve the generalization\nability of gaze estimation model by purifying gaze feature in\nsource domain[8]. The gaze generalization problem without\n1410\n𝜶𝜶Distance between Features\nGaze Difference 𝜶𝜶(°)Spherical Dist.\nL2 Dist.\nGaze\nSpherical Distance ∝𝜶𝜶Feature Space Physical Space\nProportional to α\nFigure 2. Observation: The gaze differences between samples\ncan be linearly represented by the geodesic distances between ex-\ntracted features. Such characteristic of the feature space is con-\nsistent with the physics of gaze: the distance along the spherical\nsurface is proportional to the gaze differences.\naccess to target domain data is more challenging and yet to\nbe solved.\n3. Analytical Gaze Estimation from Features\nThe aim of this paper is to design a replacement of the last\nregression FC layer to alleviate the overfitting problem. In\nthis section, we try to answer the following key questions\nby a series of validation experiments:\nQuestion 1: How many dimensions does the Principle\nComponents of Gaze (PCG) in the high-dimensional image\nfeatures have?\nQuestion 2: How to extract the PCG from the high-\ndimensional image features for generalizable gaze estima-\ntion?\n3.1. The Overfitting Problem\nTo explore above questions, we initially pretrain a com-\nmonly used baseline model, i.e. ResNet-18 in the source\ndomain (ETH-XGaze [39]) for analysis.\nGiven the source domain Ds={xi,yi|N\ni=1}where xi\nis the face image and yi= (xi, yi, zi)is the ground truth\nunit gaze direction vector, we pretrain the gaze estimation\nmodel by L1loss function:\nfi=Fθ1(xi),\narg min\nθ1,θ2(L1(yi, Lθ2(fi))|N\ni=1).(1)\nwhere Fθ1(·)is the feature extractor CNN, fiis the 512D\nhigh-dimensional image feature and Lθ2(·)is the last Fully\nConnected layer which estimates gaze from fi.\nPrevious study [8] has proven that the feature extracted\nby the pretraind baseline model fiencompasses not only\ngaze information, but also other visual contents, including\nappearance, illumination and head pose. Thus, the dimen-\nsion of the PCG should be less than the dimension of fi\nitself. Combined with the fact that gaze direction is a 3D\nTop View\nSide View\nGaze Label\nTop View\nSide View\nGaze Label\nTop View\nSide View\nPrinciple Gaze Feature\nTop View\nSide View\nPrinciple Gaze Feature \nYaw within [−50°,50°] Pitch within [−50°,50°]\nFigure 3. Following the observation in Fig. 2, we construct\nPGF by projecting high-dimensional features to the 3D space us-\ning geodesic distance. The PGF shares the same spherical distri-\nbution pattern as the gaze label. Data points are colored by gaze\nyaw and pitch angles respectively.\nTop View\nSide View\nGaze Label\nTop View\nSide View\nIsomap\nTop View\nSide View\nT-SNE\nTop View\nSide View\nLLE\nFigure 4. Projecting features extracted by the pretrain gaze estima-\ntion model into 3D space by varies distance metrics. Projections\nby geodesic distance (Isomap) show identical spherical distribu-\ntion pattern as the gaze label. Colored by gaze yaw angles.\nunit vector with 2 degrees of freedom, for the answer of\nQuestion 1, we hypothesis that the theoretical minimum\ndimension of the PCG should be 2D, identical to the gaze\nground truth. The regression FC layer Lθ2is at a high risk\nof overffiting since the number of parameters in Lθ2and\nthe dimension of fiare way beyond the minimum required\nquantity.\n3.2. Gaze on the 3D Sphere\nTo answer Question 2 , we examine the distribution of the\ngaze ground truth. In the physical space, gaze distributes\nacross the surface of the 3D unit sphere. The distance alone\nthe data manifold i.e. the spherical surface is proportional\nto the angular differences between gaze directions.\nTo verify if similar relationship also exists in the feature\nspace, we visualize the distance alone the data manifold in\n1411\nthe feature space, i.e. the geodesic distance between fiin\nFig. 2. The result reveals an important observation:\nObservation: the geodesic distance between fiis in a\nstrong direct proportion to the angular gaze differences be-\ntween samples.\nThis observation leads to a possible answer of Question\n2: The PCG within the high-dimensional image features\ncan be extracted by the geodesic distance. In the fol-\nlowing step, we leverage the geodesic distance to extract\nthe Principle Gaze Feature (PGF) from fifor generalizable\ngaze estimation.\n3.3. Mapping Features to 3D Space Analytically\nAccording to above observations, we utilize the geodesic\ndistance to extract the PGF from the high-dimensional im-\nage features. Specifically, we project the high-dimensional\nimage features to the 3D space using geodesic distance,\ni.e. Isometric Mapping (Isomap) [30]. Results in Fig. 3\ndemonstrate that the image features after projection (the\nPGF) share similar distribution pattern as gaze: the\nPGF distributes across the surface of a 3D sphere approxi-\nmately. To better demonstrate the distribution pattern, we\ncolor the PGFs within certain gaze range with pitch and\nyaw angles respectively. It is obvious that gaze pitch and\nyaw angle change monotonically alone the longitudinal and\nlatitudinal direction of the PGF Sphere. The Principle Gaze\nFeature preserves gaze information while excluding unnec-\nessary factors from fito alleviate the overffiting problem.\nGeneralizable gaze estimations could be made by simply\naligning the PGF Sphere with the unit sphere of gaze distri-\nbution using the gaze label. This alignment process consists\nof simple physical operation including rotation and scaling\nwith only 10 parameters, which is less unlikely to overfit.\nWe further utilize this idea to optimize the feature extrac-\ntor CNN Fθ1for better generalization ability. The detailed\nimplementation will be introduced in Sec. 4.\n3.4. Choice of Distance Metrics\nIn this section, we validate some other possible answers of\nQuestion 2 . Specifically, we project the image features\nfiinto the 3D space using two other dimension reduction\nmethods: the Local Linear Embedding (LLE) [24] and the\nT-distributed Stochastic Neighbor Embedding (t-SNE) [31].\nIt is obvious in Fig. 4 that only the results of Isomap exhibit\nsimilar distribution pattern to the gaze label. Although the\ndistributions of other dimension reduction methods also ex-\nhibit some directional characteristics, their overall distribu-\ntions do not show a clear geometric pattern like Isomap.\nAbove results prove that geodesic distance is the key to\nextract the Principle Components of Gaze from the high-\ndimensional image feature. In the next section, we explain\nthe specific implementation of the AGG framework4. Method\nWe propose the Analytical Gaze Generalization framework,\na domain generalization method for gaze estimation. The\nAGG framework comprises two key modules: the Geodesic\nProjection Module (GPM) module and the Sphere-Oriented\nTraining (SOT) module. The Geodesic Projection Mod-\nule predicts gaze analytically from the pretrained im-\nage feature by constructing the Principle Gaze Feature us-\ning geodesic distance. Next, the Sphere-Oriented Train-\ning module optimizes the pretrained gaze estimation net-\nwork according to the reverse process of the GPM for better\ngeneralization ability. The overview of the Analytical Gaze\nGeneralization framework is presented in Fig. 5.\n4.1. Geodesic Projection Module\nThe Geodesic Projection Module mainly consists of two\nsteps. First, we project the image features extracted by\nthe pretrained gaze estimation model into 3D space us-\ning geodesic distance, i.e. Isomap. The projected feature\n(named the Principle Gaze Feature) distributes alone the\nsurface of a sphere (named the PGF Sphere) approximately.\nThen, we align the PGF Sphere with the unit sphere repre-\nsents the gaze label distribution to predict gaze directions\nanalytically.\nFirst, we pretrain a gaze estimation model consists of a\nfeature extractor CNN Fθ1and a Fully Connected layer Lθ2\nfor gaze regression in the source domain Ds={xi,yi|N\ni=1}\nusingL1loss function according to Eq. (1). Then, we\nfreeze the gaze estimation model and extract the high-\ndimensional image features (512D in our experiments) by\n{fi=Fθ1(xi)|N′\ni=1}. The Principle Gaze Feature ei∈R3\nis constructed by projecting the image features fiinto the\n3D space using Isomap algorithm:\n{ei|N′\ni=1}=Isomap ({fi|N′\ni=1}). (2)\nSince the Principle Gaze Feature distributes across the\nsurface of a 3D sphere approximately, the next step is to\npredict gaze directions by aligning this PGF Sphere with\nthe unit gaze label sphere. First, we locate the center of the\nPGF Sphere Ocand rotate it to align the orientation with\nthe unit gaze label sphere:\ne′\ni=R(ei−Oc) = (xe′\ni, ye′\ni, ze′\ni)T, (3)\nwhere Ris the rotation matrix. Then, we calculate the Euler\nangles of e′\niand predict gaze directions yi= (θ′, ψ′)by\nsimple linear fittings:\n\uf8f1\n\uf8f2\n\uf8f3θ′\ni=k1arctan (xe′\ni\nze′\ni) +b1,\nψ′\ni=k2arcsin (ye′\ni) +b2.(4)\nThe final gaze prediction y′\ni= (x′\ni, y′\ni, z′\ni)is obtained\nthrough converting the Euler angle predictions (θ′\ni, ψ′\ni,0)\nto unit direction vectors. We formalize the process\n1412\nD𝐬𝐬\nD𝒕𝒕\nHigh-dimensional\nImage Features\n𝜓𝜓\n𝜃𝜃𝑥𝑥𝑧𝑧𝑦𝑦\nSphere Fitting & \nRotation Eq.(3)Euler Angle \nFitting Eq.(4)Principle Gaze \nFeature (PGF) in 3D\nSphere Alignment①GPM -Geodesic Projection Module (Sec. 4.1)\nGaze\nPredictions\n（𝑥𝑥 ,𝑦𝑦,𝑧𝑧）（𝑥𝑥 ,𝑦𝑦,𝑧𝑧）（𝑥𝑥 ,𝑦𝑦,𝑧𝑧）Gaze Label\nIsometric\nPropagator\nPGFIdeal \nPositionActual \nPosition\nSphere\nAlignment\nL1Loss\nSource Domain②Sphere- Oriented Training (Sec. 4.2)\nUpdateIsometric\nMapping\n③Inference —AGG (proposed) Inference (Conventional)\nFace Images Gaze Predictions FC Layer512D\n512D features\n Face Images Gaze Predictions\n①GPM10 params 1500+ params D𝐬𝐬\nD𝒕𝒕\n2DFigure 5. Overview of the proposed AGG framework. We propose two modules, the Geodesic Projection Module (GPM) and the Sphere-\nOriented Training module. We replace the last Linear layer of the pretrained baseline model with GPM to estimate gaze from the\nhigh-dimensional image feature analytically. The Sphere-Oriented Training optimizes the gaze estimation model according to the reverse\nprocess of the GPM for better generalization ability.\nfrom the Principle Gaze Feature eito gaze prediction y′\ni\nas the Sphere Alignment algorithm: y′\ni=SAθs(ei),\nwhere θsis the set of 10 learnable parameters θs=\n{Oc,R, k1, k2, b1, b2}. These parameters are obtained by\nminimizing the angular difference between gaze prediction\ny′\niand the source domian gaze label yi:\narg min\nθs(Angular (yi, SA θs(ei))|N\ni=1). (5)\nSince the GPM only contains 10 learnable parameters,\nwe only randomly choose 2000 source domain samples to\noptimize θsin our experiments. In the test time, features\nof target domain samples are concatenated to the geodesic\ndistance map built in the source domain for Isometric Map-\nping. The parameters of the SA algorithm remain fixed in\nthe Sphere-Oriented Training and test time.\n4.2. Sphere-Oriented Training\nThe Geodesic Projection Module predicts gaze from the\nimage feature analytically, thus the performance will be af-\nfected by the quality of the image feature. To extract gener-\nalizable image features, we propose Sphere-Oriented Train-\ning to optimize the pretrained feature extractor CNN Fθ1\nwith the reverse process of the GPM in the source domain.\nGiven a source domain sample {xi,yi}, we could re-\nversely calculate the ideal position of the corresponding\nPrinciple Gaze Feature since the Sphere Alignment algo-\nrithm is totally analytical: ˆei=SA−1(yi).Theoretically,the pretrained CNN could be optimized by minimizing the\ndistance between the ideal position and actual position of\nthe Principle Gaze Feature on the PGF Sphere:\narg min\nθ1(L1(ˆei, Isomap (Fθ1(xi)))|N\ni=1). (6)\nUnfortunately, it is difficult to integrate the Isomap\ninto the back propagation process because it is both time\nand space consuming. The time complexity of Isomap is\nO(N2logN)and the space demand is O(N2), where N\nis the number of samples. To solve this issue, we pro-\npose the Isometric Propagator IPθ3(·)to parameterize the\nIsomap algorithm. Isometric Propagator is a three layer\nMLP trained to simulate the Isomap function at the begin-\nning of Sphere-Oriented Training. We freeze the parameter\nof the pretrained CNN Fθ1and train the Isometric Propaga-\ntor as follow:\narg min\nθ3(L1(Isomap (fi), IPθ3(fi))|N\ni=1). (7)\nAfter the training of the Isometric Propagator, we freeze\nits parameters and replace the Isomap with it to train the fea-\nture extractor CNN. The actual Sphere-Oriented Training is\nformalized as:\narg min\nθ1(L1(ˆei, IPθ3(Fθ1(xi)))|N\ni=1). (8)\nNote that the Isometric Propagator is only used during\nthe source domain training. At test time, we predict gaze\n1413\nby the proposed GPM with Isomap for better generalization\nability. Parameters of the GPM are determined before the\nSphere-Oriented Training and remain fixed.\nOwing to the advantage that the GPM suffers less from\nthe overfitting problem than the FC layer, the purpose of\nthe SOT is to utilize this advantage to optimize the gaze\nestimation model for better generalization performance by\nincorporating the GPM into the training process.\n4.3. Implementation Details\nWe employ the AGG by PyTorch. For the training of the\npretrain model, IP and Sphere-Oriented Training, we use\nthe Adam optimizer with a learning rate of 10−4. The model\nis pretrained for 10 epochs. We choose the last epoch as the\nbaseline model. The Sphere-Oriented Training is also 10\nepochs, while the IP is trained for 100 epochs on 2000 ran-\ndomly selected samples. Batch sizes are set to 512. For\nIsomap, we use the implementation of Scikit-learn and the\nnumber of neighbor is set to 300. Pixel values are normal-\nized to [0,1], and no data augmentation is employed.\n5. Experiments\n5.1. Data Preparation\nWe conduct experiments on four commonly used gaze es-\ntimation datasets: ETH-XGaze ( DE) [39], Gaze360 ( DG)\n[12], MPIIFaceGaze ( DM) [38] and EyeDiap ( DD) [9]. We\nnormalize the data following the techniques in [38]. ETH-\nXGaze: 756kimages captured by high resolution cameras\nin laboratory environment with large gaze range. We divide\nthe last 5 subjects as test set. Gaze360: 101kimages cap-\ntured by a 360◦camera on streets with large gaze range.\nWe only use images with frontal faces in our experiments.\nMPIIFaceGaze: 45kimages (standard test set) captured by\nweb camera during daily usage of laptop computers. The\ngaze range of DMis less than half the range of DEandDG.\nThus, we only use DMas target domain. EyeDiap: 16k\nimages captured under laboratory environment with screen\nand floating targets. As the number of images is signifi-\ncantly less than other datasets, we only use DDas target\ndomain.\nIn addition, the cross-domain error between DEand\nDGis extremely large (around 20◦). Thus, we exclude the\nDE→DGandDG→DEsettings in our experiments, which\nis also excluded in previous studies [2, 8, 20, 33].\n5.2. Quantitative Evaluation\n5.2.1 Evaluation of the GPM\nWe first evaluate the proposed GPM by replacing the last\nFC layer with the GPM without changing other parameters\nof the baseline model. The mean and the standard devi-\nation (std) of the estimation error from the last 5 epochsTable 1. Results of simply replacing the last FC layer with the\nproposed GPM in inference. Results are the mean and std for the\nfinal 5 epochs. Note that the modest reduction in within-dataset\naccuracy is reasonable, since GPM is designed for generalization.\nDE→DMDE→DD within DE\nResNet-18 8.66±0.53 7.76±0.29 5.37±0.24\nResNet-18 + GPM 7.87±0.23 7.72±0.33 5.74±0.08\nResNet-50 6.92±0.86 8.61±0.88 5.27±0.56\nResNet-50 + GPM 6.56±0.41 8.10±0.57 5.29±0.07\nDG→DMDG→DD within DG\nResNet-18 8.59±0.57 10.87±1.52 12.59±0.14\nResNet-18 + GPM 8.57±0.41 10.94±0.85 12.64±0.10\nResNet-50 8.48±1.01 10.76±0.78 11.97±0.30\nResNet-50 + GPM 8.14±0.47 9.77±1.00 12.07±0.18\nare shown in Tab. 1. The proposed GPM achieves better\nperformance in 7 out of 8 cross-domain experiments. In\naddition, the GPM also performs more stably across differ-\nent epochs. These results demonstrate the advantage of the\nproposed GPM over the traditional FC layer. The within-\ndataset estimation errors of the GPM are slightly higher.\nIt is reasonable since GPM is designed for generalizing to\nunseen domains. The higher within-dataset accuracy of the\nFC layer is highly likely achieved by overfitting since it per-\nforms worse in cross-domain tests.\n5.2.2 Evaluation of the AGG Framework\nIn this section, we evaluate the effectiveness of the\nAGG framework, which optimizes the gaze estimation\nmodel to further improve generalization ability. We con-\nduct experiments in 4 cross domain settings with 3 baseline\nmodels, as shown in Tab. 2. The performances of baseline\nmodels is quite different, due to their architectures and the\ndifferent characteristics of each domain. Nevertheless, the\nproposed AGG framework achieves stable improvements in\nall 12 cross-domain settings, proves that the AGG frame-\nwork is robust to different baseline models and source do-\nmains. The AGG framework achieves improvements as\nlarge as 35.79% without target domain data. We also re-\nport the within dataset performance after generalization for\nreference. As expected, the within dataset performance de-\ncreases mildly since the model is optimized for domain gen-\neralization. Above results demonstrate the effectiveness of\nthe proposed Sphere-Oriented Training, which improves the\ngeneralization performance of varies baseline models sig-\nnificantly.\n5.2.3 Comparison with SOTA Methods\nIn Tab. 3, we compare the AGG framework with SOTA\ngaze estimation methods [6, 12, 38] and gaze general-\n1414\nTable 2. Performance of the proposed AGG framework. Results are gaze estimation error in degrees. The proposed AGG achieves stable\nimprovements up to 35.79 %in all 12 cross-domain settings without using any target domain data. The symbol∗indicates within-dataset\nexperiments for reference. Note that the modest reduction in within-dataset accuracy is to be expected for domain generalization methods.\nMethod DE→DM DE→DD DG→DM DG→DD within DE∗within DG∗\nResNet-18 8.64 7.83 8.68 12.35 5.08 12.73\nResNet-18+AGG 7.10▼17.82% 7.07▼9.71% 7.87▼9.33% 7.93▼35.79% 5.56▲9.45% 13.03 ▲2.36%\nResNet-50 6.04 7.47 10.14 11.76 5.35 12.37\nResNet-50+AGG 5.91▼2.15% 6.75▼9.64% 9.2▼9.27% 11.36 ▼3.40% 6.29▲17.57% 15.63 ▲26.35%\nVGG16 9.5 19.14 14.61 19.94 5.12 12.15\nVGG16+AGG 9.13▼3.89% 17.2▼10.14% 11.3▼22.66% 13.97 ▼29.94% 5.78▲12.89% 13.13 ▲8.07%\nTable 3. Cross domain gaze estimation error in degrees. ∗indi-\ncates methods with ResNet-50 backbone. Overall, the proposed\nAGG achieves better generalization ability than SOTA gaze esti-\nmation methods.\nMethod DE→DMDE→DDDG→DMDG→DD\nFull-Face[38] 12.35 30.15 11.13 14.42\nADL[12] 7.23 8.02 11.36 11.86\nCA-Net[6] - - 27.13 31.41\nLatentGaze[18] 7.98 9.81 - -\nPureGaze[8] 7.08* 7.48* 9.28 9.32\nResNet18+AGG 7.10 7.07 7.87 7.93\nResNet50+AGG 5.91* 6.75* 9.20* 11.36*\nization methods [8, 18]. Results demonstrate that the\nAGG outperforms other SOTA methods. The AGG with\nResNet-18 baseline achieves the best overall performances,\nit outperforms SOTA methods in DE→DD,DG→DMand\nDG→DDsettings, while achieving performance compara-\nble to the PureGaze in the DE→DMsetting. The AGG with\nResNet-50 baseline also surpasses SOTA methods in 3 out\nof 4 cross-domain settings. It performs exceptionally well\nwhen trained in the DEdomain. Overall, above experi-\nments prove that the AGG achieves better generalization\nability than SOTA gaze estimation methods.\n5.3. Verification of the AGG\n5.3.1 Verification of the Core Idea\nThe proposed Analytical Gaze Generalization framework is\ndesigned based on the observation that the geodesic distance\nbetween image features is proportional to the angular gaze\ndifferences between input samples. To explore whether this\nobservation holds true in different domains, we verify it in\nDE,DG,DMandDDrespectively. We train a baseline\nResNet-18 model according to Eq. (1) in each domain re-\nspectively to extract the image feature, and visualize the L2\nand Geodesic distance with respect to the angular gaze dif-\nferences. As shown in Fig. 6, the linear relationship holds\ntrue for all sample pairs in DE, thanks to the high image\nquality and controlled laboratory environment. For DG,\nthe pattern is evident at the beginning but becomes random\nGaze Differences / °Gaze Differences / °Gaze Differences / °Gaze Differences / °Distance between Feature\nETH-XGaze Gaze360 MPIIFaceGaze EyeDiap\nFigure 6. The L2 and Geodesic distances between image features\nwith respect to the angular differences between samples.\nwhen gaze differences surpass 140◦. We randomly visu-\nalize 4 samples from the random section at the top of the\nfigure. Since the original DGdataset includes subjects fac-\ning away from the camera, the quality of samples appears\nto deteriorate when the head pose approaches ±90◦. For\nDMandDD, the geodesic distance is also exhibits a more\ndirect proportionality to gaze differences. However, the dis-\nparity between geodesic distance and L2 distance is less ob-\nvious compared to what was observed in the DEandDG. It\nis reasonable since the gaze ranges in DMandDDare sig-\nnificantly smaller. The geodesic distance converges toward\nthe L2 distance when the features are in close proximity.\nWe further visualize the Principle Gaze Feature from\nthese four datasets in Fig. 7 for a more intuitive understand-\ning. The Principle Gaze Feature from all 4 datasets con-\nsistently demonstrate identical distribution pattern with the\ngaze label. Above results confirm that the the proportional\nrelationship between the geodesic distance and the gaze dif-\nferences remains consistent across different domains, even\nthough the image quality, gaze range, and head pose range\nexhibit significant variations among these domains. Hence,\nthe proportional relationship can be employed for domain\ngeneralization, given that it is domain-independent.\n5.3.2 Verification of the Sphere-Oriented Training\nIn this section, we assess the efficacy of the Sphere-Oriented\nTraining, i.e. whether Sphere-Oriented Training optimizes\nthe model to extract features that better conform to the pro-\n1415\nTop View\nSide View\nTop View\nSide View\nTop View\nSide View\nTop View\nSide View\nTop View\nSide View\nTop View\nSide View\nD𝐸𝐸 D𝐺𝐺 D𝑀𝑀PGF Gaze LabelD𝐸𝐸 D𝐺𝐺 D𝑀𝑀\nTop View\nSide View\nTop View\nSide View\nD𝐷𝐷\nD𝐷𝐷Figure 7. Visualization of the Principle Gaze Feature (PGF) and\ngaze label from DE,DG,DMandDD. PGF from all 4 datasets\nshare the same distribution pattern with gaze label.\n4.45\n2.606.577.73\n2.21\n1.67Sphere Error / %\nD𝐸𝐸→D𝐷𝐷 D𝐸𝐸→DM Within D𝐸𝐸4.46Sphere Error / %\nD𝐺𝐺→DD D𝐺𝐺→DM Within D𝐺𝐺3.394.14\n3.3012.70\n7.15\nFigure 8. The Sphere Error of the PGF before (ResNet-18) and\nafter Sphere-Oriented Training (Ours). Smaller sphere error in-\ndicates that the geodesic distance between extracted features are\nmore proportional to the gaze differences.\nportional relationship. To do so, we measure the Sphere\nError, defined as the ratio of distance between eiand the\nsphere surface to the radius of the sphere. The quantitative\nresults presented in Fig. 8 demonstrate that the Sphere Error\nafter the Sphere-Oriented Training reduces in both within-\ndomain and cross-domain settings. Fig. 9 provides a more\nintuitive view. The Sphere Error is significantly reduced\nin the central region after the Sphere-Oriented Training and\nthe distribution of the PGF becomes more spherical. Above\nresults validates the effectiveness of the propsoed Sphere-\nOriented Training.\n6. Limitations and Discussions\nQ1: Does the observation in Fig. 2 apply to features ex-\ntracted by different gaze estimation models? We have\nproven that the observation holds true for different model\narchitectures in Tabs. 1 and 2 and different datasets Fig. 7.\nHere we further investigate the influence of two loss func-\ntions: L1,L2loss, and two gaze representations: 3D unit\nvector (x, y, z ), 2D Euler angle (yaw, pitch ). In Fig. 10,\nwe train ResNet-18 models under above conditions and\nproject the extracted features to 3D space using geodesic\ndistance. Results show that the AGG is robust to different\nloss functions. When gaze is represented by 2D Euler an-\ngles, the projected features no longer distributes across the\nsphere surface, they approximately distribute on the surface\nof a 2D plane, similar to the gaze label. The Sphere Align-\nment algorithm needs to be altered to adapt different gaze\nrepresentations, which we have left for future work.\nTop View\nSide View\nPGF of Baseline\nTop View\nSide View\nPGF of AGG\nSmallLargeSphere Error\nFigure 9. Visualization of the Sphere Error before and after\nSphere-Oriented Training in DE. In the side view, the bottom area\nof the PGF of AGG is more spherical.\nTop View\nSide View\nTop View\nSide View\nTop View\nSide View\nTop View\nSide View\nTop View\nSide View\nTop View\nSide View\n𝑥𝑥,𝑦𝑦,𝑧𝑧 (𝑦𝑦𝑦𝑦𝑦𝑦 ,𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝 )Gaze Label\n AGG -L1 Loss AGG -L2 Loss\nGaze Label AGG -L1 Loss AGG -L2 Loss\nFigure 10. Projecting features extracted by ResNet-18 trained with\ndifferent strategy in DEinto 3D space by geodesic distance.\nQ2: Does the Isometric Propagator (IP) suffer from\nthe same overfitting issue as the last FC layer since it is\nimplemented by MLP? Although we completely replace\nthe last FC layer with the GPM in the test time, the MLP\nbased IP is still used in source domain training. The use\nof IP is an unavoidable compromise, because in the cur-\nrent deep-learning community, there has not been a perfect\nsolution for integrating Isomap into the back-propagation\nprocess. But the IP differs from the original regression FC\nlayer, since IP is only used in the training time, as a tool\nfor back-propagation with fixed parameters. Still, the im-\nplementation of IP is a limitation to our method. The per-\nformance of the AGG might be further improved if there is\na better way to integrate Isomap into the training process.\n7. Conclusion\nIn this paper we propose the Analytical Gaze Generaliza-\ntion framework for generalizing gaze estimation models to\nunseen domains. Based on the observation that the geodesic\ndistance between extracted image features is proportional to\nthe angular gaze differences, we propose the Geodesic Pro-\njection Module that estimates gaze from the image feature\nanalytically and incorporate it into the source domain train-\ning by the proposed Sphere-Oriented Training. Extensive\nexperiments show that the GPM achieves better generaliza-\ntion ability than the conventional FC layer, and the AGG im-\nproves the cross-domain accuracy significantly, outperform-\ning SOTA methods. The concept of the AGG may inspire\nmethod designs in other physical regression tasks, e.g. pose\nestimation.\n1416\nReferences\n[1] Yiwei Bao, Yihua Cheng, Yunfei Liu, and Feng Lu. Adaptive\nfeature fusion network for gaze tracking in mobile tablets. In\n2020 25th International Conference on Pattern Recognition\n(ICPR) , pages 9936–9943. IEEE, 2021. 2\n[2] Yiwei Bao, Yunfei Liu, Haofei Wang, and Feng Lu. Gen-\neralizing gaze estimation with rotation consistency. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition , pages 4207–4216, 2022. 2, 6\n[3] Alisa Burova, John M ¨akel¨a, Jaakko Hakulinen, Tuuli Keski-\nnen, Hanna Heinonen, Sanni Siltanen, and Markku Turunen.\nUtilizing vr and gaze tracking to develop ar solutions for in-\ndustrial maintenance. In Proceedings of the 2020 CHI Con-\nference on Human Factors in Computing Systems , pages 1–\n13, 2020. 1\n[4] Nora Castner, Thomas C Kuebler, Katharina Scheiter, Ju-\nliane Richter, Th ´er´ese Eder, Fabian H ¨uttig, Constanze Keu-\ntel, and Enkelejda Kasneci. Deep semantic gaze embedding\nand scanpath comparison for expertise classification during\nopt viewing. In ACM Symposium on Eye Tracking Research\nand Applications , pages 1–10, 2020. 1\n[5] Zhaokang Chen and Bertram E Shi. Appearance-based gaze\nestimation using dilated-convolutions. In Asian Conference\non Computer Vision , pages 309–324. Springer, 2018. 2\n[6] Yihua Cheng, Shiyao Huang, Fei Wang, Chen Qian, and\nFeng Lu. A coarse-to-fine adaptive network for appearance-\nbased gaze estimation. In Proceedings of the AAAI Confer-\nence on Artificial Intelligence , pages 10623–10630, 2020. 2,\n6, 7\n[7] Yihua Cheng, Xucong Zhang, Feng Lu, and Yoichi Sato.\nGaze estimation by exploring two-eye asymmetry. IEEE\nTransactions on Image Processing , 29:5259–5272, 2020. 2\n[8] Yihua Cheng, Yiwei Bao, and Feng Lu. Puregaze: Purifying\ngaze feature for generalizable gaze estimation. In Proceed-\nings of the AAAI Conference on Artificial Intelligence , pages\n436–443, 2022. 1, 2, 3, 6, 7\n[9] Kenneth Alberto Funes Mora, Florent Monay, and Jean-\nMarc Odobez. Eyediap: A database for the development and\nevaluation of gaze estimation algorithms from rgb and rgb-d\ncameras. In Proceedings of the Symposium on Eye Tracking\nResearch and Applications , pages 255–258, 2014. 2, 6\n[10] Elias Daniel Guestrin and Moshe Eizenman. General theory\nof remote gaze estimation using the pupil center and corneal\nreflections. IEEE Transactions on biomedical engineering ,\n53(6):1124–1133, 2006. 2\n[11] Dan Witzner Hansen and Qiang Ji. In the eye of the beholder:\nA survey of models for eyes and gaze. IEEE transactions on\npattern analysis and machine intelligence , 32(3):478–500,\n2009. 2\n[12] Petr Kellnhofer, Adria Recasens, Simon Stent, Wojciech Ma-\ntusik, and Antonio Torralba. Gaze360: Physically uncon-\nstrained gaze estimation in the wild. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision ,\npages 6912–6921, 2019. 1, 2, 6, 7\n[13] Jess Kerr-Gaffney, Amy Harrison, and Kate Tchanturia. Eye-\ntracking research in eating disorders: A systematic review.International Journal of Eating Disorders , 52(1):3–27, 2019.\n1\n[14] Andrew J King, Gregory F Cooper, Gilles Clermont, Harry\nHochheiser, Milos Hauskrecht, Dean F Sittig, and Shyam\nVisweswaran. Leveraging eye tracking to prioritize relevant\nmedical record data: comparative machine learning study.\nJournal of medical Internet research , 22(4):e15876, 2020. 1\n[15] Robert Konrad, Anastasios Angelopoulos, and Gordon Wet-\nzstein. Gaze-contingent ocular parallax rendering for virtual\nreality. ACM Transactions on Graphics (TOG) , 39(2):1–12,\n2020. 1\n[16] Kyle Krafka, Aditya Khosla, Petr Kellnhofer, Harini Kan-\nnan, Suchendra Bhandarkar, Wojciech Matusik, and Anto-\nnio Torralba. Eye tracking for everyone. In Proceedings of\nthe IEEE conference on computer vision and pattern recog-\nnition , pages 2176–2184, 2016. 2\n[17] Mikko Kyt ¨o, Barrett Ens, Thammathip Piumsomboon,\nGun A Lee, and Mark Billinghurst. Pinpointing: Precise\nhead-and eye-based target selection for augmented reality. In\nProceedings of the 2018 CHI Conference on Human Factors\nin Computing Systems , pages 1–14, 2018. 1\n[18] Isack Lee, Jun-Seok Yun, Hee Hyeon Kim, Youngju Na, and\nSeok Bong Yoo. Latentgaze: Cross-domain gaze estimation\nthrough gaze-aware analytic latent code manipulation. In\nProceedings of the Asian Conference on Computer Vision ,\npages 3379–3395, 2022. 7\n[19] Jiahui Liu, Jiannan Chi, and Shuo Fan. A method for ac-\ncurate 3d gaze estimation with a single camera and two\ncollinear light sources. IEEE Transactions on Instrumenta-\ntion and Measurement , 2022. 2\n[20] Yunfei Liu, Ruicong Liu, Haofei Wang, and Feng Lu. Gen-\neralizing gaze estimation with outlier-guided collaborative\nadaptation. In Proceedings of the IEEE/CVF International\nConference on Computer Vision , pages 3835–3844, 2021. 1,\n2, 6\n[21] Feng Lu, Yusuke Sugano, Takahiro Okabe, and Yoichi Sato.\nAdaptive linear regression for appearance-based gaze esti-\nmation. IEEE transactions on pattern analysis and machine\nintelligence , 36(10):2033–2046, 2014. 2\n[22] Feng Lu, Xiaowu Chen, and Yoichi Sato. Appearance-based\ngaze estimation via uncalibrated gaze pattern recovery. IEEE\nTransactions on Image Processing , 26(4):1543–1553, 2017.\n2\n[23] Seonwook Park, Shalini De Mello, Pavlo Molchanov, Umar\nIqbal, Otmar Hilliges, and Jan Kautz. Few-shot adaptive\ngaze estimation. In Proceedings of the IEEE/CVF inter-\nnational conference on computer vision , pages 9368–9377,\n2019. 2\n[24] Sam T Roweis and Lawrence K Saul. Nonlinear dimension-\nality reduction by locally linear embedding. science , 290\n(5500):2323–2326, 2000. 4\n[25] Timo Schneider, Boris Schauerte, and Rainer Stiefelhagen.\nManifold alignment for person independent appearance-\nbased gaze estimation. In 2014 22nd international confer-\nence on pattern recognition , pages 1167–1172. IEEE, 2014.\n2\n1417\n[26] Vincent Sitzmann, Ana Serrano, Amy Pavel, Maneesh\nAgrawala, Diego Gutierrez, Belen Masia, and Gordon Wet-\nzstein. Saliency in vr: How do people explore virtual envi-\nronments? IEEE transactions on visualization and computer\ngraphics , 24(4):1633–1642, 2018. 1\n[27] Sophie Stellmach, Sebastian Stober, Andreas N ¨urnberger,\nand Raimund Dachselt. Designing gaze-supported multi-\nmodal interactions for the exploration of large image collec-\ntions. In Proceedings of the 1st Conference on Novel Gaze-\nControlled Applications , New York, NY , USA, 2011. Asso-\nciation for Computing Machinery. 1\n[28] Li Sun, Zicheng Liu, and Ming-Ting Sun. Real time gaze\nestimation with a consumer depth camera. Information Sci-\nences , 320:346–360, 2015. 2\n[29] Kentaro Takemura and Kenta Yamagishi. A hybrid eye-\ntracking method using a multispectral camera. In 2017 IEEE\nInternational Conference on Systems, Man, and Cybernetics\n(SMC) , pages 1529–1534. IEEE, 2017. 2\n[30] Joshua B Tenenbaum, Vin de Silva, and John C Langford.\nA global geometric framework for nonlinear dimensionality\nreduction. science , 290(5500):2319–2323, 2000. 4\n[31] Laurens Van der Maaten and Geoffrey Hinton. Visualizing\ndata using t-sne. Journal of machine learning research , 9\n(11), 2008. 4\n[32] Kang Wang, Rui Zhao, Hui Su, and Qiang Ji. Generalizing\neye tracking with bayesian adversarial learning. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition , pages 11907–11916, 2019. 1\n[33] Yaoming Wang, Yangzhou Jiang, Jin Li, Bingbing Ni, Wen-\nrui Dai, Chenglin Li, Hongkai Xiong, and Teng Li. Con-\ntrastive regression for domain adaptation on gaze estimation.\nInProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 19376–19385, 2022.\n1, 2, 6\n[34] Zhimin Wang, Huangyue Yu, Haofei Wang, Zongji Wang,\nand Feng Lu. Comparing single-modal and multimodal in-\nteraction in an augmented reality system. In 2020 IEEE In-\nternational Symposium on Mixed and Augmented Reality Ad-\njunct, ISMAR 2020 Adjunct, Recife, Brazil, November 9-13,\n2020 , pages 165–166. IEEE, 2020. 1\n[35] Xuehan Xiong, Zicheng Liu, Qin Cai, and Zhengyou Zhang.\nEye gaze tracking using an rgbd camera: A comparison with\na rgb solution. In Proceedings of the 2014 ACM Interna-\ntional Joint Conference on Pervasive and Ubiquitous Com-\nputing: Adjunct Publication , pages 1113–1121, 2014. 2\n[36] Yu Yu and Jean-Marc Odobez. Unsupervised representa-\ntion learning for gaze estimation. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 7314–7324, 2020. 2\n[37] Xucong Zhang, Yusuke Sugano, Mario Fritz, and Andreas\nBulling. Appearance-based gaze estimation in the wild. In\nProceedings of the IEEE conference on computer vision and\npattern recognition , pages 4511–4520, 2015. 2\n[38] Xucong Zhang, Yusuke Sugano, Mario Fritz, and Andreas\nBulling. It’s written all over your face: Full-face appearance-\nbased gaze estimation. In Proceedings of the IEEE Con-\nference on Computer Vision and Pattern Recognition Work-\nshops , pages 51–60, 2017. 2, 6, 7[39] Xucong Zhang, Seonwook Park, Thabo Beeler, Derek\nBradley, Siyu Tang, and Otmar Hilliges. Eth-xgaze: A large\nscale dataset for gaze estimation under extreme head pose\nand gaze variation. In European Conference on Computer\nVision , pages 365–381. Springer, 2020. 2, 3, 6\n1418'}, 'dist': 0.9286905527114868}
Result 8: {'text': 'data', 'metadata': {'created_at': 1736932570, 'modified_at': 1736932570, 'owner': 'sayandeep', 'path': 'RAG_CONTENT/conferences/cvpr_ocr/Ben-Shabat_3DInAction_Understanding_Human_Actions_in_3D_Point_Clouds_CVPR_2024_paper.txt', 'size': 45328, 'seen_at': 1737191136, 'data': '3DInAction: Understanding Human Actions in 3D Point Clouds\r\nYizhak Ben-Shabat1;2Oren Shrout2Stephen Gould1\r\n1Australian National University2Technion, Israel Institute of Technology\r\nsitzikbs@technion.ac.il, shrout.oren@campus.technion.ac.il, stephen.gould@anu.edu.au\r\nhttps://github.com/sitzikbs/3dincaction\r\nFigure 1. t-patches for action recognition. We propose a new representation for dynamic 3D point clouds. Termed t-patches, these\r\nare locally evolving point cloud sets aggregated over time. Learning features over t-patches provides an improved temporal point cloud\r\nrepresentation for action understanding.\r\nAbstract\r\nWe propose a novel method for 3D point cloud ac-\r\ntion recognition. Understanding human actions in RGB\r\nvideos has been widely studied in recent years, however,\r\nits 3D point cloud counterpart remains under-explored de-\r\nspite the clear value that 3D information may bring. This\r\nis mostly due to the inherent limitation of the point cloud\r\ndata modality—lack of structure, permutation invariance,\r\nand varying number of points—which makes it difﬁcult to\r\nlearn a spatio-temporal representation. To address this lim-\r\nitation, we propose the 3DinAction pipeline that ﬁrst esti-\r\nmates patches moving in time (t-patches) as a key build-\r\ning block, alongside a hierarchical architecture that learns\r\nan informative spatio-temporal representation. We show\r\nthat our method achieves improved performance on existing\r\ndatasets, including DFAUST and IKEA ASM. Code is pub-\r\nlicly available at https://github.com/sitzikbs/3dincaction.\r\n1. Introduction\r\nIn this paper, we address the task of action recognition\r\nfrom 3D point cloud sequences. We propose a novel\r\npipeline wherein points are grouped into temporally evolv-ing patches that capture discriminative action dynamics.\r\nOur work is motivated by the massive growth of online me-\r\ndia, mobile and surveillance cameras that have enabled the\r\ncomputer vision community to develop many data-driven\r\naction-recognition methods [5, 12,26,31], most of which\r\nrely on RGB video data. Recently, commodity 3D sensors\r\nare gaining increased momentum, however, the 3D point\r\ncloud modality for action recognition has yet been under-\r\nexploited due to the scarcity of 3D action-labeled data.\r\nIn many cases, a pure RGB video-based inference may\r\nnot be enough and incorporating other modalities like ge-\r\nometry is required. This is especially necessary for safety\r\ncritical applications such as autonomous systems, where\r\nredundancy is crucial, or in scenarios where the video is\r\nheavily degraded (e.g., due to poor lighting). Some ap-\r\nproaches incorporate geometrical information implicitly,\r\ne.g., through intermediate pose estimation [7]. This often\r\nentails extra steps that require more time and resources and\r\nis still limited to video input. Therefore a more explicit ap-\r\nproach is desirable.\r\n3D sensors provide an alternative modality in the form of\r\npoint clouds sampled on the environment. Despite the vast\r\nresearch on 3D vision and learning, even static 3D point\r\ncloud datasets are signiﬁcantly smaller than their RGB im-\r\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\r\nExcept for this watermark, it is identical to the accepted version;\r\nthe final published version of the proceedings is available on IEEE Xplore.\r\n19978\r\nage counterparts due to difﬁculties in collecting and label-\r\ning. 3D point cloud sequence databases are even smaller,\r\nmaking it more difﬁcult to learn a meaningful 3D action\r\nrepresentation. Furthermore, learning a point cloud repre-\r\nsentation still remains an active research ﬁeld because point\r\nclouds are unstructured, unordered, and may contain a vary-\r\ning number of points. Learning a temporal point cloud rep-\r\nresentation is even more challenging since, unlike pixels,\r\nthere is no one-to-one point correspondence through time.\r\nWe address these challenges and propose the 3DinAc-\r\ntion pipeline for 3D point cloud action recognition. In our\r\npipeline, we ﬁrst extract local temporal point patches (t-\r\npatches) that reﬂect a point region’s motion in time, see\r\nFigure 1. We then learn a t-patch representation using a\r\nnovel hierarchical architecture that incorporates spatial fea-\r\ntures in the temporal domain. We ﬁnally get an action pre-\r\ndiction for each frame in a sequence by aggregating mul-\r\ntiple t-patch representations. This pipeline overcomes the\r\nneed for ground truth point temporal correspondence, grid\r\nstructure, point order, and a ﬁxed number of points in each\r\nframe. Intuitively, patches reﬂect local surface deformation\r\nand are more robust to point correspondence errors.\r\nWe conduct extended experiments to evaluate the perfor-\r\nmance of our approach compared to existing SoTA methods\r\nand show that 3DinAction provides signiﬁcant performance\r\ngains of 13% and7%in accuracy on DFAUST and IKEA\r\nASM, respectively.\r\nThe key contributions of our work are as follows:\r\n• A novel representation for dynamically evolving local\r\npoint cloud sets termed t-patches.\r\n• A hierarchical architecture that produces an informa-\r\ntive spatio-temporal representation for sequences of point\r\nclouds.\r\n2. Related Work\r\nLearning 3D point cloud representations. Point clouds\r\npose a challenge for neural networks due to their unstruc-\r\ntured and point-wise unordered nature. To address these\r\nchallenges, several approaches have been proposed. Point-\r\nNet [23, 24] uses permutation-invariant operators, such as\r\npointwise MLPs and pooling layers, to aggregate features\r\nacross a point set. Some approaches construct a graph\r\nfrom the point set. DGCNN [34] applies message pass-\r\ning and performs graph convolutions on kNN graphs, KC-\r\nNet [29] uses kernel correlation and graph pooling, and\r\nKd-Networks [15] apply multiplicative transformations and\r\nshare the parameters based on the subdivisions imposed by\r\nkd-trees. Alternatively, the structure can be imposed using\r\na grid of voxels [22, 36], or a grid of Gaussians in 3Dm-\r\nFVNet [1]. Another alternative avoids the structure by using\r\nTransformer’s attention mechanism [17, 37] . For a compre-\r\nhensive survey of point cloud architectures please see [14].\r\nRecently, various factors that can impact the training ofdifferent architectures have been investigated [13, 25]. This\r\nincludes exploring data augmentation strategies and loss\r\nfunctions that are not speciﬁc to a particular architecture.\r\nThe results of this study showed that older PointNet-based\r\narchitectures [23, 24] can perform comparably to newer ar-\r\nchitectures with minor changes.\r\nAll of the above methods deal with static, single-frame,\r\nor single-shape point clouds. In this work, the input\r\nis a temporal point cloud where a representation for a\r\nshort sequence is required and point correspondence be-\r\ntween frames is unknown. Therefore extending existing ap-\r\nproaches is not trivial.\r\nLearning temporal 3D point cloud representations.\r\nTemporal point clouds have not been as extensively stud-\r\nied as their static counterparts, in particular for action\r\nrecognition. Meteornet [21] processes 4D points using a\r\nPointNet++architecture where they appended a temporal\r\ndimension to the spatial coordinates. PSTNet [10, 11] pro-\r\nposed spatio-temporal convolutions and utilized some of\r\nthe temporal consistency for action recognition. Similarly,\r\nP4Transformer [8] uses 4D convolutions and a transformer\r\nfor capturing appearance and motion via self-attention. In\r\na follow-up work PST-Transformer [9] employs a video\r\nlevel of self-attention in search for similar points across en-\r\ntire videos and so encodes spatio-temporal structure. Some\r\nworks attempt to alleviate the full supervision requirement\r\nfor 3D action recognition. These include self-supervised\r\nfeatures learning [32] by predicting temporal order from a\r\nlarge unlabeled dataset and ﬁne-tuning on a smaller anno-\r\ntated datasets and unsupervised skeleton colorization [35].\r\nAdditional supervised approaches include MinkowskiNet\r\n[6] that uses a 4D spatio-temporal CNN after converting\r\nthe point clouds to an occupancy grid, 3DV [33] that en-\r\ncodes 3D motion information from depth videos into a com-\r\npact voxel set, and Kinet [38] that implicitly encoded fea-\r\nture level dynamics in feature space by unrolling the normal\r\nsolver of ST-surfaces.\r\nThe above methods, perform a single classiﬁcation per\r\nclip. In this paper, we focus on a related, and more chllang-\r\ning, task that requires a prediction per-frame. We propose\r\nto convert the point cloud representation into t-patches and\r\nuse an MLP based hierarchical architecture to get the spatio-\r\ntemporal representation.\r\n3D action understanding datasets. One of the major driv-\r\ning forces behind the success of learning-based approaches\r\nis the availability of annotated data. For the task of 3D point\r\ncloud action recognition, there is currently no designated\r\nstandard dataset, however, some existing datasets may be\r\nextended. The CAD 60 and CAD 120 [16, 30] datasets in-\r\nclude 60 and 120 long-term activity videos of 12 and 10\r\nclasses respectively (e.g., making cereal, microwave food).\r\nThese datasets provide raw RGB, skeletons, and depth data\r\nhowever its small scale and long-term focus limit its effec-\r\n19979\r\ntiveness. The NTU RGB+D 60 [28] and NTU RGB+D 120\r\n[20] provide\x1856K and\x18114K clips containing 60 and 120\r\nactions classes respectively, e.g., taking off a jacket, taking\r\na selﬁe. They provide three different simultaneous RGB\r\nviews, IR and depth streams as well as 3D skeletons. While\r\nthese datasets can be considered large-scale, their contrived\r\nnature makes recent skeleton-based methods (e.g., [7]) per-\r\nform well, making a prior-free approach difﬁcult to justify.\r\nThe MSR-Action3D dataset [19] includes 20 action classes\r\nperformed by 10 subjects for a total of 567 depth map se-\r\nquences, collected using a Kinect v1 device (23K frames).\r\nThe sequences in this dataset are very short and therefore\r\nusing it to evaluate learning-based approaches provides a\r\nlimited indication of generalization. The above datasets\r\nprovide per clip action annotations.\r\nSome datasets inherently provide per-frame annotations.\r\nThe IKEA ASM dataset [2] provides 371 videos clipped\r\ninto 31K clips. It contains 33 action classes related to fur-\r\nniture assembly, annotated per frame. This dataset provides\r\nseveral modalities including three RGB views, and Depth.\r\nIt is an extremely challenging dataset since the human as-\r\nsembler is often occluded and presents very unique assem-\r\nbly poses. It is also very imbalanced since different assem-\r\nbly actions have different duration and may repeat multi-\r\nple times within the same assembly. Although it was de-\r\nsigned for video action recognition, its challenges are the\r\ncore reasons for choosing to extend it to the point cloud\r\naction recognition task. The DFAUST dataset [3] provides\r\nhigh-resolution 4D scans of human subjects in motion. It\r\nincludes 14 action categories with over 100 dynamic scans\r\nof 10 subjects (1:1 male-to-female ratio) with varying body\r\nshapes represented as registrations of aligned meshes, there-\r\nfore an extension to our task is straightforward. One partic-\r\nularly important feature of this dataset is the GT point corre-\r\nspondences throughout the sequence i.e. it is possible to fol-\r\nlow each point’s movement through time. While this dataset\r\nis not as large-scale as others, it provides ground truth infor-\r\nmation (correspondence) that most other collected datasets\r\ndo not. Therefore, we extend this dataset to 3D point cloud\r\naction recognition and use it as a testbed for many ablation\r\nstudies (see Section 4.4).\r\n3. 3DinAction pipeline\r\nOur 3DinAction pipeline is illustrated in Figure 2. Given\r\na temporal sequence of 3D point clouds we ﬁrst extract a\r\nset of t-patches (Section 3.1). We then feed the t-patches\r\ninto a hierarchical neural network (Section 3.2) to produce\r\na per-frame high dimensional feature vector representation.\r\nFinally, the feature vectors are fed into a classiﬁer to obtain\r\nper-frame predictions. The proposed approach is prior-free\r\n(no skeleton extraction required) and therefore general and\r\ncan be used on different action-understanding datasets.3.1. t-patches\r\nLetS=fxj2R3jj= 1;:::;Ngdenote a 3D point\r\ncloud withNpoints. In the classic (static) setting, a patch\r\n\tqis extracted around some query point xq. For example,\r\nthe patch \tqmay be constructed by ﬁnding the k-nearest\r\nneighbors of xqinS.\r\nIn our temporal setting we are given a sequence of point\r\ncloudsS=fS0;:::;STgcomposed of point cloud frames\r\nSt=fxt\r\njjj= 1;:::;Ntg. Here the superscript tis\r\nused to denote the index of the point cloud in the sequence.\r\nInstead of extracting a patch within a single frame, we allow\r\npatches to extend temporally, and denote them as t-patches.\r\nDeﬁnition 3.1 A t-patchPqis a sequence of point sets in-\r\ndexed by a query point x0\r\nqand jointly moving in time de-\r\nﬁned by a pointwise mapping function between patches in\r\nconsecutive frames. Mathematically, Pq=h\tt\r\nqiT\r\nt=0where\r\n\t0\r\nqis the initial (static) patch and \tt\r\nq= \x08(\tt\x001\r\nq)is the\r\npatch at time twhere \x08is a pointwise mapping function.\r\nIn practice, it is difﬁcult to ﬁnd a reliable mapping func-\r\ntion\x08. Therefore we propose a simpliﬁed formulation that,\r\nfor a given query point x0\r\nq, ﬁrst extracts a patch for the\r\nﬁrst frame \t0\r\nqand then iteratively extracts corresponding\r\npatches for the next frames (iterating over time), by us-\r\ning the closest point in the next frame as the new query\r\npoint. More formally, we specify\x00 !\t0\r\nq,\t0\r\nq,\x00 !\tt\r\nq=\r\nknn(xt\x001\r\nq;St)andxt\r\nq=nn(xt\x001\r\nq;St)fort= 1;:::;T .\r\nHereknn is theknearest neighbor and nnis nearest neigh-\r\nbor. Then, the simpliﬁed t-patch formulation is given by\r\n\x00 !Pq=h\x00 !\tt\r\nqjt= 0;:::;Ti (1)\r\nSee Figure 3left for an illustration of the t-patch extrac-\r\ntion process. Note that if ground truth correspondence is\r\navailable knncan be swapped back to \x08. However, this\r\ndoes not guarantee improved performance.\r\nTemporal t-patch collapse. The simpliﬁed formulation\r\nof extracting t-patches inherently suffers from the problem\r\nof two or more t-patches collapsing into having the same\r\npoints after a certain frame. We call this scenario t-patch\r\ntemporal collapse. Temporal collapse can happen when-\r\neverxt\r\nq=xt\r\npforx0\r\nq6=x0\r\np. The main issue with tem-\r\nporal collapse is the reduction in point coverage as time\r\nprogresses, i.e. the patches covering the last point cloud\r\nhave signiﬁcant overlaps and therefore include fewer points\r\nthan the ﬁrst frame and so missing vital data. An illustra-\r\ntion of the t-patch collapse problem is available in Figure 3\r\n(right). To mitigate this issue, we propose two solutions.\r\nFirst, adding small noise to each iteration’s query points, i.e.\x00 !\tt\r\nq=knn(xt\r\nq+\x0f;St+1)where\x0f\x18N(\x16;\x1b2)is a small\r\nGaussian noise. Second, we propose to construct t-patches\r\nfrom the ﬁrst to last frame but also in reverse, initializing\r\n19980\r\nFigure 2. 3DinAction pipeline. Given a sequence of point clouds, a set of t-patches is extracted. The t-patches are fed into a neural\r\nnetwork to output an embedding vector. This is done hierarchically until ﬁnally the global t-patch vectors are pooled to get a per-frame\r\npoint cloud embedding which is then fed into a classiﬁer to output an action prediction per frame.\r\nFigure 3. t-patch construction and collapse. Illustration of t-\r\npatch construction (left) and collapse (right). Starting from an ori-\r\ngin point x0\r\nqwe ﬁnd the nearest neighbours in the next frame it-\r\neratively to construct the t-patch subset (non-black points). A col-\r\nlapse happens when two different origin points, x0\r\nqandx0\r\np, have\r\nthe same nearest neighbour at some time step, \t3\r\np= \t3\r\nqhere.\r\nwith\t0\r\nqand\tT\r\nq, respectively. We name this variation bidi-\r\nrectional t-patches. More formally bidirectional t-patches\r\nare given by,\r\n !P= [\r\nq\x00 !Pq!\r\n[ [\r\np \x00Pp!\r\n(2)\r\nwhere \x00Ppis deﬁned similarly to\x00 !Pqbut in the reverse\r\ndirection, i.e., \x00\tT\r\np,\tT\r\npand \x00\tt\r\np=knn(xt+1\r\np;St)fort=T\x001;:::; 0. Here, the ﬁnal set of t-patches is com-\r\nposed of an equal number of t-patches from both directions.\r\n3.2. Hierarchical architecture\r\nThe proposed architecture is composed of lconsecutive t-\r\npatch modules. Each module receives a point cloud se-\r\nquenceSas input. The sequence is fed into a t-patch ex-\r\ntractor where it undergoes subsampling and t-patch extrac-\r\ntion, forming ~SlandPlrespectively. Then, the t-patches\r\nare fed into t-patch Net, a network that computes a high-\r\ndimensional feature vector flfor each t-patch, parametrized\r\nby\x12l. The subsampled sequence ~Sland its corresponding\r\nt-patch features flare then fed into the next t-patch mod-\r\nule. These modules form a hierarchy in the sense that each\r\nmodule receives as input a sparser point cloud with a higher\r\ndimensional feature vector representing each point (encod-\r\ning both spatial and temporal information). Note that both\r\nthe t-patch points and their features are fed into t-patch Net.\r\nt-patch extractor. We ﬁrst subsample the ﬁrst frame\r\nin the point cloud sequence S0using farthest point\r\nsampling (FPS) to form a set of Mquery points\r\n~S0=fx0\r\nj2FPS (S0;M)g. The set ~S0is used to form\r\nthe t-patches. Subsampling is required since computing a\r\nt-patch for each point is inefﬁcient and unnecessary due to\r\noverlaps. After subsampling, we extract Mt-patches us-\r\ning Equation 2 where q2~S0. The extractor operates on\r\nboth 3D points and their corresponding features (for mod-\r\n19981\r\nules deeper in the hierarchy).\r\nModel architecture and t-patch net. The t-patch net-\r\nwork computes a high dimensional representation for each\r\nt-patch. The t-patch Net architecture is composed of sev-\r\neral MLP layers operating on the non-temporal dimensions\r\n(sharing weights across points) followed by a convolutional\r\nlayer operating on both the temporal and feature dimen-\r\nsions. Note that the network weights are also shared across\r\nt-patches. The output of each t-patch Net is a vector for\r\neach frame. The ﬁnal frame representation is obtained by\r\naggregating all of the t-patch features using a max pooling\r\noperation i.e.maxpool Ml(f3). This representation is then\r\nfed into a classiﬁer consisting of three fully connected lay-\r\ners with temporal smoothing and softmax to output the ﬁnal\r\naction prediction. To train the network we use the same\r\nlosses of RGB based approaches [2, 5] which include a per-\r\nframe prediction cross entropy loss and a per-sequence pre-\r\ndiction cross entropy loss (summed and weighted evenly)\r\nLtotal=Lframe +Lseq. For full details see supplemental.\r\n4. Experiments\r\nWe evaluate the performance of our approach on three\r\ndatasets. The results show that the 3DinAction pipeline out-\r\nperforms all baselines in DFAUST [3] and IKEA ASM [2]\r\nand is comparable in MSR-Action 3D [19]. We then con-\r\nduct an ablation study for selecting parameters and t-patch\r\nextraction method showing that adding jitter and bidirec-\r\ntional t-patches is beneﬁcial. Finally, we report time per-\r\nformance and show the tradeoff between performance and\r\ninference time. For more results and experiments, see sup-\r\nplemental material.\r\nBaselines and evaluation metrics. For evaluation, we re-\r\nport several standard metrics [4]: the top1 and top3 frame-\r\nwise accuracy are the de facto standard for action classi-\r\nﬁcation. We compute it by summing the number of cor-\r\nrectly classiﬁed frames and dividing by the total number of\r\nframes in each video and then averaging over all videos in\r\nthe test set. Additionally, since some of the datasets are im-\r\nbalanced and may contain different actions for each frame\r\nin a clip, we also report the macro-recall by separately com-\r\nputing recall for each category and then averaging (macro).\r\nFinally, we report the mean average precision (mAP) since\r\nall untrimmed videos contain multiple action labels.\r\nFor DFAUST and IKEA ASM we report static methods\r\nPointNet [23], PointNet++[24], and Set Transformer [18]\r\nby applying them on each point cloud frame individually.\r\nAdditionally, we report temporal methods like PSTNet [10]\r\nand also implemented a temporal smoothing version of\r\neach static method (PoinNet+TS, Pointnet+++TS, and Set\r\nTransformer+TS respectively) by learning the weights of a\r\nconvolutional layer over the temporal dimension. Tempo-\r\nral smoothing aims to provide a naive baseline for utiliz-\r\ning temporal information in addition to spatial information.Note that in all experiments, unless otherwise speciﬁed, our\r\nmethod uses the simpliﬁed formulation with jitter and bidi-\r\nrectional t-patches.\r\n4.1. Experiments on DFAUST dataset\r\nWe extend the DFAUST dataset for the task of action recog-\r\nnition and show that the proposed approach outperforms\r\nother methods (see Table 1).\r\nDFAUST dataset [3]. We extended the DFAUST dataset to\r\nour task by subdividing it into clips of 64 frames with train\r\nand test human subjects. The split was constructed so no\r\nsubject will appear in both training and test set as well as\r\nguarantee that all actions appear in both. The train and test\r\nsets contain 76 full-length sequences (395 clips, and \x1825K\r\nframes) and 53 sequences (313 clips, and \x1820K frames) re-\r\nspectively. Each point cloud frame contains 6890 points.\r\nThese points are mesh vertices and therefore the density\r\nvaries greatly (e.g., very dense on the face, hands, and feet\r\nand sparser on the legs). For all baselines, we sampled a set\r\nof 1024 points using the farthest point sampling algorithm\r\nto provide a more uniform set of points. For this dataset,\r\nall frames in a clip have the same label. Note that not all\r\nactions are performed by all subjects. For the full action list\r\nand dataset statistics, see the supplemental.\r\nResults. The results, reported in Table 1, show that our pro-\r\nposed approach outperforms all baselines by a large mar-\r\ngin. It also shows that temporal smoothing boosts perfor-\r\nmance signiﬁcantly for all static baselines. Additionally, to\r\nexplore the inﬂuence of our simpliﬁed knn-based tempo-\r\nral point mapping, we used the GT point correspondence\r\nto match the consecutive t-patch origin points and report\r\nthe results as another baseline (Ours + GT corr). The re-\r\nsults show that there is a mAP performance gain with GT\r\ncorrespondence, however, it is limited. Note that in most\r\ndatasets, this GT correspondence is not available. Finally,\r\nwe also experimented with a Transformer architecture to\r\nprocess the t-patch learned representations and show that\r\nit does not provide additional performance boost. This may\r\nbe attributed to the dataset size.\r\nInsight. We extended the GradCam [27] approach for our\r\n3DinAction pipeline. Using this approach we get a score\r\nper point in each t-patch proportional to its inﬂuence on\r\nclassifying the frame to a given target class. The results in\r\nFigure 4show that, as expected, our approach learns mean-\r\ningful representations since the most prominent regions are\r\nthe ones with the informative motion. For example, in the\r\nJumping jacks action (top row) the hands are most promi-\r\nnent as they are making a large and distinct motion.\r\n4.2. Experiments on IKEA ASM dataset\r\nIKEA ASM dataset [2]. This dataset consists of 371\r\nvideos (3M frames) of people assembling IKEA furniture\r\nin different indoor environments. It was collected using a\r\n19982\r\nMethodFrame acc.\r\ntop 1 top 3 mAP\r\n3DmFVNet [1] 60.86 87.68 0.7171\r\nPointNet [23] 65.67 86.44 0.7161\r\nPointNet++[24] 58.51 88.28 0.5842\r\nSet Transformer [18] 52.27 81.98 0.6209\r\nPoinNet [23] + TS 74.10 94.00 0.7863\r\nPointNet++[24] + TS 67.88 86.21 0.7563\r\nSet Transformer [18] + TS 62.95 90.33 0.7322\r\nPSTNet [10] 50.70 78.28 0.6490\r\nOurs + GT corr 77.67 95.38 0.8762\r\nOurs + Transformer 77.09 93.7 77.49\r\nOurs 87.26 99.26 0.8616\r\nTable 1. Action recognition results on DFAUST. Reporting\r\nframe-wise accuracy and mean average precision. Ours outper-\r\nforms all baselines by a large margin.\r\nFigure\r\n4.3DinAction GradCAM scores. The proposed 3Din-\r\nAction pipeline learns meaningful representations for prominent\r\nregions. The presented actions are jumping jacks (top row), hips\r\n(middle row), and knees (bottom row). The columns represent\r\nprogressing time steps from left to right. Colormap indicates high\r\nGradCAM scores in red and low scores in blue.\r\nKinect V2 camera and provides camera parameters to re-\r\nconstruct point clouds in camera coordinates. It provides\r\naction annotation for each frame (33 classes). It is a highly\r\nchallenging dataset for two main reasons: (1) It is highlyimbalanced since some actions have a long duration and\r\noccur multiple times in each video (e.g., spin leg) and some\r\nare shorter and sparser (ﬂip tabletop). (2) The assembly mo-\r\ntion includes a lot of self-occlusion as well as subtle move-\r\nments. The train/test split consists of 254 and 117 full se-\r\nquences respectively. The split is environment-based (i.e.\r\nin the test set there is no environment that appeared in the\r\ntraining set). The assembly videos have an average of \x18\r\n2735 frames per video. The point clouds provided in this\r\ndataset are aligned to the camera coordinate frame, posing\r\na challenge for methods that are sensitive to rotations since\r\nthe camera moves between different scans.\r\nResults. The results on the IKEA ASM dataset are reported\r\nin Table 2. The results show that the proposed 3DinAction\r\npipeline provides a signiﬁcant performance boost over static\r\napproaches and their temporally smooth variants. Addition-\r\nally, as expected, PointNet and Set Transformer are heavily\r\naffected by the variations in coordinate frames. PointNet++\r\non the other hand performs better since it uses local coor-\r\ndinate frames for each local region. All methods show an\r\nimproved mAP when using the temporally smooth variant\r\nwith degradation in frame-wise accuracy due to the dataset\r\nimbalance. For this dataset, the top1 metric is not always in-\r\ndicative of the quality of performance because a high top1 is\r\ndirectly correlated with many frames classiﬁed as the most\r\ncommon class. Additionally, we compare to pose-based\r\nmethods reported in [2] and show that the proposed ap-\r\nproach also outperforms these baselines. See supplemen-\r\ntary material for confusion matrices.\r\nt-patch intuition and visualization. In Figure 5we visual-\r\nize the t-patches for the ﬂip table action in the TV Bench as-\r\nsembly. A set of selected t-patches are highlighted in color\r\ndemonstrating different types of t-patches and their spatio-\r\ntemporal changes. The blue is on the moving TV Bench\r\nassembly, it moves rigidly with the assembly. The maroon\r\nis on the moving person’s arm, it exhibits nonrigid motion\r\nand deformations through time. The tealis on the static ta-\r\nble surface containing some of the TV Bench’s points in the\r\nﬁrst frame but remains static when it moves since its origin\r\nquery point is on the table. The green is on the static carpet,\r\nremaining approximately the same through time. Note that\r\nthe RGB images are for visualization purposes and are not\r\nused in our pipeline.\r\n4.3. Experiments on MSR-Action3D dataset\r\nFor this dataset, the task is to predict a single class for a\r\nsequence of frames (unlike the other datasets where a per-\r\nframe prediction is required). To that end, we replace our\r\nclassiﬁer with a single fully connected layer and max pooled\r\nthe results over the temporal domain (similar to [10]). The\r\nresults, reported in Table 3, show that all SoTA methods,\r\nincluding the proposed approach, exhibit very similar per-\r\nformance. This is mainly attributed to the small scale of the\r\n19983\r\nFigure 5. IKEA ASM example with t-patches. The ﬂip table action for the TV Bench assembly is visualization including the RGB image\r\n(top), and a grayscale 3D point cloud with t-patches (bottom). t-patches are highlighted in color. The blue is on the moving TV Bench\r\nassembly, maroon is on the moving persons arm, tealis on the static table surface, and green is on the colorful static carpet.\r\nMethodFrame acc.\r\ntop 1 top 3 macro mAP\r\nPointNet [23] 4.20 19.86 5.76 0.0346\r\nPointNet++[24] 45.97 70.10 29.48 0.1187\r\nSet Transformer [18] 14.96 57.12 13.16 0.0299\r\nPoinNet [23] + TS 6.00 19.48 5.14 0.0804\r\nPointNet++[24] +TS 27.84 60.64 27.72 0.2024\r\nSet Transformer [18] + TS 9.54 36.50 10.74 0.1471\r\nPSTNet [10] 17.94 52.24 17.14 0.2016\r\nHuman Pose HCN [2] 39.15 65.37 28.18 0.2232\r\nHuman Pose ST-GCN [2] 43.4 66.29 26.54 0.1856\r\nOurs without BD 45.16 72.83 35.06 0.2932\r\nOurs 52.91 75.03 38.84 0.2875\r\nTable 2. Action classiﬁcation on IKEA ASM. The proposed ap-\r\nproach provides a signiﬁcant performance boost over other static\r\nand dynamic approaches, including the temporal smoothing (TS).\r\ndataset and the lack of diversity in the action classes. Fur-\r\nthermore, we witnessed that the main performance gap is\r\nfor frames and sequences where the action is indistinguish-\r\nable (e.g., ﬁrst few frames of a sequence where no distin-\r\nguishable action commenced).\r\n4.4. Ablation study\r\nt-patch extraction. We studied the t-patch extraction\r\nmethod and its effects on action recognition on a noisy ver-\r\nsion of the DFAUST dataset. The results reported in Ta-\r\nble 4, show the signiﬁcance of the t-patch collapse problem\r\nand the effectiveness of adding small jitter and bidirectional\r\nt-patches to overcome it. In the DFAUST dataset, ﬁnd-\r\ning the nearest neighbor between frames provides a \x1896%\r\ncorrespondence accuracy (small motion between frames).# frames\r\nMethod 4 8 12 16 24\r\nPSTNet [10] 81.14 83.50 87.88 89.90 91.20\r\nP4Transformer [8] 80.13 83.17 87.54 89.56 90.94\r\nPST-Transformer [9] 81.14 83.97 88.15 91.98 93.73\r\nKinet [38] 79.80 83.84 88.53 91.92 93.27\r\nOurs 80.47 86.20 88.22 90.57 92.23\r\nTable 3. MSR-Action3D classiﬁcation results. Reporting classi-\r\nﬁcation accuracy for clips of different lengths. Results show that\r\nall methods are comparable since this dataset’s scale is limited.\r\nTherefore, in this experiment, we augment the dataset once\r\nby adding small Gaussian noise to each point in the dataset\r\n(\x1b= 0:01), decreasing the correspondence accuracy to\r\n\x1862:4% and introducing multiple t-patch collapse instances\r\nas well as increasing the classiﬁcation difﬁculty.\r\nSeveral variants of the t-patch extraction were explored.\r\nThe ﬁrst variation (GT) incorporates the ground truth cor-\r\nrespondence into the t-patch extraction. Using this method,\r\nthere is no t-patch collapse since there is a one-to-one map-\r\nping between frames. We expected this to produce an up-\r\nper bound on the performance, however, surprisingly the\r\nresults show that this variation is actually inferior to the\r\nproposed t-patch approach. We attribute this to the pro-\r\nposed t-patch extraction inherent augmentation caused by\r\nthe downsampling and nearest neighbor point jitter. We\r\nthen continue to explore the proposed approaches for deal-\r\ning with t-patch collapse which include jitter, i.e. adding\r\nsmall noise to each point before ﬁnding its nearest neighbor\r\nin the next frame, and the bidirectional t-patches that extract\r\npatches both from the ﬁrst to the last frame and from the last\r\nto the ﬁrst frame. The results show that adding jitter is al-\r\n19984\r\nFrame acc.\r\nData GT Jitter BD top 1 top 3 mAP\r\nclean3 7 7 77.67 95.38 0.8762\r\n7 7 7 74.73 92.14 0.8097\r\n7 3 7 80.49 96.61 0.9023\r\n7 3 3 87.26 99.26 0.8616\r\nnoisy3 7 7 76.08 95.50 0.9013\r\n7 7 7 66.74 93.76 0.7626\r\n7 3 7 81.83 98.97 0.9220\r\n7 3 3 80.03 97.57 0.8975\r\nTable 4. t-patch collapse ablation on DFAUST. Exploring adding\r\n(1) GT - ground truth correspondences, (2) jitter - small Gaussian\r\nnoise in t-patch construction, and (3) BD - bidirectional t-patches.\r\nFrame acc.\r\nn k top 1 top 3 mAP\r\n256 16 76.96 97.54 0.8430\r\n512 16 80.03 97.57 0.8975\r\n1024 16 77.30 97.88 0.8507\r\n512 8 76.87 96.21 0.7557\r\n512 32 77.91 96.60 0.7453\r\nTable 5. t-patch parameters ablation. Results for the number\r\nof neighboring points in a patch kand number of downsampled\r\npoints nshow that the method is robust.\r\nways beneﬁcial and provides a boost in performance. The\r\nbidirectional t-patches improve accuracy performance sig-\r\nniﬁcantly when the data is clean and are comparable when\r\nthe data is noisy. Note that in both dataset variations, the\r\ndegradation due to temporal t-patch collapse is low com-\r\npared to Kinect-based scan data, therefore the bidirectional\r\nbeneﬁts are not fully utilized.\r\nt-patch parameters. The core parameters for t-patch ex-\r\ntraction are the number of neighbors to extract (k ) and the\r\nnumber of points to subsample (n). Here there is a trade-\r\noff between complexity and performance i.e. whenkandn\r\nare small, the input to the model is small accordingly but\r\nthe overall coverage is reduced and therefore performance\r\nis lower. We explored their inﬂuence on the noisy DFAUST\r\ndataset and report the results in Table 5. The results show\r\nthat the method is fairly robust to the selection of these pa-\r\nrameters, producing comparable results for all. The best\r\nperformance was obtained for n= 512;k = 16. Surpris-\r\ningly, the performance slightly degrades when increasing k\r\nandnbeyond these values. This is likely due to the increase\r\nin model size, which easily overﬁts on a dataset of this size.\r\nTime and parameters. We report the time performance\r\nand the number of parameters of several baselines in Ta-\r\nble 6. The results show the tradeoff between performance\r\nand time, i.e. the temporal approaches exhibit longer pro-Method Time [ms] # parameters\r\nPointNet [23] 64.49 3.5M\r\nPointNet++[24] 23.35 1.5M\r\nPSTNet [10] 185.92 8.3M\r\nOurs t-patch extraction 180.65 0\r\nOurs feature computation 12.50 9.8M\r\nOurs classiﬁer 0.36 1.1M\r\nOurs 193.51 10.9M\r\nTable 6. Time and parameters. Temporal methods have more\r\nparameters and take longer. 3DinAction time is mostly used to\r\nextract t-patches.\r\ncessing times and more parameters while performing better.\r\nFor the proposed approach, we break down the timing of in-\r\ndividual components, namely the t-patch extraction, feature\r\ncomputation, and classiﬁer. The results show that the pro-\r\nposed approach is comparable to PSTNet in time while hav-\r\ning more parameters. Interestingly, most of the time is used\r\nfor extracting the t-patches and not for feature extraction or\r\nclassiﬁcation. This is attributed to the farthest point sam-\r\npling and the sequential knnsearch, both of which could be\r\nfurther optimized for speed. Note that results are average of\r\n50 runs, each with a batch of 4 and 1024 points per frame.\r\nLimitations. Since the simpliﬁed formulation of t-patch\r\nconstruction uses knn, it is sensitive to variations in point\r\ndensities. A t-patch in a sparse region will occupy a larger\r\nvolume than a t-patch in a dense region. We use FPS to mit-\r\nigate this, however, other approaches can be used e.g., using\r\nneighbors in a ﬁxed radius. Another limitation is data with\r\na very low frame rate or very fast motion since this breaks\r\nthe assumption that points in consecutive frames are close\r\nto each other, and will cause inconsistent t-patch motion.\r\n5. Conclusions\r\nWe introduced the 3DinAction pipeline, a novel method for\r\n3D point cloud action recognition. It showed that the cre-\r\nation of temporal patches is beneﬁcial for ﬁnding informa-\r\ntive spatio-temporal point representations. 3DinAction has\r\ndemonstrated a performance boost over SoTA methods.\r\nThis work opens many interesting future directions of\r\nresearch. These include trying to learn the t-patch construc-\r\ntion instead of the knnselection, imposing stronger tem-\r\nporal structure based on preexisting knowledge and bias\r\n(e.g., sceneﬂow or tracking), and exploring using multi-\r\nmodal inputs with this representation (e.g., RGB or text).\r\nAcknowledgement. This project has received funding from the\r\nEuropean Union’s Horizon 2020 research and innovation pro-\r\ngramme under the Marie Sklodowska-Curie grant agreement No\r\n893465. We also thank the Microsoft for Azure Credits and\r\nNVIDIA Academic Hardware Grant Program for providing high-\r\nspeed A5000 GPU.\r\n19985\r\nReferences\r\n[1] Yizhak Ben-Shabat, Michael Lindenbaum, and Anath\r\nFischer. 3DMFV: Three-dimensional point cloud clas-\r\nsiﬁcation in real-time using convolutional neural net-\r\nworks. RAL, 3:3145–3152, 2018. 2,6\r\n[2] Yizhak Ben-Shabat, Xin Yu, Fatemeh Saleh, Dylan\r\nCampbell, Cristian Rodriguez-Opazo, Hongdong Li,\r\nand Stephen Gould. The ikea asm dataset: Under-\r\nstanding people assembling furniture through actions,\r\nobjects and pose. In Proceedings of the IEEE/CVF\r\nWinter Conference on Applications of Computer Vi-\r\nsion, pages 847–859, 2021. 3,5,6,7\r\n[3] Federica Bogo, Javier Romero, Gerard Pons-Moll,\r\nand Michael J. Black. Dynamic FAUST: Registering\r\nhuman bodies in motion. In IEEE Conf. on Computer\r\nVision and Pattern Recognition (CVPR), July 2017. 3,\r\n5\r\n[4] Fabian Caba Heilbron, Victor Escorcia, Bernard\r\nGhanem, and Juan Carlos Niebles. Activitynet: A\r\nlarge-scale video benchmark for human activity un-\r\nderstanding. In Proceedings of the ieee conference on\r\ncomputer vision and pattern recognition, pages 961–\r\n970, 2015. 5\r\n[5] Joao Carreira and Andrew Zisserman. Quo vadis,\r\naction recognition? a new model and the kinet-\r\nics dataset. In proceedings of the IEEE Conference\r\non Computer Vision and Pattern Recognition , pages\r\n6299–6308, 2017. 1,5\r\n[6] Christopher Choy, JunYoung Gwak, and Silvio\r\nSavarese. 4d spatio-temporal convnets: Minkowski\r\nconvolutional neural networks. In Proceedings of the\r\nIEEE/CVF conference on computer vision and pattern\r\nrecognition, pages 3075–3084, 2019. 2\r\n[7] Haodong Duan, Yue Zhao, Kai Chen, Dahua Lin, and\r\nBo Dai. Revisiting skeleton-based action recognition.\r\nInProceedings of the IEEE/CVF Conference on Com-\r\nputer Vision and Pattern Recognition, pages 2969–\r\n2978, 2022. 1,3\r\n[8] Hehe Fan, Yi Yang, and Mohan Kankanhalli. Point 4d\r\ntransformer networks for spatio-temporal modeling in\r\npoint cloud videos. In Proceedings of the IEEE/CVF\r\nconference on computer vision and pattern recogni-\r\ntion, pages 14204–14213, 2021. 2,7\r\n[9] Hehe Fan, Yi Yang, and Mohan Kankanhalli. Point\r\nspatio-temporal transformer networks for point cloud\r\nvideo modeling. IEEE Transactions on Pattern Analy-\r\nsis and Machine Intelligence, 45(2):2181–2192, 2022.\r\n2,7\r\n[10] Hehe Fan, Xin Yu, Yuhang Ding, Yi Yang, and Mo-\r\nhan Kankanhalli. Pstnet: Point spatio-temporal con-\r\nvolution on point cloud sequences. In International\r\nConference on Learning Representations, 2021. 2,5,\r\n6,7,8[11] Hehe Fan, Xin Yu, Yi Yang, and Mohan Kankanhalli.\r\nDeep hierarchical representation of point cloud videos\r\nvia spatio-temporal decomposition. IEEE Transac-\r\ntions on Pattern Analysis and Machine Intelligence,\r\n44(12):9918–9930, 2021. 2\r\n[12] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik,\r\nand Kaiming He. Slowfast networks for video recog-\r\nnition. In Proceedings of the IEEE/CVF interna-\r\ntional conference on computer vision, pages 6202–\r\n6211, 2019. 1\r\n[13] Ankit Goyal, Hei Law, Bowei Liu, Alejandro Newell,\r\nand Jia Deng. Revisiting point cloud shape classiﬁca-\r\ntion with a simple and effective baseline. In Interna-\r\ntional Conference on Machine Learning, pages 3809–\r\n3820. PMLR, 2021. 2\r\n[14] Yulan Guo, Hanyun Wang, Qingyong Hu, Hao Liu,\r\nLi Liu, and Mohammed Bennamoun. Deep learning\r\nfor 3d point clouds: A survey. PAMI, 2020. 2\r\n[15] Roman Klokov and Victor Lempitsky. Escape from\r\ncells: Deep kd-networks for the recognition of 3d\r\npoint cloud models. In Proceedings of the IEEE inter-\r\nnational conference on computer vision, pages 863–\r\n872, 2017. 2\r\n[16] Hema Swetha Koppula, Rudhir Gupta, and Ashutosh\r\nSaxena. Learning human activities and object affor-\r\ndances from rgb-d videos. The International Journal\r\nof Robotics Research, 32(8):951–970, 2013. 2\r\n[17] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam\r\nKosiorek, Seungjin Choi, and Yee Whye Teh.\r\nSet transformer: A framework for attention-based\r\npermutation-invariant neural networks. In Interna-\r\ntional conference on machine learning, pages 3744–\r\n3753. PMLR, 2019. 2\r\n[18] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam\r\nKosiorek, Seungjin Choi, and Yee Whye Teh.\r\nSet transformer: A framework for attention-based\r\npermutation-invariant neural networks. In Interna-\r\ntional conference on machine learning, pages 3744–\r\n3753. PMLR, 2019. 5,6,7\r\n[19] Wanqing Li, Zhengyou Zhang, and Zicheng Liu. Ac-\r\ntion recognition based on a bag of 3d points. In 2010\r\nIEEE computer society conference on computer vi-\r\nsion and pattern recognition-workshops , pages 9–14.\r\nIEEE, 2010. 3,5\r\n[20] Jun Liu, Amir Shahroudy, Mauricio Perez, Gang\r\nWang, Ling-Yu Duan, and Alex C Kot. Ntu rgb+ d\r\n120: A large-scale benchmark for 3d human activity\r\nunderstanding. IEEE transactions on pattern analysis\r\nand machine intelligence, 42(10):2684–2701, 2019. 3\r\n[21] Xingyu Liu, Mengyuan Yan, and Jeannette Bohg. Me-\r\nteornet: Deep learning on dynamic 3d point cloud se-\r\nquences. In Proceedings of the IEEE/CVF Interna-\r\n19986\r\ntional Conference on Computer Vision, pages 9246–\r\n9255, 2019. 2\r\n[22] Daniel Maturana and Sebastian Scherer. V oxnet: A\r\n3d convolutional neural network for real-time object\r\nrecognition. In 2015 IEEE/RSJ international confer-\r\nence on intelligent robots and systems (IROS), pages\r\n922–928. IEEE, 2015. 2\r\n[23] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J\r\nGuibas. Pointnet: Deep learning on point sets for\r\n3d classiﬁcation and segmentation. In IEEE Conf.\r\non Computer Vision and Pattern Recognition (CVPR),\r\npages 652–660, 2017. 2,5,6,7,8\r\n[24] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J\r\nGuibas. Pointnet++: Deep hierarchical feature learn-\r\ning on point sets in a metric space. In NeurIPS, vol-\r\nume 30, 2017. 2,5,6,7,8\r\n[25] Guocheng Qian, Yuchen Li, Houwen Peng, Jinjie Mai,\r\nHasan Abed Al Kader Hammoud, Mohamed Elho-\r\nseiny, and Bernard Ghanem. Pointnext: Revisiting\r\npointnet++ with improved training and scaling strate-\r\ngies. arXiv preprint arXiv:2206.04670, 2022. 2\r\n[26] Zhaofan Qiu, Ting Yao, and Tao Mei. Learning spatio-\r\ntemporal representation with pseudo-3d residual net-\r\nworks. In The IEEE International Conference on\r\nComputer Vision (ICCV), Oct 2017. 1\r\n[27] Ramprasaath R Selvaraju, Michael Cogswell, Ab-\r\nhishek Das, Ramakrishna Vedantam, Devi Parikh, and\r\nDhruv Batra. Grad-cam: Visual explanations from\r\ndeep networks via gradient-based localization. In\r\nProceedings of the IEEE international conference on\r\ncomputer vision, pages 618–626, 2017. 5\r\n[28] Amir Shahroudy, Jun Liu, Tian-Tsong Ng, and Gang\r\nWang. Ntu rgb+ d: A large scale dataset for 3d human\r\nactivity analysis. In Proceedings of the IEEE con-\r\nference on computer vision and pattern recognition,\r\npages 1010–1019, 2016. 3\r\n[29] Yiru Shen, Chen Feng, Yaoqing Yang, and Dong Tian.\r\nMining point cloud local structures by kernel correla-\r\ntion and graph pooling. In IEEE Conf. on Computer\r\nVision and Pattern Recognition (CVPR), pages 4548–\r\n4557, 2018. 2\r\n[30] Jaeyong Sung, Colin Ponce, Bart Selman, and\r\nAshutosh Saxena. Unstructured human activity detec-\r\ntion from rgbd images. In 2012 IEEE international\r\nconference on robotics and automation, pages 842–\r\n849. IEEE, 2012. 2\r\n[31] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Tor-\r\nresani, and Manohar Paluri. Learning spatiotemporal\r\nfeatures with 3d convolutional networks. In Proceed-\r\nings of the IEEE international conference on computer\r\nvision, pages 4489–4497, 2015. 1\r\n[32] Haiyan Wang, Liang Yang, Xuejian Rong, Jinglun\r\nFeng, and Yingli Tian. Self-supervised 4d spatio-temporal feature learning via order prediction of se-\r\nquential point cloud clips. In Proceedings of the\r\nIEEE/CVF Winter Conference on Applications of\r\nComputer Vision, pages 3762–3771, 2021. 2\r\n[33] Yancheng Wang, Yang Xiao, Fu Xiong, Wenxiang\r\nJiang, Zhiguo Cao, Joey Tianyi Zhou, and Junsong\r\nYuan. 3dv: 3d dynamic voxel for action recognition\r\nin depth video. In Proceedings of the IEEE/CVF con-\r\nference on computer vision and pattern recognition,\r\npages 511–520, 2020. 2\r\n[34] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma,\r\nMichael M Bronstein, and Justin M Solomon. Dy-\r\nnamic graph cnn for learning on point clouds. Acm\r\nTransactions On Graphics (tog), 38:1–12, 2019. 2\r\n[35] Siyuan Yang, Jun Liu, Shijian Lu, Meng Hwa Er, and\r\nAlex C Kot. Skeleton cloud colorization for unsu-\r\npervised 3d action representation learning. In Pro-\r\nceedings of the IEEE/CVF International Conference\r\non Computer Vision, pages 13423–13433, 2021. 2\r\n[36] Cheng Zhang, Haocheng Wan, Shengqiang Liu,\r\nXinyi Shen, and Zizhao Wu. Pvt: Point-voxel\r\ntransformer for 3d deep learning. arXiv preprint\r\narXiv:2108.06076, 2021. 2\r\n[37] Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip H.S.\r\nTorr, and Vladlen Koltun. Point transformer. In Pro-\r\nceedings of the IEEE/CVF International Conference\r\non Computer Vision (ICCV), pages 16259–16268, Oc-\r\ntober 2021. 2\r\n[38] Jia-Xing Zhong, Kaichen Zhou, Qingyong Hu, Bing\r\nWang, Niki Trigoni, and Andrew Markham. No pain,\r\nbig gain: classify dynamic point cloud sequences with\r\nstatic models by ﬁtting feature-level space-time sur-\r\nfaces. In Proceedings of the IEEE/CVF Conference\r\non Computer Vision and Pattern Recognition , pages\r\n8510–8520, 2022. 2,7\r\n19987'}, 'dist': 0.9286905527114868}
Result 9: {'text': 'data', 'metadata': {'created_at': 1736932570, 'modified_at': 1736932570, 'owner': 'sayandeep', 'path': 'RAG_CONTENT/conferences/cvpr_ocr/Banani_Probing_the_3D_Awareness_of_Visual_Foundation_Models_CVPR_2024_paper.txt', 'size': 57275, 'seen_at': 1737191136, 'data': 'Probing the 3D Awareness of Visual Foundation Models\nMohamed El Banani1Amit Raj2Kevis-Kokitsi Maninis2Abhishek Kar2Yuanzhen Li2\nMichael Rubinstein2Deqing Sun2Leonidas Guibas2Justin Johnson1Varun Jampani2⇤\n1University of Michigan2Google Research\nAbstract\nRecent advances in large-scale pretraining have yielded\nvisual foundation models with strong capabilities. Not only\ncan recent models generalize to arbitrary images for their\ntraining task, their intermediate representations are useful\nfor other visual tasks such as detection and segmentation.\nGiven that such models can classify, delineate, and local-\nize objects in 2D, we ask whether they also represent their\n3D structure? In this work, we analyze the 3D awareness\nof visual foundation models. We posit that 3D awareness\nimplies that representations (1) encode the 3D structure of\nthe scene and (2) consistently represent the surface across\nviews. We conduct a series of experiments using task-speciﬁc\nprobes and zero-shot inference procedures on frozen fea-\ntures. Our experiments reveal several limitations of the\ncurrent models. Our code and analysis can be found at\nhttps://github.com/mbanani/probe3d .\n1. Introduction\nLarge-scale pretraining on image datasets has yielded visual\nfoundation models with impressive generalization capabili-\nties. Such models can classify [ 46,65], segment [ 36], and\ngenerate [ 10,69,70] arbitrary images. Furthermore, the\ndense representations learned by such models extend beyond\ntheir training tasks and exhibit strong zero-shot capabilities\nin other tasks such as segmentation [ 56,95] and part discov-\nery [ 1,27]. This suggests that models are learning strong\nimage representations, but how well do they represent the\n3D world that images depict?\nRecent work suggests that visual foundation models are\nuseful for some 3D tasks despite being trained with 2D\ndata. For instance, models implicitly represent depth and\nsurface normals when generating images of scenes and\nfaces [ 6,12]. Furthermore, the intermediate representations\nof self-supervised and generative models can be used to esti-\nmate semantic correspondence [ 1,27,30,83,99] and object\npose [ 25]. However, when reconstructing 3D objects, they\ngenerate artifacts that suggest a lack of 3D consistency [ 50];\n* Current afﬁliation is Stability AI.\nEvaluation Tasks\nSingle-Image 3D Multiview ConsistencySingle-Image 3D Multiview ConsistencyObjectsScenes\nEvaluated ModelsMAE iBOTDeiT IIICLIPMiDaS \nDINO DINOv2StableDiffusionSigLIPSAM \nFigure 1. Are current visual foundation models 3D aware? We\nprobe the 3D awareness of the learned representations by evaluating\ntheir ability to encode the 3D structure of the visible surface and\ntheir consistency across views.\ne.g., animals with multiple faces. Therefore, it remains un-\nclear how those modes represent or understand the 3D world.\nThe goal of this paper is to study the 3D awareness of\nvisual foundation models. Previous benchmarks evaluate\nvisual models on semantic tasks [ 24,26,87], but their 3D\nunderstanding remains understudied. Representations can\nvary from having no 3D awareness ( e.g., class label or bag\nof words) to accurately representing the 3D geometry of\nthe scene ( e.g., 3D surface map or mesh). We posit that\n3D awareness can be evaluated through two distinct capa-\nbilities: single-view surface reconstruction andmultiview\nconsistency . If a model is 3D aware, we expect that its repre-\nsentations would encode the geometry of the surfaces visible\nin the image; i.e., how far is each point in the scene? what\nis the orientation of the surface? Moreover, the representa-\ntions should be consistent for different views of the scene;\nallowing them to establish accurate correspondence.\n1\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n21795\nTo this end, we conduct an empirical analysis of the 3D\nawareness of visual foundation models. Our analysis con-\nsiders a range of large-scale pretrained models that have\nexhibited strong generalization, regardless of their pretrain-\ning objective. We evaluate the models on their ability to\nestimate 3D quantities that match the aforementioned ca-\npabilities: depth, surface normals, and 3D correspondence.\nFurthermore, we evaluate those capabilities at both the scene\nlevel [ 13,78] and for individual objects [ 32,57] to provide\nfurther differentiation. We show the models and tasks con-\nsidered in Fig. 1. Since we are interested in what the models\nrepresent, we probe the frozen representations through task-\nspeciﬁc probes or zero-shot inference methods. This allows\nus to evaluate the models’ representations, rather than the\ntransferability of their pretrained weights.\nOur experiments reveal a large variation in the 3D aware-\nness of the models. We present the aggregated ratings (higher\nis better) of different models in single-image and multiview\ntasks in Fig. 1. We ﬁnd that recent self-supervised models\nsuch as DINOv2 [ 60] learn representations that encode depth\nand surface normals, with StableDiffusion [ 69] being a close\nsecond. Meanwhile, the training in vision language for mod-\nels such as CLIP [ 65] exhibits very poor performance despite\nits impressive semantic generalization capabilities. At their\nbest, some of the probes achieve a performance close to that\nof state-of-the-art models despite being pretrained with a\nvery different objective. Meanwhile, we ﬁnd that the models\nstruggle with multiview consistency. Although most models\ncan accurately match objects and scenes with small view-\npoint changes, they perform very poorly at large viewpoint\nvariations. Our analysis further suggests that consistency\nacross images is semantic in nature; i.e., models accurately\nmatch semantic parts but struggle to incorporate the global\nobject pose. We hope that our ﬁndings will spark more in-\nterest in better understanding the 3D awareness of vision\nfoundation models and contribute to more comprehensive\nbenchmarks of visual representation learning approaches.\n2. 3D Aware Visual Representations\nWe ﬁrst discuss what we mean by 3D aware visual repre-\nsentations. When we view a scene, we seem to effortlessly\nunderstand its 3D structure despite only seeing its 2D pro-\njection. Research in developmental psychology and psy-\nchophysics suggests that our perception encodes surface\nproperties such as depth and orientation [ 39,79]. Research\non mental imagery has suggested that our internal represen-\ntations of objects encode their 3D shape and are subject to\n3D constraints [ 76]. Inspired by this work, we posit that\n3D aware representations encode basic 3D properties of the\nsurface as distances and orintations. Beyond a single image,\n3D aware representations are consistent across views of the\nsame object or scene, as they are projections of the same\nunderlying 3D geometry.Representations in computer vision have varied a lot in\nhow well they represented the 3D shapes of objects. Early\nrepresentations such as 2.5D sketches [ 55] and generalized\ncylinders [ 7,8] explicitly depicted the 3D geometry of the\nobejcts and their spatial relationships. Recent advances have\ndeviated from explicit modeling and instead rely on the rep-\nresentation of visual information as dense feature grids [ 28]\nor sets of tokens [ 15]. While 3D awareness of early repre-\nsentations was obvious, it remains unclear what the learned\nrepresentations encode or how 3D aware they are. Popular\ninterpretability mechanisms such as GradCAM [ 74] are not\nhelpful here, as they tell us which components of the im-\nage led to a speciﬁc inference, not what information was\nrepresented by the network.\nWe propose evaluating the 3D awareness of visual models\nby probing them on two capabilities: single-view 3D and\nmultiview consistency. We take inspiration from the work on\nhuman perception [ 38,75,79] and evaluate models on how\nwell they encode basic 3D properties and how 3D consistent\nthey are. For a single image, we expect a 3D aware model\nto accurately represent the visible surface and encode prop-\nerties such as depth and surface orientation. When given\nmultiple images of the same object or scene, we expect a 3D\naware representation to capture the relationships between\nthe images and provide accurate correspondence. Although\nthese two capabilities are not exhaustive, they capture two\nfundamental aspects of 3D understanding. Furthermore, they\ncan be directly mapped to three well-studied problems in\ncomputer vision, namely, estimating monocular depth, sur-\nface normals, and correspondence.\n3. Experimental Setup\nThe goal of our experiments is to evaluate the 3D awareness\nof visual foundation models: i.e., large-scale pretrained mod-\nels that are proposed as general backbones for a wide variety\nof downstream tasks or applications. Speciﬁcally, we hope\nto answer the following questions:\n1.Do models learn to represent the visible surface?\n2.Are the representations consistent across views?\n3.How does the training objective impact 3D awareness?\nModels. We primarily focus our experiments on vision trans-\nformers that were proposed as general purpose backbones or\nthat exhibit strong generalization performance across tasks\nor domains. Moreover, we are interested in evaluating mod-\nels that were trained with different supervisory signals. First,\nwe consider three forms of supervision that commonly serve\nas pretraining tasks: classiﬁcation [ 86], language supervi-\nsion [ 31,65], and self-supervision [ 9,29,60,102]. Recent\nwork has also shown that text-conditioned image genera-\ntion [ 69] can learn strong representations and provide strong\nbackbones for other vision tasks [ 45,95,101]. We also\nconsider two forms of dense supervision that have recently\n2\n21796\nTable 1. Evaluated Visual Models. We consider a range of visual\nmodels spanning several forms of supervision. We evaluate publicly\navailable checkpoints and choose checkpoints of comparable model\nand training size whenever possible.\nModel Architecture Supervision Dataset\nDeIT III [ 86] ViT-B/16 Classiﬁcation ImageNet-22k\nMAE [ 29] ViT-B/16 SSL ImageNet-1k\niBOT [ 102] ViT-B/16 SSL ImageNet-1k\nDINO [ 9] ViT-B/16 SSL ImageNet-1k\nDINO v2 [ 60] ViT-B/14 SSL LVD-142M\nCLIP [ 65] ViT-B/16 VLM WIT-400M\nSigLIP [ 97] ViT-B/16 VLM WebLI\nStableDiffusion [ 69] UNet Generation LAION\nSAM [ 36] ViT-B/16 Segmentation SA-1B\nMiDaS [ 67] ViT-L/16 Depth MIX-6\nbeen scaled up: depth estimation [ 67,68] and class-agnostic\nsegmentation [ 36]. While there models have not been used\nas general purpose backbones yet, they exhibit impressive\ngeneralize to a wide range of domains and provide an inter-\nesting point of comparison. We present an overview of the\nmodels considered in Tab. 1, and more details can be found\nin App. A.1.\nOne challenge is how to fairly compare models that have\ndifferent data and compute requirements. This challenge is\nfurther ampliﬁed by considering the scale used to achieve the\nstrong performance displayed by such models. Furthermore,\nthe data used to train many of these models is private [ 60,65]\nand even replicating the data collection and curation process\nrequires extensive resources as shown by Xu et al. [94]. Be-\nyond data scale and curation, models have different data\nrequirements that range from class labels [ 86], captions [ 65],\nmasks [ 36], or even simple curation [ 60]. As a result, it is\nunclear which dataset would provide a fair comparison. We\nmake a pragmatic choice of relying on publicly available\ncheckpoints and selecting checkpoints of comparable archi-\ntectures and training scale to provide some fair comparison.\nWe provide additional comparisons in App. Band discuss\nthe impact of this on our results in App. C.\nAnother important question is how to evaluate those prop-\nerties. One common approach is transfer learning, where\nthe pretrained model is ﬁne-tuned using task-speciﬁc super-\nvision. This is often a good practical choice, as it results\nin strong downstream performance. However, it is not suit-\nable for our analysis, as good ﬁne-tuning performance may\nindicate two different things: the model has good 3D aware-\nness or the model weights are a good initialization for other\ntasks [ 26]. Furthermore, ﬁne-tuning specializes the mod-\nels by sacriﬁcing its generality [ 42]. Instead, we probe the\nfrozen features with trainable probes and zero-shot inference\nmethods that do not change model weights or signiﬁcantly\nalter model capacity. This allows us to evaluate pretrained\nrepresentations of models with the assumption that the same\nmodel may be used for a wide range of tasks.3.1. Single Image Surface Reconstruction\nIn this section, we analyze how well the models represent the\nvisible surface in the image. We consider two tasks for single-\nview 3D understanding: depth estimation and surface normal\nestimation. Those tasks are well established in computer\nvision and are commonly studied in human perception and\ndevelopment [ 79]. Although depth and surface normals\nare closely related quantities, they are different prediction\ntasks as they rely on different visual cues, as discussed by\nKoenderink and Van Doorn [39]and Fouhey [22]. We brieﬂy\noutline our evaluation setup below, and refer the reader to\nApp. Aand our code release for more speciﬁc details.\nMonocular depth estimation is the task of predicting the\ndepth for each pixel in the image. Although early work\nframed the task as regression [ 17], recent work has shown\nthat the use of binned prediction results in better perfor-\nmance Bhat et al. [4]. We follow the AdaBins [ 4] formu-\nlation and train dense probes using their proposed losses.\nWe report the root-mean-squared prediction error for depth\nestimation as well as recall at different threshold rations,\nsimilar to Eigen et al. [ 17].\nWe ﬁnd that estimating the depth for object-centric\ndatasets is particularly challenged by scale ambiguity. While\nscale ambiguity affects both objects and scene, we ﬁnd that\nmodels trained to estimate metric depth on objects end up fo-\ncus on predicting the object’s mean depth without capturing\nany details. As a result, we use a scale-invariant formulation\nfor objects by normalizing their depth between 0 and 1.\nSurface normal estimation is the task of predicting the\norientation of the surface at every pixel. We adopt the setup\nof Bae et al. [2], which utilizes an uncertainty-aware angular\nloss. Similarly to Fouhey et al. [23], we report the root-mean\nsquare angular prediction error as well as the precentage\nrecall at different angular thresholds.\nProbe. We use a dense multiscale probe similar to the\nDPT decoder [ 68]. This deviates from the common choice\nof linear probing commonly used in self-supervised model\nbenchmarking [ 41]. Linear probing is useful for semantic\ntasks since linear separability of classes is a desired and\nexpected property. However, it is unclear why we should\nrequire the encoding of 3D properties to be linear. Further-\nmore, the model may represent such properties at different,\nor multiple, locations within the network. Hence, instead of\ntraining a linear probe on a speciﬁc linear, we use a multi-\nscale dense probe to map the features from multiple layers\nto either depth or surface normals.\nOptimization. We train the probes for 10 epochs using the\nAdamW [ 35,52] optimizer with a linear warm and cosine\ndecay learning rate scheduler. While longer training fur-\nther improves performance, trends stabilize after 5 training\nepochs due to the relatively small capacity of the probe.\n3\n21797\nFigure 2. Depth Estimation Results. While pretrained representations exhibit large variation in their ability to represent depth, their\nperformance is consistent on objects and scenes. CLIP and MAE features do not encode depth and appear to instead capture rough priors\nsuch as ”ﬂoor pixels are close”. Most models appear to capture the rough structure of the scene and vary in the degree to which they capture\ndetails. DINOv2 performs best and accurately captures ﬁne details; e.g., cow’s ear, desk chair, and coffee table.\nDatasets. We evaluate the performance on both scenes\nand objects. We use the NYUv2 dataset [ 78] to evaluate\nscene-level performance as it is a common benchmark for\nindoor scene understanding. We evaluate object-level per-\nformance using the NA VI dataset [ 32] which includes a set\nof object instances in a wide range of scenes and orienta-\ntions. Both datasets provide aligned depth maps. For surface\nnormals, we use the annotations generated by Ladick `y et al.\n[43] and generate the surface normal annotations for NA VI.\nResults. We evaluate all models and report the performance\nin App. Bdue to space limitations. We focus here on qual-\nitative results and performance trends, and analyze them\nthrough a series of questions:\nDo models learn to represent depth? We observe that\nthe ability of the models to encode depth is highly variable.\nThis can be clearly seen in Fig. 2where DINOv2 and Sta-\nbleDiffusion predict accurate and detailed depth maps that\ncapture the cow’s ear and chair legs, while CLIP and MAE\ngenerate blurry and inaccurate estimates. It is worth noting\nthat the models compared are all highly performant models\nthat are often used within as backbones for downstreams\ntaks. The disparity seen highlights the importance of consid-\nering a wider range of tasks for benchmarking such models,\nas well as the utility of 3D awareness as a domain for such\nbenchmarking.\nDo models learn to represent surface normals? Surface\nnormal probe results reveal similar trends to depth estimation,\nwith some models achieving very high performance, while\nothers struggle to capture any information beyond the coarse\npriors such as “ﬂoor pixels point up.” The reliance on priorsbecomes more clear when comparing predictions for objects\nand scenes since objects have fewer priors due to the large\npose variation. This is useful when analyzing the qualitative\nresults of CLIP, which may appear blurry but correct for\nscenes, but are clearly inaccurate for objects. However, the\nbest-performing model, DINOv2, achieves an impressive\nperformance that is competitive with state-of-the-art models.\nHow is performance correlated across both tasks? We\nobserve that the performance of models is strongly corre-\nlated across domains and tasks as shown in Fig. 4. This\nsupports our experimental design choices as it suggests that\nwe are measuring a single capability using different meth-\nods. Furthermore, the consistent performance across indoor\nscenes and objects suggests that such models are learning to\nrepresent some information about the visible surface without\nany task-speciﬁc supervision. Although recent work has\nfocused on the ability of generative models to learn this in-\nformation [ 6,12], we ﬁnd that it is not unique to such models\ntrained with classiﬁcation or discriminative self-supervision\nachieving comparable performance.\nWe note that, while depth and surface normal perfor-\nmance are well correlated at the model level, the correlation\nis far weaker when considering performance at the image or\npixel level. We ﬁnd that model performance is not consistent\nat the image or patch level; e.g., we ﬁnd that the correlation\nbetween errors made by DINOv2 on NYU is 0.37 when\naggregating at the image level, and 0.13 when considering\npixel-level errors. Hence, while the underlying ability to\nrepresent the surface is shared, surface normals and depth\nestimation rely on different visual cues[ 22,40,59] resulting\nin model errors being weakly correlated.\n4\n21798\nFigure 3. Surface Normal Qualitative Examples. With the exception of CLIP, models can capture the rough orientation of object and\nscene surfaces; e.g., ﬂoors, walls, ceilings. The main distinction seems to be in how well they capture ﬁner details. Similarly to depth results,\nwe ﬁnd that DINOv2 and StableDiffusion perform best and can capture ﬁne details such as the edges of the toy car and the white seat.\nSurprisingly, we ﬁnd that SAM’s predictions are not as detailed despite its ability to predict accurate segmentation boundaries.\nFigure 4. Single view performance correlation. Depth and\nsurface normal performance is highly correlated across domains.\nWhat is the impact of the training objective? We ob-\nserve that discriminative self-supervised models perform\nbest across both tasks and domains. This is surprising\nsince it is unclear why the self-distillation and instance de-\nscrimination losses used to train such models would encour-\nage this behavior. Consistent with other work [ 6,12], we\nﬁnd that StableDiffusion also captures surface properties\nwell. Interestingly, models trained with dense supervision,\neven depth supervision, perform worse than self-supervised\nand text-conditioned generation, and perform on par with\nclassiﬁcation-trained models. Finally, language-supervised\nmodels appear to perform poorly despite their common util-\nity as backbones for a variety of tasks. This could be related\nto previous ﬁndings that vision language models struggle\nwith spatial relations and compositionality [ 44,48,81].\nOverall, our experiments suggest that most visual models\nsuggest that most visual foundation models end up learning\nrepresentations that encode properties of the visual surface\ndespite being trained with just image data.3.2. Multiview Consistency\nWe previously evaluated the models’ ability to represent the\nvisible surfaces. Although this is important for 3D under-\nstanding, the evaluation is limited to a single image. As\ndiscussed previously, 3D awareness also implies consistency\nof representations across multiple views. We evaluate this\nusing correspondence estimation, where the goal is to iden-\ntify image patches across views that depict the same 3D\npoint. This capability is important because it would allow\nthe model to correctly aggregate information across views,\nwhich is central to reconstruction and localization pipelines.\nGeometric correspondence estimation. Given two views\nof the same object or scene, identify pixels across views\nthat depict the same point in 3D space. Rather than training\na probe, we directly compute correspondence between the\ndense feature maps extracted from each image as this allows\nus to directly evaluate the consistency of the representations.\nThis inference procedure is derived from keypoint-free cor-\nrespondence estimation pipelines [ 18,19,82] and is similar\nto recent approaches to assess feature quality [ 1,83,99]\nDatasets. We consider both scenes and objects. For scenes,\nwe evaluate our model on the Paired ScanNet [ 13] split\nproposed by Sarlin et al. [72]. For objects, we sample view\npairs from the NA VI wild set which depict the same object in-\nstances in different environments. We sample views that have\na maximum rotation of 120 degrees to ensure that there exists\na mutually visible surface. We also evaluate performance\non the SPair dataset [ 57] which provides keypoint-labeled\nimages allowing us to analyze the models’ performance on a\nclosely related task: semantic correspondence estimation.\n5\n21799\nImage PairSAMDeiT IIIStableDiffusionDINOv2\nFigure 5. Correspondence Estimation Qualitative Results. We observe that models can estimate accurate correspondence for small\nviewpoint changes, but struggle with large viewpoint changes. This is true even if the change is an in-plane rotation as shown with the eagle.\nThis pattern is consistent for both objects and scenes, although performance is not well correlated: SAM and StableDiffusion perform better\nfor scenes, while DeiT and DINOv2 are more consistent for objects. Correspondence color-coded for accuracy.\nEvaluation. We report the correspondence recall; i.e., the\npercentage of correspondence that falls within some deﬁned\ndistance. Correspondence error is often computed in pixels\nto account for the large variation in depth; e.g., a prediction\noff by 1 pixel can be a few millimeters on a near-by surface or\nseveral meters for outdoor scenes. This choice is less suitable\nfor objects, since they do not have the same large variation\ndepth. Object can also suffer from self-occlusion and re-\npeated parts, which makes a pixel-wise threshold potentially\nerrenous. Therefore, we use a metric threshold for objects.\nSince layer selection can greatly affect performance [ 87],\nwe evaluated model performance at four different intermedi-\nate points. Finally, we ﬁnd that model performance varies\ngreatly depending on the viewpoint differnce between the\nview pairs, as we discuss next. As a result, we bin the per-\nformance depending on the magnitude of the transformation\nbetween the view pairs. For more details on the evaluation\nsetup, we refer the reader to App. A.\nWe evaluate all models on the three datasets and report\nthe results in App. B. We present qualitative results and\nperformance trends in Fig. 5and Fig. 6.\nAre the representations 3D consistent? While models can\nestimate accurate correspondence between objects for small\nviewpoint changes, the performance quickly deteriorates for\nlarger viewpoint changes, as seen in Fig. 6. Although we\nexpect performance to be lower for larger viewpoint changes\nas they are more difﬁcult, the rate of deterioration is interest-\ning. Speciﬁcally, StableDiffusion and SAM experience very\nsharp drops from being among the top performers for the\nFigure 6. While all models experience performance drops with\nlarger viewpoint changes, some experience sharper drops suggest-\ning a lack of 3D awareness.\nsmallest viewpoint changes to being the worst models for the\nlarger viewpoint changes. This can be clearly seen in Fig. 5\nwhere both models predict accurate dense correspondence\nfor the dinosaur in the top row, where the viewpoint variation\nis minimal, but perform very poorly for the rotated eagle\nviews. This rapid deterioration is not universal, as shown by\nthe wide baseline performance of DINOv2 and DeiT.\nWe observe similar trends for indoor scenes where the\nmodels predict accurate correspondence when viewing the\nscene from a very similar vantage point, but struggle with\neven small viewpoint changes as seen in the last two rows\nof Fig. 5. Although DINOv2 performs better than the other\nmodels, the absolute performances for all models are very\nlow for wide baseline correspondence estimation. In general,\nour results suggest that current models are not 3D consistent\ndespite encoding surface properties as shown in Sec. 3.1.\n6\n21800\nFigure 7. Semantic Correspondence. StableDiffusion represents\nsemantics well, but lack 3D consistency. This results in accurate\ncorrespondence for objects viewed from similar angles and system-\natic errors when viewing objects from different viewpoints.\nSemantic vs. Geometric Correspondence. Recent work\nhas shown that self-supervised and generative models excel\nat estimating semantic correspondence [ 1,83,99]. Semantic\ncorrespondence [ 3] generalizes the correspondence problem\nfrom matching the same points across views of the same\nobject to matching similar semantic parts across different\ninstances of the same class; e.g., matching a dog’s left ear\nin images of two different dogs. At ﬁrst glance, this seems\nto contradict our results, since semantic correspondence ap-\npears to capture both 3D structure and semantics.\nSemantic correspondence is commonly evaluated using\nkeypoint recall. This evaluation makes the model’s perfor-\nmance succeptible to semantic biases and priors in the data.\nKeypoints are often selected to be unique and easily identi-\nﬁable; e.g., beaks and tails. Although some keypoints ( e.g.,\neyes and knees) are repeated, they often appear in consistent\nspatial arrangements due to photographer bias.\nWe illustrate the disparity between semantic and geomet-\nric correspondence by evaluating StableDiffusion on SPair-\n71k chairs in Fig. 7. We evaluate performance using keypoint\nconfusion rather than recall. We do this by matching the clos-\nest keypoint to the predicted correspondence location and\nplotting the confusion matrix. This is only computed for key-\npoints with a true match. While StableDiffusion estimates\naccurate correspondence for small viewpoint changes, it ex-\nhibits interesting error patterns for large viewpoint changes.\nErrors seem restricted to semantically related classes ( e.g.,\nseat corners, and chair legs). Furthermore, the qualitative re-\nsults suggest that the representation captures a combination\nof semantics and 2D location: i.e., the chair leg on the right.\nWe suspect that this observation is related to the Janus prob-\nlem observed in diffusion-based 3D reconstruction, since the\nsame ear can be repurposed for two different faces.\nFigure 8. Cross-task performance correlation. Performance\non single view tasks is strongly correlated with itself as well as\nsemantic correspondence, but we see a drop in correlation perfor-\nmance of scene-level correspondence estimation and correspodence\nestimation with large viewpoint variation.\n3.3. Analysis\nOne important question is how correlated are different tasks;\ni.e., if a model’s representations accurately represent depth,\nhow likely is it that they are also useful for correspondence?\nTo address this question, we compute the correlations be-\ntween the models’ aggregated performance across multiple\ntasks. We are particularly interested in understanding the\nrelationship between training objectives and 3D awareness.\nWe note that while we highlighted speciﬁc models in our\nanalysis, we evaluated a much larger set of model variants\nand computed the cross-task performance correlations on the\nfull set. See App. Bfor the complete set of results.\nWe compute the Pearson correlation between all pairs\nof tasks as shown in Fig. 8. For single-view 3D, we report\nrecall for depth and surface normal estimation on objects\nand scene. We also report recall for correspondence estima-\ntion and separate the performance based on the amount of\nviewpoint variation by considering the smallest and largest\nviewpoint bins for NA VI and ScanNet. Finally, we also\nreport the aggregated performance for semantic correspon-\ndence estimation.\nWe ﬁnd that performance on all single view tasks is\nstrongly correlated with correlation coefﬁcients larger than\n0.82. On the other hand, the correlation across multiview\ntasks is much lower, as shown by the values on the bottom\nright corner of the correlation matrix. Interestingly, seman-\ntic correspondence performance is more strongly correlated\nwith single-view tasks than it is with multiview tasks despite\nhaving a similar evaluation procedure to the latter. This fur-\nther supports our claim that semantic correspondence is not\na good measure of 3D consistency.\n7\n21801\n4. Related Work\nOur work is broadly related to other efforts to understand the\nrepresentations learned by vision models and to use them for\n3D vision tasks. Since the recent revival of deep learning,\nthere has been a lot of work on understanding how and what\nthese models learn with a focus on classiﬁcation models.\nEarly work focused on analyzing what those models could\nbe used for [ 11,26,41] and providing some interpretability\ninto what they were learning [ 74]. Our work is inspired by\nrecent efforts to benchmark the semantic and localization\ncapabilities of visual backbones [ 24,26,44,48,85,87]\nwhich we try to extend towards 3D awareness.\nRecent work has attempted to evaluate the 3D under-\nstanding of vision models. One line of work has explored\nhow well generative models capture single image geome-\ntry [6,12,16,71]. Although this line of work shared our\ngoals, their probing techniques are often speciﬁc to gener-\native models, making it difﬁcult to extend to other visual\nmodels. More closely related to our analysis is the recent\nwork of Zhan et al. [98], who proposed analyzing the 3D\nunderstanding of StableDiffusion through a series of binary\nclassiﬁcation tasks. Instead, we focus on dense probing tasks\nand multiview consistency, as they are less susceptible to\nsemantic priors, which can confound 3D undersanding, as\nshown by Tatarchenko et al. [84]. Furthermore, we explore\nmultiview consistency as another facet of 3D awareness.\nAnother line of work has focus on using large-scale mod-\nels for 3D tasks. One line of work extracts features from\nmodels for correspondence estimation [ 1,30,54,60,83,99]\nand pose estimation [ 25,100]. Others have shown how\nthese models could be ﬁne-tuned for accurate depth estima-\ntion with [ 33,101] achieving state-of-the-art performance\nby ﬁne-tuning StableDiffusion. Another line of work com-\nbines image generation with 3D representations for text- or\nimage-conditioned 3D reconstruction [ 62,88,93]. While\nthose methods generate impressive 3D shapes, it has been\nobserved that their generations are not 3D consistent and can\ngenerate animals with multiple heads (the Janus problem).\nRecent efforts have shown that ﬁne-tuning with 3D data can\nimprove generation quality [ 34,50,63,66,77]. We are in-\nspired by this line of work, but note that it differs in objective\nfrom our analysis, as we are interested in understanding 3D\nawareness in models trained without 3D supervision.\n5. Discussion\nThis paper presents an exploratory study of the 3D aware-\nness of visual models; i.e., how well do the representations\ncapture the 3D-ness of the scenes and objects? We posit that\n3D awareness implies representations that (1) encode the\ngeometry of the visible surface and (2) are consistent across\nviews. We used trainable probes and zero-shot inference\nmethods to evaluate the frozen features of those models.Our results show that visual foundation models learn rep-\nresentations that encode the depth and orientation of the\nvisible surface, with vision-language models being the no-\ntable exception. We also ﬁnd that while models can estimate\naccurate semantic correspondence as well as correspondence\nacross images of a similar viewpoint, they struggle with large\nviewpoint changes. This indicates a lack of multiview consis-\ntency and suggests that models are learning representations\nthat are view-consistent, not 3D consistent. One possibility is\nthat the models are learning view-dependent representations.\nThis could be similar to the theories of shape perception pro-\nposed by Koenderink and Van Doorn [ 37,38], where shape\nperception is achieved by a series of view-speciﬁc represen-\ntations connected with an aspect graph. Another possibility\nis that current models are simply good “image models” and\nthat good discriminative features are sufﬁcient for strong\n2.5D understanding. We hope that our ﬁndings can simulate\nmore interest in understanding the 3D awareness of visual\nmodels and that future work can provide better answers.\nOur analysis struggles with several limitations, which we\ndiscuss in more detail in App. C. First, we used pretrained\ncheckpoints that were often trained on different datasets\nand with different compute scales. While this allowed us\nto explore a broader set of models and tasks, it would be\nuseful to make more targeted and fair comparisons to bet-\nter understand the impact of training signals. Second, we\nfocused on minimal probing approaches to analyze the pre-\ntrained representations. It would be useful to explore other\nprobing techniques, as it remains unclear what is the best\nway to understand the distributed representations learned\nby visual models. Finally, our analysis only explored two\nbasic aspects of 3D understanding. However, 3D awareness\nand understanding are closely related to more complex and\nhigher-order tasks such as perceiving 3D shapes, reasoning\nabout spatial relationships, as well as making predictions\nabout deformation and dynamics.\nThis work is only a ﬁrst step towards understanding the\n3D awareness of visual models. This is becoming more\nrelevant, as recent image and video generation models have\nachieved impressive feats of photorealism and temporal con-\nsistency. This makes this a very exciting time to delve into\nunderstanding what those models have learned and whether\nor not they learned about the 3D structure of the world in the\nprocess of learning to generate it. We hope that our ﬁndings\nwill stimulate more interest in understanding the 3D aware-\nness of visual models and that future work can provide more\ninsight into how models represent the world and the impact\nof the learning objectives of such representations.\nAcknowledgments: This work was done during an intership with\nthe VisCAM team at Google Research. We thank Prafull Sharma,\nShivam Duggal, Karan Desai, Junhwa Hur, and Charles Herrmann\nfor many helpful discussions. We also thank Alyosha Efros, David\nFouhey, Stella Yu, and Andrew Owens for their feedback.\n8\n21802\nReferences\n[1]Shir Amir, Yossi Gandelsman, Shai Bagon, and Tali Dekel.\nDeep vit features as dense visual descriptors. arXiv preprint\narXiv:2112.05814 , 2021. 1,5,7,8,15\n[2]Gwangbin Bae, Ignas Budvytis, and Roberto Cipolla. Es-\ntimating and exploiting the aleatoric uncertainty in surface\nnormal estimation. In ICCV , 2021. 3,16\n[3]Alexander C Berg, Tamara L Berg, and Jitendra Malik.\nShape matching and object recognition using low distortion\ncorrespondences. In CVPR , pages 26–33. Citeseer, 2005. 7\n[4]Shariq Farooq Bhat, Ibraheem Alhashim, and Peter Wonka.\nAdabins: Depth estimation using adaptive bins. In CVPR ,\n2021. 3,15\n[5]Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter\nWonka, and Matthias M ¨uller. Zoedepth: Zero-shot trans-\nfer by combining relative and metric depth. arXiv preprint\narXiv:2302.12288 , 2023. 14\n[6]Anand Bhattad, Daniel McKee, Derek Hoiem, and DA\nForsyth. Stylegan knows normal, depth, albedo, and more.\narXiv preprint arXiv:2306.00987 , 2023. 1,4,5,8\n[7]Thomas O. Binford. Visual perception by computer. In\nProceedings of the IEEE Conference on Systems and Control ,\n1971. 2\n[8]Rodney A Brooks. Symbolic reasoning among 3-d models\nand 2-d images. Artiﬁcial intelligence , 17(1-3):285–348,\n1981. 2\n[9]Mathilde Caron, Hugo Touvron, Ishan Misra, Herv ´eJ´egou,\nJulien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-\ning properties in self-supervised vision transformers. In\nIEEE Conf. Comput. Vis. Pattern Recog. , 2021. 2,3,13,21,\n22,23,24,25\n[10] Huiwen Chang, Han Zhang, Jarred Barber, Aaron\nMaschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang,\nKevin Patrick Murphy, William T Freeman, Michael Ru-\nbinstein, Yuanzhen Li, and Dilip Krishnan. Muse: Text-to-\nimage generation via masked generative transformers. In\nICML , 2023. 1\n[11] K Chatﬁeld, K Simonyan, A Vedaldi, and A Zisserman.\nReturn of the devil in the details: delving deep into convolu-\ntional nets. In BMVC , 2014. 8\n[12] Yida Chen, Fernanda Vi ´egas, and Martin Wattenberg. Be-\nyond surface statistics: Scene representations in a latent\ndiffusion model. arXiv preprint arXiv:2306.05720 , 2023. 1,\n4,5,8\n[13] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber,\nThomas Funkhouser, and Matthias Nießner. Scannet: Richly-\nannotated 3d reconstructions of indoor scenes. In CVPR ,\n2017. 2,5,15\n[14] Timoth ´ee Darcet, Maxime Oquab, Julien Mairal, and Piotr\nBojanowski. Vision transformers need registers. arXiv\npreprint arXiv:2309.16588 , 2023. 13,21,22,23,24,25\n[15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. In Int. Conf. Learn.\nRepresent. , 2020. 2[16] Xiaodan Du, Nicholas Kolkin, Greg Shakhnarovich, and\nAnand Bhattad. Generative models: What do they know?\ndo they know things? let’s ﬁnd out! arXiv , 2023. 8\n[17] David Eigen, Christian Puhrsch, and Rob Fergus. Depth\nmap prediction from a single image using a multi-scale deep\nnetwork. In NeruIPS , 2014. 3,15\n[18] Mohamed El Banani and Justin Johnson. Bootstrap Your\nOwn Correspondences. In ICCV , 2021. 5\n[19] Mohamed El Banani, Luya Gao, and Justin Johnson. Unsu-\npervisedR&R: Unsupervised Point Cloud Registration via\nDifferentiable Rendering. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\npages 7129–7139, 2021. 5\n[20] Mohamed El Banani, Karan Desai, and Justin Johnson.\nLearning Visual Representations via Language-Guided Sam-\npling. In CVPR , 2023. 19\n[21] Mark Everingham, Luc Van Gool, Christopher KI Williams,\nJohn Winn, and Andrew Zisserman. The pascal visual object\nclasses (voc) challenge. International Journal of Computer\nVision , 88(2):303–338, 2010. 15\n[22] David Fouhey. Factoring Scenes into 3D Structure and\nStyle . PhD thesis, Carnegie Mellon University, Pittsburgh,\nPA, 2016. 3,4\n[23] David F. Fouhey, Wajahat Hussain, Abhinav Gupta, and\nMartial Hebert. Single image 3D without a single 3D image.\nInICCV , 2015. 3,16\n[24] Micah Goldblum, Hossein Souri, Renkun Ni, Manli Shu, Vi-\nraj Prabhu, Gowthami Somepalli, Prithvijit Chattopadhyay,\nMark Ibrahim, Adrien Bardes, Judy Hoffman, Rama Chel-\nlappa, Andrew Gordon Wilson, and Tom Goldstein. Battle\nof the backbones: A large-scale comparison of pretrained\nmodels across computer vision tasks. In NeurIPS Datasets\nand Benchmarks Track , 2023. 1,8,13\n[25] Walter Goodwin, Sagar Vaze, Ioannis Havoutis, and Ingmar\nPosner. Zero-shot category-level object pose estimation. In\nECCV , 2022. 1,8\n[26] Priya Goyal, Dhruv Mahajan, Abhinav Gupta, and Ishan\nMisra. Scaling and benchmarking self-supervised visual\nrepresentation learning. In ICCV , 2019. 1,3,8\n[27] Kamal Gupta, Varun Jampani, Carlos Esteves, Abhinav Shri-\nvastava, Ameesh Makadia, Noah Snavely, and Abhishek Kar.\nAsic: Aligning sparse in-the-wild image collections. arXiv\npreprint arXiv:2303.16201 , 2023. 1\n[28] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDelving deep into rectiﬁers: Surpassing human-level per-\nformance on imagenet classiﬁcation. In ICCV , pages 1026–\n1034, 2015. 2\n[29] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr\nDoll´ar, and Ross Girshick. Masked autoencoders are scal-\nable vision learners. In CVPR , 2022. 2,3,13,21,22,23,24,\n25\n[30] Eric Hedlin, Gopal Sharma, Shweta Mahajan, Hossam Isack,\nAbhishek Kar, Andrea Tagliasacchi, and Kwang Moo Yi.\nUnsupervised semantic correspondence using stable diffu-\nsion. arXiv preprint arXiv:2305.15581 , 2023. 1,8\n[31] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade\nGordon, Nicholas Carlini, Rohan Taori, Achal Dave,\n9\n21803\nVaishaal Shankar, Hongseok Namkoong, John Miller, Han-\nnaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Open-\nCLIP, 2021. 2,14,21,22,23,24,25\n[32] Varun Jampani, Kevis-Kokitsi Maninis, Andreas Engel-\nhardt, Arjun Karpur, Karen Truong, Kyle Sargent, Stefan\nPopov, Andr ´e Araujo, Ricardo Martin-Brualla, Kaushal Pa-\ntel, Daniel Vlasic, Vittorio Ferrari, Ameesh Makadia, Ce Liu,\nYuanzhen Li, and Howard Zhou. Navi: Category-agnostic\nimage collections with high-quality 3d shape and pose anno-\ntations. In NeurIPS Datasets and Benchmarks Track , 2023.\n2,4,14\n[33] Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Met-\nzger, Rodrigo Caye Daudt, and Konrad Schindler. Repurpos-\ning diffusion-based image generators for monocular depth\nestimation. arXiv preprint arXiv:2312.02145 , 2023. 8\n[34] Gyeongnyeon Kim, Wooseok Jang, Gyuseong Lee, Susung\nHong, Junyoung Seo, and Seungryong Kim. Dag: Depth-\naware guidance with denoising diffusion probabilistic mod-\nels.arXiv preprint arXiv:2212.08861 , 2022. 8\n[35] Diederik Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. In ICLR , 2015. 3\n[36] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\nhead, Alexander C Berg, Wan-Yen Lo, et al. Segment any-\nthing. arXiv preprint arXiv:2304.02643 , 2023. 1,3,14,21,\n22,23,24,25\n[37] Jan J Koenderink and Andrea J Van Doorn. The singularities\nof the visual mapping. Biological cybernetics , 24(1):51–59,\n1976. 8\n[38] Jan J Koenderink and Andrea J Van Doorn. The internal rep-\nresentation of solid shape with respect to vision. Biological\ncybernetics , 32(4):211–216, 1979. 2,8\n[39] Jan J Koenderink and Andrea J Van Doorn. Surface shape\nand curvature scales. Image and vision computing , 10(8):\n557–564, 1992. 2,3\n[40] Jan J Koenderink, Andrea J Van Doorn, and Astrid ML Kap-\npers. Pictorial surface attitude and local depth comparisons.\nPerception & Psychophysics , 58(2):163–173, 1996. 4\n[41] Simon Kornblith, Jonathon Shlens, and Quoc V Le. Do\nbetter imagenet models transfer better? In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition , pages 2661–2671, 2019. 3,8\n[42] Ananya Kumar, Aditi Raghunathan, Robbie Matthew Jones,\nTengyu Ma, and Percy Liang. Fine-tuning can distort pre-\ntrained features and underperform out-of-distribution. In In-\nternational Conference on Learning Representations , 2022.\n3\n[43] L’ubor Ladick `y, Bernhard Zeisl, and Marc Pollefeys. Dis-\ncriminatively trained dense surface normal estimation. In\nECCV , 2014. 4,14\n[44] Martha Lewis, Qinan Yu, Jack Merullo, and Ellie Pavlick.\nDoes clip bind concepts? probing compositionality in large\nimage models. arXiv preprint arXiv:2212.10537 , 2022. 5,\n8,17\n[45] Alexander C. Li, Mihir Prabhudesai, Shivam Duggal, Ellis\nBrown, and Deepak Pathak. Your diffusion model is secretly\na zero-shot classiﬁer. In Proceedings of the IEEE/CVFInternational Conference on Computer Vision (ICCV) , pages\n2206–2217, 2023. 2\n[46] Junnan Li, Ramprasaath R. Selvaraju, Akhilesh Deepak Got-\nmare, Shaﬁq Joty, Caiming Xiong, and Steven Hoi. Align\nbefore fuse: Vision and language representation learning\nwith momentum distillation. In Advances in Neural Infor-\nmation Processing Systems , 2021. 1\n[47] Zhengqi Li and Noah Snavely. Megadepth: Learning single-\nview depth prediction from internet photos. In Proceedings\nof the IEEE conference on computer vision and pattern\nrecognition , pages 2041–2050, 2018. 15\n[48] Zhuowan Li, Cihang Xie, Benjamin Van Durme, and Alan\nYuille. Localization vs. semantics: Visual representations in\nunimodal and multimodal models. In EACL , 2024. 5,8,17\n[49] Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen,\nXiaodong Liu, Jianfeng Gao, and Jiawei Han. On the vari-\nance of the adaptive learning rate and beyond. In ICLR ,\n2020. 16\n[50] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-\nmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3:\nZero-shot one image to 3d object. In ICCV , 2023. 1,8\n[51] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feicht-\nenhofer, Trevor Darrell, and Saining Xie. A convnet for the\n2020s. In CVPR , 2022. 14,21,22,23,24,25\n[52] Ilya Loshchilov and Frank Hutter. SGDR: Stochastic\ngradient descent with warm restarts. arXiv preprint\narXiv:1608.03983 , 2016. 3\n[53] David G Lowe. Distinctive image features from scale-\ninvariant keypoints. IJCV , 2004. 16\n[54] Grace Luo, Lisa Dunlap, Dong Huk Park, Aleksander Holyn-\nski, and Trevor Darrell. Diffusion hyperfeatures: Searching\nthrough time and space for semantic correspondence. arXiv ,\n2023. 8\n[55] David Marr and Tomaso Poggio. A computational theory of\nhuman stereo vision. Royal Society of London , 1979. 2\n[56] Luke Melas-Kyriazi, Christian Rupprecht, Iro Laina, and\nAndrea Vedaldi. Deep spectral methods: A surprisingly\nstrong baseline for unsupervised semantic segmentation and\nlocalization. In IEEE Conf. Comput. Vis. Pattern Recog. ,\n2022. 1\n[57] Juhong Min, Jongmin Lee, Jean Ponce, and Minsu Cho.\nSpair-71k: A large-scale benchmark for semantic correspon-\ndence. arXiv prepreint arXiv:1908.10543 , 2019. 2,5,15,\n17\n[58] Norman Mu, Alexander Kirillov, David Wagner, and Sain-\ning Xie. Slip: Self-supervision meets language-image pre-\ntraining. In Eur. Conf. Comput. Vis. , 2022. 19\n[59] J Farley Norman, James T Todd, Hideko F Norman,\nAnna Marie Clayton, and T Ryan McBride. Visual discrimi-\nnation of local surface structure: Slant, tilt, and curvedness.\nVision research , 46(6-7):1057–1069, 2006. 4\n[60] Maxime Oquab, Timoth ´ee Darcet, Th ´eo Moutakanni, Huy\nV o, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,\nDaniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mah-\nmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell\nHowes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael\nRabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herv ´e Je-\ngou, Julien Mairal, Patrick Labatut, Armand Joulin, and\n10\n21804\nPiotr Bojanowski. DINOv2: Learning Robust Visual Fea-\ntures without Supervision, 2023. 2,3,8,13,15,18,21,22,\n23,24,25\n[61] Luigi Piccinelli, Christos Sakaridis, and Fisher Yu. idisc:\nInternal discretization for monocular depth estimation. In\nCVPR , 2023. 16\n[62] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-\nhall. Dreamfusion: Text-to-3d using 2d diffusion. In ICLR ,\n2022. 8\n[63] Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren,\nAliaksandr Siarohin, Bing Li, Hsin-Ying Lee, Ivan Sko-\nrokhodov, Peter Wonka, Sergey Tulyakov, and Bernard\nGhanem. Magic123: One image to high-quality 3d ob-\nject generation using both 2d and 3d diffusion priors. arXiv\npreprint arXiv:2306.17843 , 2023. 8\n[64] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario\nAmodei, Ilya Sutskever, et al. Language models are unsu-\npervised multitask learners. OpenAI blog , 1(8):9, 2019. 21,\n22,23,24,25\n[65] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language su-\npervision. In Int. Conf. Machine Learning , 2021. 1,2,3,\n14\n[66] Amit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer,\nNataniel Ruiz, Ben Mildenhall, Shiran Zada, Kﬁr Aberman,\nMichael Rubinstein, Jonathan Barron, Yuanzhen Li, and\nVarun Jampani. Dreambooth3d: Subject-driven text-to-3d\ngeneration. ICCV , 2023. 8\n[67] Ren´e Ranftl, Katrin Lasinger, David Hafner, Konrad\nSchindler, and Vladlen Koltun. Towards robust monocular\ndepth estimation: Mixing datasets for zero-shot cross-dataset\ntransfer. TPAMI , 2020. 3,14\n[68] Ren´e Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vi-\nsion transformers for dense prediction. In ICCV , 2021. 3,\n14,15,21,22,23,24,25\n[69] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj ¨orn Ommer. High-resolution image\nsynthesis with latent diffusion models. In CVPR , pages\n10684–10695, 2022. 1,2,3,14,21,22,23,24,25\n[70] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\nJonathan Ho, David J. Fleet, and Mohammad Norouzi. Pho-\ntorealistic text-to-image diffusion models with deep lan-\nguage understanding. In NeurIPS , 2022. 1\n[71] Ayush Sarkar, Hanlin Mai, Amitabh Mahapatra, Svetlana\nLazebnik, David Forsyth, and Anand Bhattad. Shadows\ndon’t lie and lines can’t bend! generative models don’t know\nprojective geometry...for now. 2023. 8\n[72] Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz,\nand Andrew Rabinovich. Superglue: Learning feature match-\ning with graph neural networks. In CVPR , 2020. 5,15\n[73] Christoph Schuhmann, Richard Vencu, Romain Beaumont,\nRobert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo\nCoombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m:Open dataset of clip-ﬁltered 400 million image-text pairs.\narXiv preprint arXiv:2111.02114 , 2021. 14\n[74] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek\nDas, Ramakrishna Vedantam, Devi Parikh, and Dhruv Ba-\ntra. Grad-cam: Visual explanations from deep networks via\ngradient-based localization. In Int. Conf. Comput. Vis. , 2017.\n2,8\n[75] Roger N Shepard and Susan Chipman. Second-order iso-\nmorphism of internal representations: Shapes of states. Cog-\nnitive psychology , 1(1):1–17, 1970. 2\n[76] Roger N Shepard and Jacqueline Metzler. Mental rotation\nof three-dimensional objects. Science , 171(3972):701–703,\n1971. 2\n[77] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie\nLi, and Xiao Yang. Mvdream: Multi-view diffusion for 3d\ngeneration. arXiv preprint arXiv:2308.16512 , 2023. 8\n[78] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob\nFergus. Indoor segmentation and support inference from\nrgbd images. In ECCV , 2012. 2,4,13,14\n[79] Elizabeth Spelke, Sang Ah Lee, and V ´eronique Izard. Be-\nyond core knowledge: Natural geometry. Cognitive science ,\n34(5):863–884, 2010. 2,3\n[80] Andreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross\nWightman, Jakob Uszkoreit, and Lucas Beyer. How to train\nyour vit? data, augmentation, and regularization in vision\ntransformers. arXiv preprint arXiv:2106.10270 , 2021. 14,\n18\n[81] Sanjay Subramanian, William Merrill, Trevor Darrell, Matt\nGardner, Sameer Singh, and Anna Rohrbach. Reclip: A\nstrong zero-shot baseline for referring expression compre-\nhension. In Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long\nPapers) , 2022. 5\n[82] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and\nXiaowei Zhou. LoFTR: Detector-free local feature matching\nwith transformers. CVPR , 2021. 5\n[83] Luming Tang, Menglin Jia, Qianqian Wang, Cheng Perng\nPhoo, and Bharath Hariharan. Emergent correspondence\nfrom image diffusion. arXiv preprint arXiv:2306.03881 ,\n2023. 1,5,7,8,14,17\n[84] Maxim Tatarchenko, Stephan R Richter, Ren ´e Ranftl,\nZhuwen Li, Vladlen Koltun, and Thomas Brox. What do\nsingle-view 3d reconstruction networks learn? In CVPR ,\n2019. 8\n[85] Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet\nSingh, Adina Williams, Douwe Kiela, and Candace Ross.\nWinoground: Probing vision and language models for visio-\nlinguistic compositionality. In CVPR , 2022. 8\n[86] Hugo Touvron, Matthieu Cord, and Herv ´eJ´egou. Deit III:\nRevenge of the ViT. In ECCV , 2022. 2,3,13,21,22,23,\n24,25\n[87] Matthew Walmer, Saksham Suri, Kamal Gupta, and Abhinav\nShrivastava. Teaching matters: Investigating the role of\nsupervision in vision transformers. In IEEE Conf. Comput.\nVis. Pattern Recog. , 2023. 1,6,8,15\n[88] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh,\nand Greg Shakhnarovich. Score jacobian chaining: Lifting\n11\n21805\npretrained 2d diffusion models for 3d generation. In CVPR ,\n2023. 8\n[89] Ross Wightman. Pytorch image models. https://github.\ncom/rwightman/pytorch-image-models , 2019. 13\n[90] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chau-\nmond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim\nRault, R ´emi Louf, Morgan Funtowicz, Joe Davison, Sam\nShleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien\nPlu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama\nDrame, Quentin Lhoest, and Alexander M. Rush. Trans-\nformers: State-of-the-art natural language processing. In\nProceedings of the 2020 Conference on Empirical Methods\nin Natural Language Processing: System Demonstrations ,\npages 38–45, Online, 2020. Association for Computational\nLinguistics. 13\n[91] Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei\nChen, Zhuang Liu, In So Kweon, and Saining Xie. Con-\nvnext v2: Co-designing and scaling convnets with masked\nautoencoders. In CVPR , 2023. 13,21,22,23,24,25\n[92] Yu Xiang, Roozbeh Mottaghi, and Silvio Savarese. Beyond\npascal: A benchmark for 3d object detection in the wild.\nInIEEE Winter Conference on Applications of Computer\nVision (WACV) , 2014. 15\n[93] Dejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Yi Wang,\nand Zhangyang Wang. Neurallift-360: Lifting an in-the-wild\n2d photo to a 3d object with 360deg views. In CVPR , 2023.\n8\n[94] Hu Xu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang,\nRussell Howes, Vasu Sharma, Shang-Wen Li, Gargi Ghosh,\nLuke Zettlemoyer, and Christoph Feichtenhofer. Demys-\ntifying clip data. arXiv preprint arXiv:2309.16671 , 2023.\n3\n[95] Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiao-\nlong Wang, and Shalini De Mello. ODISE: Open-V ocabulary\nPanoptic Segmentation with Text-to-Image Diffusion Mod-\nels. In CVPR , 2023. 1,2,14\n[96] Wei Yin, Chi Zhang, Hao Chen, Zhipeng Cai, Gang Yu,\nKaixuan Wang, Xiaozhi Chen, and Chunhua Shen. Metric3d:\nTowards zero-shot metric 3d prediction from a single image.\nInICCV , 2023. 15\n[97] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and\nLucas Beyer. Sigmoid loss for language image pre-training.\nICCV , 2023. 3,14,21,22,23,24,25\n[98] Guanqi Zhan, Chuanxia Zheng, Weidi Xie, and Andrew\nZisserman. What does stable diffusion know about the 3d\nscene?, 2023. 8,14,15\n[99] Junyi Zhang, Charles Herrmann, Junhwa Hur, Luisa Pola-\nnia Cabrera, Varun Jampani, Deqing Sun, and Ming-Hsuan\nYang. A tale of two features: Stable diffusion complements\ndino for zero-shot semantic correspondence. In NeurIPS ,\n2023. 1,5,7,8,14,15,17\n[100] Kaifeng Zhang, Yang Fu, Shubhankar Borse, Hong Cai,\nFatih Porikli, and Xiaolong Wang. Self-supervised geo-\nmetric correspondence for category-level 6d object pose\nestimation in the wild. In ICLR , 2023. 8\n[101] Wenliang Zhao, Yongming Rao, Zuyan Liu, Benlin Liu,\nJie Zhou, and Jiwen Lu. Unleashing text-to-image dif-fusion models for visual perception. arXiv preprint\narXiv:2303.02153 , 2023. 2,8,14\n[102] Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang\nXie, Alan Yuille, and Tao Kong. ibot: Image bert pre-training\nwith online tokenizer. International Conference on Learning\nRepresentations (ICLR) , 2022. 2,3,13,21,22,23,24,25\n12\n21806'}, 'dist': 0.9286905527114868}
Result 10: {'text': 'data', 'metadata': {'created_at': 1736932570, 'modified_at': 1736932570, 'owner': 'sayandeep', 'path': 'RAG_CONTENT/conferences/cvpr_ocr/Chen_Think_Twice_Before_Selection_Federated_Evidential_Active_Learning_for_Medical_CVPR_2024_paper.txt', 'size': 52987, 'seen_at': 1737191136, 'data': 'Think Twice Before Selection: Federated Evidential Active Learning for Medical\nImage Analysis with Domain Shifts\nJiayi Chen1∗Benteng Ma2∗Hengfei Cui1Yong Xia1,3†\n1School of Computer Science and Engineering, Northwestern Polytechnical University, China\n2Hong Kong University of Science and Technology, Hong Kong SAR, China\n3Research & Development Institute of Northwestern Polytechnical University in Shenzhen, China\njiayichen@mail.nwpu.edu.cn, bentengma@ust.hk, hfcui@nwpu.edu.cn, yxia@nwpu.edu.cn\nAbstract\nFederated learning facilitates the collaborative learning\nof a global model across multiple distributed medical in-\nstitutions without centralizing data. Nevertheless, the ex-\npensive cost of annotation on local clients remains an ob-\nstacle to effectively utilizing local data. To mitigate this\nissue, federated active learning methods suggest leverag-\ning local and global model predictions to select a rela-\ntively small amount of informative local data for annota-\ntion. However, existing methods mainly focus on all lo-\ncal data sampled from the same domain, making them un-\nreliable in realistic medical scenarios with domain shifts\namong different clients. In this paper, we make the first at-\ntempt to assess the informativeness of local data derived\nfrom diverse domains and propose a novel methodology\ntermed Federated Evidential ActiveLearning (FEAL) to\ncalibrate the data evaluation under domain shift. Specif-\nically, we introduce a Dirichlet prior distribution in both\nlocal and global models to treat the prediction as a distribu-\ntion over the probability simplex and capture both aleatoric\nand epistemic uncertainties by using the Dirichlet-based\nevidential model. Then we employ the epistemic uncer-\ntainty to calibrate the aleatoric uncertainty. Afterward, we\ndesign a diversity relaxation strategy to reduce data re-\ndundancy and maintain data diversity. Extensive experi-\nments and analysis on five real multi-center medical im-\nage datasets demonstrate the superiority of FEAL over the\nstate-of-the-art active learning methods in federated sce-\nnarios with domain shifts. The code will be available at\nhttps://github.com/JiayiChen815/FEAL .\n∗Equal contribution.†Yong Xia is the corresponding author. This\nwork was supported in part by Shenzhen Science and Technology Pro-\ngram under Grants JCYJ20220530161616036, National Natural Science\nFoundation of China under Grants 62171377 and 62271405, Ningbo Clin-\nical Research Center for Medical Imaging under Grant 2021L003 (Open\nProject: 2022LYKFZD06), and Foshan HKUST Projects under Grants\nFSUST21-HKUST10E and FSUST21-HKUST11E.\n?𝑈!𝐿!\n?\n𝑈"𝐿"\n𝑈#𝐿#\nAnnotateDistributeAggregate⋯Client KServerClient 1Client 2Train\n(a) FAL scheme\n0.0 2.5 5.0 7.5 10.0 12.5\nEnergy score0.000.050.100.150.200.25DensityClient 1\nClient 2\nClient 3\nClient 4\n(b) KDE of energy score\n1 2 3 4\nClient1 2 3 4Client1.0000 0.0001 0.0001 0.0001\n0.0001 1.0000 0.0130 0.0001\n0.0001 0.0130 1.0000 0.0031\n0.0001 0.0001 0.0031 1.0000\n0.00.20.40.60.81.0\n (c)p-value\nFigure 1. Illustration of federated active learning (FAL) in the\npresence of domain shift. (a) FAL comprises model distribution,\nlocal training, model aggregation, and data annotation. (b) The\nKDE of energy scores depicts domain shifts across clients. (c)\nThe low p-values in cross-client KDE of energy scores indicate\nthe existence of significant domain shifts between all client pairs.\n1. Introduction\nFederated learning enables collaborative learning across\nmultiple clinical institutions ( i.e., clients) to learn a uni-\nfied model on the central server through model aggregation\nwhile preserving the data privacy at each client [21, 36, 57]\n(see Fig. 1 (a)). Unfortunately, such a learning pipeline re-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n11439\nquires each client to prepare its own labeled data, whose\nscale is constrained by the available expertise, time, and\nbudget for data annotation.\nOne possible solution to alleviate the annotation cost is\nto select a part of highly informative data to annotate. Ac-\ntive learning (AL) has shown great potential in guiding the\ndata selection process [1, 3, 20, 61], leading to the federated\nAL (FAL) framework. Such a pipeline [4, 14, 37, 41, 46, 65]\nallows each client to assess the informativeness of unla-\nbeled data using either the local model at each client or the\nglobal model from the server, greatly alleviating the heavy\nannotation costs while retaining great performance. Nev-\nertheless, when using a local model to select data, there\nis a bias toward prioritizing the data that improves the lo-\ncal updates while disregarding the overall generalizability\nof the global model. Client models trained on diverse do-\nmains may exhibit significant divergence within the param-\neter space, making the use of a global model aggregated\nfrom these models for data selection unreliable.\nRecent advances in FAL, e.g., LoGo [20] and\nKAFAL [3], tend to harness the knowledge of both local and\nglobal models to identify informative samples. Although\nthis strategy has been proven to be more effective than em-\nploying a single model, these methods focus mainly on the\nclass imbalance issue while assuming that the data at mul-\ntiple clients is from the same domain. However, the do-\nmain shift across clients is commonly seen in real-world\napplications, which is evidenced by the extremely low p-\nvalues of the kernel density estimation (KDE) of energy\nscores [31] (see Fig. 1 (b) and (c)). The existence of domain\nshift renders two major challenges for FAL. (1) Overcon-\nfidence; Existing FAL methods evaluate data uncertainty\nbased on the softmax prediction made by a deterministic\nmodel, which is essentially a point estimate and can be mis-\ncalibrated easily on data with domain shifts [31, 33, 42],\nresulting in unreliable uncertainty evaluation. (2) Limited\nuncertainty representation. Uncertainty can be divided\ninto aleatoric uncertainty (or data uncertainty) and epis-\ntemic uncertainty (or knowledge uncertainty) [43]. The for-\nmer reflects the inherent complexity of data, such as class\noverlap and instance noise [54]. The latter captures the re-\nstricted knowledge of a model caused by insufficient data\nor domain shifts. The softmax prediction can represent\nthe aleatoric uncertainty but fails to capture the epistemic\nuncertainty, resulting in incomplete evaluations, which are\nparticularly noticeable in the presence of domain shift.\nTo address both challenges, we propose the Federated\nEvidential Active Learning (FEAL) method. Built upon\nthe Dirichlet-based evidential model [47, 62], FEAL treats\nthe categorical prediction of a sample as following a Dirich-\nlet distribution, thus allowing multiple potential predictions\nfor a sample. FEAL comprises two key modules, i.e.,\ncalibrated evidential sampling (CES) and evidential modellearning (EML). CES is a novel FAL sampling strategy that\nincorporates both uncertainty and diversity measures. It uti-\nlizes the expected entropy of potential predictions to quan-\ntify aleatoric uncertainty and aggregates the aleatoric un-\ncertainty in both global and local models. Further, CES em-\nploys the differential entropy of the Dirichlet distribution to\ncharacterize the epistemic uncertainty [51] and utilizes the\nepistemic uncertainty in the global model to calibrate the\naggregated aleatoric uncertainty. To enhance data selection,\ndiversity relaxation is also employed with the local model\nto reduce redundancy and maintain diversity among the se-\nlected samples. In addition to active sampling, we introduce\nevidence regularization in EML for accurate evidence rep-\nresentation and data assessment. The main contributions of\nthis work are summarized as follows:\n• We explore a rarely studied problem, FAL with domain\nshifts, which aims to attain a global model with a limited\nannotation budget for local clients amidst domain shifts.\n• We propose the FEAL method, with a sampling strategy\nCES and a local training scheme EML, to tackle the chal-\nlenges in FAL with domain shifts. CES is designed to se-\nlect informative samples by leveraging aleatoric and epis-\ntemic uncertainty with both global and local models and\nretaining sample diversity. EML is developed to regular-\nize the evidence for improved data evaluation.\n• We conduct extensive experiments on five real multi-\ncenter medical image datasets, comprising two datasets\nfor classification and three datasets for segmentation. The\nresults suggest the superiority of our FEAL method over\nits AL and FAL counterparts.\n2. Related Work\n2.1. Federated Learning with Domain Shifts\nDomain shift is a long-standing challenge for feder-\nated learning. Previous approaches can be divided into\nregularization-based, aggregation-based, and personalized\nones. Regularization-based methods implemented regu-\nlarization on model parameters [19, 27, 52] or feature em-\nbeddings [12, 15, 26, 60] to address the objective inconsis-\ntency induced by domain shift. Aggregation-based meth-\nodsdynamically adjust aggregation weights based on data\nquality [32], estimated client contribution [17], general-\nization gap between global and local models [67], layer-\nwise divergence [44] or performance on proxy dataset [29].\nPersonalized methods aggregated domain-agnostic layers,\nwhile customizing domain-specific layers for local clients,\nincluding batch normalization (BN) [28], high-frequency\nconvolution [6] and prediction layers [57]. Additionally,\nseveral methods enhanced data diversity [30, 68] to refine\ndata distribution and mitigate statistical heterogeneity [64].\nThese approaches strive to mitigate the impact of domain\nshifts across clients in supervised scenarios with fully an-\n11440\nnotated training samples. Unfortunately, they ignore the\nsubstantial annotation costs for each client. In contrast, we\nfurther leverage active learning to reduce annotation costs\nby selecting the most informative data and propose a label-\nefficient method for federated learning with domain shifts.\n2.2. AL Methods\nConventional AL methods can be categorized into\nuncertainty-based, diversity-based, and hybrid ones.\nUncertainty-based AL methods aim to select the most\nambiguous unlabeled samples for annotation. Classical ap-\nproaches such as least confidence sampling [49], margin-\nbased sampling [37], and entropy-based sampling [50] eval-\nuate the data uncertainty based on categorical probabilities.\nYoo et al. [65] and Huang et al. [14] estimated the loss for\nuncertainty assessment. Moreover, several approaches as-\nsess the data uncertainty by analyzing the prediction incon-\nsistency among multiple augmented samples [11], standard\nand dropout inferences [9, 10], or original and disturbed\nfeatures [41]. Diversity-based AL methods aim to iden-\ntify a subset of samples that captures the distribution of the\ncomplete dataset. A variety of approaches have been pro-\nposed that exploit core-set techniques [4, 46] or clustering\nmethods [23, 38, 55] in the latent feature space, incorporate\na diversity constraint in the optimization process [8, 63], or\nmodel the distribution discrepancy between labeled and un-\nlabeled samples [24] in order to identify a diverse collection\nof samples. Hybrid AL methods exploit both uncertainty\nand diversity in their sampling strategies. Ash et al. [2] clus-\ntered the gradient embeddings to guarantee both uncertainty\nand diversity. A two-stage sampling strategy has also been\nimplemented [41, 56, 66]. However, these methods primar-\nily focus on data selection driven by aleatoric uncertainty,\noften neglecting its sufficiency and reliability in practical\nscenarios. In this work, we developed a Dirichlet-based ev-\nidential model to capture both aleatoric and epistemic un-\ncertainties. We further leveraged the epistemic uncertainty\nto calibrate uncertainty estimates, enhancing their reliability\nin the context of domain shifts.\n2.3. FAL Methods\nFAL aims to enhance the annotation efficacy of each local\nclient in decentralized learning. In contrast to the central-\nized scenarios, there exist two potential query-selector mod-\nels in FAL [20], including the global model and the local\nmodel. Both Wu et al. [61] and Ahn et al. [1] exclusively\nutilized a singular model for data evaluation. Specifically,\nWuet al. [61] introduced a hybrid metric that considers both\nthe locally predicted loss and the local feature distances be-\ntween unlabeled and labeled samples. By contrast, Ahn\net al. [1] argued that evaluating samples with the global\nmodel contributes to the objectives of federated learning\nand recommended applying sampling strategies solely withthe global model. Nevertheless, as demonstrated in [20],\nthe superiority of the two query-selector models depends\non the global and local heterogeneous levels, and it is nec-\nessary to leverage the knowledge of both global and local\nmodels. Kim et al. [20] proposed a hybrid metric called\nLoGo, which applies k-means clustering technique [34] on\nthe gradient space of the local model and subsequently con-\nducts cluster-wise sampling using the global model. Cao et\nal.[3] proposed a knowledge-specialized sampling strategy,\nwhich leverages the discrepancy between the global model\nand local model to assess data uncertainty. However, these\nmethods focus on the local data from a singular domain,\nwhich is less realistic. Though partial approaches [3, 20]\naccount for heterogeneity caused by class imbalance, they\noften neglect another heterogeneous property known as do-\nmain shifts. In this work, we propose the uncertainty cal-\nibration method to achieve reliable uncertainty evaluation\nwith domain shifts across multiple clients.\n3. Methodology\n3.1. Problem Formulation\nThe overview of our FEAL framework is displayed in Fig. 2\nand Appendix A. Under this framework, we maintain K\nlocal models {θk}K\nk=1on clients and a global model θon\nthe central server. The k-th local client contains a labeled\nsetLkand an unlabeled set Uk. FAL comprises two iterative\nphases: federated model training and local data annotation.\nFederated model training involves model distribution, local\ntraining, and model aggregation. In the first round, the k-th\nclient randomly selects Bkunlabeled samples and annotates\nthem to form the initial labeled set L1\nk={(xi,yi)}Bk\ni=1, and\nthe unlabeled set is updated to U1\nk=Uk\\L1\nk. In the r-th\nFAL round, the k-th client constructs the query set Qr\nk=\n{(xj,yj)}Bk\nj=1for annotation using the sampling strategy\nand updates the labeled set to Lr\nk=Lr−1\nk∪Qr\nk, whereas the\nunlabeled set is updated to Ur\nk=Ur−1\nk\\Qr\nk. Subsequent\nfederated model training proceeds with the updated labeled\nsetLr\nk. The FAL process is repeated for Rtimes as required.\n3.2. Dirichlet-based Evidential Model in FAL\nFor federated active learning, we employ a Dirichlet-based\nevidential model to effectively capture aleatoric and epis-\ntemic uncertainties in both global and local models. In this\nsection, we begin by presenting the foundational formula-\ntion of the Dirichlet-based evidential model.\nWe start with the general C-class classification task.\nGiven an input sample xfrom the k-th client, a model f\nparameterized with θprojects xinto a C-dimensional log-\nitsf(x,θ). The classical CNN utilizes the softmax operator\nto transform the logits f(x,θ)into the prediction of class\nprobabilities ρ. However, this approach essentially provides\na single-point estimate of ρand can be easily miscalibrated\n11441\n(a) Federated Evidential Active Learning (FEAL)\nDistributeAggregate\n⋯\nServer\n𝑈!𝐿!\n⋮𝜽⋮𝜽!Query with\nClient 1Train\n𝑈"𝐿"\nQuery with⋮𝜽⋮𝜽"Annotate\n(b) Calibrated Evidential Sampling (CES)\n𝑈!𝐿!𝒙\nLowHighSort𝑈(𝒙,𝜽,𝜽#)\nLowHighSelect𝑈(𝒙,𝜽,𝜽#)𝑈(𝒙,𝜽,𝜽#)\n𝑈"#$(𝒙,𝜽)𝑈%&"(𝒙,𝜽)\n𝑈%&"(𝒙,𝜽!)Local model 𝜽!Global model 𝜽Evidential head\nFeatureextractorFeatureextractor⋮\nEvidential head𝐶"𝐶!𝐶$𝜶⋮\n𝐶"𝐶!𝐶$𝜶\nUncertainty Calibration\nDiversity RelaxationMin neighbor size 𝑛=5\n𝑄!Query set\nOracle\nLabeled neighborUnlabeled neighborCadidate sample 𝒙% (Select)Cadidate sample 𝒙% (Ignore)\n𝑈&𝐿&\nQuery with⋮𝜽⋮𝜽#Unlabeled setLabeled set\nClient 2\nClient KFigure 2. Illustration of the proposed FEAL method. (a) Overview of FEAL. (b) Illustration of CES module, including uncertainty\ncalibration and diversity relaxation.\non local data from diverse domains. The Dirichlet-based ev-\nidential model, on the other hand, views the categorical pre-\ndiction ρas a random variable with a Dirichlet distribution\nDir(ρ|α). The probability density function of ρ[47, 62],\ngivenxandθ, is formulated as:\np(ρ|x,θ) =\uf8f1\n\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f3Γ(PC\nc=1αc)QC\nc=1Γ(αc)CY\nc=1ραc−1\nc,(ρ∈∆C)\n0 ,(otherwise )(1)\nwhere αdenotes the parameters of the Dirichlet distri-\nbution for sample x,Γ(·)is the Gamma function, and\n∆C={ρ|PC\nc=1ρc=1and0<ρc<1}represents the C-\ndimensional unit simplex.\nThe posterior probability P(y=c|x,θ)for class c,\na.k.a. , the expected categorical prediction ρc, is given by:\nP(y=c|x,θ) =Z\np(y=c|ρ)·p(ρ|x,θ)dρ=αc\nS,(2)\nwhere S=PC\nc=1αcrepresents the Dirichlet strength. The\nderivation of Eq. 2 is provided in Appendix B.1.\nDrawing on concepts from Dempster-Shafer theory [48]\nand subjective logic [18], the parameter αis linked to the\naccumulated evidence ewhich quantifies the degree of sup-\nport for the prediction on sample x. The parameter αis\nderived as α=e+ 1 = A(f(x,θ)) + 1 , where A(·)is\na non-negative activation function that transforms the logits\nf(x,θ)into evidence e.\nIn our study, all local models adopt the same Dirichlet-\nbased evidential architecture with the global model to com-\nmunicate between local clients and the central server.\n3.3. Calibrated Evidential Sampling\nIn the context of FAL with domain shifts, we integrate both\nuncertainty and diversity measures to identify the most in-\nformative samples for annotation (see Fig. 2(b)). As for un-\ncertainty evaluation, we leverage the epistemic uncertaintyin the global model to calibrate the aleatoric uncertainty in\nboth global and local models. We now delve into its details.\nAleatoric uncertainty. Dirichlet-based evidential models\ninterpret the categorical prediction ρas a distribution rather\nthan a singular point estimate, which acknowledges a range\nof possible predictions. We use the expected entropy of all\npossible predictions to deliver the aleatoric uncertainty [62]\nto quantify the inherent complexity or ambiguity present in\nlocal data. Given a sample xand the global model θ, the\naleatoric uncertainty of the sample xin the global model θ\nis represented as:\nUale(x,θ) =Ep(ρ|x,θ)[H[P(y|ρ)]]\n=−CX\nc=1Ep(ρc|x,θ)[ρc·logρc]\n=CX\nc=1αc\nS·[ψ(S+ 1)−ψ(αc+ 1)],(3)\nwhere H(·)denotes the Shannon entropy [50]. Similarly,\nthe aleatoric uncertainty in the local model kisUale(x,θk).\nThe derivation of Eq. 3 is in Appendix B.2.\nEpistemic uncertainty. In the Dirichlet distribution, the\ndifferential entropy quantifies how dispersed the probabil-\nities are across different categories [35]. We employ the\ndifferential entropy of the Dirichlet distribution to quantify\nthe epistemic uncertainty linked to domain shifts between\nthe global model and local data. Specifically, given a sam-\nplexand the global model θ, the epistemic uncertainty of\nthe sample xin the global model θis represented as:\nUepi(x,θ) =H[p(ρ|x,θ)]\n=−Z\np(ρ|x,θ)·logp(ρ|x,θ)dρ\n=CX\nc=1logΓ(αc)\nΓ(S)−(αc−1)·[ψ(αc)−ψ(S)].(4)\nThe derivation of Eq. 4 is in Appendix B.2.\n11442\nUncertainty calibration. Given a sample x, the global\nmodel θ, and the local model θk, we calculate the aleatoric\nuncertainty (Eq. 3) in both global and local models and sub-\nsequently calibrate the aleatoric uncertainty by incorporat-\ning the epistemic uncertainty (Eq. 4) from the global model.\nThe overall calibrated uncertainty for sample xis\nU(x,θ,θk) = [Uale(x,θ) +Uale(x,θk)]·Uepi(x,θ). (5)\nDiversity relaxation. We adopt local constraints to ensure\ndiversity among selected samples, contrasting with core-set\ntechniques that impose global diversity constraints. As out-\nlined in Alg. 1, we initially sort the unlabeled set Ur−1\nkby\ndescending calibrated uncertainty U(x,θ,θk), and then ex-\ntract feature embeddings with local model θk. During the it-\neration over the unlabeled set Ur−1\nk, we compute the cosine\nsimilarity s(xi,xj)for each candidate sample xiagainst\nall other samples xj∈Ur−1\nk\\xiand form a neighbor set\nN(xi)based on the similarity threshold τ. A sample xi\nis selected if its neighbor counts |N(xi)|are less than the\nminimum neighbor size nor if these neighbors remain un-\nlabeled. Following this criterion, Bkunlabeled samples are\nchosen to constitute the final set Qr\nkfor annotation, effec-\ntively balancing diversity and uncertainty in data selection.\nAlgorithm 1 Diversity Relaxation for Local Client k\nInput: unlabeled set Ur−1\nk, local model θk, annotation budget\nBk, similarity threshold τ, minimum neighbor size n\nOutput: query set Qr\nk\n1:SortUr−1\nkby descending calibrated uncertainty.\n2:Initialize index i= 1and query set Qr\nk=∅.\n3:while|Qr\nk|< B kandi≤ |Ur−1\nk|do\n4: Select a candidate sample xifromUr−1\nk.\n5: Compute feature similarity s(xi,xj)using θk, where\nxj∈Ur−1\nk\\xi.\n6: Form neighbor set N(xi), including xjwiths(xi,xj)≥τ.\n7: if|N(xi)|< n orN(xi)∩Qr\nk=∅then\n8: AddxitoQr\nk.\n9: end if\n10: Increment i.\n11:end while\n12:return Qr\nk\n3.4. Evidential Model Learning\nDirichlet-based evidential models treat the categorical pre-\ndiction of a sample as a distribution, enabling multiple po-\ntential predictions to occur with specific probabilities. Con-\nsidering all possible predictions, we adopt the Bayes risk\nof cross-entropy loss [47] as the task loss for classification\ntasks, formulated as follows:\nLtask(x,θk,y) =Z\n(CX\nc=1−yclogρc)·p(ρ|x,θk)dρ\n=CX\nc=1yc·[ψ(S)−ψ(αc)],(6)where ψ(·)is the digamma function and ycis the label indi-\ncator for class c. Similarly, the Bayes risk of Dice loss [25]\nfor segmentation tasks is:\nLtask(x,θk,y) =Z\n(1−2\nCCX\nc=1∥yc◦ρc∥1\n∥y2\nc∥1+∥ρ2\nc∥1)·p(ρ|x,θk)dρ\n= 1−2\nCCX\nc=1∥yc◦ρc∥1\n∥y2\nc∥1+∥ρ2\nc∥1+∥ρc◦(1−ρc)\nS+1∥1,\n(7)\nwhere ◦is the Hadamard product and the expected categor-\nical probability of xisρc=αc\nS. The derivation of Eq. 6\nand Eq. 7 are in Appendix B.3.\nWe incorporate evidence regularization to further reduce\nincorrect evidence [47] and improve correct evidence [40].\nLreg(x,θk,y)=KL[Dir(ρ|˜α)∥Dir(ρ|1)]−C\nS·f(x,θk),(8)\nwhere ˜α=y+ (1−y)⊙αandKL(·)denotes the\nKullback-Leibler divergence [22]. Notably, we calculate\nthe average pixel-wise Lregin segmentation.\nThe overall training objective, combining task loss Ltask\nand evidence regularization Lreg, is formulated as:\nL(x,θk,y) =Ltask(x,θk,y) +λ· Lreg(x,θk,y),(9)\nwhere λis the trade-off weight. between the task loss and\nthe regularization term.\n4. Experiments\n4.1. Experimental Settings\nDatasets. We evaluated FEAL on five real multi-center\nmedical image datasets, comprising two classification and\nthree segmentation datasets. The classification datasets in-\ncluded\n•Fed-ISIC : A skin lesion dataset from 4 data sources [39]\ncontaining {12413, 3954, 3363, 2259 }images.\n•Fed-Camelyon : A breast cancer histology dataset from 5\ncenters [16] comprising {59436, 34904, 85054, 129838,\n146722 }patches.\nThe segmentation datasets included\n•Fed-Polyp : A endoscopic polyp dataset from 4 cen-\nters [57] with {1000, 196, 379, 612 }samples.\n•Fed-Prostate : A prostate MRI dataset from 6 data\nsources [30] with {261, 384, 158, 468, 421, 175 }slices.\n•Fed-Fundus : A retinal fundus dataset from 4 centers [30]\nwith{101, 159, 400, 400 }samples.\nIn our study, each dataset was divided using an 8:2 train-to-\ntest split ratio at the patient level. Details of these datasets\nare provided in Appendix C.1.\nEvaluation metrics. For classification, we utilized the Bal-\nanced Multi-class Accuracy (BMA) for skin lesion classi-\nfication [5] and measured accuracy (ACC) for breast can-\ncer histology classification. In the context of segmentation,\n11443\n1 2 3 4 5\nAL round525456586062646668BMA (%)\nFull\nRandom\nEntropy (G)\nTOD (G)Gradnorm (G)\nCoreSet (G)\nBADGE (G)\nOurs+ 1.98+ 1.04+ 2.08 + 2.84(a) Fed-ISIC ( G)\n1 2 3 4 5\nAL round525456586062646668BMA (%)\nFull\nRandom\nEntropy (L)\nTOD (L)Gradnorm (L)\nCoreSet (L)\nBADGE (L)\nOurs+ 2.02+ 0.92+ 1.48 + 2.06 (b) Fed-ISIC ( L)\n1 2 3 4 5\nAL round525456586062646668BMA (%)\nFull\nRandom\nEntroopy (E)\nTOD (E)\nGradnorm (E)CoreSet (E)\nBADGE (E)\nLoGo\nKAFAL\nOurs+ 1.95+ 1.21+ 1.39 + 1.62 (c) Fed-ISIC ( E)\n1 2 3 4 5\nAL round939495969798ACC (%)\nFull\nRandom\nEntropy (G)\nTOD (G)Gradnorm (G)\nCoreSet (G)\nBADGE (G)\nOurs+ 0.88+ 0.57+ 0.51+ 0.44\n(d) Fed-Camelyon ( G)\n1 2 3 4 5\nAL round939495969798ACC (%)\nFull\nRandom\nEntropy (L)\nTOD (L)Gradnorm (L)\nCoreSet (L)\nBADGE (L)\nOurs+ 0.69+ 0.51+ 0.35+ 0.44 (e) Fed-Camelyon ( L)\n1 2 3 4 5\nAL round939495969798ACC (%)\nFull\nRandom\nEntropy (E)\nTOD (E)\nGradnorm (E)CoreSet (E)\nBADGE (E)\nLoGo\nKAFAL\nOurs+ 0.73+ 0.46+ 0.32+ 0.37 (f) Fed-Camelyon ( E)\nFigure 3. Comparison of FAL methods in medical image classification. (a)-(c) and (d)-(f) depict the results of the Fed-ISIC and Fed-\nCamelyon datasets, respectively. Performance enhancements over the second-best method in each FAL round are emphasized in red text.\nwe used the Dice score and the 95% Hausdorff Distance\n(HD95) to assess segmentation results.\nImplementation details. We conducted R= 5 rounds of\nFAL involving federated model training and data annota-\ntion. The annotation budget Bkis500for Fed-ISIC and\nFed-Camelyon, 50for Fed-Polyp, and 20for Fed-Prostate\nand Fed-Fundus. During model training, we followed the\nprevious work [16, 57, 59] to utilize EfficientNet-B0 [53]\nfor Fed-ISIC, DenseNet-121 [13] for Fed-Camelyon, and\nU-Net [30, 45] for segmentation datasets. Notably, both\nEfficientNet-B0 and DenseNet-121 were pre-trained on Im-\nageNet [7]. Each experiment was conducted three times us-\ning different random seeds, and the average results were re-\nported. More details are in Appendix C.1.\nComparison methods. We compared FEAL with eight\nFAL methods, including random sampling (Random),\nentropy-based sampling (Entropy) [50], TOD [14], Grad-\nnorm [58], CoreSet [46], BADGE [2], LoGo [20], and\nKAFAL [3]. The first six strategies are primarily developed\nfor standard active learning, whereas LoGo and KAFAL\nare specifically tailored for decentralized scenarios. To in-\ncorporate these standard AL strategies into the FAL frame-\nwork, we implemented them in three distinct manners: us-\ning only the global model (referred to as G), depending\nsolely on the local model ( L), or employing a simple en-\nsemble method with both models ( E). It guarantees a com-\nprehensive evaluation of these strategies in FAL. Details of\ncomparison methods are summarized in Appendix C.1.4.2. Results\nImage classification. The comparative analysis of image\nclassification results in Fig. 3 indicates that FEAL achieves\nsuperior results on both Fed-ISIC and Fed-Camelyon\ndatasets. As depicted in Fig. 3, the performance of all meth-\nods exhibits a general trend of improvement with the incre-\nmental inclusion of labeled samples. However, an exception\nto this trend is observed in the Fed-ISIC dataset as shown\nin Fig. 3(a). As observed, the exclusive use of Entropy,\nGradnorm, and CoreSet with a single model, whether it is a\nglobal (see Fig. 3(a)) or local model (see Fig. 3(b)), results\nin suboptimal performance, leading to a notable decrease in\neffectiveness beginning from the third round. The global\nmodel delivers unreliable uncertainty evaluations, which\nmay result in suboptimal data selection and adversely affect\nthe ability of the model to generalize effectively. Moreover,\nselecting data based on evaluations from the local model can\ncause overfitting to its specific client, negatively impacting\nthe performance. Conversely, methods like Gradnorm ( E)\nand TOD ( E) that combine both global and local models\noften outperform those relying solely on the global model,\nbenefiting from the additional domain-specific knowledge\nof the local model. However, it is important to note that\nwithout proper calibration of the global model, the com-\nbined use of both models does not always guarantee better\nperformance than solely using the local model.\nRemarkably, FEAL consistently outperforms state-of-\nthe-art FAL methods on Fed-ISIC, as shown in Fig. 3(a)-\n(c). This superiority is especially noticeable in the fifth FAL\nround, where FEAL achieves a substantial performance\n11444\nTable 1. Comparison of FAL methods in medical image segmentation. Dice scores for three segmentation datasets are reported. For\nFed-Fundus, Dice scores for both optic disc and optic cup segmentation and their average are presented. GandLstand for sampling solely\nwith the global or local model, while Erepresents sampling with both models. Red and blue highlight the Top-1 and Top-2 results.\nModel MethodFed-Polyp (%) Fed-Prostate (%) Fed-Fundus (%)\nR2 R3 R4 R5 R2 R3 R4 R5 R2 R3 R4 R5\n- Full 78.18 88.02 94.32 / 85.70 (90.01)\n- Random 67.70 72.16 75.58 76.32 80.29 82.70 83.94 84.77 92.30 / 81.41 (86.85) 93.33 / 84.45 (88.89) 94.29 / 84.80 (89.54) 94.46 / 85.05 (89.76)\nGEntropy [50] 67.45 74.65 75.30 76.69 82.17 82.53 84.05 86.10 93.19 / 82.61 (87.90) 93.84 / 84.35 (89.10) 94.27 / 85.34 (89.80) 94.47 / 85.29 (89.88)\nTOD [14] 64.99 74.61 76.24 78.26 80.75 83.48 84.31 85.82 92.70 / 82.49 (87.60) 93.95 / 85.01 (89.48) 94.27 / 85.63 (89.95) 94.71 / 85.58 (90.14)\nGradnorm [58] 69.14 74.58 75.79 78.51 82.10 83.01 84.85 86.02 93.20 / 82.01 (87.60) 94.12 / 84.71 (89.41) 94.33 / 85.38 (89.85) 94.56 / 85.43 (89.99)\nCoreSet [46] 69.50 73.37 76.71 78.18 82.11 83.68 84.56 85.86 93.00 / 83.07 (88.03) 93.90 / 84.75 (89.32) 94.16 / 85.35 (89.75) 94.51 / 85.63 (90.07)\nBADGE [2] 70.09 74.11 76.38 76.55 82.78 83.91 85.39 85.97 93.17 / 82.54 (87.85) 94.07 / 84.46 (89.26) 94.40 / 85.37 (89.89) 94.58 / 85.19 (89.89)\nLEntropy 67.48 73.41 75.07 78.63 81.08 82.22 84.36 85.19 93.19 / 83.22 (88.21) 93.83 / 84.49 (89.16) 94.36 / 84.97 (89.66) 94.63 / 85.68 (90.15)\nTOD [14] 65.95 72.92 75.19 77.97 79.59 83.74 85.50 86.03 92.82 / 82.34 (87.58) 93.98 / 85.00 (89.49) 94.37 / 85.28 (89.83) 94.65 / 85.56 (90.10)\nGradnorm [58] 70.06 74.69 77.25 78.84 80.52 83.43 84.94 86.04 93.29 / 83.04 (88.16) 94.13 / 84.69 (89.41) 94.33 / 85.60 (89.97) 94.42 / 85.53 (89.98)\nCoreSet [46] 68.92 74.06 75.59 77.75 81.49 83.49 84.65 86.19 92.80 / 83.20 (88.00) 93.87 / 84.70 (89.28) 94.28 / 85.42 (89.85) 94.47 / 85.54 (90.00)\nBADGE [2] 70.28 73.96 76.21 77.63 82.07 83.54 85.30 86.06 93.06 / 82.65 (87.85) 93.95 / 84.44 (89.19) 94.34 / 85.02 (89.68) 94.54 / 85.52 (90.03)\nEEntropy [50] 67.85 75.10 76.80 77.20 80.95 83.66 84.81 85.42 93.26 / 82.77 (88.01) 94.04 / 84.69 (89.36) 94.33 / 85.31 (89.82) 94.38 / 85.10 (89.74)\nTOD [14] 67.25 70.43 74.84 77.53 81.45 84.46 84.51 85.65 93.13 / 82.70 (87.92) 93.63 / 84.64 (89.14) 94.31 / 85.30 (89.81) 94.54 / 85.82 (90.18)\nGradnorm [58] 68.01 75.75 77.73 75.67 81.21 83.43 85.30 85.13 93.36 / 83.09 (88.23) 93.83 / 84.91 (89.37) 94.33 / 85.59 (89.96) 94.65 / 85.52 (90.08)\nCoreSet [46] 67.77 74.28 77.69 75.87 81.30 84.52 84.75 86.50 93.24 / 82.55 (87.89) 93.63 / 84.86 (89.24) 94.20 / 85.50 (89.85) 94.62 / 85.89 (90.25)\nBADGE [2] 69.12 75.45 77.37 76.24 81.31 84.34 85.92 85.55 93.37 / 82.95 (88.16) 93.99 / 85.00 (89.50) 94.50 / 85.22 (89.86) 94.62 / 85.44 (90.03)\nLoGo [20] 69.07 75.76 74.63 77.24 82.35 84.56 85.53 85.97 93.14 / 83.01 (88.08) 93.93 / 84.55 (89.24) 94.18 / 85.68 (89.93) 94.61 / 85.64 (90.12)\nKAFAL [3] 69.69 73.83 75.38 77.97 82.65 83.49 85.58 85.96 93.11 / 82.75 (87.93) 94.01 / 84.12 (89.06) 94.37 / 85.16 (89.77) 94.46 / 85.02 (89.74)\nFEAL (Ours) 72.06 76.39 78.62 80.18 82.94 85.29 86.77 87.42 93.53 / 83.72 (88.63) 94.25 / 85.19 (89.72) 94.60 / 85.96 (90.28) 94.89 / 86.27 (90.58)\ngain of 1.62% over the second-best method, CoreSet ( E),\nas demonstrated in Fig. 3(c). Additionally, it is noteworthy\nthat FEAL achieves a performance comparable to training\nwith the fully annotated dataset in the third round and even\nexceeds the fully supervised performance by 0.84% in the\nfifth round. These advancements are primarily attributable\nto the effective uncertainty calibration and demonstrate the\nefficacy of FEAL. It is noteworthy that the baseline meth-\nods KAFAL and LoGo, designed for FAL underperform in\nreal-world federated scenarios. Despite showing impressive\nresults in simulated federated datasets, they fail to repli-\ncate this success in actual multi-center federated scenarios.\nThis is mainly due to the inherent domain shift characteris-\ntics of multi-center medical data. As depicted in Fig. 3(d)-\n(f), FEAL also achieves superior performance on the large-\nscale dataset Fed-Camelyon, where each local client con-\ntains tens of thousands of patches. By employing a low-\ndata regime, where merely about 3.43% of the total train-\ning samples are annotated in the active learning process,\nFEAL attains 99.40% of fully supervised performance af-\nter five rounds of FAL. This achievement represents a sig-\nnificant improvement compared to the second-best method,\nKAFAL, which reaches 98.93% of the fully supervised per-\nformance, demonstrating the effectiveness of uncertainty\ncalibration in FEAL. Additional results with different an-\nnotation budgets/ratios are available in Appendix C.2.\nImage segmentation. To further evaluate the effective-\nness of FEAL in segmentation tasks, we conducted experi-\nments on three real multi-center datasets: Fed-Polyp, Fed-\nProstate, and Fed-Fundus, with the results summarized in\nTab. 1. As can be seen, FEAL exhibits superior perfor-\nmance on three multi-center segmentation datasets, as ev-\nidenced by its higher Dice scores. Specifically, for Fed-\nPolyp, FEAL yields a Dice score of 80.18% in the fifthround, outperforming the second-best method Gradnorm\n(L) by1.34% and surpassing fully-supervised training by\n2.00%. For Fed-Prostate, FEAL demonstrates improve-\nments of 0.85% and0.92% over the second-best method\nin the fourth and fifth FAL rounds, respectively. For Fed-\nFundus, FEAL not only surpasses other methods in seg-\nmenting both the optic disc and optic cup but also outper-\nforms fully supervised training in the fourth and fifth rounds\nof FAL. Complete results including HD95 and standard de-\nviation are available in Appendix C.2.\n4.3. Discussion\nEffect of uncertainty calibration. We conducted exper-\niments on Fed-ISIC to evaluate the effects of different un-\ncertainty combinations: UG\nepi,UG\nale, andUL\nale. As summarized\nin Tab. 2, combining aleatoric uncertainty from both global\nand local models proves more effective than relying on just\none model. The best results are obtained with UG\nepi,UG\nale, and\nUL\nale, showcasing the effectiveness of the proposed uncer-\ntainty calibration. The ablation results on Fed-Polyp are in\nAppendix C.3. Moreover, we visualize the aleatoric uncer-\ntainty in both models on Fed-Polyp in Fig. 4. It is noticeable\nthatUG\naleandUL\nalehighlight different regions, underscoring\nthe importance of combining aleatoric uncertainty in both\nmodels for a more comprehensive assessment.\nTable 2. Ablation study of uncertainty calibration on Fed-ISIC.\nUG\nepiUG\naleUL\naleRound 2 Round 3 Round 4 Round 5\n-✓ -60.61±1.5766.60±0.3367.09±1.0266.57±1.21\n- - ✓ 62.20±3.5666.84±1.9966.13±1.5267.45±0.69\n-✓ ✓ 63.43±1.1167.18±0.5566.58±1.0266.70±0.28\n✓ - - 61.97±1.2565.87±0.5967.09±1.2466.41±1.10\n✓ ✓ -61.95±2.1266.08±0.4067.19±1.0266.85±0.84\n✓ -✓ 61.07±1.2465.17±1.5867.16±0.7365.92±1.95\n✓✓✓ 65.18±0.4167.77±1.3168.41±1.0168.46±0.37\n11445\nInputGround truth𝑈!"#$𝑈!"#%𝑈!"#$+𝑈!"#%Figure 4. Visualization of aleatoric uncertainty on Fed-Polyp. UG\nale\nandUL\naledenote the aleatoric uncertainty in the global and local\nmodels, respectively.\nEffect of diversity relaxation. We analyzed the impact of\nhyperparameters, i.e.minimum neighbor size nand sim-\nilarity threshold τ, on Fed-ISIC. As depicted in Fig. 5(a),\neliminating diversity relaxation (‘w/o relaxation’) results in\na notable reduction in BMA in the fifth round, and the best\nperformance is achieved with n=5andτ=0.85. The abla-\ntion results on Fed-Polyp are reported in Appendix C.3.\n1 2 3 4 5\nAL round52.555.057.560.062.565.067.5BMA (%)\nw/o relaxation\n3 neighbors\n5 neighbors (Ours)\n7 neighbors\n9 neighbors\n(a) Minimum neighbor size n\n1 2 3 4 5\nAL round52.555.057.560.062.565.067.5BMA (%)\n=0.75\n=0.80\n=0.85 (Ours)\n=0.90\n (b) Similarity threshold τ\nFigure 5. Ablation study of diversity relaxation on Fed-ISIC.\nEffect of evidential model training. We performed exper-\niments to compare the evidential loss ( Lin Eq. 9) against\ncross-entropy loss (CE) on Fed-ISIC and against dice loss\n(Dice) on Fed-Polyp. The results are detailed in Tab. 3. As\ncan be seen, training with evidential loss results in an aver-\nage performance gain of 1.03% on Fed-ISIC and 1.16% on\nFed-Polyp, respectively. This improvement can be primar-\nily attributed to evidence regularization, demonstrating the\nefficacy of evidential model training. The ablation results\non the other three datasets are available in Appendix C.3.\nTable 3. Ablation study of loss function.\nDataset Loss Round 2 Round 3 Round 4 Round 5\nFed-ISICCE 64.28±1.6466.69±0.9567.32±1.1667.40±0.22\nL 65.18±0.4167.77±1.3168.41±1.0168.46±0.37\nFed-PolypDice 70.14±0.1075.77±0.6777.23±0.2179.48±0.62\nL 72.06±0.7276.39±0.6678.62±1.4480.18±0.10\nEffect of trade-off weight λ. We further de-\ntermined the optimal setting for the hyperparame-\nterλon Fed-ISIC, choosing from the candidate set\n{1e−3,5e−3,1e−2,5e−2,1e−1}, the results are detailed\nin Tab. 4. As can be seen, the best performance is achieved\nwhen λ= 1e−2. The ablation results on Fed-Polyp are\nreported in Appendix C.3.\nAnalysis of Dirichlet simplex. We analyze the Dirich-\nlet simplex on a subset of Fed-ISIC encompassing threeTable 4. Ablation study of trade-off weight λon Fed-ISIC.\nλ Round 2 Round 3 Round 4 Round 5\n1e−3 63.49±3.00 64.57±2.70 66.25±1.17 65.45±1.06\n5e−3 63.10±2.07 65.79±2.57 66.00±2.09 66.48±0.86\n1e−2 65.18±0.41 67.77±1.31 68.41±1.01 68.46±0.37\n5e−2 62.12±0.99 67.21±1.42 66.92±0.70 66.90±0.93\n1e−1 63.53±2.03 66.21±0.35 66.03±2.34 67.78±1.17\nclasses. As illustrated in Fig. 6, when selecting samples\nwith FEAL, the Dirichlet distribution becomes more con-\ncentrated at the simplex’s corner for unlabeled samples, in-\ndicating reduced epistemic uncertainty in the global model.\nThis trend verifies the effectiveness of CES in addressing\ndomain shifts. Additionally, starting with an identical set\nof labeled samples, we tracked the selection of samples in\nthe second FAL round utilizing various FAL methods. The\nDirichlet simplexes of different methods are visualized in\nFig. 7. As can be seen, the Dirichlet distribution of sam-\nples selected by FEAL showcases a broader spread across\nthe simplex, indicating that FEAL effectively models the\nglobal model’s knowledge of local data and prioritizes se-\nlecting samples characterized by high epistemic uncertainty.\nMore details and results are available in Appendix C.3.\n𝑈!"#(𝒙,𝜽)=−4.03\nMELBCCBKLRound 1\n𝑈!"#(𝒙,𝜽)=−4.42\nMELBCCBKLRound 2\n𝑈!"#(𝒙,𝜽)=−5.03\nMELBCCBKLRound 3\nMELBCCBKL𝑈!"#(𝒙,𝜽)=−5.55Round 4\nMELBCCBKL𝑈!"#(𝒙,𝜽)=−6.22Round 5\nMELBCCBKLEntropy\nMELBCCBKLCoreSet\nMELBCCBKLLoGo\nMELBCCBKLKAFAL\nMELBCCBKLOurs\nMELBCCBKL\nMELBCCBKLBKLBKLBKLMELBCCMELBCCMELBCC\nBKLMELBCCBKLMELBCCBKLMELBCCBKLMELBCCBKLMELBCC\nFigure 6. Visualization of the Dirichlet simplex for unlabeled sam-\nples across five FAL rounds using FEAL.\nMELBCCBKLEntropy\nMELBCCBKLCoreSet\nMELBCCBKLLoGo\nMELBCCBKLKAFAL\nMELBCCBKLOurs𝑈!"#(𝒙,𝜽)=−4.03\nMELBCCBKLRound 1\n𝑈!"#(𝒙,𝜽)=−4.42\nMELBCCBKLRound 2\n𝑈!"#(𝒙,𝜽)=−5.03\nMELBCCBKLRound 3\nMELBCCBKL𝑈!"#(𝒙,𝜽)=−5.55Round 4\nMELBCCBKL𝑈!"#(𝒙,𝜽)=−6.22Round 5\nFigure 7. Visualization of the Dirichlet simplex for samples se-\nlected in the second FAL round using various sampling strategies.\n5. Conclusion and Social Impact\nTo address the challenge of unreliable data assessment us-\ning the global model under domain shifts, we proposed a\nmethod FEAL, which places a Dirichlet prior over categori-\ncal probabilities to treat the prediction as a distribution over\nthe probability simplex and leverages both aleatoric uncer-\ntainty and epistemic uncertainty to calibrate the uncertainty\nevaluation, enhancing the reliability of data assessment and\nincorporating diversity relaxation to maintain sample diver-\nsity. Extensive results verify the effectiveness. This work\nholds the potential to advance healthcare by preserving data\nprivacy and facilitating collaborative research, ultimately\nleading to more accessible and effective patient care.\n11446\nReferences\n[1] Jin-Hyun Ahn, Kyungsang Kim, Jeongwan Koh, and\nQuanzheng Li. Federated active learning (f-al): an efficient\nannotation strategy for federated learning. arXiv preprint\narXiv:2202.00195 , 2022. 2, 3\n[2] Jordan T Ash, Chicheng Zhang, Akshay Krishnamurthy,\nJohn Langford, and Alekh Agarwal. Deep batch active learn-\ning by diverse, uncertain gradient lower bounds. In ICLR ,\n2020. 3, 6, 7\n[3] Yu-Tong Cao, Ye Shi, Baosheng Yu, Jingya Wang, and\nDacheng Tao. Knowledge-aware federated active learning\nwith non-iid data. In ICCV , pages 22279–22289, 2023. 2, 3,\n6, 7\n[4] Razvan Caramalau, Binod Bhattarai, and Tae-Kyun Kim. Se-\nquential graph convolutional network for active learning. In\nCVPR , pages 9583–9592, 2021. 2, 3\n[5] Bill Cassidy, Connah Kendrick, Andrzej Brodzicki, Joanna\nJaworek-Korjakowska, and Moi Hoon Yap. Analysis of the\nisic image datasets: Usage, benchmarks and recommenda-\ntions. Med. Image. Anal. , 75:102305, 2022. 5\n[6] Zhen Chen, Meilu Zhu, Chen Yang, and Yixuan Yuan. Per-\nsonalized retrogress-resilient framework for real-world med-\nical federated learning. In MICCAI , pages 347–356, 2021.\n2\n[7] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In CVPR , pages 248–255. IEEE, 2009. 6\n[8] Ehsan Elhamifar, Guillermo Sapiro, Allen Yang, and\nS Shankar Sasrty. A convex optimization framework for ac-\ntive learning. In ICCV , pages 209–216, 2013. 3\n[9] Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian\napproximation: representing model uncertainty in deep\nlearning. In ICML , 2016. 3\n[10] Yarin Gal, Riashat Islam, and Zoubin Ghahramani. Deep\nbayesian active learning with image data. In ICML , 2017. 3\n[11] Mingfei Gao, Zizhao Zhang, Guo Yu, Sercan ¨O Arık,\nLarry S Davis, and Tomas Pfister. Consistency-based semi-\nsupervised active learning: Towards minimizing labeling\ncost. In ECCV , pages 510–526, 2020. 3\n[12] Yongxin Guo, Xiaoying Tang, and Tao Lin. Fedbr: improv-\ning federated learning on heterogeneous data via local learn-\ning bias reduction. In ICML , pages 12034–12054, 2023. 2\n[13] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil-\nian Q Weinberger. Densely connected convolutional net-\nworks. In CVPR , pages 4700–4708, 2017. 6\n[14] Siyu Huang, Tianyang Wang, Haoyi Xiong, Jun Huan, and\nDejing Dou. Semi-supervised active learning with temporal\noutput discrepancy. In ICCV , pages 3447–3456, 2021. 2, 3,\n6, 7\n[15] Wenke Huang, Mang Ye, Zekun Shi, He Li, and Bo Du. Re-\nthinking federated learning with domain shift: A prototype\nview. In CVPR , pages 16312–16322. IEEE, 2023. 2\n[16] Meirui Jiang, Zirui Wang, and Qi Dou. Harmofl: Harmoniz-\ning local and global drifts in federated learning on heteroge-\nneous medical images. In AAAI , pages 1087–1095, 2022. 5,\n6[17] Meirui Jiang, Holger R Roth, Wenqi Li, Dong Yang, Can\nZhao, Vishwesh Nath, Daguang Xu, Qi Dou, and Ziyue Xu.\nFair federated medical image segmentation via client contri-\nbution estimation. In CVPR , pages 16302–16311, 2023. 2\n[18] Audun Jøsang. Subjective logic . 2016. 4\n[19] Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri,\nSashank Reddi, Sebastian Stich, and Ananda Theertha\nSuresh. Scaffold: Stochastic controlled averaging for fed-\nerated learning. In ICML , pages 5132–5143, 2020. 2\n[20] SangMook Kim, Sangmin Bae, Hwanjun Song, and Se-\nYoung Yun. Re-thinking federated active learning based on\ninter-class diversity. In CVPR , pages 3944–3953, 2023. 2, 3,\n6, 7\n[21] Jakub Kone ˇcn`y, H Brendan McMahan, Daniel Ramage, and\nPeter Richt ´arik. Federated optimization: Distributed ma-\nchine learning for on-device intelligence. arXiv preprint\narXiv:1610.02527 , 2016. 1\n[22] Solomon Kullback and Richard A Leibler. On information\nand sufficiency. The annals of mathematical statistics , 22(1):\n79–86, 1951. 5\n[23] Natsumaro Kutsuna, Takumi Higaki, Sachihiro Matsunaga,\nTomoshi Otsuki, Masayuki Yamaguchi, Hirofumi Fujii, and\nSeiichiro Hasezawa. Active learning framework with itera-\ntive clustering for bioimage classification. Nat. Commun. , 3\n(1):1032, 2012. 3\n[24] Haohan Li and Zhaozheng Yin. Attention, suggestion and\nannotation: a deep active learning framework for biomedical\nimage segmentation. In MICCAI , pages 3–13, 2020. 3\n[25] Hao Li, Yang Nan, Javier Del Ser, and Guang Yang. Region-\nbased evidential deep learning to quantify uncertainty and\nimprove robustness of brain tumor segmentation. Neural\nComput. Appl. , pages 1–15, 2022. 5\n[26] Qinbin Li, Bingsheng He, and Dawn Song. Model-\ncontrastive federated learning. In CVPR , pages 10713–\n10722, 2021. 2\n[27] Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi,\nAmeet Talwalkar, and Virginia Smith. Federated optimiza-\ntion in heterogeneous networks. MLSys , 2:429–450, 2020.\n2\n[28] Xiaoxiao Li, Meirui Jiang, Xiaofei Zhang, Michael Kamp,\nand Qi Dou. Fedbn: Federated learning on non-iid\nfeatures via local batch normalization. arXiv preprint\narXiv:2102.07623 , 2021. 2\n[29] Zexi Li, Tao Lin, Xinyi Shang, and Chao Wu. Revisiting\nweighted aggregation in federated learning with neural net-\nworks. arXiv preprint arXiv:2302.10911 , 2023. 2\n[30] Quande Liu, Cheng Chen, Jing Qin, Qi Dou, and Pheng-Ann\nHeng. Feddg: Federated domain generalization on medical\nimage segmentation via episodic learning in continuous fre-\nquency space. In CVPR , pages 1013–1023, 2021. 2, 5, 6\n[31] Weitang Liu, Xiaoyun Wang, John Owens, and Yixuan Li.\nEnergy-based out-of-distribution detection. NeurIPS , 33:\n21464–21475, 2020. 2\n[32] Benteng Ma, Yu Feng, Geng Chen, Changyang Li, and Yong\nXia. Federated adaptive reweighting for medical image clas-\nsification. Pattern Recogn. , 144:109880, 2023. 2\n11447\n[33] Benteng Ma, Jing Zhang, Yong Xia, and Dacheng Tao. Vnas:\nVariational neural architecture search. Int. J. Comput. Vis. ,\n2024. 2\n[34] James MacQueen et al. Some methods for classification\nand analysis of multivariate observations. In Proceedings of\nthe fifth Berkeley symposium on mathematical statistics and\nprobability , pages 281–297, 1967. 3\n[35] Andrey Malinin and Mark Gales. Predictive uncertainty es-\ntimation via prior networks. NeurIPS , 31, 2018. 4\n[36] Brendan McMahan, Eider Moore, Daniel Ramage, Seth\nHampson, and Blaise Aguera y Arcas. Communication-\nefficient learning of deep networks from decentralized data.\nInAISTATS , pages 1273–1282, 2017. 1\n[37] Robert Munro Monarch. Human-in-the-Loop Machine\nLearning: Active learning and annotation for human-\ncentered AI . Simon and Schuster, 2021. 2, 3\n[38] Hieu T Nguyen and Arnold Smeulders. Active learning using\npre-clustering. In ICML , page 79, 2004. 3\n[39] Jean Ogier du Terrail, Samy-Safwan Ayed, Edwige Cyffers,\nFelix Grimberg, Chaoyang He, Regis Loeb, Paul Mangold,\nTanguy Marchand, Othmane Marfoq, Erum Mushtaq, et al.\nFlamby: Datasets and benchmarks for cross-silo federated\nlearning in realistic healthcare settings. NeurIPS , 35:5315–\n5334, 2022. 5\n[40] Deep Shankar Pandey and Qi Yu. Learn to accumulate ev-\nidence from all training samples: Theory and practice. In\nICML , pages 26963–26989, 2023. 5\n[41] Amin Parvaneh, Ehsan Abbasnejad, Damien Teney, Gholam-\nreza Reza Haffari, Anton van den Hengel, and Javen Qinfeng\nShi. Active learning by feature mixing. In CVPR , pages\n12237–12246, 2022. 2, 3\n[42] Tim Pearce. Uncertainty in neural networks; bayesian en-\nsembles, priors & prediction intervals . PhD thesis, Univer-\nsity of Cambridge, 2020. 2\n[43] Tim Pearce, Alexandra Brintrup, and Jun Zhu. Under-\nstanding softmax confidence and uncertainty. arXiv preprint\narXiv:2106.04972 , 2021. 2\n[44] Yasar Abbas Ur Rehman, Yan Gao, Pedro Porto Buarque\nDe Gusm ˜ao, Mina Alibeigi, Jiajun Shen, and Nicholas D\nLane. L-dawa: Layer-wise divergence aware weight ag-\ngregation in federated self-supervised visual representation\nlearning. In ICCV , pages 16464–16473, 2023. 2\n[45] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:\nConvolutional networks for biomedical image segmentation.\nInMICCAI , pages 234–241, 2015. 6\n[46] Ozan Sener and Silvio Savarese. Active learning for convolu-\ntional neural networks: A core-set approach. arXiv preprint\narXiv:1708.00489 , 2017. 2, 3, 6, 7\n[47] Murat Sensoy, Lance Kaplan, and Melih Kandemir. Evi-\ndential deep learning to quantify classification uncertainty.\nNeurIPS , 31, 2018. 2, 4, 5\n[48] Kari Sentz and Scott Ferson. Combination of evidence in\ndempster-shafer theory. 2002. 4\n[49] Burr Settles. Active learning literature survey. 2009. 3\n[50] Claude Elwood Shannon. A mathematical theory of commu-\nnication. Bell Syst. Tech. J. , 27(3):379–423, 1948. 3, 4, 6,\n7[51] Maohao Shen, Yuheng Bu, Prasanna Sattigeri, Soumya\nGhosh, Subhro Das, and Gregory Wornell. Post-hoc uncer-\ntainty learning using a dirichlet meta-model. In AAAI , pages\n9772–9781, 2023. 2\n[52] Neta Shoham, Tomer Avidor, Aviv Keren, Nadav Israel,\nDaniel Benditkis, Liron Mor-Yosef, and Itai Zeitak. Over-\ncoming forgetting in federated learning on non-iid data.\narXiv preprint arXiv:1910.07796 , 2019. 2\n[53] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model\nscaling for convolutional neural networks. In ICML , pages\n6105–6114, 2019. 6\n[54] Dennis Ulmer, Christian Hardmeier, and Jes Frellsen. Prior\nand posterior networks: A survey on evidential deep learning\nmethods for uncertainty estimation. TMLR , 2023. 2\n[55] Ruth Urner, Sharon Wulff, and Shai Ben-David. Plal:\nCluster-based active learning. In COLT , pages 376–397,\n2013. 3\n[56] Fan Wang, Zhongyi Han, Zhiyan Zhang, Rundong He, and\nYilong Yin. Mhpl: Minimum happy points learning for ac-\ntive source free domain adaptation. In CVPR , pages 20008–\n20018, 2023. 3\n[57] Jiacheng Wang, Yueming Jin, and Liansheng Wang. Per-\nsonalizing federated medical image segmentation via local\ncalibration. In ECCV , pages 456–472, 2022. 1, 2, 5, 6\n[58] Tianyang Wang, Xingjian Li, Pengkun Yang, Guosheng Hu,\nXiangrui Zeng, Siyu Huang, Cheng-Zhong Xu, and Min Xu.\nBoosting active learning via improving test performance. In\nAAAI , pages 8566–8574, 2022. 6, 7\n[59] Jeffry Wicaksana, Zengqiang Yan, and Kwang-Ting\nCheng. Fca: Taming long-tailed federated medical im-\nage classification by classifier anchoring. arXiv preprint\narXiv:2305.00738 , 2023. 6\n[60] Nannan Wu, Li Yu, Xin Yang, Kwang-Ting Cheng, and\nZengqiang Yan. Fediic: Towards robust federated learning\nfor class-imbalanced medical image classification. In MIC-\nCAI, pages 692–702, 2023. 2\n[61] Xing Wu, Jie Pei, Cheng Chen, Yimin Zhu, Jianjia Wang,\nQuan Qian, Jian Zhang, Qun Sun, and Yike Guo. Federated\nactive learning for multicenter collaborative disease diagno-\nsis.IEEE Trans. Med. Imaging. , 2022. 2, 3\n[62] Mixue Xie, Shuang Li, Rui Zhang, and Chi Harold Liu.\nDirichlet-based uncertainty calibration for active domain\nadaptation. arXiv preprint arXiv:2302.13824 , 2023. 2, 4\n[63] Yi Yang, Zhigang Ma, Feiping Nie, Xiaojun Chang, and\nAlexander G Hauptmann. Multi-class active learning by un-\ncertainty sampling with diversity maximization. Int. J. Com-\nput. Vis. , 113:113–127, 2015. 3\n[64] Mang Ye, Xiuwen Fang, Bo Du, Pong C Yuen, and Dacheng\nTao. Heterogeneous federated learning: State-of-the-art and\nresearch challenges. ACM Comput. Surv. , 56(3):1–44, 2023.\n2\n[65] Donggeun Yoo and In So Kweon. Learning loss for active\nlearning. In CVPR , 2019. 2, 3\n[66] Jiakang Yuan, Bo Zhang, Xiangchao Yan, Tao Chen, Botian\nShi, Yikang Li, and Yu Qiao. Bi3d: Bi-domain active learn-\ning for cross-domain 3d object detection. In CVPR , pages\n15599–15608, 2023. 3\n11448\n[67] Ruipeng Zhang, Qinwei Xu, Jiangchao Yao, Ya Zhang, Qi\nTian, and Yanfeng Wang. Federated domain generalization\nwith generalization adjustment. In CVPR , pages 3954–3963,\n2023. 2\n[68] Tianfei Zhou and Ender Konukoglu. Fedfa: Federated fea-\nture augmentation. arXiv preprint arXiv:2301.12995 , 2023.\n2\n11449'}, 'dist': 0.9286905527114868}
Result 11: {'text': 'data', 'metadata': {'created_at': 1736932570, 'modified_at': 1736932570, 'owner': 'sayandeep', 'path': 'RAG_CONTENT/conferences/cvpr_ocr/Cai_L-MAGIC_Language_Model_Assisted_Generation_of_Images_with_Coherence_CVPR_2024_paper.txt', 'size': 40276, 'seen_at': 1737191136, 'data': 'L-MAGIC: Language ModelAssisted Generation of Images with Coherence\r\nZhipeng Cai*Matthias Mueller Reiner Birkl Diana Wofk Shao-Yen Tseng\r\nJunda Cheng Gabriela Ben-Melech Stan Vasudev Lai Michael Paulitsch\r\nIntel Labs\r\nAbstract\r\nIn the current era of generative AI breakthroughs, gener-\r\nating panoramic scenes from a single input image remains\r\na key challenge. Most existing methods use diffusion-based\r\niterative or simultaneous multi-view inpainting. However,\r\nthe lack of global scene layout priors leads to subpar out-\r\nputs with duplicated objects ( e.g., multiple beds in a bed-\r\nroom) or requires time-consuming human text inputs for\r\neach view. We propose L-MAGIC, a novel method leverag-\r\ning large language models for guidance while diffusing mul-\r\ntiple coherent views of 360\x00panoramic scenes. L-MAGIC\r\nharnesses pre-trained diffusion and language models with-\r\nout ﬁne-tuning, ensuring zero-shot performance. The output\r\nquality is further enhanced by super-resolution and multi-\r\nview fusion techniques. Extensive experiments demonstrate\r\nthat the resulting panoramic scenes feature better scene lay-\r\nouts and perspective view rendering quality compared to re-\r\nlated works, with >70% preference in human evaluations.\r\nCombined with conditional diffusion models, L-MAGIC can\r\naccept various input modalities, including but not limited\r\nto text, depth maps, sketches, and colored scripts. Apply-\r\ning depth estimation further enables 3D point cloud gen-\r\neration and dynamic scene exploration with ﬂuid camera\r\nmotion. Code is available at https://github.com/\r\nZhipengCai/L-MAGIC-code-release .\r\n1. Introduction\r\nDiffusion models have achieved state-of-the-art perfor-\r\nmance in image generation. However, generating a 360\x00\r\npanoramic scene from a single perspective image remains\r\na challenge, which is an important problem in many com-\r\nputer vision applications, such as architecture design, movie\r\nscene creation, and virtual reality (VR).\r\nTraining a model to directly generate panoramic im-\r\nages is challenging due to the lack of diverse large-scale\r\ndatasets. Hence, most existing works separate panoramic\r\nscenes into multiple perspective views, and inpaint them us-\r\n*Corresponding author (zhipeng.cai@intel.com)\r\nL-MAGIC\r\nDepth estimation\r\nFigure 1. Teaser. L-MAGIC is a novel method to generate a 360\x00\r\npanoramic scene from a single input image. L-MAGIC utilizes\r\nlarge language models to control perspective diffusion models to\r\ngenerate multiple views with coherent 360\x00layout. L-MAGIC is\r\nalso compatible with images synthesized by conditional genera-\r\ntive models, making it capable of creating panoramic scenes from\r\nvarious input modalities. A set of perspective images rather than\r\na single panoramic image also allows our method to leverage off-\r\nthe-shelf monocular depth estimation models to enable immersive\r\nexperiences, e.g., scene ﬂy-through or 3D point cloud generation.\r\ning pre-trained diffusion models. To ensure generalization,\r\nthe diffusion model is either frozen without any architec-\r\nture change [ 11] or combined with extra modules trained on\r\nsmall datasets for integrating multi-view information [ 25].\r\nA common approach to encode the scene informa-\r\ntion during multi-view inpainting is to provide a text-\r\nconditioned diffusion model with the description of the in-\r\nput image, which is generated by a user or an image cap-\r\ntioning model [ 13]. Though effective for extending local\r\nscene content, such approaches suffer from incoherence\r\nin the overall scene layout. Speciﬁcally, using the same\r\ntext description for diffusing different views along the 360\x00\r\npanorama leads to artifacts and unnecessarily repeated ob-\r\njects. Current inpainting methods have no mechanism to\r\nleverage global scene information in individual views.\r\nIn this work, we show that state-of-the-art (vision) lan-\r\nguage models, without ﬁne-tuning, can be used to control\r\nmulti-view diffusion and effectively address the above prob-\r\nlem. We propose L-MAGIC (Fig. 1), a novel framework\r\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\r\nExcept for this watermark, it is identical to the accepted version;\r\nthe final published version of the proceedings is available on IEEE Xplore.\r\n7049\r\nleveraging large language models to enable the automatic\r\ngeneration of diverse yet coherent 360\x00views from a given\r\ninput image. L-MAGIC relies on iterative warping-and-\r\ninpainting. Pre-trained (vision) language models are used\r\nto: (1) generate layout descriptions of different views that\r\nare used in text-conditioned inpainting, (2) automatically\r\ndetermine whether salient objects should be repeated or not\r\nfor a speciﬁc scene, and (3) monitor the inpainting outputs\r\nto avoid challenging cases where diffusion models violate\r\nthe text guidance. A key contribution is the prompt de-\r\nsign for language and diffusion models to make L-MAGIC\r\nfully automatic . In addition, smooth multi-view fusion and\r\nsuper-resolution techniques are used to ensure high resolu-\r\ntion and quality when producing the ﬁnal panorama.\r\nExperiments show that L-MAGIC generates 360\x00\r\npanoramic scenes with higher quality and more coherent\r\nlayouts compared to state-of-the-art methods. Not rely-\r\ning on model ﬁne-tuning makes L-MAGIC effective on in-\r\nthe-wild images, and extendable, using conditional diffu-\r\nsion models [ 19,29], to other types of inputs such as text,\r\ndepth maps, sketch drawings and color scripts/segmentation\r\nmasks. Applying depth estimation further enables the cre-\r\nation of 3D point clouds and immersive scene ﬂy-through\r\nexperiences with both camera rotation and translation.\r\n2. Related Work\r\nDiffusion models. Diffusion models learn to generate data\r\nby inverting the diffusion process, i.e., removing the noise\r\nin the data (see [ 26] for a detailed survey). By separating the\r\ndata generation process into multiple noise removal steps\r\n(reverse process) [ 10], diffusion models learn image syn-\r\nthesis much more effectively than GANs [ 9]. Recently, la-\r\ntent diffusion models [ 19] have been proposed to improve\r\nthe training speed and image synthesis quality by perform-\r\ning the diffusion in latent space. By training the model on\r\nlarge-scale image-caption pairs [ 21], they achieve remark-\r\nable quality and robustness in text-conditioned image syn-\r\nthesis. Further ﬁne-tuning of latent diffusion models using\r\nlarge mask strategies [ 24] produces robust text-conditioned\r\ninpainting models, which are used in this work.\r\nPanoramic scene generation. Various approaches have\r\nbeen proposed for panoramic scene generation. Some of\r\nthem [ 8,22] treat the panorama as a single equirectangular\r\nimage, and generate it in a single forward pass. However,\r\nsuch approaches struggle to close the loop at both ends of\r\nthe generated equirectangular image, even with purposely\r\ndesigned spherical positional embeddings [ 8]. Meanwhile,\r\nthe lack of large scale training data makes it impractical\r\nto train a generalizable model for image-to-panorama, and\r\nlimits the robustness of the model, resulting in inconsistent\r\noutputs with the input descriptions.\r\nSome recent methods [ 11,25] create panoramas by gen-erating multiple perspective views using robust pre-trained\r\ndiffusion models trained on large-scale perspective data.\r\nText2room [ 11] generates 10 rotated views of a panorama\r\nusing stable diffusion v2 inpainting without layout control\r\nand focuses on indoor scenes and mesh generation. MVD-\r\niffusion [ 25] ensures multi-view local texture consistency\r\nby extra multi-view attention modules ﬁne-tuned on small\r\ndatasets. Though applicable to both text-to-panorama and\r\nimage-to-panorama, these methods struggle to generate di-\r\nverse 360\x00views. Speciﬁcally, there is no mechanism to\r\nencode the global scene layout into the generation or in-\r\npainting of different views. Hence, conditioning the method\r\nonly on the input image or text results in salient objects\r\n(e.g., beds in a bedroom) being generated repeatedly across\r\nviews. In this work, we guide the multi-view diffusion pro-\r\ncess with large language models to automatically generate\r\npanoramic scenes with coherent and diverse 360\x00layouts.\r\nLanguage models. Recent language model advancements\r\nhave enabled many important applications (see [ 30] for\r\ndetails). Trained on large-scale data with humans in the\r\nloop, ChatGPT [ 4] has demonstrated super-human perfor-\r\nmance on various language-based tasks. In this work, we\r\nutilize ChatGPT to automatically generate coherent multi-\r\nview scene descriptions for the consumption of a pre-trained\r\ndiffusion model that generates multiple perspective views\r\nof a panoramic scene. Leveraging multi-modal data, vision\r\nlanguage models [ 28] have further enhanced language mod-\r\nels to understand visual inputs. In this work, we utilize pre-\r\ntrained VQA models [ 13] to automatically generate a scene\r\ndescription for the input image, and to avoid unnecessarily\r\nrepeated objects across the generated panoramic scene.\r\n3. Methdology\r\nThe goal of this work is to generate a coherent 360\x00\r\npanoramic scene given a single (perspective) image. Note\r\nthat this setting is very general since the input image could\r\nbe either captured in the real world or synthesized. For ex-\r\nample using conditional diffusion models [ 19,29], one can\r\nsynthesize the input image using inputs such as text descrip-\r\ntions, sketches, depth images, and so on.\r\nAs shown in Fig. 2, L-MAGIC generates the panoramic\r\nscene with iterative warping-and-inpainting. The warping\r\nstep generates an incomplete perspective view and the mask\r\nof the missing region (Sec. 3.1). The inpainting step com-\r\npletes the masked region with assistance from language\r\nmodels (Sec. 3.2). The ﬁnal panorama is created by fusing\r\nthe generated views with some post-processing to enhance\r\nthe quality and resolution (Sec. 3.3).\r\n3.1. Warping\r\nAt each warping step, we project all completed views onto a\r\nunit sphere representing the panoramic scene, and then ren-\r\n7050\r\nCaptureorWarp\r\n1.Scene layout in each view? 2.Avoid repeated objects?\r\nMerge\r\nNovel views/video3DRenderDepth estimationConditional diffusion\r\noror\r\n…\r\n+Diffusion InpaintLLMs\r\nA living room\r\nFigure 2. L-MAGIC pipeline. The input is an image Ieither captured in the real-world or synthesized, e.g., by conditional diffusion\r\nmodels. Multiple novel views to compose a 360\x00panoramic scene are generated by iterative warping and inpainting. Pre-trained diffusion\r\nmodels assisted by pre-trained language models are used to generate views with both high-quality local textures and coherent 360\x00layouts.\r\nFurther quality enhancement techniques ensure smooth blending of multiple views into high-resolution panoramic scenes. L-MAGIC can\r\ngenerate panorama images, immersive videos, and 3D point clouds from various types of inputs, such as images, text, and sketch drawings.\r\nder the next incomplete view to inpaint based on the relative\r\ncamera pose. To project an image to the unit sphere, we ﬁrst\r\nconstruct a mesh by deﬁning the vertices Von each image\r\npixel and creating edges Ebetween adjacent pixels. Then,\r\nwe project the vertices to a unit sphere by\r\nvsp=K\x001v\r\n||K\x001v||, (1)\r\nwhere Kis the intrinsic matrix, v2V is the homoge-\r\nneous coordinate [ 23] of a pixel, and vspis the projected\r\nlocation. To warp a completed view Ato a novel view\r\nB, where Ris the rotation matrix from AtoB, we ro-\r\ntate each projected vertex vspofAbyvrot=Rvsp, and\r\nthen perform rasterization-based rendering [ 18](I,M)=\r\nrasterize (Vrot,E,K0)where Vrotis the set of rotated vertices\r\nandK0is the intrinsic matrix of image B. The output Iis a\r\nwarped image and Mis a binary mask indicating whether\r\ninpainting is required for a pixel (obtained by checking for\r\neach pixel whether ray-casting hits a valid mesh face).\r\nTo ensure the local inpainting consistency of each\r\nperspective view, we use a large ﬁeld of view (FoV) and\r\nadjust the rotation angles so that both known and unknown\r\nregions are reasonably large after warping. In practice, a\r\nFoV of 100 degrees with roughly 40 degrees of rotation\r\nbetween adjacent views works well. To further reduce\r\nthe iterative error accumulation, we expand the scene\r\nalternatively from both sides of the input image, rather\r\nthan expanding in a single direction. To ensure a smooth\r\n360\x00loop closure, we tune the rotation angles so that\r\nthe ﬁnal view has a large incomplete region at the center,\r\nresulting in a sequence of views with rotation angles of\r\n{0\x00,41\x00,\x0041\x00,82\x00,\x0082\x00,123\x00,200.5\x00(for loop closure) }.\r\n3.2. Inpainting with Language Model Assistance\r\nThe inpainting step completes a warped view with a con-\r\nsistent local style and a coherent 360\x00scene layout. Weutilize the Stable Diffusion v2 inpainting model [ 19], which\r\ncan effectively extrapolate the large missing region of each\r\nwarped view while maintaining local style consistency.\r\nHowever, naive inpainting without any prior will generate\r\nsevere artifacts. One common prior explored before [ 11,25]\r\nis a user-provided text description of the scene or input\r\nimage. Yet, using the same description in different views\r\nmay generate duplicate objects such as multiple beds in a\r\nbedroom (see Fig. 4), since perspective inpainting methods\r\nhave no mechanism to split the layout into different views.\r\nPlease refer to Sec. 4.3for a detailed ablation study.\r\nTo address these problems, we use a vision language\r\nmodel Lv(·)(BLIP-2 [ 13]) and a language model L(·)\r\n(ChatGPT4 [ 4]) to guide the inpainting process. In the fol-\r\nlowing, we describe our method (Alg. 1) in detail. The exact\r\nprompts used in Alg. 1to interact with language and diffu-\r\nsion models are provided in Appendix A.\r\nBefore warping and inpainting, we ﬁrst prompt Lv(·)to\r\ngenerate the description dIfor the input image I(line 2).\r\nWe ask two questions so that dIcontains both coarse and\r\nﬁne levels of detail. Next, we ask L(·)to imagine the global\r\nscene layout d360(line 3) based on dI, where each line of\r\nd360corresponds to the description of a speciﬁc view. To\r\navoid duplicate objects, we request a compact description of\r\nindividual views without mentioning objects in other views.\r\nd360mostly contains objects of individual views. Us-\r\ning such descriptions as the inpainting prompt can lead to\r\ninconsistent style at distant views. Hence, we ask L(·)to\r\nremove objects from dIand obtain the ﬁnal scene-level de-\r\nscription dscene,e.g., ‘a bedroom with a wooden bed’ be-\r\ncomes ‘a bedroom’ (line 4).dscene is later used together\r\nwith d360to ensure a consistent multi-view style.\r\nThough dsceneensures the multi-view style consistency,\r\nthe training data bias of diffusion models may still result in\r\nobjects commonly associated with a particular scene being\r\ngenerated, even if not explicitly mentioned in d360. For ex-\r\n7051\r\nAlgorithm 1 L-MAGIC\r\n1:Input : Initial image I, intrinsic matrix KofI, intrin-\r\nsicsK0and poses P={Ri}N\r\ni=1of warped views, vi-\r\nsion language model Lv(·), language model L(·), text-\r\nconditioned inpainting model finpaint (·).\r\n2:dI L v(I,‘Description of input image’ )\r\n3:d360 L(dI,‘Layout of individual views’ )\r\n4:dscene L(dI,‘Remove object-level descriptions’ )\r\n5:drepeat  L(dI,‘Avoid duplicated objects’ )\r\n6:W1,M1 warp (C,K,K0,R1)\r\n7:I1 finpaint (W1,M1,dscene).expand the FoV of I\r\n8:C  {I 1}\r\n9:fori=2toNdo\r\n10: c 0\r\n11: Wi,Mi warp (C,K,K0,Ri)\r\n12: di generate prompt (d360,dscene,drepeat,i)\r\n13: Ii finpaint (Wi,Mi,di)\r\n14: foreach object Oindrepeat do\r\n15: ifLv(Ii,‘IsOinIi?’)=’yes’Tc<20then\r\n16: c c+1\r\n17: Go to line 13.\r\n18: end if\r\n19: end for\r\n20: C CS{Ii}\r\n21:end for\r\n22:return merge( C)\r\nample, a bed is often generated with the word ‘bedroom’\r\nin the prompt, resulting in duplicate beds in multiple views.\r\nTo avoid this problem, we further let L(·)automatically de-\r\ntermine whether there are some objects in the scene that\r\nrequire repetition avoidance (line 5).\r\nAfter each warping step, we use the outputs from\r\nlines 2to5to automatically generate the prompt for text-\r\nconditioned inpainting (line 12). Speciﬁcally, for the\r\nwarped view with 0\x00rotation (i = 1), we use dsceneas the\r\nprompt ( di) for text-conditioned inpainting (line 7). For\r\nother views (line 13), if there is no object in drepeat,i.e., no\r\nrepetition avoidance required, we perform inpainting with\r\nthe prompt ‘a peripheral view of <dscene>where we see\r\n<the corresponding description in d360>’. If any object\r\nexists in drepeat, we use the positive prompt of ‘a peripheral\r\nview of <dscene>where we only see <the corresponding\r\ndescription in d360>’, and the negative prompt of ‘any type\r\nof<the object in drepeat>’(one sentence for each object in\r\ndrepeat). The positive prompt template prevents Stable Dif-\r\nfusion from generating common objects of an environment\r\n(e.g., the bed in a bedroom). The negative prompt template\r\navoids duplication of objects mentioned in drepeat.\r\nBias exists in the training data of diffusion models —\r\nan image with the caption of ‘a bedroom’ mostly contains\r\na bed. Therefore, repeated objects can still be generatedeven with constraints from the prompt. To further alleviate\r\nthis problem, we use Lv(·)to check whether each inpainted\r\nimage Iicontains objects mentioned in drepeat (line 15). If\r\nthe answer is ‘yes’, we re-run inpainting until the answer\r\nbecomes ‘no’ or the maximum number of trials cis reached.\r\n3.3. Quality and Resolution Enhancement\r\nSeveral techniques are also proposed to enhance the quality\r\nand resolution of the ﬁnal panorama.\r\nAs shown in Appendix. B, adjacent pixels at the center\r\nof an image have a larger angular distance than the ones at\r\nthe side of an image. When warping a completed view to\r\na novel view, the original central region becomes the side\r\nregion, making the rendered image blurry due to interpola-\r\ntion. Meanwhile, the resolution of the Stable Diffusion out-\r\nput is 512⇤512, the panorama created by these images has a\r\nlow resolution. To address both problems, we apply super-\r\nresolution [ 3] to the output Iiof each inpainting step, in-\r\ncreasing the resolution of Iito2048 ⇤2048. Then, we warp\r\nthe high-resolution image to a low-resolution novel view so\r\nthat no (strong) interpolation is required. After performing\r\nall warping and inpainting steps, we simply fuse the super-\r\nresolution images to generate a high-resolution panorama.\r\nDuring warping and panorama generation, multiple per-\r\nspective images might have overlaps at the same region. To\r\navoid sharp boundaries when merging them, we perform a\r\nweighted average, i.e., given multiple warped pixels at the\r\nsame location with colors ci, the ﬁnal merged pixel color\r\niscmerge =P\r\niwiciP\r\niwi, where the weight wiis computed as\r\nthe distance to the nearest image boundary at the original\r\nview i. This strategy effectively down-weights the pixels\r\nnear the warping boundaries, ensuring a smooth transition\r\nduring multi-view fusion.\r\nTo create the ﬁnal panorama (line 22), we ﬁrst project\r\neach view to the unit sphere same as in Sec. 3.1. Then, we\r\nperform the equirectangular projection [ 5] to warp multi-\r\nple projected views to the same equirectangular plane, and\r\nmerge them into a single equirectangular image.\r\n3.4. Discussion\r\nL-MAGIC is fully automatic — no human interaction is re-\r\nquired to link language models and diffusion models. This\r\nis realized by 1) careful prompt engineering, which enables\r\nlanguage models to output texts that can be automatically\r\nconverted into the inpainting prompt, and 2) handling the\r\nedge cases where language models or diffusion models do\r\nnot generate outputs that satisfy the requirements in the in-\r\nput prompt. For example, ChatGPT sometimes still outputs\r\nthe layout description d360with an erroneous format, mak-\r\ning automatic prompt extraction fail catastrophically. We\r\nautomatically detect such cases, and re-run line 3to ensure\r\nthe algorithm ﬂow, see Appendix Afor more details.\r\n7052\r\nL-MAGIC requires no model ﬁne-tuning , which ensures\r\nthe zero-shot performance and makes it capable of accept-\r\ning other types of inputs leveraging conditional generative\r\nmodels (see Sec. 4for results). This advantage also allows\r\nindividual modules to be replaced by future methods to en-\r\nhance the performance, e.g., change BLIP-2+ChatGPT to\r\nGPT-4V [ 1], or use other inpainting models.\r\n4. Experiments\r\nWe describe our experimental setup in Sec. 4.1and com-\r\npare our method with other panorama generation methods\r\nin Sec. 4.2. We then analyze the contribution of individual\r\ncomponents of our methods in Sec. 4.3. Finally, we demon-\r\nstrate several down-stream applications, such as scene ﬂy-\r\nthrough and 3D scene generation in Sec. 4.4.\r\n4.1. Experimental Setup\r\nBaselines. We evaluate our method on both image-to-\r\npanorama and text-to-panorama. For image-to-panorama ,\r\nwe compare against:\r\n1.Stable Diffusion v2 [ 19]: we use the prompt ‘360 de-\r\ngree panorama of <scene description >’ to inpaint the\r\npanorama image in a single diffusion process.\r\n2.Text2room [ 11]: we take the panorama generation com-\r\nponent (at steps 11-20 of the pipeline) for comparison.\r\n3.MVDiffusion image-to-panorama model [ 25].\r\nTo enable text conditioning in these methods, we use BLIP-\r\n2 to obtain the description of the input image. For text-to-\r\npanorama , we compare against:\r\n1.Text2light [ 8]: GAN-based text to panorama model.\r\n2.Stable Diffusion v2 [ 19]: we use the prompt ‘360 de-\r\ngree panorama of <input prompt >’ to generate the\r\npanoramic image in a single diffusion process.\r\n3.LDM3D [ 22] panorama model: we only use the output\r\nRGB image and the prompt ‘360 degree panorama of\r\n<input prompt >’ to generate the panoramic image in a\r\nsingle diffusion process.\r\n4.Text2room panorama generation module [ 11].\r\n5.MVDiffusion text-to-panorama model [ 25].\r\nImplementation Details. L-MAGIC is implemented with\r\nPyTorch [ 15] and the ofﬁcial release of BLIP-2 [ 13], Stable\r\nDiffusion [ 19] and the ChatGPT4 API [ 4]. It takes 2-5 min-\r\nutes to generate a 360\x00panorama depending on the number\r\nof repetitions in line 17of Alg. 1. We take the ofﬁcial code\r\nand model for all other methods. In text-to-panorama, L-\r\nMAGIC uses Stable Diffusion v2 conditioned on the given\r\ntext prompt to generate the input image.\r\nData. To evaluate the in-the-wild performance, we col-\r\nlect data that does not overlap with the training data of any\r\nmethod for both tasks. For image-to-panorama, we use 20\r\nindoor and 20 outdoor images from tanks-and-temples [ 12]\r\nand RealEstate10K [ 31]. For text-to-panorama, we useChatGPT to generate 20 random scene descriptions (10 in-\r\ndoor and 10 outdoor, see Appendix C).\r\nMetrics. To evaluate different methods with respect to\r\nquality and multi-view diversity, we compute the Inception\r\nScore (IS) [ 20] for the perspective views of the panorama.\r\nSince existing quantitative metrics do not capture all as-\r\npects of human perception of quality [ 7], we follow existing\r\nworks [ 8,11,14] for a complementary human evaluation.\r\nTo this end, we set up a voting web page that shows side-by-\r\nside panoramic scenes generated using the same input, one\r\nwith our method and one with a baseline. We ask 15 anony-\r\nmous voters to choose which panorama has higher quality\r\nand scene structure (see Appendix Dfor an example voting\r\npage). To minimize voting bias, we randomly shufﬂe the or-\r\nder of the side-by-side panoramas and hide the generation\r\nmethod names. We use the votes to compute a preference\r\nscore from 0 to 1 for our method compared to the baselines.\r\nThis score is simply the percentage of votes for our method\r\nwith respect to quality and structure.\r\n4.2. Main Results\r\nAs shown in Fig. 3, our method performs better for both\r\nimage-to-panorama and text-to-panorama, even compared\r\nto task-speciﬁc methods. To further understand the perfor-\r\nmance of different methods, we provide in Fig. 4and5the\r\nqualitative results for both tasks. Stable Diffusion v2 treats\r\na panorama as a single image. It cannot close the 360\x00\r\nloop since equirectangular projection [ 5] splits the loop-\r\nclosure area to two sides of an image (moved to the middle\r\nof the rendered panorama for better visualization). In the\r\nmeantime, due to the lack of large-scale panorama training\r\ndata, it still generates unnecessarily repeated objects such\r\nas multiple beds in a bedroom. Text2room andMVDiffu-\r\nsion separate a panorama into multiple perspective views.\r\nInpainting them using the same prompt results in unreason-\r\nably repeated objects in multiple views. Due to the limited\r\npanorama training data, Text2light cannot fully understand\r\nzero-shot scene descriptions generated by ChatGPT, result-\r\ning in scenes not consistent with the input prompt. Similar\r\nto Stable Diffusion v2, treating panorama as a single image\r\nmakes it fail on loop closure. LDM3D is ﬁne-tuned on top\r\nof a perspective latent diffusion model. Though better than\r\nText2light, it still cannot close the loop and sometimes fails\r\nto generate scenes that are consistent with the details of the\r\nprompt ( e.g., generating a non-modern living room when\r\nasked for a modern one). Our method works robustly on\r\nvarious inputs, generating panoramic scenes with high per-\r\nspective rendering quality and reasonable 360\x00scene lay-\r\nouts (see supplementary videos for more details).\r\n4.3. Analysis\r\nWe further analyze different components of our method in\r\nthis section. The analysis is conducted from 3 aspects: 1)\r\n7053\r\n0%10%20%30%40%50%60%70%80%90%100%\r\nStable Diffusionv2Text2roomMVDiffusionPreference on our method Image-to-panoramaQualityLayout\r\n0%10%20%30%40%50%60%70%80%90%100%\r\nText2lightStableDiffusion v2LDM3DText2roomMVDiffusionPreference on our method Text-to-panoramaQualityLayout\r\n(a) Human evaluations.(b) Algorithmic evaluations\r\nFigure 3. Quantitative results for image-to-panorama and text-to-panorama. (a) Human evaluations. Each baseline has two bars\r\nrepresenting respectively the quality of rendered perspective views and the 360\x00layout. The value of the bar means the frequency where\r\nour method is preferred in the voting. Above 50%(dashed line) means our method is more preferred than the corresponding baseline. (b)\r\nAlgorithmic evaluation by computing the Inception Score (IS). L-MAGIC consistently outperforms previous methods on both metrics.\r\nStable Diffusion v2Text2roomMVDiffusionOursFigure 4. Image-to-panorama visualizations. Stable Diffusion v2 cannot close the 360\x00loop (sharp boundaries at the middle). Text2room\r\nand MVDiffusion lack mechanisms to avoid duplicate objects. L-MAGIC outputs have high local view quality and coherent scene layouts.\r\nthe scene prior, 2) the inpainting method and 3) quality en-\r\nhancement techniques. For each aspect, we remove a com-\r\nponent or replace it with other methods, and perform the\r\nsame evaluation as in the main experiments. We use the\r\nsame data used in the image-to-panorama main experiment\r\nfor analysis. The results are reported in Fig. 6. Please refer\r\nto Appendix Efor the visualization comparisons.\r\nFor the scene prior , we remove the prior from chatGPT\r\n(no GPT) and all text guidance (no prompt) respectively.\r\nWithout the global layout prior from chatGPT, the structure\r\nof the outputs becomes worse. Without any prompt guid-\r\nance, both the scene layout and the perspective view ren-dering quality degrade severely.\r\nFor the inpainting method , we replace the Stable Diffu-\r\nsion v2 model with 3 state-of-the-art text-conditioned in-\r\npainting methods, namely, Blended Latent Diffusion (BLD\r\ninpaint) [ 6], Deep Floyd (DF inpaint) [ 2] and Stable Diffu-\r\nsion XL (SDXL inpaint) [ 16]. Interestingly, though some\r\nof the methods [ 16] are published later than Stabld Dif-\r\nfusion v2, their capacity to perform large mask inpainting\r\nis limited on in-the-wild images, resulting in worse perfor-\r\nmance in terms of both scene layouts and rendering qual-\r\nity. Note that the inception scores for some methods ( e.g.,\r\nBLD inpaint) are higher than ours despite a much worse\r\n7054\r\nText2lightStable Diffusion v2LDM3DText2roomMVDiffusionOursInput Prompt: Modern living room with a sofa and a TV.Input Prompt: Underwater coral reef scene.Figure 5. Text-to-panorama visualizations. Text2light, Stable Diffusion v2 and LDM3D cannot close the 360\x00loop (sharp boundaries\r\nat the middle). Text2room and MVDiffusion generate panoramas with duplicate objects. L-MAGIC effectively addresses these problems,\r\nresulting in high-quality panorama with reasonable scene layouts.\r\nperformance from human evaluation. This is caused by the\r\nadversarial samples generated by these methods (see Ap-\r\npendix Ffor examples), where the local patches of the ad-\r\nversarial samples are not consistent with the input image,\r\nyet leading to a high diversity in the inception score. Simi-\r\nlar issues have also been discovered in other problems [ 7],\r\nwhich shows the importance of human evaluation.\r\nFor the quality enhancement techniques , we remove re-\r\nspectively the super-resolution (no SR) and smoothing (no\r\nsmooth) techniques mentioned in Sec. 3.3. Though the\r\nscene layout does not degrade much, the perspective view\r\nrendering quality is lower due to the blur or artifacts.\r\n4.4. Applications\r\nCombining matured computer vision techniques makes our\r\npipeline applicable to a wide range of applications.\r\nAnything-to-panorama. Conditional diffusion mod-\r\nels [19,29] can now generate an image from diverse typesof inputs. The strong zero-shot performance of L-MAGIC\r\nmakes it possible to generate panoramic scenes from po-\r\ntentially any inputs compatible with conditional diffusion\r\nmodels. As shown in Fig. 7, we can use [ 29] to generate a\r\nsingle image from 1) a depth map, 2) a sketch drawing or\r\n3) a colored script or segmentation image. Then, this gen-\r\nerated image can be used in L-MAGIC to produce realistic\r\npanoramic scenes. This ﬂexibility makes L-MAGIC bene-\r\nﬁcial to a wide range of design applications.\r\n3D scene generation. Applying state-of-the-art depth esti-\r\nmation models, we can further generate 3D scenes from the\r\noutput of L-MAGIC. Speciﬁcally, we compute the depth\r\nmap for multiple perspective views, then we merge the\r\ndepth maps into the equirectangular image plane by align-\r\ning all views to the initial view. Then we convert the corre-\r\nsponding panoramic depth map to a 3D point cloud. We use\r\nMetric3D [ 27] and DPT-hybrid [ 17] to estimate the depth\r\nfor indoor and outdoor scenes respectively. The alignment\r\n7055\r\n(a) Human evaluation.(b) Algorithmic evaluation.0%10%20%30%40%50%60%70%80%90%100%\r\nNopromptNo GPTBLDinpaintDFinpaintSDXLinpaintNo SRNosmoothPreference on our methodQualityLayout\r\nFigure 6. Analysis. We follow the same evaluation protocol as\r\nin the main experiment (Fig. 3). From human evaluations, we see\r\nthat removing the language assistance results in severe degration in\r\neither the scene layout (No GPT) or both the quality and the layout\r\n(No Prompt). Replacing Stable Diffusion v2 with other inpainting\r\nmethods results in performance drop on both the quality and the\r\nlayout. The rendering quality decreased with quality enhancement\r\ntechniques removed. The Inception Score cannot accurately re-\r\nﬂect the performance on adversarial examples (see Appendix F),\r\nresulting in inconsistency with human evaluation results.\r\nFigure 7. Panorama generated from other input modalities.\r\nL-MAGIC can effectively create panoramas from various input\r\nmodalities, such as an input depth map (top), a sketch drawing\r\n(middle) and a colored script or a segmentation mask (bottom).\r\nThe dotted bounding box indicates the region of the initial per-\r\nspective view, which is generated by conditional diffusion models.\r\nis done by optimizing the scale and shift of each depth map\r\nto enforce the depth from multiple views to be the same\r\nat the same pixel. Fig. 8shows sampled results. Despite\r\nsome artifacts caused by the limitation of monocular depth\r\nmodels, L-MAGIC can generate diverse indoor and outdoor\r\npoint clouds from various types of inputs.\r\nImmersive video. One can also render immersive videos\r\nfrom our panorama. Speciﬁcally, we ﬁrst generate a\r\npanorama using our pipeline, and then generate a series of\r\ncamera poses for individual video frames. Then we warp\r\nthe panorama to each frame view according to the cam-\r\nera poses. To further enable scene ﬂy-through with camera\r\ntranslations, we apply depth-based warping when transla-\r\ntion is involved in a frame, and inpaint the missing region\r\nintroduced after translation. See Appendix Gfor implemen-\r\nUnderwater coral reef scene.\r\nFigure 8. 3D point cloud generation. Performing depth estima-\r\ntion on the generated panorama further enables the creation of 3D\r\npoint clouds from diverse inputs. We can generate point clouds for\r\nboth indoor and outdoor scenes, even the underwater scene with\r\nclear geometry of the ﬁshes and coral reefs.\r\ntation details and the supplementary videos for the results.\r\n5. Conclusion\r\nWe have proposed L-MAGIC, a novel method that can gen-\r\nerate 360\x00panoramic scenes from a single input image. L-\r\nMAGIC leverages large (vision) language models to guide\r\ndiffusion models to smoothly extend the local scene con-\r\ntent with a coherent 360\x00layout. We have also proposed\r\ntechniques to enhance the quality and resolution of the\r\ngenerated panorama. Extensive experiments demonstrate\r\nthe effectiveness of L-MAGIC, outperforming state-of-the-\r\nart methods for image-to-panorama and text-to-panorama\r\nacross metrics. Combined with state-of-the-art computer\r\nvision techniques such as conditional diffusion models and\r\ndepth estimation models, our method can consume various\r\ntypes of inputs (text, sketch drawings, depth maps etc.) and\r\ngenerate outputs beyond a single panoramic image (videos\r\nwith camera translations, 3D point clouds, etc.). See Ap-\r\npendix Hfor discussions about limitations and future works.\r\n7056\r\nReferences\r\n[1]https : / / openai . com / blog / chatgpt - can -\r\nnow-see-hear-and-speak .5\r\n[2]https://github.com/deep-ﬂoyd/if. 6\r\n[3]https://huggingface.co/stabilityai/stable-diffusion-x4-\r\nupscaler. 4\r\n[4]https : / / openai . com / blog / introducing -\r\nchatgpt-and-whisper-apis .2,3,5\r\n[5]https://en.wikipedia.org/wiki/equirectangular projection. 4,\r\n5\r\n[6]Omri Avrahami, Ohad Fried, and Dani Lischinski. Blended\r\nlatent diffusion. ACM Transactions on Graphics (TOG) , 42\r\n(4):1–11, 2023. 6\r\n[7]Shane Barratt and Rishi Sharma. A note on the inception\r\nscore. arXiv preprint arXiv:1801.01973 , 2018. 5,7\r\n[8]Zhaoxi Chen, Guangcong Wang, and Ziwei Liu. Text2light:\r\nZero-shot text-driven hdr panorama generation. ACM Trans-\r\nactions on Graphics (TOG) , 41(6):1–16, 2022. 2,5\r\n[9]Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\r\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\r\nYoshua Bengio. Generative adversarial nets. Advances in\r\nneural information processing systems , 27, 2014. 2\r\n[10] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\r\nfusion probabilistic models. Advances in neural information\r\nprocessing systems , 33:6840–6851, 2020. 2\r\n[11] Lukas H ¨ollein, Ang Cao, Andrew Owens, Justin Johnson,\r\nand Matthias Nießner. Text2room: Extracting textured\r\n3d meshes from 2d text-to-image models. arXiv preprint\r\narXiv:2303.11989 , 2023. 1,2,3,5\r\n[12] Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen\r\nKoltun. Tanks and temples: Benchmarking large-scale scene\r\nreconstruction. ACM Transactions on Graphics (ToG) , 36\r\n(4):1–13, 2017. 5\r\n[13] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\r\nBlip-2: Bootstrapping language-image pre-training with\r\nfrozen image encoders and large language models. arXiv\r\npreprint arXiv:2301.12597 , 2023. 1,2,3,5\r\n[14] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher\r\nYu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting\r\nusing denoising diffusion probabilistic models. In Proceed-\r\nings of the IEEE/CVF Conference on Computer Vision and\r\nPattern Recognition , pages 11461–11471, 2022. 5\r\n[15] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\r\nJames Bradbury, Gregory Chanan, Trevor Killeen, Zeming\r\nLin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An im-\r\nperative style, high-performance deep learning library. Ad-\r\nvances in neural information processing systems , 32, 2019.\r\n5\r\n[16] Dustin Podell, Zion English, Kyle Lacey, Andreas\r\nBlattmann, Tim Dockhorn, Jonas M ¨uller, Joe Penna, and\r\nRobin Rombach. Sdxl: improving latent diffusion mod-\r\nels for high-resolution image synthesis. arXiv preprint\r\narXiv:2307.01952 , 2023. 6\r\n[17] Ren´e Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vi-\r\nsion transformers for dense prediction. In Proceedings of\r\nthe IEEE/CVF international conference on computer vision ,\r\npages 12179–12188, 2021. 7,13[18] Nikhila Ravi, Jeremy Reizenstein, David Novotny, Tay-\r\nlor Gordon, Wan-Yen Lo, Justin Johnson, and Georgia\r\nGkioxari. Accelerating 3d deep learning with pytorch3d.\r\narXiv preprint arXiv:2007.08501 , 2020. 3\r\n[19] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\r\nPatrick Esser, and Bj ¨orn Ommer. High-resolution image\r\nsynthesis with latent diffusion models. In Proceedings of\r\nthe IEEE/CVF conference on computer vision and pattern\r\nrecognition , pages 10684–10695, 2022. 2,3,5,7\r\n[20] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki\r\nCheung, Alec Radford, and Xi Chen. Improved techniques\r\nfor training gans. Advances in neural information processing\r\nsystems , 29, 2016. 5\r\n[21] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\r\nCade Gordon, Ross Wightman, Mehdi Cherti, Theo\r\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\r\nman, et al. Laion-5b: An open large-scale dataset for training\r\nnext generation image-text models. Advances in Neural In-\r\nformation Processing Systems , 35:25278–25294, 2022. 2\r\n[22] Gabriela Ben Melech Stan, Diana Wofk, Scottie Fox, Alex\r\nRedden, Will Saxton, Jean Yu, Estelle Aﬂalo, Shao-Yen\r\nTseng, Fabio Nonato, Matthias Muller, et al. Ldm3d: Latent\r\ndiffusion model for 3d. arXiv preprint arXiv:2305.10853 ,\r\n2023. 2,5\r\n[23] Peter Sturm. Multi-view geometry for general camera mod-\r\nels. In 2005 IEEE Computer Society Conference on Com-\r\nputer Vision and Pattern Recognition (CVPR’05) , pages\r\n206–212. IEEE, 2005. 3\r\n[24] Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin,\r\nAnastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov,\r\nNaejin Kong, Harshith Goka, Kiwoong Park, and Victor\r\nLempitsky. Resolution-robust large mask inpainting with\r\nfourier convolutions. In Proceedings of the IEEE/CVF winter\r\nconference on applications of computer vision , pages 2149–\r\n2159, 2022. 2\r\n[25] Shitao Tang, Fuyang Zhang, Jiacheng Chen, Peng Wang, and\r\nYasutaka Furukawa. Mvdiffusion: Enabling holistic multi-\r\nview image generation with correspondence-aware diffusion.\r\narXiv preprint arXiv:2307.01097 , 2023. 1,2,3,5\r\n[26] Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Run-\r\nsheng Xu, Yue Zhao, Wentao Zhang, Bin Cui, and Ming-\r\nHsuan Yang. Diffusion models: A comprehensive survey of\r\nmethods and applications. ACM Computing Surveys , 2022.\r\n2\r\n[27] Wei Yin, Chi Zhang, Hao Chen, Zhipeng Cai, Gang Yu,\r\nKaixuan Wang, Xiaozhi Chen, and Chunhua Shen. Metric3d:\r\nTowards zero-shot metric 3d prediction from a single image.\r\nInProceedings of the IEEE/CVF International Conference\r\non Computer Vision , pages 9043–9053, 2023. 7\r\n[28] Jingyi Zhang, Jiaxing Huang, Sheng Jin, and Shijian Lu.\r\nVision-language models for vision tasks: A survey. arXiv\r\npreprint arXiv:2304.00685 , 2023. 2\r\n[29] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\r\nconditional control to text-to-image diffusion models. In\r\nProceedings of the IEEE/CVF International Conference on\r\nComputer Vision , pages 3836–3847, 2023. 2,7\r\n[30] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei\r\nWang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie\r\n7057\r\nZhang, Zican Dong, et al. A survey of large language mod-\r\nels.arXiv preprint arXiv:2303.18223 , 2023. 2\r\n[31] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe,\r\nand Noah Snavely. Stereo magniﬁcation: Learning\r\nview synthesis using multiplane images. arXiv preprint\r\narXiv:1805.09817 , 2018. 5\r\n7058'}, 'dist': 0.9286905527114868}
Result 12: {'text': 'data', 'metadata': {'created_at': 1736932571, 'modified_at': 1736932571, 'owner': 'sayandeep', 'path': 'RAG_CONTENT/conferences/cvpr_ocr/Liu_Shadow_Generation_for_Composite_Image_Using_Diffusion_Model_CVPR_2024_paper.txt', 'size': 41202, 'seen_at': 1737191136, 'data': 'Shadow Generation for Composite Image Using Diffusion Model\nQingyang Liu1, Junqi You1, Jianting Wang1, Xinhao Tao1, Bo Zhang1, Li Niu1,2*\n1Shanghai Jiao Tong University2miguo.ai\n1{narumimaria,yjqsjtu2022,glory1299,taoxinhao,bo-zhang,ustcnewly }@sjtu.edu.cn\nAbstract\nIn the realm of image composition, generating realistic\nshadow for the inserted foreground remains a formidable\nchallenge. Previous works have developed image-to-image\ntranslation models which are trained on paired training\ndata. However, they are struggling to generate shadows\nwith accurate shapes and intensities, hindered by data\nscarcity and inherent task complexity. In this paper, we\nresort to foundation model with rich prior knowledge of\nnatural shadow images. Specifically, we first adapt Con-\ntrolNet to our task and then propose intensity modulation\nmodules to improve the shadow intensity. Moreover, we\nextend the small-scale DESOBA dataset to DESOBAv2 us-\ning a novel data acquisition pipeline. Experimental results\non both DESOBA and DESOBAv2 datasets as well as real\ncomposite images demonstrate the superior capability of\nour model for shadow generation task. The dataset, code,\nand model are released at https://github.com/bcmi/Object-\nShadow-Generation-Dataset-DESOBAv2.\n1. Introduction\nImage composition [28] aims to merge the foreground of\none image with another background image to produce a\ncomposite image, which has a wide range of applications\nlike virtual reality, artistic creation, and E-commerce. Sim-\nply pasting the foreground onto the background often re-\nsults in visual inconsistencies, including the incompatible\nillumination between foreground and background [3], lack\nof foreground shadow/reflection [12, 34], and so on. In this\npaper, we focus on the shadow issue, i.e., the inserted fore-\nground does not have plausible shadow on the background,\nwhich could significantly degrade the realism and quality of\ncomposite image.\nAs illustrated in Figure 1, shadow generation is a chal-\nlenging task because the foreground shadow is determined\nby many complicated factors like the lighting information\nand the geometry of foreground/background. The exist-\n*Corresponding author.\nFigure 1. A composite image can be obtained by pasting the fore-\nground on the background. Shadow generation aims to generate\nplausible shadow for the inserted foreground in the composite im-\nage to produce a more realistic image.\ning shadow generation methods can be divided into ren-\ndering based methods [34–36] and non-rendering based\nmethods [12, 22, 53]. Rendering based methods usually\nimpose restrict assumptions on the geometry and lighting,\nwhich could hardly be satisfied in real-world scenarios. Be-\nsides, [35, 36] require users to specify the lighting infor-\nmation, which hinders its direct application in our task.\nNon-rendering based methods usually train an image-to-\nimage translation network, based on pairs of composite im-\nages without foreground shadows and real images with fore-\nground shadows. However, due to the training data scarcity\nand task difficulty, these methods are struggling to generate\nshadows with reasonable shapes and intensities.\nRecently, foundation model ( e.g., stable diffusion [32])\npretrained on large-scale dataset has demonstrated unprece-\ndented potential for image generation and editing. In previ-\nous works [44, 48] on object-guided inpainting or composi-\ntion, they show that the generated foregrounds are accom-\npanied by shadows even without considering the shadow is-\nsue, probably because of the rich prior knowledge of natural\nshadow images in foundation model. However, they could\nonly generate satisfactory shadows in simple cases and the\nobject appearance could be altered unexpectedly.\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n8121\nWe build our method upon conditional foundation model\n[52] and propose several key innovations. First, we mod-\nify the control encoder input and the noise loss to fit our\ntask. Then, we observe that the generated shadow intensity\n(the level of darkness) is unsatisfactory. Especially when\nthe background objects has shadows, the intensity incon-\nsistency between foreground shadow and background shad-\nows make the whole image unrealistic. Therefore, we intro-\nduce another intensity encoder to modulate the foreground\nshadow intensity. Specifically, the denoising U-Net is modi-\nfied to output both noise map and foreground shadow mask.\nThe intensity encoder takes in the composite image and\nbackground shadow mask, producing the scale/bias to mod-\nulate the predicted noise within the foreground shadow re-\ngion. Finally, we devise a post-processing network to rec-\ntify the color shift and background variation.\nThe model training requires abundant pairs of composite\nimages without foreground shadows and real images with\nforeground shadows. The existing real-world shadow gen-\neration dataset DESOBA [12] is limited by scale ( i.e., 1,012\nreal images and 3,623 pairs) due to the high cost of manual\nshadow removal, which is insufficient to train our model. To\nensure sufficient supervision, we design a novel data con-\nstruction pipeline, which extends DESOBA to DESOBAv2\n(i.e., 21,575 real images and 28,573 pairs) using object-\nshadow detection and inpainting techniques. Specifically,\nwe first collect a large number of real-world images with\none or more object-shadow pairs. Then, we use pretrained\nobject-shadow detection model [41] to predict object and\nshadow masks for object-shadow pairs. Next, we apply pre-\ntrained inpainting model [32] to inpaint the detected shadow\nregions to get deshadowed images. Finally, based on real\nimages and deshadowed images, we construct pairs of syn-\nthetic composite images and ground-truth target images.\nWe conduct experiments on both DESOBAv2 and DES-\nOBA datasets. The results reveal remarkable improvement\nin shadow generation task, after leveraging the benefits of\nlarge-scale data and foundation model. Our main contri-\nbutions can be summarized as follows: 1) We contribute\nDESOBAv2, a large-scale real-world shadow generation\ndataset, which could greatly facilitate the shadow genera-\ntion task. 2) We propose a cutting-edge diffusion model\nspecifically designed to produce shadows for the compos-\nite foregrounds. 3) Through comprehensive experiments,\nwe validate the efficacy of our dataset construction pipeline\nand the superiority of our proposed model.\n2. Related Work\n2.1. Image Composition\nImage composition aims to overlay a foreground object on\na background image to yield a composite result [20, 22,\n42, 46, 47]. Previous research works have tackled differ-ent issues that can compromise the quality of composite im-\nages. For instance, image blending methods [31, 42, 49, 51]\ntarget at combining the foreground and background seam-\nlessly. Image harmonization methods [3–6, 40] aim to\nrectify the illumination disparity between foreground and\nbackground. Nonetheless, the above methods largely over-\nlook the shadow cast by the foreground onto the back-\nground. Recently, generative image composition methods\n[38, 44, 48] can insert a foreground object into a bounding\nbox in the background and the inserted object is likely to\nhave shadow effect. However, they could only generate sat-\nisfactory shadows in simple cases and the object appearance\ncould be altered unexpectedly.\n2.2. Shadow Generation\nIn this paper, the goal of shadow generation task is gener-\nating plausible shadow for the composite foreground. Ex-\nisting methods can be broadly categorized into rendering\nbased methods and non-rendering based methods. The ren-\ndering based methods necessitate a comprehensive under-\nstanding of factors like illumination, reflectance, material\nproperties, and scene geometry to produce shadows for the\ninserted objects. However, such detailed knowledge re-\nlies on user input [15, 16, 21, 35, 36] or model predic-\ntion [1, 7, 19, 50], which is either labor-intensive or unreli-\nable [53]. For example, [35, 36] could produce compelling\nresults with user control. However, in the composite im-\nage, the lighting information should be inferred automati-\ncally from background instead of requested by users.\nNon-rendering based methods [12, 22, 25, 53] aim to\ntranslate an input composite image without foreground\nshadow to an output with foreground shadow, bypassing\nthe need for explicit knowledge of the aforementioned fac-\ntors. For instance, ShadowGAN [53] utilizes both global\nand local conditional discriminator to enhance the realism\nof generated shadows. ARShadowGAN [22] emphasizes\nthe importance of background shadow and uses it to guide\nforeground shadow generation. SGRNet [12] encourages\nthe information exchange between foreground and back-\nground, and employs a classic illumination model for bet-\nter shadow effect. The work [25] produces multiple under-\nexposure images and fuses them to get the final shadow re-\ngion. DMASNet [39] decomposes shadow mask prediction\ninto box prediction and shape prediction, achieving better\ncross-domain transferability.\nTo the best of our knowledge, we are the first diffusion-\nbased method focusing on shadow generation.\n2.3. Diffusion Models\nIn recent years, diffusion models have emerged as a pow-\nerful tool in image generation and image editing. These\nmodels approach image generation as a series of stochastic\ntransitions, moving from a basic distribution to the desired\n8122\nFigure 2. The pipeline of dataset construction. We use object-shadow detection model [41] to predict pairs of object and shadow masks\nin the real image Ir. Then we obtain the union Msof all shadow masks as the inpainting mask and apply inpainting model [32] to get a\ndeshadowed image Id. After designating a foreground object, we replace the background shadow regions MbsinIdwith the counterparts\ninIrto synthesize a composite image Ic, and replace all the shadow regions MsinIdwith the counterparts in Irto obtain the ground-\ntruth target image Ig.\ndata distribution [11]. Diffusion models can be divided into\nunconditional diffusion models [11, 37] and conditional dif-\nfusion models [27, 32, 52]. Unconditional diffusion models\nfocus on generating realistic images by capturing the dis-\ntribution of natural images, without the need of any specific\ninput conditions. Conditional diffusion models are designed\nto produce images under the guidance of specific condi-\ntional inputs, such as text descriptions, semantic masks, and\nso on. ControlNet [52] is a popular conditional diffusion\nmodel, which equips large pretrained text-to-image diffu-\nsion models with spatial-aware and task-specific conditions.\nWe build our model upon ControlNet and propose several\ninnovations to meet the specific requirements of shadow\ngeneration.\n3. Dataset Construction\nThe pipeline of our dataset construction is illustrated in Fig-\nure 2, which will be detailed next.\n3.1. Shadow Image Collection\nWe harvest an extensive collection of real-world outdoor\nimages with natural lighting across various scenes from two\nsources. On one hand, we crawl online images from public\nwebsites that have licenses for reuse. On the other hand, we\nhire photographers to capture photos in the outdoor scenes\nthat satisfy our requirements. We only preserve the images\nwith at least one object-shadow pair, arriving at 44,044 im-\nages.\n3.2. Shadow Removal\nGiven a real image Irwith object-shadow pairs, we use the\npretrained object-shadow detection model [41] to predict Kpairs of object and shadow masks. We use Mo,k(resp. ,\nMs,k) to denote the object ( resp. , shadow) mask of the k-\nth object. We refer to one detected object-shadow pair as\none detected instance. We eliminate the images without any\ndetected instance.\nSubsequently, we attempt to erase all the detected shad-\nows. We have tried some state-of-the-art shadow removal\nmodels [8, 9], but the performance in the wild is below our\nexpectation due to poor generalization ability. Considering\nthe recent rapid advance of image inpainting [14, 23, 29, 32,\n45, 56] techniques, we resort to image inpainting to remove\nthe shadows. Although image inpainting cannot preserve\nthe background information precisely, we observe that the\nbackground textures in the shadow region are usually very\nsimple, and the inpainted result has similar textures with the\noriginal background. Thus, we roughly treat the inpainted\nresults as deshadowed results.\nWe obtain the union of all detected shadow masks Ms=\nMs,1∪Ms,2∪···∪ Ms,Kas the inpainting mask and apply\nthe pretrained inpainting model [32] to get a deshadowed\nimage Id. In practice, we observe that the inpainting model\nis prone to generate low-quality shadow in the inpainted re-\ngion in some cases. To prevent the inpainting model from\ngenerating undesirable shadows in the inpainted region, we\nadopt some tricks like dilating the inpainting mask and flip-\nping images vertically, which can effectively obstruct un-\ndesirable shadow generation during inpainting. However,\nthere may still exist undesirable shadows or noticeable arti-\nfacts in the inpainted region.\nAfter inpainting, we manually filter the object-shadow\npairs according to the following rules: 1) We remove\nthe object-shadow pairs with low-quality object masks or\nshadow masks. 2) We remove those object-shadow pairs\n8123\nwith generated shadows or noticeable artifacts in the in-\npainted region. After manual filtering, we refer to the re-\nmaining object-shadow pairs as valid instances. We have\n21,575 images with 28,573 valid instances.\n3.3. Composite Image Synthesis\nGiven a pair of a real image Irand a deshadowed image\nId, we randomly select the k-th foreground object from\nvalid instances and synthesize the composite image. Mo,k\n(resp. ,Ms,k) is referred to as the foreground object ( resp. ,\nshadow) mask Mfo(resp. ,Mfs). One strategy is replacing\nthe shadow region Mfsof this foreground object in Irwith\nthe counterpart in Idto erase the foreground shadow. How-\never, this strategy may leave traces along the shadow bound-\nary, in which case the model may find a shortcut to generate\nthe shadow. Another strategy is replacing the shadow re-\ngionsMbs=Ms,1∪···∪ Ms,k−1∪Ms,k+1∪···∪ Ms,K\nof the other objects in Idwith the counterparts in Irto syn-\nthesize a composite image Ic, in which only the selected\nforeground object does not have shadow while all the other\nobjects have shadows. We adopt the second strategy.\nAfter inpainting, the background may undergo slight\nchanges, so the background of Icmay be slightly different\nfrom that of Ir. To ensure consistent background, we obtain\nthe ground-truth target image Igby replacing the shadow\nregions Msof all objects in Idwith the counterparts in Ir.\nThen, IcandIgform a pair of input composite image and\nground-truth target image. So far, we obtain tuples in the\nform of {Ic,Mfo,Mfs,Mbs,Ig}, which will be used for\nmodel training. Example images and more statistics of our\ndataset can be found in the supplementary.\n4. Background\nStable Diffusion [32] is latent diffusion model operating in a\nlatent space. First, 512×512images are converted to 64×64\nlatent images using V AE [18] with encoder Erand decoder\nDr. The image space is projected to the latent space using\nEr, and back to the image space using Dr. Then, the for-\nward diffusion process and backward denoising process are\nperformed in the latent space. The denoising U-Net [33]\nconsists of an encoder with 12blocks, a middle block, and\na skip-connected decoder with 12blocks.\nDuring training, random Gaussian noise ϵis added to the\nlatent image z0in the denoising step t, producing a noisy\nlatent image zt. Given time step tand text prompt ctxt,\nthe denoising U-Net with model parameters ϵθis trained to\npredict the added noise ϵ.\nTo support spatial conditional information ( e.g., edge,\npose, depth), ControlNet [52] integrates a control encoder\nEcwith pre-trained Stable Diffusion. Specifically, the\ncontrol encoder contains trainable replicas of its 12 en-\ncoding blocks and middle block across four resolutions(64×64,32×32,16×16,8×8). It takes a 512×512\nconditional image as input.\nThe conditional feature maps cimgoutput from control\nencoder are used to enhance the 12 skip-connections and\nmiddle block in denoising U-Net via zero convolution lay-\ners. While the original Stable Diffusion is fixed to retain\nprior knowledge, control encoder could incorporate addi-\ntional conditions to guide image generation. The objective\ncould be rewritten as\nLctrl=Et,ϵ∼N (0,1)h\n∥ϵ−ϵθ(zt, t,ctxt,cimg)∥2\n2i\n.(1)\n5. Method\nGiven a composite image Icwithout foreground shadow as\nwell as the foreground object mask Mfo, our Shadow Gen-\neration Diffusion (SGDiffusion) model aims to produce ˜Ig\nwith plausible foreground shadow. We will adapt Control-\nNet [52] to shadow generation task in Section 5.1, and pro-\npose novel modules to improve the shadow intensity in Sec-\ntion 5.2. Finally, we will briefly introduce post-processing\ntechniques to enhance the image quality in Section 5.3.\n5.1. Adapting ControlNet to Shadow Generation\nFor shadow generation task, the useful conditional infor-\nmation is input composite image Icand foreground object\nmaskMfo, in which the foreground object mask indicates\nthe target object we need to generate shadow for. We con-\ncatenate IcwithMfoas the input of control encoder Ec.\nThe control encoder outputs the conditional feature maps\ncsg, which are injected into the denoising decoder to pro-\nvide guidance. For the text prompt, we have tried sev-\neral variants like “the [object category] with shadow”, but\nthey have no significant impact on the generated shadows.\nTherefore, we use null text prompt by default.\nGiven a set of conditions including time step tand con-\nditional feature maps csg, the denoising U-Net with model\nparameters ϵθpredicts the noise ϵadded to the noisy latent\nimage zt:\nLsg=Et,ϵ∼N (0,1)h\n∥ϵ−ϵθ(zt, t,csg))∥2\n2i\n. (2)\nTo enforce the model to place more emphasis on the fore-\nground shadow region, we introduce weighted noise loss,\nwhich assigns higher weights to the foreground shadow re-\ngion. We expand the foreground shadow mask by a dilated\nkernel to get the expanded mask ˆMfs. The weights in the\nexpanded foreground shadow region are wwhile the other\nweights are 1, leading to the weight map Wfs. If we do\nnot expand the foreground shadow region, the model will\nbe misled to generate large shadows, overlooking the de-\ntails of shadow shapes and boundaries. By applying weight\nmapWfsto the noise loss, we can arrive at\nLwsg=Et,ϵ∼N (0,1)h\n∥Wfs◦(ϵ−ϵθ(zt, t,csg))∥2\n2i\n,(3)\n8124\nFigure 3. The framework of our SGDiffusion. We adapt ControlNet (Control Encoder and Denoising U-Net) to shadow generation task.\nWe also introduce an intensity encoder to modulate the foreground shadow region in the noise map ˜ϵ, leading to ˆϵ. The output noise ˆϵis\nsupervised by weighted noise loss Lmwsg based on the expanded foreground shadow mask ˆMfs\nwhere ◦denotes element-wise multiplication.\nDuring inference, to retain more information of input\ncomposite image Icin the initial noise, we obtain zTby\nadding noise to the latent image of Ic, rather than directly\nsampling from the Gaussian distribution N(0,1).\n5.2. Shadow Intensity Modulation\nBy using the adapted ControlNet in Section 5.1, we ob-\nserve that the intensity of generated foreground shadow is\nunsatisfactory. Especially when the background has object-\nshadow pairs, the generated foreground shadow is often no-\ntably darker or brighter than background shadows. Such in-\nconsistency between foreground shadow intensity and back-\nground shadow intensity makes the whole image unrealistic.\nTherefore, we introduce another intensity encoder to\nmodulate the foreground shadow intensity. Specifically,\nwe use encoder Eito extract intensity-relevant information.\nIntuitively, by observing background shadows and its sur-\nrounding unshadowed areas, we can estimate the intensity\nof foreground shadows. Thus, the input of intensity encoder\nEishould include the composite image Icand background\nshadow mask Mbs. When there is no background shadow,\nthe mask is all black. We concatenate Icwith background\nshadow mask Mbsas the input of intensity encoder.\nThe intensity encoder outputs scales and biases to adjust\nthe intensity of noise map within the foreground shadow re-\ngion. The modulated noise map results in the modulated la-\ntent image, and further results in the modulated foregroundshadow. Therefore, the intensity adjustment of noise map\nis finally embodied in the intensity variation of generated\nforeground shadow. Specifically, when the noise map has\ncchannels, Eioutputs the c-dim scale vector sandc-dim\nbias vector b, containing channel-wise scales and biases. s\nandbare used to modulate the predicted noise map within\nthe foreground shadow region.\nOne problem is that the foreground shadow region is un-\nknown in the testing stage, so we need to predict the fore-\nground shadow mask. To avoid much extra computational\ncost, we take advantage of the feature maps in the denois-\ning U-Net to predict the foreground shadow mask. Previ-\nous works usually combine different layers of feature maps\nin denoising U-Net for mask prediction [24, 43]. We try\ndifferent layers of feature maps and find that decoder fea-\nture maps are more effective in shadow mask prediction.\nWe also use foreground object mask, which could provide\nuseful hints for the location of foreground shadow. We re-\nsize all decoder feature maps and foreground object mask\nto the same size, and concatenate them channel-wisely. The\nconcatenation passes through several convolutional layers\nto predict the foreground shadow mask ˜Mfs.˜Mfsis su-\npervised with ground-truth foreground shadow mask Mfs\nby Binary Cross-Entropy (BCE) loss and Dice loss [26]:\nLmask =Lbce(˜Mfs,Mfs) +Ldice(˜Mfs,Mfs).(4)\nWhen tis large, ztis close to random noise and thus the\ndecoder feature maps are not informative to predict shadow\n8125\nmask. Hence, we only employ the loss Lmask when the\ntime step tis small. We set the threshold of tasσT, in\nwhich Tis the total number of steps. Accordingly, shadow\nintensity modulation is only applied when tis smaller than\nthe threshold σT.\nProvided with the predicted foreground shadow mask\n˜Mfs, we can modulate the noise map within the fore-\nground shadow region. Given the predicted noise map ˜ϵ=\nϵθ(zt, t,csg), we multiply ˜ϵby channel-wise scales sand\nadd channel-wise biases bto get ˜ϵ′. Then, based on ˜Mfs,\nwe combine the modulated noise map and original noise\nmap to get the final noise map: ˆϵ=˜ϵ′◦˜Mfs+˜ϵ◦(1−˜Mfs).\nWe replace the predicted noise map in Eqn. (3) with the\nfinal noise map ˆϵand get\nLmwsg =Et,ϵ∼N (0,1)h\n∥Wfs◦(ϵ−ˆϵ)∥2\n2i\n. (5)\nWe summarize the mask prediction loss in Eqn. (4) and\nweighted noise loss in Eqn. (5) as\nLall=Lmask +λLmwsg, (6)\nwhere λis a trade-off parameter.\n5.3. Post-processing\nWe observe that the generated images could have color shift\nand background variation issues. Color shift means that the\noverall color tone deviates from the input composite image.\nBackground variation means that some background details\nare changed. To solve these issues, we create a multi-task\npost-processing network which yields the rectified image\ntogether with the foreground shadow mask. Then, we com-\nbine input composite image and rectified image based on\nthe predicted foreground shadow mask to produce the final\nimage. The technical details are left to supplementary.\n6. Experiments\n6.1. Datasets and Evaluation Metrics\nWe conduct experiments on both DESOBA [12] and our\ncontributed DESOBAv2 dataset. We split DESOBAv2 into\n21,088 training images with 27,718 tuples and 487 test im-\nages with 855 tuples. Following [12], the test set contains\nBOS images (with background object-shadow pairs) and\nBOS-free images. Most of our experiments are based on\nDESOBAv2 dataset due to the following two concerns: 1)\nDESOBAv2 has larger test set which supports more com-\nprehensive evaluation. 2) DESOBA has the artifacts caused\nby manual shadow removal and the existing methods ( e.g.,\nSGRNet) tend to overfit such artifacts.\nFor the generated results, we evaluate both image qual-\nity and mask quality. For image evaluation, following [12],\nwe adopt RMSE and SSIM, which are calculated based\non the ground-truth target image and the generated image.Global RMSE (GR) and Global SSIM (GS) are calculated\nover the whole image, while Local RMSE (LR) and Local\nSSIM (LS) are calculated over the ground-truth foreground\nshadow region. For the mask evaluation, following [12], we\nadopt Balanced Error Rate (BER), which is calculated based\non the ground-truth binary foreground shadow mask and the\npredicted foreground shadow mask obtained by threshold\n0.5. Global BER (GB) is calculated over the whole image,\nwhile Local BER (LB) is calculated over the ground-truth\nforeground shadow region. Note that diffusion model has\nstochastic property and shadow generation is a multi-modal\ntask, that is, one input has multiple plausible outputs. Simi-\nlar to multi-modal inpainting evaluation [54, 55], we gener-\nate 5 results for one test image with different random seeds\nand select the one closest to the ground-truth (the highest\nLocal SSIM) to calculate evaluation metrics.\n6.2. Implementation Details\nWe develop our method with PyTorch 1.12.1 [30]. Our\nmodel is trained using the Adam optimizer [17] with a con-\nstant learning rate of 1e−5over 50 epochs, on four NVIDIA\nRTX A6000 GPUs. Our method is built upon ControlNet\n[52]. We employ ResNet18 [10] as the intensity encoder.\nThe mask predictor passes the concatenation of decoder fea-\nture maps and foreground object mask through four con-\nvolutional layers, with ReLU activation following the first\nthree layers and Sigmoid activation following the last layer.\nWe set the hyper-parameters w,σ, and λas10,0.7, and 1,\nrespectively.\n6.3. Comparison with Baselines\nFollowing [12], we compare with ShadowGAN [53], Mask-\nShadowGAN [13], ARShadowGAN [22], and SGRNet\n[12]. We train and test all methods on DESOBAv2 dataset.\nThe quantitative results are summarized in Table 1. We ob-\nserve that our SGDiffusion achieves the lowest GRMSE,\nLRMSE and the highest GSSIM, LSSIM, which demon-\nstrates that our method could generate shadow images that\nare closer to the ground-truth shadow images. The best GB\nand LB results demonstrate that the shapes and locations of\nour generated shadows are more accurate.\nFor qualitative comparison, we show several example re-\nsults in Figure 4. Compared with the baseline methods,\nthe shadows produced by our model have more reasonable\nshapes and intensities. Moreover, as shown in row 1, our\nmethod can take into account the self-occlusion of objects\nto generate discontinuous shadows. As shown in row 4, our\nmethod can also consider the material of the objects, pro-\nducing shadows with translucency effects. We provide more\nexamples in the supplementary.\n8126\nFigure 4. Visual comparison of different methods on DESOBAv2 dataset. From left to right are input composite image (a), foreground\nobject mask (b), results of ShadowGAN [53] (c), MaskshadowGAN [13] (d), ARShadowGAN [22] (e), SGRNet [12] (f), our SGDiffusion\n(g), ground-truth (h).\nMethodBOS Test Images BOS-free Test Images\nGR↓ LR↓ GS↑LS↑GB↓ LB↓ GR↓ LR↓ GS↑LS↑GB↓ LB↓\nShadowGAN [53] 7.511 67.464 0.961 0.197 0.446 0.890 17.325 76.508 0.901 0.060 0.425 0.842\nMaskshadowGAN [13] 8.997 79.418 0.951 0.180 0.500 1.000 19.338 94.327 0.906 0.044 0.500 1.000\nARShadowGAN [22] 7.335 58.037 0.961 0.241 0.383 0.761 16.067 63.713 0.908 0.104 0.349 0.682\nSGRNet [12] 7.184 68.255 0.964 0.206 0.301 0.596 15.596 60.350 0.909 0.100 0.271 0.534\nSGDiffusion 6.098 53.611 0.971 0.370 0.245 0.487 15.110 55.874 0.913 0.117 0.233 0.452\nTable 1. The results of different methods on DESOBAv2 dataset. The best results are highlighted in boldface.\n6.4. Ablation Studies\nWe study the impact of weighted noise loss (WL), intensity\nmodulation (IM), and post-processing (PP) of our SGDiffu-\nsion on BOS test images from DESOBAv2. The quantita-\ntive results are summarized in Table 2.\nIn row 1, we report the results of basic ControlNet with-\nout weighted noise loss. For WL, the comparison between\nrow 3 and row 1 emphasizes the importance of paying more\nattention to the foreground shadow region. We also report a\nspecial case †in row 2, where the foreground shadow mask\nis not expanded when constructing the weight map. The re-\nsults in row 2 are comparable or even worse than those inrow 1, as the model tends to generate larger shadow size\nwhile ignoring shape and edge details. For IM, the compar-\nison between row 1 and row 5 shows that the intensity mod-\nulation can significantly improve the shadow quality by ad-\njusting the shadow intensity. We also report a special case ◦\nin row 4, where the intensity encoder input does not contain\nbackground shadow mask. The comparison between row\n4 and row 5 shows that background shadow mask is help-\nful, because the background shadow regions and their sur-\nrounding regions could provide useful clues to infer shadow\nintensity. For PP, the comparison between row 6 and row 7\ndemonstrates that post-processing effectively corrects color\nshift and background variations, substantially reducing the\n8127\nFigure 5. Visual comparison of different methods on real composite images. From left to right are input composite image (a), foreground\nobject mask (b), results of ShadowGAN [53] (c), MaskshadowGAN [13] (d), ARShadowGAN [22] (e), SGRNet [12] (f), SGDiffusion (g).\nRow WL IM PP GR↓ LR↓ GB↓ LB↓\n1 - - + 8.285 59.753 0.271 0.534\n2 † - + 8.319 59.491 0.282 0.563\n3 + - + 7.041 53.829 0.249 0.492\n4 -◦ + 7.410 56.121 0.269 0.536\n5 - + + 7.357 54.159 0.262 0.526\n6 + + - 13.447 55.231 0.245 0.487\n7 + + + 6.098 53.611 0.245 0.487\nTable 2. Ablation studies of our method on BOS test images from\nDESOBAv2 dataset. WL is short for weighted loss and †means\nwithout expanding shadow mask. IM is short for intensity modu-\nlation and ◦means without using background shadow mask. PP is\nshort for post-processing.\nglobal RMSE. We also provide the visual results of ablated\nversions in the supplementary.\n6.5. Real Composite Images\nWe compare different methods on real composite images\nprovided by [12], where background images and foreground\nobjects are from the DESOBA [12] test set. We train all\nmethods on DESOBAv2 and finetune them on DESOBA.\nThe visual results of different methods are showcased in\nFigure 5. These results confirm that SGDiffusion adeptly\nsynthesizes lifelike shadows with precise contours, loca-tions, and directions, which are compatible with the back-\nground object-shadow pairs and foreground object informa-\ntion. In contrast, previous methods often produce vague\nand misdirected shadows. We provide more examples in\nthe supplementary.\nGiven the absence of ground-truth images for real com-\nposite images, following [12], we opt for subjective eval-\nuation, engaging 50human raters in the user study. Each\nparticipant is presented with image pairs from the results\ngenerated by 5methods, and asked to choose the image with\nmore realistic foreground shadow. Using the Bradley-Terry\nmodel [2], we report the B-T scores in the supplementary,\nwhich again proves the advantage of our method.\n7. Conclusion\nIn this paper, we have contributed a large-scale shadow gen-\neration dataset DESOBAv2. We have also designed a novel\ndiffusion-based shadow generation method. Extensive ex-\nperimental results show that our method is able to generate\nplausible shadows for composite foregrounds, significantly\nsurpassing previous methods.\n8. Acknowledgement\nThe work was supported by the National Natural Sci-\nence Foundation of China (Grant No. 62076162),\nthe Shanghai Municipal Science and Technology Ma-\njor/Key Project, China (Grant No. 2021SHZDZX0102).\n8128\nReferences\n[1] Ibrahim Arief, Simon McCallum, and Jon Yngve Hardeberg.\nRealtime estimation of illumination direction for augmented\nreality on mobile devices. In CIC, 2012. 2\n[2] Ralph Allan Bradley and Milton E Terry. Rank analysis of\nincomplete block designs: I. the method of paired compar-\nisons. Biometrika , 39(3/4):324–345, 1952. 8\n[3] Wenyan Cong, Jianfu Zhang, Li Niu, Liu Liu, Zhixin Ling,\nWeiyuan Li, and Liqing Zhang. Dovenet: Deep image har-\nmonization via domain verification. In CVPR , 2020. 1, 2\n[4] Wenyan Cong, Li Niu, Jianfu Zhang, Jing Liang, and Liqing\nZhang. Bargainnet: Background-guided domain translation\nfor image harmonization. In ICME , 2021.\n[5] Wenyan Cong, Xinhao Tao, Li Niu, Jing Liang, Xuesong\nGao, Qihao Sun, and Liqing Zhang. High-resolution im-\nage harmonization via collaborative dual transformations. In\nCVPR , 2022.\n[6] Xiaodong Cun and Chi-Man Pun. Improving the harmony of\nthe composite image by spatial-separated attention module.\nTIP, 2020. 2\n[7] Marc-Andr ´e Gardner, Yannick Hold-Geoffroy, Kalyan\nSunkavalli, Christian Gagn ´e, and Jean-Francois Lalonde.\nDeep parametric indoor lighting estimation. In ICCV , 2019.\n2\n[8] Lanqing Guo, Siyu Huang, Ding Liu, Hao Cheng, and Bihan\nWen. Shadowformer: Global context helps image shadow\nremoval. In AAAI , 2023. 3\n[9] Lanqing Guo, Chong Wang, Wenhan Yang, Siyu Huang,\nYufei Wang, Hanspeter Pfister, and Bihan Wen. Shadowd-\niffusion: When degradation prior meets diffusion model for\nshadow removal. In CVPR , 2023. 3\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In CVPR ,\n2016. 6\n[11] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. In NeurlPS , 2020. 3\n[12] Yan Hong, Li Niu, and Jianfu Zhang. Shadow generation for\ncomposite image in real-world scenes. AAAI , 2022. 1, 2, 6,\n7, 8\n[13] Xiaowei Hu, Yitong Jiang, Chi-Wing Fu, and Pheng-Ann\nHeng. Mask-shadowgan: Learning to remove shadows from\nunpaired data. In ICCV , 2019. 6, 7, 8\n[14] Shaozong Huang and Lan Hong. Diffusion model for mural\nimage inpainting. In ITOEC , 2023. 3\n[15] Kevin Karsch, Kalyan Sunkavalli, Sunil Hadap, Nathan Carr,\nHailin Jin, Rafael Fonte, Michael Sittig, and David Forsyth.\nAutomatic scene inference for 3d object compositing. ACM\nTOG , 2014. 2\n[16] Eric Kee, James F. O’Brien, and Hany Samir Farid. Exposing\nphoto manipulation from shading and shadows. ACM TOG ,\n2014. 2\n[17] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. CoRR , abs/1412.6980, 2014. 6\n[18] Diederik P Kingma and Max Welling. Auto-encoding varia-\ntional bayes. CoRR , abs/1312.6114, 2013. 4[19] Bin Liao, Yao Zhu, Chao Liang, Fei Luo, and Chunxia Xiao.\nIllumination animating and editing in a single picture using\nscene structure estimation. Computers & Graphics , 82:53–\n64, 2019. 2\n[20] Chen-Hsuan Lin, Ersin Yumer, Oliver Wang, Eli Shechtman,\nand Simon Lucey. St-gan: Spatial transformer generative\nadversarial networks for image compositing. In CVPR , 2018.\n2\n[21] Bin Liu, Kun Xu, and Ralph R Martin. Static scene illumi-\nnation estimation from videos with applications. JCST , 32\n(3):430–442, 2017. 2\n[22] Daquan Liu, Chengjiang Long, Hongpan Zhang, Hanning\nYu, Xinzhi Dong, and Chunxia Xiao. Arshadowgan: Shadow\ngenerative adversarial network for augmented reality in sin-\ngle light scenes. In CVPR , 2020. 1, 2, 6, 7, 8\n[23] Guilin Liu, Fitsum A. Reda, Kevin J. Shih, Ting-Chun Wang,\nAndrew Tao, and Bryan Catanzaro. Image inpainting for ir-\nregular holes using partial convolutions. In ECCV , 2018. 3\n[24] Jian Ma, Mingjun Zhao, Chen Chen, Ruichen Wang, Di Niu,\nHaonan Lu, and Xiaodong Lin. Glyphdraw: Learning to\ndraw chinese characters in image synthesis models coher-\nently. CoRR , abs/2303.17870, 2023. 5\n[25] Quanling Meng, Shengping Zhang, Zonglin Li, Chenyang\nWang, Weigang Zhang, and Qingming Huang. Automatic\nshadow generation via exposure fusion. IEEE Transactions\non Multimedia , 2023. 2\n[26] Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi.\nV-net: Fully convolutional neural networks for volumetric\nmedical image segmentation. In 3DV, 2016. 5\n[27] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhon-\ngang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning\nadapters to dig out more controllable ability for text-to-image\ndiffusion models. CoRR , abs/2302.08453, 2023. 3\n[28] Li Niu, Wenyan Cong, Liu Liu, Yan Hong, Bo Zhang, Jing\nLiang, and Liqing Zhang. Making images real again: A\ncomprehensive survey on deep image composition. CoRR ,\nabs/2106.14490, 2021. 1\n[29] Sibam Parida, Vignesh Srinivas, Bhavishya Jain, Rajesh\nNaik, and Neeraj Rao. Survey on diverse image inpainting\nusing diffusion models. In PCEMS , 2023. 3\n[30] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\nJames Bradbury, Gregory Chanan, Trevor Killeen, Zeming\nLin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An im-\nperative style, high-performance deep learning library. NIPS ,\n32, 2019. 6\n[31] Patrick P ´erez, Michel Gangnet, and Andrew Blake. Poisson\nimage editing. In SIGGRAPH . 2003. 2\n[32] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj ¨orn Ommer. High-resolution image syn-\nthesis with latent diffusion models. In CVPR , 2022. 1, 2, 3,\n4\n[33] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:\nConvolutional networks for biomedical image segmentation.\nInMICCAI , 2015. 4\n[34] Yichen Sheng, Jianming Zhang, and Bedrich Benes. Ssn:\nSoft shadow network for image compositing. In CVPR ,\n2021. 1\n8129\n[35] Yichen Sheng, Yifan Liu, Jianming Zhang, Wei Yin, A Cen-\ngiz Oztireli, He Zhang, Zhe Lin, Eli Shechtman, and Bedrich\nBenes. Controllable shadow generation using pixel height\nmaps. In ECCV , 2022. 1, 2\n[36] Yichen Sheng, Jianming Zhang, Julien Philip, Yannick Hold-\nGeoffroy, Xin Sun, He Zhang, Lu Ling, and Bedrich Benes.\nPixht-lab: Pixel height based light effect generation for im-\nage compositing. In CVPR , 2023. 1, 2\n[37] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-\ning diffusion implicit models. CoRR , abs/2010.02502, 2020.\n3\n[38] Yi-Zhe Song, Zhifei Zhang, Zhe L. Lin, Scott D. Cohen,\nBrian L. Price, Jianming Zhang, Soo Ye Kim, and Daniel G.\nAliaga. Objectstitch: Generative object compositing. In\nCVPR , 2023. 2\n[39] Xinhao Tao, Junyan Cao, Yan Hong, and Li Niu. Shadow\ngeneration with decomposed mask prediction and attentive\nshadow filling. In AAAI , 2024. 2\n[40] Yi-Hsuan Tsai, Xiaohui Shen, Zhe Lin, Kalyan Sunkavalli,\nXin Lu, and Ming-Hsuan Yang. Deep image harmonization.\nInCVPR , 2017. 2\n[41] Tianyu Wang, Xiaowei Hu, Pheng-Ann Heng, and Chi-Wing\nFu. Instance shadow detection with a single-stage detector.\nTPAMI , 2022. 2, 3\n[42] Huikai Wu, Shuai Zheng, Junge Zhang, and Kaiqi Huang.\nGp-gan: Towards realistic high-resolution image blending.\nInACM MM , 2019. 2\n[43] Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiao-\nlong Wang, and Shalini De Mello. Open-vocabulary panop-\ntic segmentation with text-to-image diffusion models. In\nCVPR , 2023. 5\n[44] Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin\nChen, Xiaoyan Sun, Dong Chen, and Fang Wen. Paint by\nexample: Exemplar-based image editing with diffusion mod-\nels. In CVPR , 2023. 1, 2\n[45] Shiyuan Yang, Xiaodong Chen, and Jing Liao. Uni-paint:\nA unified framework for multimodal image inpainting with\npretrained diffusion model. In ACM MM , 2023. 3\n[46] Fangneng Zhan, Jiaxing Huang, and Shijian Lu. Adaptive\ncomposition gan towards realistic image synthesis. CoRR ,\nabs/1905.04693, 2019. 2\n[47] Fangneng Zhan, Shijian Lu, Changgong Zhang, Feiying Ma,\nand Xuansong Xie. Towards realistic 3d embedding via view\nalignment. CoRR , abs/2007.07066, 2020. 2\n[48] Bo Zhang, Yuxuan Duan, Jun Lan, Yan Hong, Huijia Zhu,\nWeiqiang Wang, and Li Niu. Controlcom: Controllable\nimage composition using diffusion model. arXiv preprint\narXiv:2308.10040 , 2023. 1, 2\n[49] He Zhang, Jianming Zhang, Federico Perazzi, Zhe Lin, and\nVishal M Patel. Deep image compositing. In WACV , 2021.\n2\n[50] Jinsong Zhang, Kalyan Sunkavalli, Yannick Hold-Geoffroy,\nSunil Hadap, Jonathan Eisenman, and Jean-Franc ¸ois\nLalonde. All-weather deep outdoor lighting estimation. In\nCVPR , 2019. 2\n[51] Lingzhi Zhang, Tarmily Wen, and Jianbo Shi. Deep image\nblending. In WACV , 2020. 2[52] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models. In\nICCV , 2023. 2, 3, 4, 6\n[53] Shuyang Zhang, Runze Liang, and Miao Wang. Shadowgan:\nShadow synthesis for virtual objects with conditional adver-\nsarial networks. Computational Visual Media , 5:105–115,\n2019. 1, 2, 6, 7, 8\n[54] Lei Zhao, Qihang Mo, Sihuan Lin, Zhizhong Wang, Zhiwen\nZuo, Haibo Chen, Wei Xing, and Dongming Lu. Uctgan:\nDiverse image inpainting based on unsupervised cross-space\ntranslation. In CVPR , 2020. 6\n[55] Chuanxia Zheng, Tat-Jen Cham, and Jianfei Cai. Pluralistic\nimage completion. In CVPR , 2019. 6\n[56] Haitian Zheng, Zhe Lin, Jingwan Lu, Scott Cohen, Eli\nShechtman, Connelly Barnes, Jianming Zhang, Ning Xu,\nSohrab Amirghodsi, and Jiebo Luo. Image inpainting with\ncascaded modulation gan and object-aware training. In\nECCV , 2022. 3\n8130'}, 'dist': 0.9286905527114868}
Result 13: {'text': 'data', 'metadata': {'created_at': 1736932570, 'modified_at': 1736932570, 'owner': 'sayandeep', 'path': 'RAG_CONTENT/conferences/cvpr_ocr/Bialer_RadSimReal_Bridging_the_Gap_Between_Synthetic_and_Real_Data_in_CVPR_2024_paper.txt', 'size': 49887, 'seen_at': 1737191136, 'data': 'RadSimReal: Bridging the Gap Between Synthetic and Real Data in Radar\nObject Detection With Simulation\nOded Bialer*and Yuval Haitman*\nGeneral Motors, Technical Center Israel\noded.bialer8@gmail.com, haitman@post.bgu.ac.il\nAbstract\nObject detection in radar imagery with neural networks\nshows great potential for improving autonomous driving.\nHowever, obtaining annotated datasets from real radar im-\nages, crucial for training these networks, is challenging,\nespecially in scenarios with long-range detection and ad-\nverse weather and lighting conditions where radar perfor-\nmance excels. To address this challenge, we present Rad-\nSimReal, an innovative physical radar simulation capable\nof generating synthetic radar images with accompanying\nannotations for various radar types and environmental con-\nditions, all without the need for real data collection. Re-\nmarkably, our findings demonstrate that training object de-\ntection models on RadSimReal data and subsequently eval-\nuating them on real-world data produce performance lev-\nels comparable to models trained and tested on real data\nfrom the same dataset, and even achieves better perfor-\nmance when testing across different real datasets. Rad-\nSimReal offers advantages over other physical radar sim-\nulations that it does not necessitate knowledge of the radar\ndesign details, which are often not disclosed by radar sup-\npliers, and has faster run-time. This innovative tool has the\npotential to advance the development of computer vision al-\ngorithms for radar-based autonomous driving applications.\nOur GitHub: https://yuvalhg.github.io/RadSimReal.\n1. Introduction\nAutomotive radar plays an important role in autonomous\ndriving systems, offering long-range object detection ca-\npabilities and robustness against challenging weather and\nlighting conditions. The radar emits radio frequency (RF)\nsignals and, through the processing of reflected echoes from\nthe surrounding environment, creates a radar reflection in-\ntensity image [6]. The image contains reflection intensities\n*Both authors contributed equally to this work.\nBoth authors are with General Motors, Yuval Haitman is also with the\nSchool of Electrical and Computer Engineering in Ben Gurion University\nof the Negev.\n-30 0  30 90 \nAzimuth [deg]10 15 20 25 30 35 Range [m]\n-53 -37 -24 -12 0  12 24 37 53 90 \nAzimuth [deg]10 15 20 25 30 35 Range [m]Image \nSimulation Radar Image \nSimulation Radar Image \nRadDet \nImage \nRadDet \n-30 0  30 90 \nAzimuth [deg]510 15 20 Range [m]\n-53 -37 -24 -12 0  12 24 37 53 90 \nAzimuth [deg]46810 12 14 16 18 20 Range [m]\n(c) (d) (a) (b) Figure 1. Comparison between synthetic and real radar images\nfrom four different scenarios. Each scenario shows the camera\nimage and the corresponding radar image. (a) and (b) simulation\nscenarios. (c) and (d) real scenarios.\ncorresponding to range and angle coordinates, providing a\nvisual representation of the scene. Afterwards, computer vi-\nsion algorithms are employed to identify objects within this\nvisual image.\nNumerous Deep Neural Network (DNN) methods have\nemerged for detecting objects in radar images [11, 22, 25,\n30, 44, 45]. These techniques involve training the DNN\nusing annotated real data. Several datasets containing real\nannotated radar images have been introduced [27–29, 36,\n40, 44]. These datasets vary in terms of the radar type and\nenvironmental conditions. However, the primary challenge\nwith object detection DNNs trained on real data lies in the\nconsiderable effort required to collect and annotate the data.\nThis challenge is particularly hard in the case of radar since\nit is used to detect objects at long range, adverse weather\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n15407\nand lightening conditions in which annotations are difficult\nto obtain.\nIn an effort to address the challenge posed by data anno-\ntations, an alternative approach that generates training data\nthrough generative methods has been proposed. Several\nstudies have explored the training of a Generative Adver-\nsarial Network (GAN) using unlabeled real radar data to\nproduce synthetic radar data that closely mimics actual real\nradar data [10, 16, 42, 43]. These studies have demonstrated\nthat when training a detection DNN with synthetic data gen-\nerated by the GAN and testing on real data, the performance\ngap compared to training with real data is small.\nDespite the advantage of not requiring data annotation,\ngenerative data generation still presents the hurdle of col-\nlecting a large volume of unlabeled real data. This poses\na significant limitation in system development, as it neces-\nsitates collecting a substantial domain-specific dataset for\neach unique radar sensor, distinct sensor mounting condi-\ntions, and environmental conditions in order to effectively\ntrain the GAN for generating radar images that match the\nspecific distribution of the data. This problem is resolved\nwhen using physical radar simulation instead of generative\nradar simulation.\nIn physical simulation, synthetic radar images are cre-\nated through the physical modeling of the environment and\nthe radar sensor [3, 33]. Consequently, for each distinct\nradar sensor, mounting setup, scenario distribution, and en-\nvironmental conditions a simulated dataset can be generated\nwithout the necessity of collecting any real radar data. Phys-\nical radar simulations have been extensively explored in the\nradar domain [3, 20, 21, 33, 35, 38]. Their process involves\nseveral steps. It begins with the creation of 3D automotive\nscenarios, followed by the calculation of radar reflections\nachieved through signal ray tracing from the radar to objects\nand back. Subsequently, the radar’s received signal is gen-\nerated based on these reflections and the specific radar hard-\nware configuration. Finally, the radar image is produced by\napplying radar-specific signal processing algorithms to the\nreceived signal.\nThe specific hardware and signal processing design of a\nradar significantly influence the output radar image. Thus,\nthe domain shift from one radar type to another is signifi-\ncant, possibly more pronounced than in other sensors like\ncameras. This underscores a major limitation of current\nphysical radar simulations, as they demand a comprehen-\nsive understanding of the radar’s hardware parameters and\nsignal processing algorithms to produce a radar image that\naccurately replicates the real-world image. These details\nare not always disclosed by radar suppliers, and even when\navailable, their implementation within the simulation neces-\nsitates radar expertise. Another issue with physical radar\nsimulation is its high computational demand, resulting in\nlong processing times when generating large datasets.The similarity between real radar images and synthetic\nimages generated by physical radar simulation, when the\nhardware and signal processing details of the radar are avail-\nable, has been evaluated both qualitatively [20, 21, 35] and\nquantitatively. The quantitative evaluation involves calcu-\nlating correlation coefficients between synthetic and real\nimages [33], and measuring distances between correspond-\ning reflection points or objects in synthetic and real images\n[26]. However, there has been a lack of evaluation concern-\ning the performance gap between an object detection DNN\ntrained with physical radar data and tested on real data, in\ncomparison to a DNN solely trained and tested on real data.\nIn this paper, we present RadSimReal an innovative\nphysical radar simulation method that does not require prior\nknowledge of the radar hardware specifications or its signal\nprocessing algorithms. Consequently, it can be applied to\na wide range of radar sensors without necessitating exper-\ntise in radar-specific details. Additionally, this novel sim-\nulation approach offers considerably faster processing time\nin comparison to conventional physical radar simulations.\nFig. 1 illustrates the similarity between radar images gener-\nated by RadSimReal and real radar images, which raises the\nquestion of whether synthetic images can replace real ones\nfor training object detection neural networks. To answer\nthis, we conduct a novel analysis comparing the perfor-\nmance of DNN object detection models trained with Rad-\nSimReal data and tested on real data versus models trained\nand tested exclusively with real data. Our findings reveal\nthat object detection DNNs trained on RadSimReal data and\ntested on real data exhibit performance levels comparable to\nthose trained on real data when both the training and testing\ndatasets are sourced from the same real dataset, and outper-\nform them when the training and testing are from different\nreal datasets. RadSimReal is a powerful tool for efficiently\ngenerating extensive annotated training data with flexibility\nin radar sensor type, mounting configurations and environ-\nmental conditions, all without the overhead effort of collect-\ning real data for each specific setting.\nThe main contributions of the paper are as follows:\n1. A pioneer analysis revealing that object detection DNNs,\ntrained with physical radar simulation data and tested on\nreal data, perform comparably to models trained on real\ndata when the train and test sets of the real data are from\nthe same dataset, and outperforms them when they are\nfrom different datasets. This highlights the benefits of\nphysical simulation, efficiently generating training data\nwith annotations for diverse radar configurations without\nthe challenges of collecting real data, overcoming a main\nlimitation in generative approaches.\n2. An innovative physical radar simulation technique that\noffers an important advantage over other reference phys-\nical simulation methods by not necessitating in-depth\nknowledge of specific radar implementation details,\n15408\nwhich are often undisclosed, nor radar expertise, and\nalso has a significantly faster run-time.\n3. A simulation tool offering efficient data generation with\nground truth information that conveniently supports var-\nious radar types. This tool will contribute to advancing\nradar-based computer vision research.\n2. Related Work\n2.1. Radar Object Detection\nVarious studies in the literature extensively investigate DNN\nmethods for object detection in radar images. Kim et al.\n[22] applied YOLO [30] to radar images, surpassing the per-\nformance of conventional radar detection methods. Xu et al.\n[11] used a ResNet-18 encoder with a CNN decoder to esti-\nmate 3D bounding box properties. RADDet [44] integrated\nresidual blocks and YOLO detection heads [7], while Zhang\net al. [45] employed a CNN-based version of U-Net for\nradar object detection. Meyer et al. [25] utilized graph con-\nvolution networks for radar object detection. A two-stage\nobject detection approach was introduced in [17]. An al-\nternative approach for object detection involves using radar\npoint clouds [9, 13, 15, 24, 32, 34], which are generated\nfrom the reflection points detected in radar images using the\nConstant False Alarm Rate (CFAR) algorithm [31]. CFAR\nintroduces significant information loss during subsequent\nDNN processing [22], leading to a considerable degrada-\ntion in object detection performance when compared to us-\ning radar images. Therefore, this paper focuses on DNN\nprocessing of radar images.\n2.2. Radar Datasets\nNumerous publicly accessible automotive radar datasets of-\nfer real radar images along with object annotations. The\nRADIATE [36] and Oxford RobotCar [4] datasets use a\n360° mechanical scanning antenna, differing from conven-\ntional radar images generated by antennas arrays. The\nCRUW dataset [41] provides radar images with a limited\nrange of up to 25 meters, and the annotations consist of ob-\nject center points without bounding box information. RA-\nDIal [29] and K-Radar [28] provide high-resolution radar\nreflection intensity images, but details about their radar\nhardware are undisclosed, making them unsuitable for con-\nventional physical radar simulation. The RADDet [44]\ndataset features radar images from 15 diverse automotive\nscenarios captured using a Texas Instruments (TI) automo-\ntive radar prototype [1, 2]. The CARRADA dataset [27]\ncomprises radar images from 30 controlled scenarios, uti-\nlizing the same TI radar as RADDet, allowing for cross-\ndataset performance evaluation. Furthermore, the availabil-\nity of hardware specifications for the TI prototype radar\nin these datasets facilitates the generation of synthetic data\nthrough traditional physical radar simulation.2.3. Generative Radar Data Generation\nWeston et al. [42] introduced a GAN method that generates\nsynthetic radar images conditioned on a 3D representation\nfrom CARLA simulation [12], which was trained on a rel-\natively large dataset comprising 222,420 images from the\nOxford RobotCar [4]. Their study showed that a segmenta-\ntion model, trained on synthetic data and tested on real data,\nachieves performance similar to a model trained and tested\nexclusively on real data. L2R GAN [39] used the same ex-\ntensive dataset to train a GAN for converting LIDAR point\nclouds into radar images. Synthetic image quality was eval-\nuated against real images using PSNR and SSIM metrics,\nalong with qualitative assessment through a human subjec-\ntive study. Oliveira and Bekooij [10] employed a GAN to\ngenerate synthetic radar images conditioned on bounding\nbox layouts. They showed a small performance gap be-\ntween an object detection DNN trained on synthetic data\nand tested on real data versus one trained solely on real data.\nFidelis [16] used a GAN to generate radar received signals\nand derived radar images through signal processing. Their\nstudy showed a close distribution resemblance between syn-\nthetic and real images, assessed by the FID metric [19].\nWheeler et al. [43] used a Variational Autoencoder con-\nditioned on an object list and raster grid of the ground road-\nways. They validated synthetic image similarity to real im-\nages using K-L divergence for clutter and average squared\ndeviation for objects. While generative methods bridge the\nsynthetic-real data gap, they require extensive training data\nfor each radar type, mounting configuration, and environ-\nmental condition, posing a significant overhead.\n2.4. Physical Radar Simulation Data Generation\nIn physical simulation, synthetic radar images are gener-\nated by simulating the environment, the radar sensor, and\nits installation on the vehicle. Numerous studies have pro-\nposed methods for physically modeling both the environ-\nment and radar systems. MaxRay [3] created realistic sce-\nnarios in Blender [18], simulated reflections through ray\ntracing with RF propagation properties, and tested radar de-\ntection and clutter removal on synthetic data of an OFDM\nradar simulation. ViRa [33] used the Unity game engine [8]\nto generate environments and simulated an FMCW radar.\nThey demonstrated the similarity of their simulated radar\ndata to real-world radar measurements in a laboratory set-\nting using correlation metrics. Thieling et al. [38] pro-\nposed a radar simulation with environmental influences like\nrain. Their study assessed the simulation’s performance\nin qualitative terms. In another approach [35], a ray trac-\ning technique for a MIMO radar simulation was presented,\nwith the realism of the generated radar images evaluated\nthrough qualitative comparisons to real radar images for the\nsame scenarios. Several other ray tracing radar simulations\nfor realistic automotive scenarios have also been published\n15409\n[14, 20, 21], which verified the simulations’ accuracy by\ncomparing them to real measurements.\nAll these radar simulation studies haven’t explored the\nuse of DNN detection methods with physical radar simu-\nlation data. The performance gap between training object\ndetection DNNs with radar simulation synthetic data versus\nreal data and testing on real data remains unexplored. Fur-\nthermore, implementing these simulations requires detailed\nknowledge of radar hardware design and signal processing\nalgorithms, factors often undisclosed by radar suppliers.\n3. Proposed Physical Simulation Method\nFig. 2 presents a block diagram comparing our proposed\nphysical simulation approach, RadSimReal , with the con-\nventional physical simulation method. The diagram con-\nsists of three primary components. RadSimReal encom-\npasses Fig. 2(a) and (c), whereas the conventional simula-\ntion method involves Fig. 2(a) and (b). Fig. 2(a) pertains\nto the environmental simulation, a shared element in both\napproaches, independent of the specific radar sensor. In this\npart, a 3D scene is generated using a graphics engine; in our\nimplementation, we employed CARLA [12]. Then, dense\nreflection points from objects within the scene are acquired\nby ray-tracing of RF propagation paths, starting from the\nradar to objects in the environment and then returning to\nthe radar [35]. Subsequently, the RF reflectivity of these\npoints is determined using physical formulas [5], account-\ning for surface material, orientation, and distance from the\nradar. Following the environmental simulation, the subse-\nquent step involves the radar simulation part, responsible\nfor translating the intensities of reflection points into the\nradar’s output image. Fig. 2(c) illustrates the radar simula-\ntion block diagram of RadSimReal , which deviates from the\nconventional radar simulation method shown in Fig. 2(b).\nIn the conventional radar simulation presented in\nFig. 2(b), the radar’s received signal, stemming from all re-\nflection points within the scene, is acquired based on the\nspecific hardware design details of the radar. Additionally,\nnoise is introduced into the received signal. For detailed\ninformation regarding this part, we refer readers to the sup-\nplementary material. Following this, radar signal process-\ning algorithms are applied to the received signal, yielding\nthe radar 3D tensor. This tensor encapsulates the radar re-\nflection intensity across range, azimuth angle, and Doppler\ncells. Notably, we consider a radar with only azimuth angle\nand without elevation, although the simulation is not limited\nto such radar and could be expanded to include elevation. In\nthe final step of Fig. 2(b), the 3D tensor is transformed into\nan image format with dimensions of range and azimuth an-\ngle by selecting the maximum value along the Doppler di-\nmension. It’s worth mentioning that the simulation provides\nflexibility in outputting the entire radar tensor or converting\nit into an image using various techniques, depending on theimplementation of the object detection DNN.\nThe conventional radar simulation method depicted in\nFig. 2(b), necessitates an in-depth comprehension of the\nradar hardware design and the associated software pro-\ncessing to faithfully mimic a specific radar. This entails\na comprehensive understanding of specific details such as\nthe transmit signal waveform, antenna array configuration,\nsampling rate, beamforming algorithms, and more. How-\never, this presents a major challenge, as these details are\noften proprietary and not publicly disclosed by radar man-\nufacturers. This limitation is resolved in Fig. 2(c) by Rad-\nSimReal , as elaborated in the following.\nRadSimReal relies on the radar’s point spread function\n(PSF). Every reflection point is apparent in the radar tensor\nby a multi-dimensional PSF, spanning dimensions of range,\nangle, and Doppler, and centered on the coordinates of the\nreflection point. Fig. 3(a) shows an example of a 2D slice of\na radar’s PSF in range and angle centered at a range of 25m\nand an azimuth angle of 0◦. The PSF exhibits a wide spread\nin the angular domain but a narrow spread in both range\nand Doppler, although the Doppler aspect is not depicted in\nthe figure. This characteristic arises from the radar’s coarse\nangular resolution and high range and Doppler resolution.\nIt is essential to note that the shape of the PSF depends\nupon the specific radar hardware design and signal process-\ning algorithms [37]. Fortunately, it can be acquired from a\nradar tensor of a narrow object, such as a pole or a dedi-\ncated corner reflector [23]. Importantly, obtaining the PSF\ndoes not require knowledge of any details about the radar\ndesign, thereby circumventing a key limitation of conven-\ntional radar simulation. The straightforward procedure for\nobtaining the radar’s PSF from a radar tensor is detailed in\nthe supplementary material.\nIn Fig. 3(b), we present a radar image generated through\nthe conventional simulation outlined in Fig. 2(b). This im-\nage illustrates a scenario with three reflection points and\nis free from noise. In Fig. 3(c), we depict the outcome of\nthe conventional simulation for the same scenario, this time\nwith the addition of noise. The white points in the figures\ndenote the locations of these reflection points. Notably, the\nimage displays a superposition of the radar’s PSF centered\non each point. Therefore, the same image can be obtained\nby convolution between the reflection points and the PSF.\nAs a result, RadSimReal generates the radar tensor by 3D\nconvolution of the reflection points with the PSF, followed\nby the addition of noise, as presented in Fig. 2(c). The ten-\nsor is then converted to an image similarly as in Fig. 2(b).\nDespite the differences in the simulation implementa-\ntions in Fig. 2(c) and Fig. 2(b), they lead to the generation\nof an identical radar image when the full PSF is applied.\nFor a detailed mathematical explanation of this equivalence,\nplease refer to the supplementary material. Notably, the en-\nergy of the PSF diminishes rapidly from its central point in\n15410\nScene \nGeneration \n+ Ray \nTracing Assign \nReflection \nIntensity Generate \nReceived \nSignal Radar \nSignal \nProcessing Convert to \nRadar \nImage \nConvolution of \nReflection \nPoints & PSF Convert to \nRadar \nImage Radar HW \nParameters \nReceived \nSignal Noise \nReflection \nPoints Reflections \nIntensity \nAngle Range Radar \nTensor Radar \nImage \nRange \nAngle \nNoise \nRadar ’s PSF Radar \nTensor \nAngle Range Radar \nImage \nRange \nAngle (a) Environment Simulation (b) Conventional Radar Simulation \n(c) Proposed Radar Simulation Figure 2. Block diagram illustrating the processing steps for conventional simulation (a)+(b) and RadSimReal (a)+(c). (a) Simulates the\nenvironment to generate reflection points with RF reflectivity of an automotive scene, while (b) and (c) represent the conventional approach\nandRadSimReal ’s approach, respectively, for transforming the reflection points into a radar image.\nboth the range and Doppler domains. To reduce the compu-\ntational complexity of the simulation, we truncate the PSF\nto encompass up to 99% of its energy. This truncation leads\nto a substantial reduction in the PSF size by a factor exceed-\ning 1000, thereby significantly enhancing the run-time of\nthe convolution operation in the simulation. In Fig. 3(d), the\ntruncated PSF is displayed, containing 99% of the energy of\nthe original PSF from Fig. 3(a). The radar image, obtained\nthrough convolution of the truncated PSF with the same\nthree reflection points as depicted in Fig. 3(b), is showcased\nin Fig. 3(e) without noise and in Fig. 3(f) with the addition\nof noise. The truncated portion of the PSF has very low in-\ntensity value (about 80 dB lower than PSF peak), which is\nsignificantly lower than the noise level. Consequently, the\ndifference between the radar image from the conventional\nsimulation (Fig. 3(c)) and our proposed simulation utilizing\nthe truncated PSF (Fig. 3(f)) is practically indistinguishable.\nWhile RadSimReal (Fig. 2(a)+(c)) produces radar im-\nages similar to the conventional simulation (Fig. 2(a)+(b)),\nit possesses two significant advantages over the conven-\ntional simulation. Firstly, it eliminates the necessity of pos-\nsessing in-depth radar design information by relying on a\nsimple radar measurement of its PSF and noise variance, as\ndetailed in the supplementary material. Secondly, it exhibits\na significantly faster run-time, as evaluated and demon-\nstrated in Section 3.2.3.1. Simulation Fidelity Evaluation\nIn this section, we assess the resemblance between real\nradar images and synthetic images generated by RadSim-\nReal. Our evaluation initiates with a comparison be-\ntween synthetic and real radar images generated from the\nsame scenario. Creating a simulation scene that faith-\nfully replicates a real-world scenario for which an actual\nradar measurement was taken poses a considerable chal-\nlenge. To overcome this challenge, we substituted the re-\nflection points derived from the CARLA simulation engine\nin Fig. 2(a) with points obtained from a high-resolution LI-\nDAR sensor. This LIDAR was positioned in close proxim-\nity to the radar and captured measurements from the same\nscene at the same time as the real radar measurement. For\nthe assignment of the RF reflection intensity to the LIDAR\npoints the surface material and orientation of the LIDAR\npoints were obtained by the following three steps: (1) Man-\nual segmentation of LIDAR points into object types such\nas vehicles, poles, signs, roads, and buildings. (2) Surface\nmaterial allocation of each point based on its object type.\n(3) Computation of the angle of the surface normal vector\nusing a polygon mesh generated from the LIDAR points.\nThe remaining stages of the simulation processing were as\noutlined in Fig. 2(c).\nAn example of a real radar image is shown in Fig. 4(c)\nand a synthetic radar image in Fig. 4(d) generated from the\nsame scene (as explained above). The prototype radar used\n15411\n-80 -60 -40 -20 0 20 40 60 80 \nAzimuth [deg]15 20 25 30 Range [m]\n-80 -70 -60 -50 -40 -30 -20 -10 0[db]\n-80 -60 -40 -20 0 20 40 60 80 \nAzimuth [deg]15 20 25 30 Range [m]\n-50 -45 -40 -35 -30 -25 -20 -15 -10 -50[db]\n-80 -60 -40 -20 0 20 40 60 80 \nAzimuth [deg]15 20 25 30 Range [m]\n-80 -70 -60 -50 -40 -30 -20 -10 0[db]\n-80 -60 -40 -20 0 20 40 60 80 \nAzimuth [deg]15 20 25 30 Range [m]\n-50 -45 -40 -35 -30 -25 -20 -15 -10 -50[db](a) \nPoint \nReflector 1\nPoint \nReflector 2 Point \nReflector 3 \n-80 -60 -40 -20 0 20 40 60 80 \nAzimuth [deg]15 20 25 30 35 Range [m]\n-80 -70 -60 -50 -40 -30 -20 -10 0[db]\n-80 -60 -40 -20 0 20 40 60 80 \nAzimuth [deg]15 20 25 30 35 Range [m]\n-80 -70 -60 -50 -40 -30 -20 -10 0[db]\n(b) \n(c) (d) \nPoint \nReflector 1\nPoint \nReflector 2 Point \nReflector 3 \n(e) \n(f) Figure 3. Radar image generated with conventional simulation vs.\nRadSimReal . (a) Radar’s PSF 2D slice in range and angle dimen-\nsions. (b) Radar image without noise generated by conventional\nsimulation for a scenario with 3 reflection points. (c) Radar image\nof (b) with noise. (d) Truncated PSF with 99% of its energy. (e)\nRadar image obtained for the same scenario as in (b) by RadSim-\nReal without noise, (f) The radar image of (e) with noise.\nin Fig. 4(c) had 3.9◦azimuth resolution, and 0.28mrange\nresolution, and the same radar was simulated in Fig. 4(d).\nThe camera image of the scene is presented in Fig. 4(a),\nand the LIDAR points segmented to different object types\nare shown in Fig. 4(b). It is observed that the synthetic and\nreal reflection intensity images have close resembles, which\nshows that the simulation models well the real radar.\nNext, we present a comparison between real radar im-\nages extracted from the RADDet dataset [44], and syn-\nthetic images generated by simulating the same radar as\nin RADDet with RadSimReal . As the RADDet dataset\nlacks LIDAR measurements, simulating the precise scenar-\nios of real radar images, as depicted in Fig. 4, is unfea-\nsible. Hence, we present synthetic and real radar images\nfrom different scenarios and assess their characteristic re-\nsemblance. In Fig. 1, the upper two rows illustrate syn-\nthetic radar images along with their corresponding camera\nimages, produced by simulating the radar used in the RAD-\nDet dataset. Meanwhile, the lower two rows exhibit real\nradar images and their respective camera images from the\n(c) Real radar image \ndB \n0\n-80 -60 -40 -20 Vehicle \nBuilding \nGround Light poles & signs (a) Camera image (b) Segmented LIDAR points \n(d) Simulated radar image Figure 4. Comparison between RadSimReal image and a real radar\nimages for the same scenario. (a) Camera image of the scenario.\n(b) High-resolution LIDAR points segmented by object type. (c)\nReal radar image. (d) RadSimReal image. The black points in (c)\nand (d) represent the LIDAR points.\nRADDet dataset. Although the synthetic and real images\nstem from different scenarios and cannot be compared one\nto one, they exhibit analogous characteristics. Both real and\nsynthetic radar images display reflections with varying in-\ntensities and have a similar spreading functions. The close\nresemblance between the synthetic and real images makes\nit hard to distinguish between them, which is another indi-\ncation that RadSimReal models well the real radar.\nWe also assessed the statistical similarity between the\nRadSimReal data and real data using the Frechet Inception\nDistance (FID) [19]. The FID score is commonly employed\nin the literature to quantify the statistical resemblance be-\ntween synthetic and real datasets. The FID score between\nthe RADDet training set (comprising 8196 images) and the\nRADDet test set (comprising 1962 images) is 6.76. On\nthe other hand, the FID score between the RADDet train-\ning set and a synthetic dataset of 10,000 images generated\nbyRadSimReal is 6.54. The similarity between these two\nscores indicates that the statistical characteristics of the syn-\nthetic dataset produced by RadSimReal closely resemble\nwith those of the RADDet data.\n3.2. Computation Efficiency\nThe complexity ratio between conventional simulation and\nRadSimReal corresponds to the ratio between the entire\nradar tensor volume and the PSF volume. As explained in\nSection 3, our simulation truncates the PSF to preserve 99%\nof its energy, drastically reducing its volume. This results\nin a substantial complexity reduction, approximately by a\nfactor of 1000, with RadSimReal compared to conventional\nradar simulation. Further details on the computational com-\nplexity calculation and run time measurements are available\nin the supplementary material.\n15412\n4. Simulation to Real Domain Gap Analysis\nIn this section, we analyze the object detection performance\ngap between models trained with RadSimReal data and\nthose trained with real data, both tested on real data. For this\nanalysis we use three different object detection methods:\n‘U-Net’, ‘RADDet’, and ‘Probabilistic’. ‘U-Net’ employs a\nU-Net as proposed in [45] with an additional input channel\nof the input image Cartesian coordinates. ‘RADDet’ refers\nto the object detection network introduced in the RADDet\npaper [44], while ‘Probabilistic’ is the network from [11].\nWe assess performance using the RADDet [44], CAR-\nRADA [27], and CRUW [41] datasets. These datasets fea-\nture automotive radar reflection images captured with a TI\nradar prototype [1, 2] in diverse scenes, providing extensive\ntesting coverage. While RADDet and CARRADA feature\n2D bounding box annotations for objects, CRUW provides\nannotations indicating the center points of objects, to which\nwe’ve subsequently added bounding box extensions using\nthe CFAR algorithm [31]. RADDet includes images from\n15 densely populated automotive scenarios with favorable\nweather conditions, while CARRADA comprises 30 staged\nscenarios with varying object densities and weather condi-\ntions, including challenging conditions like snow. In both\ndatasets the radar is mounted on a stationary platform. In\nthe CRUW dataset the radar is mounted on a vehicle, captur-\ning radar images from scenarios where the ego-vehicle was\nin motion (highway and city streets) and scenarios where it\nwas stationary (campus roads and parking lots).\nWe employed a dataset split for RADDet that ensures\ndifferent scenarios in the train and test sets thereby prevent-\ning potential overfitting in the split proposed in the dataset\npaper [44]. Our RADDet split consists of a training set with\n8196 images and a test set with 1962 images. For CAR-\nRADA, the training set comprises 2208 images, and the\ntest set includes 276 images. As for CRUW, our training\nset comprises 9623 images, with a test set of 2226 images.\nThe synthetic dataset generated by RadSimReal comprised\n10000 training images, comparable in size to that of RAD-\nDet and CRUW. The simulated scenarios involved a radar\nmounted on a vehicle driving in city streets, experiencing\nboth stationary and moving phases.\nWe conducted performance tests on the three object de-\ntection models mentioned above using three separate test\ndatasets: RADDet, CARRADA and CRUW. These mod-\nels were individually trained with four distinct datasets:\nthe RADDet training set, the CARRADA training set, the\nCRUW training set, or the RadSimReal dataset. Perfor-\nmance was assessed using Average Precision (AP) for class\n’car’ at IOU thresholds 0.1, 0.3 and 0.5. The AP was de-\ntermined by the area under the precision-recall curve. Ta-\nble 1 presents AP results, comparing performance between\ntraining and testing on individual datasets versus training\nwith RadSimReal and testing on different datasets. It addi-tionally includes cross-dataset evaluation between RADDet\nand CARRADA, both characterized by a stationary radar\nsetup (unlike CRUW), with the primary difference lying\nin their scenes. The results reveal several important in-\nsights. All three models trained using RadSimReal exhibit\nperformance on both the RADDet and CRUW test sets that\nclosely resembles their performance when trained on the\ncorresponding RADDet or CRUW training sets. In eval-\nuations with the CARRADA test set, models trained with\nRadSimReal consistently outperform those trained with the\nCARRADA training set, likely due to the small size of\nthe latter. Notably, models trained with RADDet experi-\nence a significant performance decline on the CARRADA\ntest set compared to their performance when trained with\nRadSimReal . These results demonstrate that object detec-\ntion DNNs trained with RadSimReal perform comparable\nto those trained on real data and even outperform DNNs\ntrained on real data when subjected to cross-dataset evalua-\ntion or when dealing with limited training data.\nSubsequently, we assess the performance of object de-\ntection models on the RADDet test set when trained using a\ncombination of data from RadSimReal and the training set\nof RADDet. The outcomes of this evaluation are presented\nin Table 2. The findings reveal that augmenting the Rad-\nSimReal dataset with real datasets from RADDet does not\nyield a significant performance enhancement. This suggests\nthat the domain shift from RadSimReal data to real data is\ninsignificant.\nNext, we provide qualitative examples comparing be-\ntween an object detection model trained with RadSimReal\nand one trained with real data. In Fig. 5, we showcase the\ndetection scores at the output of the ‘U-Net’ model for two\nexamples taken from the RADDet test set, comparing the\nperformance of the model trained with RadSimReal against\nthe same model trained with the RADDet training set. Each\nexample is displayed in a separate row, featuring the origi-\nnal image alongside the output detection scores and bound-\ning boxes of both models. Notably, the detection scores and\nboxes of both models resemble each other, indicating that\nthe model trained with synthetic data delivers similar per-\nformance to the one trained exclusively on real data.\nThe analysis in this section shows RadSimReal ’s suc-\ncess in bridging the object detection performance gap be-\ntween synthetic and real data. It is important to note that\nthe images generated by RadSimReal are similar to those\nproduced by other existing physical radar simulations. Con-\nsequently, training object detection models with other phys-\nical radar simulations could achieve a similar performance\nas with RadSimReal . The significance of our study lies in\nunveiling this key discovery for the first time and introduc-\ningRadSimReal , which holds advantages over existing sim-\nulations. It eliminates the need for in-depth knowledge of\nthe radar design, typically undisclosed, and has faster run-\n15413\nTable 1. AP at different IOU for three object detection models trained on RadSimReal or real data and tested on real data\nTest Set Train SetU-Net Model Probabilistic Model RADDet Model\n@0.1 @0.3 @0.5 @0.1 @0.3 @0.5 @0.1 @0.3 @0.5\nRADDetRADDet 84.76 83.01 55.53 83.31 74.80 40.68 83.69 72.96 47.95\nRadSimReal 85.63 82.16 57.64 82.75 75.83 52.36 83.48 73.91 46.63\nCARRADA 22.33 19.46 14.04 24.58 22.53 16.98 19.18 15.37 9.47\nCARRADACARRADA 50.99 49.00 44.65 50.16 48.22 39.51 31.94 26.65 17.79\nRadSimReal 70.77 62.47 43.96 63.51 56.92 41.07 72.39 65.65 28.76\nRADDet 62.40 56.84 30.78 57.02 54.23 19.53 69.63 61.63 20.49\nCRUWCRUW 86.66 78.83 56.54 81.90 77.22 52.95 85.74 70.10 54.36\nRadSimReal 86.82 77.51 55.47 80.04 76.50 51.45 86.21 69.60 52.48\nconstructions constructions \n(a) (c)\n-25 -20 -15 -10 -5 0 5 10 15 20 \nx [m]25 30 35 40 45 y [m]\n-25 -20 -15 -10 -5 0 5 10 15 20 \nx [m]25 30 35 40 45 y [m]\n-25 -20 -15 -10 -5 0 5 10 15 20 \nx [m]25 30 35 40 45 y [m]\n-20 -15 -10 -5 0 5 10 15 \nx [m]15 20 25 30 y [m]\n-20 -15 -10 -5 0 5 10 15 \nx [m]15 20 25 30 y [m]\n-20 -15 -10 -5 0 5 10 15 \nx [m]15 20 25 30 y [m]\n(b)\nFigure 5. Qualitative comparison of object detection DNN trained on RadSimReal vs. real data from the RADDet dataset. Rows correspond\nto different scenarios from RADDet test set. (a) Input radar image, (b) ‘U-Net’ model’s detection score and bounding boxes trained with\nRADDet. (c) ‘U-Net’ trained with RadSimReal data. Detected and ground truth bounding boxes marked in pink and white, respectively.\nTable 2. Object detection AP at various IOU for ‘U-Net’ trained\nwith combined RadSimReal ’s data (Sim) and real data (Real)\nTest Set Train Set @0.1 @0.3 @0.5\nRADDetSim 85.63 82.16 57.64\nSim + Real 86.09 83.38 58.13\nCARRADASim 70.77 62.47 43.96\nSim + Real 71.11 62.63 44.52\ntime. Additionally, we acknowledge that the synthetic to\nreal performance gap can also be closed by generating data\nwith generative methods such as GAN. However, these ap-\nproaches have a major drawback compared to the physi-\ncal simulation model; they require the collection of a large\namount of real data for each distinct variation in radar type,\nits mounting configuration and environmental conditions.\n5. Conclusion\nThis paper introduces RadSimReal , a novel physical radar\nsimulation that generates synthetic radar images for train-ing object detection DNNs. We have shown that the Rad-\nSimReal images closely resemble real radar images both\nqualitatively and statistically. Most importantly, our results\nreveal that training object detection DNNs with these syn-\nthetic images and testing them on real data yield results sim-\nilar to those obtained when training exclusively with real\ndata. Moreover, it attains superior performance in cross-\ndataset evaluations with different real datasets.\nRadSimReal offers distinct advantages over alternative\nmethods of synthetic data generation. It can efficiently sim-\nulate diverse radar types without the need for extensive real\ndata collection, a process demanding substantial resources,\nor in-depth knowledge of proprietary radar implementation\ndetails, which are often confidential. Instead, it only re-\nquires a measurement of the radar’s PSF. This work high-\nlights the great potential of radar simulation in radar-based\ncomputer vision applications, paving the way for its in-\ncreased adoption and further exploration in this field.\n15414\nReferences\n[1] AWR1843 data sheet, product information and support,\nhttps://www.ti.com/product/awr1843, . 3, 7\n[2] TI mmwave-sdk software development kit (SDK),\nhttps://www.ti.com/tool/mmwave-sdk, . 3, 7\n[3] Maximilian Arnold, M Bauhofer, Silvio Mandelli, Marcus\nHenninger, Frank Schaich, Thorsten Wild, and Stephan ten\nBrink. Maxray: A raytracing-based integrated sensing and\ncommunication framework. In 2022 2nd IEEE International\nSymposium on Joint Communications & Sensing (JC&S) ,\npages 1–7. IEEE, 2022. 2, 3\n[4] Dan Barnes, Matthew Gadd, Paul Murcutt, Paul Newman,\nand Ingmar Posner. The oxford radar robotcar dataset: A\nradar extension to the oxford robotcar dataset. In 2020\nIEEE International Conference on Robotics and Automation\n(ICRA) , pages 6433–6438. IEEE, 2020. 3\n[5] R Mahafza Bassem and Z Elsherbeni Atef. Matlab simula-\ntions for radar systems design. CRC, London , 2004. 4\n[6] Igal Bilik, Oded Bialer, Shahar Villeval, Hasan Sharifi,\nKeerti Kona, Marcus Pan, Dave Persechini, Marcel Musni,\nand Kevin Geary. Automotive mimo radar for urban envi-\nronments. In 2016 IEEE Radar Conference (RadarConf) ,\npages 1–6. IEEE, 2016. 1\n[7] Alexey Bochkovskiy, Chien-Yao Wang, and Hong-\nYuan Mark Liao. Yolov4: Optimal speed and accuracy of\nobject detection. arXiv preprint arXiv:2004.10934 , 2020. 3\n[8] Ismail Buyuksalih, Serdar Bayburt, Gurcan Buyuksalih, AP\nBaskaraca, Hairi Karim, and Alias Abdul Rahman. 3d mod-\nelling and visualization based on the unity game engine–\nadvantages and challenges. ISPRS Annals of the Photogram-\nmetry, Remote Sensing and Spatial Information Sciences , 4:\n161–166, 2017. 3\n[9] Andreas Danzer, Thomas Griebel, Martin Bach, and Klaus\nDietmayer. 2d car detection in radar data with pointnets.\nIn2019 IEEE Intelligent Transportation Systems Conference\n(ITSC) , pages 61–66. IEEE, 2019. 3\n[10] Marcio L Lima de Oliveira and Marco JG Bekooij. Gen-\nerating synthetic short-range fmcw range-doppler maps us-\ning generative adversarial networks and deep convolutional\nautoencoders. In 2020 IEEE Radar Conference (Radar-\nConf20) , pages 1–6. IEEE, 2020. 2, 3\n[11] Xu Dong, Pengluo Wang, Pengyue Zhang, and Langechuan\nLiu. Probabilistic oriented object detection in automotive\nradar. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition Workshops , pages 102–\n103, 2020. 1, 3, 7\n[12] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Anto-\nnio Lopez, and Vladlen Koltun. Carla: An open urban driv-\ning simulator. In Conference on robot learning , pages 1–16.\nPMLR, 2017. 3, 4\n[13] Maria Dreher, Emec ¸ Erc ¸elik, Timo B ¨anziger, and Alois\nKnol. Radar-based 2d car detection using deep neural net-\nworks. In 2020 IEEE 23rd International Conference on In-\ntelligent Transportation Systems (ITSC) , pages 1–8. IEEE,\n2020. 3\n[14] Manuel Dudek, Ren ´e Wahl, Dietmar Kissinger, Robert\nWeigel, and Georg Fischer. Millimeter wave fmcw radarsystem simulations including a 3d ray tracing channel sim-\nulator. In 2010 Asia-Pacific Microwave Conference , pages\n1665–1668. IEEE, 2010. 4\n[15] Zhaofei Feng, Shuo Zhang, Martin Kunert, and Werner\nWiesbeck. Point cloud segmentation with a high-resolution\nautomotive radar. In AmE 2019-Automotive meets Electron-\nics; 10th GMM-Symposium , pages 1–5. VDE, 2019. 3\n[16] Eduardo C Fidelis, Fabio Reway, Herick Ribeiro, Pietro L\nCampos, Werner Huber, Christian Icking, Lester A Faria,\nand Torsten Sch ¨on. Generation of realistic synthetic raw\nradar data for automated driving applications using genera-\ntive adversarial networks. arXiv preprint arXiv:2308.02632 ,\n2023. 2, 3\n[17] Yuval Haitman and Oded Bialer. Boostrad: Enhancing object\ndetection by boosting radar reflections. In Proceedings of the\nIEEE/CVF Winter Conference on Applications of Computer\nVision , pages 1638–1647, 2024. 3\n[18] Roland Hess. Blender foundations: The essential guide to\nlearning blender 2.5 . Taylor & Francis, 2013. 3\n[19] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. Gans trained by a\ntwo time-scale update rule converge to a local nash equilib-\nrium. Advances in neural information processing systems ,\n30, 2017. 3, 6\n[20] Nils Hirsenkorn, Paul Subkowski, Timo Hanke, Alexander\nSchaermann, Andreas Rauch, Ralph Rasshofer, and Erwin\nBiebl. A ray launching approach for modeling an fmcw radar\nsystem. In 2017 18th International Radar Symposium (IRS) ,\npages 1–10. IEEE, 2017. 2, 4\n[21] Martin Holder, Clemens Linnhoff, Philipp Rosenberger, and\nHermann Winner. The fourier tracing approach for modeling\nautomotive radar sensors. In 2019 20th International Radar\nSymposium (IRS) , pages 1–8. IEEE, 2019. 2, 4\n[22] Woosuk Kim, Hyunwoong Cho, Jongseok Kim, Byungkwan\nKim, and Seongwook Lee. Yolo-based simultaneous target\ndetection and classification in automotive fmcw radar sys-\ntems. Sensors , 20(10):2897, 2020. 1, 3\n[23] Eugene F Knott, John F Schaeffer, and Michael T Tulley.\nRadar cross section . SciTech Publishing, 2004. 4\n[24] Florian Kraus, Nicolas Scheiner, Werner Ritter, and Klaus\nDietmayer. Using machine learning to detect ghost images\nin automotive radar. In 2020 IEEE 23rd International Con-\nference on Intelligent Transportation Systems (ITSC) , pages\n1–7. IEEE, 2020. 3\n[25] Michael Meyer, Georg Kuschk, and Sven Tomforde. Graph\nconvolutional networks for 3d object detection on radar data.\nInProceedings of the IEEE/CVF International Conference\non Computer Vision , pages 3060–3069, 2021. 1, 3\n[26] Anthony Ngo, Max Paul Bauer, and Michael Resch. A multi-\nlayered approach for measuring the simulation-to-reality gap\nof radar perception for autonomous driving. In 2021 IEEE\nInternational Intelligent Transportation Systems Conference\n(ITSC) , pages 4008–4014. IEEE, 2021. 2\n[27] Arthur Ouaknine, Alasdair Newson, Julien Rebut, Florence\nTupin, and Patrick P ´erez. Carrada dataset: Camera and\nautomotive radar with range-angle-doppler annotations. In\n2020 25th International Conference on Pattern Recognition\n(ICPR) , pages 5068–5075. IEEE, 2021. 1, 3, 7\n15415\n[28] Dong-Hee Paek, Seung-Hyun Kong, and Kevin Tirta Wijaya.\nK-radar: 4d radar object detection dataset and benchmark\nfor autonomous driving in various weather conditions. arXiv\npreprint arXiv:2206.08171 , 2022. 3\n[29] Julien Rebut, Arthur Ouaknine, Waqas Malik, and Patrick\nP´erez. Raw high-definition radar for multi-task learning. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition , pages 17021–17030, 2022. 1,\n3\n[30] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali\nFarhadi. You only look once: Unified, real-time object de-\ntection. In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pages 779–788, 2016. 1, 3\n[31] Hermann Rohling. Radar cfar thresholding in clutter and\nmultiple target situations. IEEE transactions on aerospace\nand electronic systems , (4):608–621, 1983. 3, 7\n[32] Nicolas Scheiner, Nils Appenrodt, J ¨urgen Dickmann, and\nBernhard Sick. Radar-based road user classification and nov-\nelty detection with recurrent neural network ensembles. In\n2019 IEEE Intelligent Vehicles Symposium (IV) , pages 722–\n729. IEEE, 2019. 3\n[33] Christian Sch ¨offmann, Barnaba Ubezio, Christoph B ¨ohm,\nStephan M ¨uhlbacher-Karrer, and Hubert Zangl. Virtual\nradar: Real-time millimeter-wave radar sensor simulation for\nperception-driven robotics. IEEE Robotics and Automation\nLetters , 6(3):4704–4711, 2021. 2, 3\n[34] Ole Schumann, Markus Hahn, J ¨urgen Dickmann, and Chris-\ntian W ¨ohler. Semantic segmentation on radar point clouds.\nIn2018 21st International Conference on Information Fu-\nsion (FUSION) , pages 2179–2186. IEEE, 2018. 3\n[35] Christian Sch ¨ußler, Marcel Hoffmann, Johanna Br ¨aunig, In-\ngrid Ullmann, Randolf Ebelt, and Martin V ossiek. A realistic\nradar ray tracing simulator for large mimo-arrays in automo-\ntive environments. IEEE Journal of Microwaves , 1(4):962–\n974, 2021. 2, 3, 4\n[36] Marcel Sheeny, Emanuele De Pellegrin, Saptarshi Mukher-\njee, Alireza Ahrabian, Sen Wang, and Andrew Wallace.\nRadiate: A radar dataset for automotive perception in bad\nweather. In 2021 IEEE International Conference on Robotics\nand Automation (ICRA) , pages 1–7. IEEE, 2021. 1, 3\n[37] Merrill Ivan Skolnik. Introduction to radar systems. New\nYork, 1980. 4\n[38] J ¨orn Thieling, Susanne Frese, and J ¨urgen Roßmann. Scalable\nand physical radar sensor simulation for interacting digital\ntwins. IEEE Sensors Journal , 21(3):3184–3192, 2020. 2, 3\n[39] Leichen Wang, Bastian Goldluecke, and Carsten Anklam.\nL2r gan: Lidar-to-radar translation. In Proceedings of the\nAsian Conference on Computer Vision , 2020. 3\n[40] Yizhou Wang, Zhongyu Jiang, Yudong Li, Jenq-Neng\nHwang, Guanbin Xing, and Hui Liu. Rodnet: A real-time\nradar object detection network cross-supervised by camera-\nradar fused object 3d localization. IEEE Journal of Selected\nTopics in Signal Processing , 15(4):954–967, 2021. 1\n[41] Yizhou Wang, Zhongyu Jiang, Yudong Li, Jenq-Neng\nHwang, Guanbin Xing, and Hui Liu. Rodnet: A real-time\nradar object detection network cross-supervised by camera-\nradar fused object 3d localization. IEEE Journal of Selected\nTopics in Signal Processing , 15(4):954–967, 2021. 3, 7[42] Rob Weston, Oiwi Parker Jones, and Ingmar Posner. There\nand back again: Learning to simulate radar data for real-\nworld applications. In 2021 IEEE International Conference\non Robotics and Automation (ICRA) , pages 12809–12816.\nIEEE, 2021. 2, 3\n[43] Tim A Wheeler, Martin Holder, Hermann Winner, and\nMykel J Kochenderfer. Deep stochastic radar models. In\n2017 IEEE Intelligent Vehicles Symposium (IV) , pages 47–\n53. IEEE, 2017. 2, 3\n[44] Ao Zhang, Farzan Erlik Nowruzi, and Robert Laganiere.\nRaddet: Range-azimuth-doppler based radar object detection\nfor dynamic road users. In 2021 18th Conference on Robots\nand Vision (CRV) , pages 95–102. IEEE, 2021. 1, 3, 6, 7\n[45] Guoqiang Zhang, Haopeng Li, and Fabian Wenger. Object\ndetection and 3d estimation via an fmcw radar using a fully\nconvolutional network. In ICASSP 2020-2020 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal Process-\ning (ICASSP) , pages 4487–4491. IEEE, 2020. 1, 3, 7\n15416'}, 'dist': 0.9286905527114868}
Result 14: {'text': 'data', 'metadata': {'created_at': 1736932570, 'modified_at': 1736932570, 'owner': 'sayandeep', 'path': 'RAG_CONTENT/conferences/cvpr_ocr/Chen_Towards_High-fidelity_Artistic_Image_Vectorization_via_Texture-Encapsulated_Shape_Parameterization_CVPR_2024_paper.txt', 'size': 42700, 'seen_at': 1737191136, 'data': 'Towards High-fidelity Artistic Image Vectorization via Texture-Encapsulated\nShape Parameterization\nYe Chen1Bingbing Ni1,2∗Jinfan Liu1Xiaoyang Huang1Xuanhong Chen1,2\n1Shanghai Jiao Tong University, Shanghai 200240, China\n2USC-SJTU Institute of Cultural and Creative Industry\n{chenye123, nibingbing, chen19910528 }@sjtu.edu.cn\nAbstract\nWe develop a novel vectorized image representation\nscheme accommodating both shape/geometry and texture\nin a decoupled way, particularly tailored for reconstruction\nand editing tasks of artistic/design images such as Emojis\nand Cliparts. In the heart of this representation is a set\nof sparsely and unevenly located 2D control points. On\none hand, these points constitute a collection of paramet-\nric/vectorized geometric primitives ( e.g., curves and closed\nshapes) describing the shape characteristics of the target\nimage. On the other hand, local texture codes, in terms\nof implicit neural network parameters, are spatially dis-\ntributed into each control point, yielding local coordinate-\nto-RGB mappings within the anchored region of each con-\ntrol point. In the meantime, a zero-shot learning algorithm\nis developed to decompose an arbitrary raster image into\nthe above representation, for the sake of high-fidelity im-\nage vectorization with convenient editing ability. Extensive\nexperiments on a series of image vectorization and editing\ntasks well demonstrate the high accuracy offered by our\nproposed method, with a significantly higher image com-\npression ratio over prior art.\n1. Introduction\nHow to represent images is a fundamental problem in the\nfield of computer vision. Traditionally, images are rep-\nresented by pixels stored on fixed and discrete grids ( i.e.,\nraster images). One main advantage of such representation\nis that, there is virtually no limit to its ability to express im-\nage details with enough pixels. Nevertheless, pixel-based\nrepresentation is plagued by numerous limitations. On one\nhand, it suffers from excessive redundancy due to its stor-\nage on fixed grids, which inherently constrains image res-\nolution, along with inevitable information loss during im-\nage re-scaling. On the other hand, most importantly, pixel-\n∗Corresponding author: Bingbing Ni\n,\nLossy Texture Details\nLossy Pixel EditingRedundant ParametersRaster\nCompact Parameters!!……,!"!#!!……!"!#,!!……!"!#"×$×3……Vector&ℎ(!)_+,-×(2-+4)3145728(1024×1024×3)\n256×(2×12+4)=7168---Figure 1. Illustration of our motivation. Raster images that\nstore pixel values on fixed and discrete grids suffer from excessive\nparameter redundancy and a high coupling between image geom-\netry and texture, resulting in challenging image editing. Vector\nimages represent visual concepts with vectorized geometric prim-\nitives in a very compact parameter format, endowed with great\neditability. However, vector images are not suitable for express-\ning image texture details. This work explores a compact vector-\nized image representation that decouples images in geometric\nand texture space, tailored for high-fidelity texture-rich image vec-\ntorization, facilitating easy image editing.\nbased representation stores all the information of an image\nin the RGB values of the pixels, resulting in a high coupling\nbetween the representation of image geometry and texture.\nHence, it is challenging to edit the shape/geometry and tex-\nture of a raster image separately.\nVector images embody an alternative paradigm for rep-\nresenting visual information, which describe images with\na collection of parametric/vectorized geometric primitives\n(e.g., curves and closed shapes), defined as control points\nin the continuous space. Compared to representing images\nwith fixed discrete grids, vector images present many ad-\nvantages. 1) Resolution-Agnostic: Pixel-based image re-\nscaling often introduces unexpected noise/artifact that is\nhard to identify and remove. In contrast, modeling im-\nages with continuous parameters enables the storage and\ngeneration of images in arbitrary resolutions without in-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n15877\nformation loss. 2) Easy Editing: Vector images have ex-\nplicit/explainable parameter format, facilitating convenient\nimage editing ability by simply modifying shape or color\nparameters in a structured way. 3) Lossless Compression:\nThe shape primitives in vector images are typically defined\nby mathematical functions, allowing for the expression of\ncomplex shapes with a minimal number of control points.\nTherefore, the number of parameters required to store a vec-\ntor image is significantly lower than that of a raster image.\nUnfortunately, while vector images possess indisputable\nadvantages in representing geometric information, they fall\nshort in effectively capturing the texture details found in\nimages, which restricts their applications to simple images\n(e.g. Emojis, Fonts and Icons). As depicted in Fig. 1, cur-\nrent image vectorization representations [7, 15, 17, 25, 26]\nsuffer from significant loss of fine texture details when con-\nfronted with regions containing intricate textures. The un-\nderlying reason can be attributed to the fact that traditional\nvectorization representations store a single color within\neach shape, rather than capturing the distribution of colors.\nConsequently, this representation inherently exhibits ineffi-\nciency in conveying rich image textures.\nTo address both limitations, we propose a memory-\nefficient texture-encapsulated shape-vectorized image\nrepresentation that decomposes an image into dual-\nparameterized geometry and texture space, facilitating\nhigh-fidelity representation of complicated image geometry\nand texture as well as decoupled editing ability. Our\nnovel image representation features the following designs.\nIn a nutshell, geometry is encoded with closed B ´ezier\nshapes parameterized into the coordinates of control\npoints, similar to contemporary image vectorization algo-\nrithms [7, 15, 17, 26]. In the meantime, to represent texture,\nwe follow the emerging implicit neural representation\nscheme [22, 29], which is parameterized into a lightweight\nMLP that takes local latent encodings of coordinate\ninformation as input and predicts local texture ( i.e. RGB\nvalues). Drawing inspiration from distributed implicit\nrepresentations [23], these local texture codes are spatially\ndistributed into each control point of our geometric repre-\nsentation, namely, each control point stores a latent texture\ncode which takes charge of the local coordinate-to-RGB\nmappings within the surrounding region. To this end, an\nimage is parameterized to a set of coordinates of control\npoints assigned with corresponding latent texture codes and\na lightweight MLP shared across the whole image space,\nwhich fully capture image characteristics including color,\ntexture, and geometry, in a decoupled way. Compared\nto current methods that store texture parameters in fixed\ngrids, our shape control point anchored distributed texture\nrepresentation scheme substantially minimizes storage\noverhead, while assigning more resources for describing\nthose more crucial local visual regions (patches).Leveraging our developed representation, we accom-\nplish remarkably high-fidelity image vectorization ( i.e. pa-\nrameterized reconstruction and editing) with a straight-\nforward zero-shot learning algorithm based on self-\nsupervision. Unlike previous algorithms that rely on dif-\nferentiable rasterization to estimate gradients and optimize\ncontrol point coordinates, which often encounters large gra-\ndient error, we efficiently constrain the B ´ezier shapes to\nmatch crucial areas in image space by constructing a ge-\nometric field, thus encouraging accurate parameter fitting\nand fast convergence. We extensively experiment the pro-\nposed framework for a series of image vectorization tasks\nincluding Emoji [1], Icon [2] and our developed Clipart\nbenchmarks. It is demonstrated that our method achieves\nmuch better image vectorization quality with a significantly\nreduced number of parametric shapes, compared to prior\nart. In addition, compared to image reconstruction meth-\nods based on implicit neural representations, our method\nachieves better reconstruction results with only 10×fewer\nlatent codes, which significantly demonstrates the effective-\nness and efficiency of our representation. Moreover, our\nmethod can edit images by simply editing the shape and\ntexture parameters in a decoupled way.\n2. Related Works\nImage Vectorization. Image vectorization is an impor-\ntant research area in computer graphics. Traditional ap-\nproaches [8, 33] typically utilize a two-stage algorithm\nthat involves an initial step of image segmentation, fol-\nlowed by targeted vectorization of the segmented regions.\nIn the era of deep learning, several neural network-based\nmethods [7, 26] are proposed inspired by the differentiable\nrasterization method DiffVG [15]. Im2Vec [26] uses a\nvariable-complexity closed B ´ezier path as the fundamental\ngraphic primitive. Chen et al. [7] utilize several geometric\nprimitives like triangle and rectangle to fit the image. Nev-\nertheless, these methods all face challenges when it comes\nto vectorizing complex images, as predicting a large num-\nber of control points simultaneously solely based on pixel\nloss is very difficult. LIVE [17] is a remarkable work that\ngenerates compact vectorized representations for complex\nimages with a meticulously designed layer-wise path ini-\ntialization technique. Although LIVE can generate decent\nvectorization results for complex images in almost any do-\nmain, it has inherent limitations in representing image tex-\ntures due to a lack of exploration of internal color distri-\nbutions within closed B ´ezier shapes. As a result, it relies\non stacking local shapes to express texture details. Du et\nal. [9] propose to use linear gradients to define the spa-\ntially varying color within local image regions, endowed\nwith the ability to edit images easily in a structured way.\nHowever, the linear gradient layers rely heavily on an in-\ntricate layer configuration strategy. In this work, we in-\n15878\ntroduce a texture-encapsulated shape-vectorized represen-\ntation to capture complex image textures, achieving high-\nfidelity image vectorization with a simplistic framework.\nImplicit Neural Representation. Implicit neural repre-\nsentation is widely applied in the field of 3D vision [3, 5,\n11, 12, 21, 28], which models 3D shapes and appearances\nwith MLPs that map coordinates to signals. Particularly,\nNeRF [22] inspires a series of outstanding works [4, 13,\n16, 19, 24, 32]. Recent advances [5, 10, 20, 23] investigate\nthe utilization of latent representations to balance between\nmemory usage and computational expenses of heavy MLPs.\nWhile implicit neural representation achieves success in\n3D tasks, its applications in 2D domain [6, 14, 29–31] are\nrelatively unexplored. Deep Image Prior [31] is a pioneer-\ning work to represent images using deep ConvNets with\nabundant neural parameters. SIREN [29] parameterizes 2D\nimages with MLP and novel periodic activation functions.\nHowever, the MLPs of SIREN are highly redundant and in-\nefficient, and unacceptable reconstruction errors occur as\nthe hidden layers are further reduced. This work proposes\nan efficient and flexible representation by encapsulating la-\ntent codes into crucial shape parameters, which can be op-\ntimized with a zero-shot learning framework.\n3. Methodology\n3.1. Overview\nThe overall framework is illustrated in Fig. 2. Our frame-\nwork decouples the raster image I∈RH×W×3in geo-\nmetric space and texture space using a texture-encapsulated\nshape-vectorized representation, i.e., a parameterized shape\nrepresentation (Sec. 3.2) and a shape-anchored implicit neu-\nral texture representation (Sec. 3.3).\nWe utilize parameterized closed B ´ezier shapes (defined\nas coordinates of control points, as implemented in [15, 17])\nto represent the fundamental geometric information in the\nimage. We establish a geometric field based on image edge\npoints to constrain the coordinates of control points of all\nB´ezier shapes, enabling B ´ezier shapes to efficiently cover\nregions with rich geometric information in the image space.\nThen we introduce a shape-anchored implicit neural tex-\nture representation to explore image textures. Specifically,\nour framework assigns an optimizable latent code zto each\ncontrol point in the shape representation. Then a shape-\naware interpolation function ϕ(no parameters) and a train-\nable lightweight MLP fθ(with θas its parameters) are uti-\nlized to represent the texture in an implicit coordinate-to-\nRGB way, where the texture information at arbitrary posi-\ntion can be spatially interpolated and retrieved.\nTo conclude, we parameterize the input image as a set of\nB´ezier shapes attached with trainable texture latents and a\nlightweight MLP fθ, which can be formulated as:\nI∼ {S={s1,s2, ...,sn}, θ}, (1)where ndenotes the shapes needed to represent an image.\nAndsitakes the form:\nsi={(pi1,zi1),(pi2,zi2), ...,(pim,zim)}, (2)\nwhere p∗= (x∗, y∗)∈R2denotes the coordinate of con-\ntrol points and z∗∈Rdrepresents the latent texture code\nassigned to the corresponding control point. mdenotes the\nnumber of control points for each shape. More details are\nelaborated in the following sections.\n3.2. Parameterized Shape Representation\nWe use B ´ezier shapes defined by control points to repre-\nsent image geometry. Common image vectorization meth-\nods tend to optimize the control points with pixel loss by uti-\nlizing the gradients approximated by differentiable renderer\n(e.g., DiffVG [15]), which suffers from the issue of gradient\nsparsity. In contrast, we introduce a geometric field to opti-\nmize the coordinates of control points. For the input raster\nimage, we use the Canny operator to extract the edge points\nE={ei}and generate the corresponding geometric field.\nMore concretely, we think that the closer a point is to the\nedge of an image, the more geometric information it con-\ntains. Hence, we define the geometric field as a probability\nfield, which describes the importance of each query point in\ndescribing the image geometry. The ground-truth geometric\nfieldGedefined by edge points can be formulated as:\nGe(q) = 1−tanh(min\ni||q−ei||2\n2\nσ2), (3)\nwhere q= (x, y)∈R2denotes any query coordinate in the\nimage space and σis used to control the range of the field.\nWe utilize the geometric field to optimize the coordinates\nof control points of all B ´ezier shapes. Specifically, we use\ncontrol points in Sto densely sample a set of B ´ezier curve\npoints B={bi(t)}:\nbi(t) =Bpi1pi2...pim(t), (4)\nwhere bi(t)is a curve point of B ´ezier shape siconditioned\nontandBis the B ´ezier function. Then we compute the\npredicted geometric field Gb:\nGb(q) = 1−tanh(min\ni,t||q−bi(t)||2\n2\nσ2). (5)\nWe use the binary cross-entropy loss to compare the distri-\nbutions of GeandGc:\nLg=BCE (Gb(∗), Ge(∗)). (6)\nIn addition, we use the chamfer distance loss to encour-\nage the B ´ezier shapes to fit the key edge points in the image:\nLc=X\nb∈Bmin\ne∈E∥b−e∥2\n2+X\ne∈Emin\nb∈B∥e−b∥2\n2.(7)\n15879\n$!%!!%!"%!#……$#%#!%#"%##………………$$%$!%$"%$#Geometric Space…Texture Space\n…&$!&$#&$"&!!…&!#&!"…\n""\'#\n""(#"#Shape-aware InterpolationControl PointsLatent Codes#Implicit RepresentationB́$zierShapes\nLatent codes…\nMLPTextureTexture Feature Interpolation(%,\')Feature Fusion(,,.)(),*,+)#"\'#Weighted Sum#"(#$%$CoordinateTexture%#=1/\'Figure 2. Overview of our framework. We propose a novel texture-encapsulated shape-vectorized representation to decouple the raster\nimage in geometric and texture space. We utilize B ´ezier shapes defined as coordinates of control points to represent image geometry and\na shape-anchored implicit neural representation to explore image texture efficiently by distributing latent texture codes into control points.\nWith such representation, the texture feature at arbitrary coordinate can be spatially interpolated and fused within B ´ezier shapes and the\ntexture information can be retrieved through a very lightweight MLP.\n!!!!!"\n!#$Grid-based latent code distributionShape-anchored latent code distribution\nLatent codesParams:!(ℎ$)Params:!(&)Latent codesLinear grid interpolationShape-aware interpolation\n!!"!!!!!%!""!"%!"!\nFigure 3. An example of the comparison between our shape-\nanchored latent code distribution and general grid-based latent\ncode distribution. “ n” denotes shape number. Our strategy greatly\nreduces storage overhead. Moreover, our shape-aware interpola-\ntion restricts the interpolation process to the interior of shapes,\nmaking the interpolation smoother and accelerating convergence\nbecause the texture inside shapes tends to be relatively smooth.\nWith Eqn.(6&7), we can iteratively optimize the coordi-\nnates of control points to constrain B ´ezier shapes to cover\nthe crucial areas with rich geometric information.\n3.3. Shape-anchored Implicit Neural Texture Rep-\nresentation\nWe use implicit neural representation in the continuous pa-\nrameter domain to explore complex image texture. Follow-\ning recent progress in image reconstruction via implicit neu-\nral representation [6], we also distribute some latent codes\nin the image spatial positions.\nHowever, we abandon the usual strategy that assigns la-\ntent codes to fixed and discrete grid points, which is very in-efficient and inflexible because it not only leads to storage\nredundancy but also makes the training process extremely\ncomplex (as shown in Fig. 3). For our problem, we no-\ntice that there are some smooth regions in the image space\nthat can be fitted with only a small amount of latent codes,\nwhile more latent codes are needed to be allocated for those\nregions with sharp gradients/changes ( e.g., the edge points),\nwhich significantly aligns with our utilization of geometric\nrepresentation in Sec. 3.2 in that texture-rich regions well\ncorrespond to edge regions. Based on the above consid-\nerations, we distribute the texture latent codes on the con-\ntrol points of the B ´ezier shapes in a distributed manner,\nwhich significantly improves the utilization efficiency of la-\ntent codes and reduces storage costs.\nSpecifically, in our texture representation, the image tex-\nture information is parameterized by a set of latent codes\nstored in control points and a lightweight mapping function\n(fθ: [x,z]7→c) which decodes the latent codes to texture\nvalues (RGB), where xandzare the coordinate and the\ncorresponding latent vector/texture feature at query point in\nthe continuous image space. zis obtained via a shape-aware\ninterpolation method, i.e., interpolating the latent codes of\nthe shape that covers the query point. Assume a query point\nq∈R2is within a shape si, the shape-aware interpolation\nfunction ϕis formulated as:\nϕ(si,q) =mX\nj=1wi\nj(q)Pm\nj=1wi\nj(q)zij, (8)\nwith\nwi\nj(q) =1\n∥q−pij∥2\n2, (9)\nwhere (pij,zij)∈si, as defined in Eqn. 2.\n15880\nFor points in the image space, we adopt a divide-and-\nconquer strategy to assign them with latent vectors. Specif-\nically, for points covered by B ´ezier shapes, we can obtain\ntheir latents by interpolating the latent codes of the con-\ntrol points of the shapes they belong to by utilizing Eqn. 8.\nFor the points that are not covered by any B ´ezier shape,\nwe assign them a default latent vector. Note that a pixel\nmay be covered by several B ´ezier shapes, thus we propose\nan inverse distance weighting feature fusion method (IDW-\nFusion) to fuse the latent vectors interpolated by all shapes.\nWe can formulate this process as:\n1d,ifeSq=∅,(10) with:\nwsi(q) =mPm\nj=1∥q−pij∥2\n2, (11)\nwhereeSq⊂Sdenotes the set of all shapes that cover point\nq, which can be obtained with the Winding Number Algo-\nrithm. The RGB value at point qcan be predicted as:\ncq=fθ([q,zq]), (12)\nwhere [·,·]means concatenation.\nCompared to previous image vectorization methods that\nuse only one color within a B ´ezier shape, our shape-\nanchored implicit neural texture representation models the\ncolor distribution inside the B ´ezier shape efficiently. In ad-\ndition, in our texture representation, the number of latent\ncodes is resolution-agnostic, which achieves a significant\ncompression of parameters and efficient texture representa-\ntional capability with the content-adaptive latent code dis-\ntribution.\n3.4. Parameterized Representation Optimization\nGiven an input raster I∈RH×W×3, our task is to recon-\nstruct it with the above proposed parameterized representa-\ntion in a zero-shot learning framework ( i.e., single-image\noptimization). We have to optimize the parameters in a\nself-supervised manner because we only have raster images\nfor optimization without any image-to-vector or image-to-\nparameter labeling. Thus we have to rasterize our param-\neterized representation into a raster image ˆI∈RH×W×3,\nwith the same size as the input image and then measure the\npixel errors. Specifically, for each pixel [i, j]of the output\nimage ˆI, we query its color with our parameterized repre-\nsentation by viewing it as a point in the image space with\ncoordinate xij= [i\nH−0.5,j\nW−0.5]as in [18]. Then we\ncan obtain the corresponding texture feature zxijand use\nfθto approximate the RGB value:\nˆI[i, j] =fθ([xij,zxij]). (13)Then we can utilize the pixel-wise mean square error loss\nto optimize the parameters, which is formulated as:\nLr=∥I−ˆI∥2\n2. (14)\nAfter optimization, the input image is parameterized into\ndecoupled geometric and texture space, facilitating high-\nfidelity reconstruction with easy editing by simply editing\nthe shape and texture parameters in a decoupled way.\nOptimization objectives. The input image is decomposed\ninto our parameterized representation in a zero-shot learn-\ning algorithm. As shown in Alg. 1, we firstly randomly\ninitialize all parameters of our representation including the\ncontrol points and corresponding latent codes of all shapes\n(i.e.,S) and the MLP parameters ( i.e.,θ). Then we iter-\natively optimize all the parameters with a combination of\nLg,LcandLr, and the optimal problem of our framework\ncan be expressed as:\nmin\n{S,θ}Lr+Lg+Lc. (15)\nAlgorithm 1: Zero-shot Representation Learning\nInput : I,n,m,d,iters ;\nRandom Init: S,θ;// Parameters\nGenerate: E,Ge,Q;\n// Edge points, Geometric field, All pixel points\nforiin range(iters) do\nB= sample curve points( S);\n// pred geometric field\nGb= geometric field(B);\n//eSfor all pixel points\neSQ=winding number parallel( S,Q);\n// latent vector for all pixel points\nzQ= latent interp(eSQ,Q);\n// rasterize\nˆI=fθ(Q,zQ);\n// compute loss\nL=Lr(I,ˆI) +Lg(Gb, Ge) +Lc(B,E);\nupdate parameters S,θ;\nend\nOutput: Parameterized image representation: {S,θ}.\n4. Experiments\n4.1. Experimental Setups\nDatasets. Current image vectorization algorithms are lim-\nited to handling images with simple geometric structures\nlike Emoji [1] and Icon [2], and there is a lack of explo-\nration for images with complex textures. In this paper, in\naddition to comparing our method with existing methods\non commonly used Emojis and Icons, we also introduce a\nClipart Dataset consisting of 200 clipart images with com-\nplex shapes, textures, and rich backgrounds, which is very\nchallenging for image vectorization task.\n15881\nn Dataset DiffVG LIVE NPA Ours\n5Emoji 0.0212 0.0019 0.0049 0.0007\nIcon 0.0573 0.0026 0.0093 0.0009\n10Emoji 0.0092 0.0016 0.0020 0.0006\nIcon 0.0285 0.0024 0.0017 0.0007\nTable 1. Image vectorization results on Emoji&Icon datasets.\nnmeans the number of shapes. MSE results are reported. Our\nmethod achieves significantly better results than state-of-the-art.\nImplementation Details. By default, we use four segments\nfor each closed B ´ezier shape, thus each shape contains 12\ncontrol points ( i.e.,m= 12 ). The number of shapes nis set\nto5and128for Emoji/Icon dataset and Clipart dataset re-\nspectively. Definitely, more shapes lead to better vectorized\nresults. The control factor σof the geometric field is set\nto0.005. We set the dimension of latent codes as d= 16 ,\nand the MLP fθis implemented as a linear layer with input\ndimension 18and output dimension 3. With the help of a\nhighly parallel optimization process, we can reduce the time\nit takes to optimize an image to the order of minutes.\n4.2. Image Vectorization\nWe evaluate our texture-encapsulated shape representation\non image vectorization task by measuring the differences\nbetween the input raster images and the rasterized vectors.\nThe comparison with SOTA methods ( i.e., DiffVG [15],\nLIVE [17] and NPA [7]) are performed both quantitatively\nand qualitatively on Emoji, Icon and Clipart dataset.\nEmoji&Icon Datasets. The quantitative comparisons on\nEmoji and Icon datasets are shown in Tab. 1. Our method\noutperforms all previous image vectorization methods on\nboth benchmarks, especially when fewer B ´ezier shapes are\nutilized. The results fully demonstrate the compactness\nof our texture-encapsulated shape representation, especially\nin the efficient utilization of shapes. Qualitative compar-\nisons are shown in Fig 4. We can see that our representa-\ntion achieves high-fidelity image vectorization results even\nwith a minimal number of shapes. Note that LIVE also\ngenerates very compact representation, but it relies heavily\non the initialization strategy and still struggles to generate\nregular shapes when dealing with shape intersections. We\nalso show some decoupled editing examples in Fig. 5. Our\nmethod can easily transform geometric shapes and replace\nthe textures within the corresponding shapes with specific\nones like frosted glass texture.\nClipart Dataset. The Clipart dataset contains clipart im-\nages with complex texture information, which is very chal-\nlenging for image vectorization task. We use this dataset\nprimarily to demonstrate the efficiency of our texture-\nencapsulated shape representation in expressing complex\ntextures, especially when the number of shapes is limited.\nNPA [7] is limited in its ability to simultaneously opti-\nmize a large number of shape parameters only with gra-\nRasterInput!=5!=10!=5!=10!=5!=10!=5!=10DiffVGLIVEOursNPAFigure 4. Qualitative comparison on Emoji&Icon datasets. n\nmeans the number of shapes. Our method can accurately represent\nthe image geometry even with limited number of shapes.\nfliprotate +translate translatescale-upGeometric Editing\nTexture Editing\nscale-uprotatenew textureadd shaperotatenew texture\nFigure 5. We showcase the editability of our parameterized repre-\nsentation. We can edit the image geometry and image texture in\na decoupled way simply by editing the corresponding parameters.\nIn the bottom row, we show the shape decompositions with the “x”\nmarks indicating the control points.\ndients approximated by differentiable rasterization method,\nwhich prevents it from producing acceptable results on this\ndataset. Therefore, we only compare our method with\nDiffVG [15] and LIVE [17]. The quantitative results are\nshown in Tab. 2. Our method performs significantly better\nthan DiffVG and LIVE when utilizing the same number of\nshapes. In addition, despite the layer-wise representation of\nLIVE is highly compact, we are still able to achieve compa-\nrable reconstruction results by further reducing the number\nof shapes by half. The qualitative results are visualized in\nFig. 6. We can observe that our representation models the\ntexture details much better than other methods, and effec-\ntively prevents local texture artifacts caused by the stacking\nof redundant shapes.\n4.3. Learning Implicit Image Representations\nConsidering that we utilize implicit neural representation\nto parameterize images, we also make comparisons with\ngeneral implicit image representations. SIREN [29] is a\nclassical method to learn implicit representations for im-\nages parameterized by neural networks. In this section,\nwe compare our method with SIREN on the task of im-\nage reconstruction with continuous representation. To com-\npare with methods that store latent codes on fixed grids,\nwe also utilize the framework of LIIF [6] to perform zero-\n15882\n!=128!=256!=128!=256!=128!=256OursLIVEDiffVGRasterInput\nFigure 6. Qualitative comparison on Clipart dataset. nmeans the number of shapes. We use red boxes to emphasize the differences.\nOur representation can express complex image details with a small number of shapes. Please zoom in for more details.\nn Method MSE ↓LPIPS↓SSIM↑\n128DiffVG [15] 0.0089 0.3741 0.7763\nLIVE [17] 0.0090 0.3424 0.7916\nOurs 0.0009 0.2492 0.8803\n256DiffVG [15] 0.0035 0.3271 0.8092\nLIVE [17] 0.0028 0.2894 0.8631\nOurs 0.0006 0.2218 0.9014\nTable 2. Image vectorization results on Clipart dataset. Pixel\nMSE, LPIPS and SSIM are reported. nmeans the number of\nshapes. The LPIPS is computed based on VGG [27]. Our method\nachieves significantly better reconstruction results than state-of-\nthe-arts, especially when limited number of shapes are used.\nshot image reconstruction. Specifically, instead of train-\ning a heavy encoder to generate latent codes, we directly\ndistribute randomly initialized latent codes on fixed grid\npoints and iteratively fit the implicit parameters (includ-\ning latent codes and an MLP-based decoding function) for\neach image. We demonstrate the efficiency of our texture-\nencapsulated shape representation by comparing the image\nreconstruction errors and the corresponding number of pa-\nrameters required for the implicit representation on the Cli-\npart dataset. The results are shown in Tab. 3. We can see\nthat our method achieves better image reconstruction results\nwith a significantly higher parameter compression ratio than\nother methods. When using the same order of magnitude ofparameters, our reconstruction results are obviously better\nthan other methods. Some visualization results are shown in\nFig. 7. For SIREN, more hidden layers lead to better recon-\nstruction quality, but it lacks the ability to reconstruct im-\nages with lightweight networks. For methods that store la-\ntent codes on fixed grids, a large number of latent codes are\nneeded to achieve acceptable reconstruction results. In con-\ntrast, our representation captures rich image textures with\nonly a small number of latent codes and a very lightweight\nMLP because we assign more resources for describing cru-\ncial local visual regions by flexibly encapsulating texture\nparameters in shape control points. Both quantitative and\nqualitative results demonstrate that our method learns effi-\ncient continuous representations of images.\n4.4. Ablation Study\nIn this section, we explore the crucial designs and hyper-\nparameters of our framework. For simplicity, we only use\nMSE as the metric for all quantitative comparisons.\nComponent Analyses. We first investigate the effective-\nness of each training objective. The MSE results are shown\nin Tab. 4. We see that the geometric field ( Lg) effectively\nimproves the reconstruction results among all datasets, and\nthe chamfer distance loss ( Lc) further improves reconstruc-\ntion qualities in the complex Clipart dataset. The results\ndemonstrate that, compared to only using pixel loss, our\n15883\nMethod MSE↓ LPIPS↓SSIM↑Codes↓Params. ↓\nSIREN-1 0.0058 0.4424 0.7035 - 17152\nSIREN-3 0.0020 0.2642 0.8325 - 49920\nGrid-/4 0.0010 0.2602 0.8506 16384 262198\nGrid-/16 0.0043 0.4896 0.6724 1024 16438\nOurs 0.0009 0.2492 0.8803 1536 27702\nTable 3. Comparison with general implicit image represen-\ntations. “Codes” denote the number of latent codes utilized.\n“Params” denote the number of parameters. “SIREN” does not\ndistribute latent codes. “SIREN- ∗” means the SIREN version with\n∗hidden layers. In “Grid-/ ∗”, the “∗” denotes the proportion of the\nspatial dimension of the original image to the grids storing latent\nvariables. Our representation achieves comparable reconstruction\nresults with a significant parameter compression. When using the\nsame order of magnitude of parameters, our method achieves ob-\nviously higher reconstruction quality.\nSIREN-1SIREN-3Grid-/4Grid-/16OursInput\nFigure 7. Qualitative comparison with general implicit image\nrepresentations. Compared to SIREN and the usual strategy to\nstore latent codes on grids, our representation achieves clearer and\nmore faithful reconstruction results with fewer parameters.\nLosses Emoji Icon Clipart\nLr 0.0013 0.0018 0.0043\nLr+Lg 0.0008 0.0011 0.0013\nLr+Lg+Lc0.0007 0.0009 0.0009\nTable 4. Component Analyses on the training objectives. MSE\nresults on several datasets are reported.\nmethod makes the shapes more accurately fit key geometric\nregions by explicitly constraining the coordinates of control\npoints utilizing geometric information in the image.\nA second experiment is conducted to explore how the\ntexture feature fusion method affects the image reconstruc-\ntion results. More concretely, we compare our inverse dis-\ntance weighting feature fusion method ( i.e., IDW-Fusion)\nwith the ablated version of directly adding the features inter-\npolated by all shapes together ( i.e., Sum-Fusion). We also\ninvestigate the effectiveness of attaching the coordinates of\nquery points to the features. The results are shown in Tab. 5.\nWe can observe that the IDW-Fusion method is very effec-Emoji Icon Clipart\nIDW-Fusion 0.0007 0.0009 0.0009\nSum-Fusion 0.0013 0.0012 0.0016\nw/o coords 0.0015 0.0020 0.0042\nTable 5. Component Analyses on the feature fusion method.\nMSE results on several datasets are reported.\nMSE@ClipartShape Number !Latent Code Dimension "\nFigure 8. Parameter Analyses on the shape number and latent\ncode dimension. MSE results on Clipart dataset are reported.\ntive by considering the varying degrees of influence of each\nshape on the query point. Notably, concatenating the fea-\ntures with coordinates is effective and crucial in our method\nbecause points in the background do not belong to any ex-\nplicit shapes and the position information can serve as the\ndiscriminative features for these points.\nParameter Analyses. We explore how shape number nand\nlatent code dimension daffect the reconstruction results.\nThe MSE results on Clipart dataset are shown in Fig. 8. We\ncan see that more shapes lead to better reconstruction results\nand our method can achieve competitive results with only\n64 shapes in the complex Clipart dataset. In addition, we\ncan see that higher dimensions only marginally improve the\nreconstruction capability. Considering the computational\nand storage costs it incurs, d= 16 is an efficient setting.\n5. Conclusion\nThis work presents a novel vectorized image representation\nto decompose images into parameterized shape and tex-\nture space. Along with our representation, we introduce\na straightforward zero-shot learning framework in a self-\nsupervised manner for image vectorization task. Extensive\nexperimental results on various benchmarks and tasks prove\nthat our representation achieves high-fidelity image recon-\nstruction with a significantly high image parameters com-\npression, endowed with convenient image editing by sim-\nply editing corresponding shape and texture parameters in a\ndecoupled way.\n6. Acknowledgment\nThis work was supported by National Science Foundation\nof China (U20B2072, 61976137). This work was also\npartially supported by Grant YG2021ZD18 from Shang-\nhai Jiaotong University Medical Engineering Cross Re-\nsearch. This work was partially supported by STCSM\n22DZ2229005.\n15884\nReferences\n[1] Note emoji. https://github.com/googlefonts/\nnoto-emoji. Accessed: 2021-09-30. 2, 5\n[2] creativestall. https : / / thenounproject . com /\ncreativestall/. 2, 5\n[3] Matan Atzmon and Yaron Lipman. Sal: Sign agnos-\ntic learning of shapes from raw data. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 2565–2574, 2020. 3\n[4] Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter\nHedman, Ricardo Martin-Brualla, and Pratul P Srinivasan.\nMip-nerf: A multiscale representation for anti-aliasing neu-\nral radiance fields. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision , pages 5855–5864,\n2021. 3\n[5] Rohan Chabra, Jan E Lenssen, Eddy Ilg, Tanner Schmidt,\nJulian Straub, Steven Lovegrove, and Richard Newcombe.\nDeep local shapes: Learning local sdf priors for detailed 3d\nreconstruction. In Computer Vision–ECCV 2020: 16th Eu-\nropean Conference, Glasgow, UK, August 23–28, 2020, Pro-\nceedings, Part XXIX 16 , pages 608–625. Springer, 2020. 3\n[6] Yinbo Chen, Sifei Liu, and Xiaolong Wang. Learning\ncontinuous image representation with local implicit image\nfunction. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition , pages 8628–8638,\n2021. 3, 4, 6\n[7] Ye Chen, Bingbing Ni, Xuanhong Chen, and Zhangli Hu.\nEditable image geometric abstraction via neural primitive as-\nsembly. In ICCV , pages 23514–23523, 2023. 2, 6\n[8] James Richard Diebel. Bayesian Image Vectorization: the\nprobabilistic inversion of vector image rasterization . Stan-\nford University, 2008. 2\n[9] Zheng-Jun Du, Liang-Fu Kang, Jianchao Tan, Yotam Gin-\ngold, and Kun Xu. Image vectorization and editing via linear\ngradient layer decomposition. ACM Transactions on Graph-\nics (TOG) , 42(4):1–13, 2023. 2\n[10] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong\nChen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels:\nRadiance fields without neural networks. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 5501–5510, 2022. 3\n[11] Amos Gropp, Lior Yariv, Niv Haim, Matan Atzmon, and\nYaron Lipman. Implicit geometric regularization for learning\nshapes. arXiv preprint arXiv:2002.10099 , 2020. 3\n[12] Chiyu Jiang, Avneesh Sud, Ameesh Makadia, Jingwei\nHuang, Matthias Nießner, Thomas Funkhouser, et al. Local\nimplicit grid representations for 3d scenes. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 6001–6010, 2020. 3\n[13] Verica Lazova, Vladimir Guzov, Kyle Olszewski, Sergey\nTulyakov, and Gerard Pons-Moll. Control-nerf: Editable\nfeature volumes for scene rendering and manipulation. In\nProceedings of the IEEE/CVF Winter Conference on Appli-\ncations of Computer Vision , pages 4340–4350, 2023. 3\n[14] Jaewon Lee and Kyong Hwan Jin. Local texture estima-\ntor for implicit representation function. In Proceedings ofthe IEEE/CVF conference on computer vision and pattern\nrecognition , pages 1929–1938, 2022. 3\n[15] Tzu-Mao Li, Michal Luk ´aˇc, Micha ¨el Gharbi, and Jonathan\nRagan-Kelley. Differentiable vector graphics rasterization\nfor editing and learning. TOG , 39(6):1–15, 2020. 2, 3, 6, 7\n[16] Jinxian Liu, Ye Chen, Bingbing Ni, Jiyao Mao, and Zhenbo\nYu. Inferring fluid dynamics via inverse rendering. arXiv\npreprint arXiv:2304.04446 , 2023. 3\n[17] Xu Ma, Yuqian Zhou, Xingqian Xu, Bin Sun, Valerii Filev,\nNikita Orlov, Yun Fu, and Humphrey Shi. Towards layer-\nwise image vectorization. In CVPR , pages 16314–16323,\n2022. 2, 3, 6, 7\n[18] Xu Ma, Yuqian Zhou, Huan Wang, Can Qin, Bin Sun, Chang\nLiu, and Yun Fu. Image as set of points. arXiv preprint\narXiv:2303.01494 , 2023. 5\n[19] Ricardo Martin-Brualla, Noha Radwan, Mehdi SM Sajjadi,\nJonathan T Barron, Alexey Dosovitskiy, and Daniel Duck-\nworth. Nerf in the wild: Neural radiance fields for uncon-\nstrained photo collections. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\npages 7210–7219, 2021. 3\n[20] Ishit Mehta, Micha ¨el Gharbi, Connelly Barnes, Eli Shecht-\nman, Ravi Ramamoorthi, and Manmohan Chandraker. Mod-\nulated periodic activations for generalizable local functional\nrepresentations. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision , pages 14214–14223,\n2021. 3\n[21] Mateusz Michalkiewicz, Jhony K Pontes, Dominic Jack,\nMahsa Baktashmotlagh, and Anders Eriksson. Implicit sur-\nface representations as layers in neural networks. In Proceed-\nings of the IEEE/CVF International Conference on Com-\nputer Vision , pages 4743–4752, 2019. 3\n[22] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. Communications of the ACM , 65(1):99–106, 2021. 2,\n3\n[23] Thomas M ¨uller, Alex Evans, Christoph Schied, and Alexan-\nder Keller. Instant neural graphics primitives with a mul-\ntiresolution hash encoding. ACM Transactions on Graphics\n(ToG) , 41(4):1–15, 2022. 2, 3\n[24] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and\nFrancesc Moreno-Noguer. D-nerf: Neural radiance fields\nfor dynamic scenes. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition , pages\n10318–10327, 2021. 3\n[25] Antoine Quint. Scalable vector graphics. IEEE MultiMedia ,\n10(3):99–102, 2003. 2\n[26] Pradyumna Reddy, Michael Gharbi, Michal Lukac, and\nNiloy J Mitra. Im2vec: Synthesizing vector graphics without\nvector supervision. In CVPR , pages 7342–7351, 2021. 2\n[27] Karen Simonyan and Andrew Zisserman. Very deep convo-\nlutional networks for large-scale image recognition. arXiv\npreprint arXiv:1409.1556 , 2014. 7\n[28] Vincent Sitzmann, Michael Zollh ¨ofer, and Gordon Wet-\nzstein. Scene representation networks: Continuous 3d-\nstructure-aware neural scene representations. Advances in\nNeural Information Processing Systems , 32, 2019. 3\n15885\n[29] Vincent Sitzmann, Julien Martel, Alexander Bergman, David\nLindell, and Gordon Wetzstein. Implicit neural representa-\ntions with periodic activation functions. Advances in neural\ninformation processing systems , 33:7462–7473, 2020. 2, 3,\n6\n[30] Matthew Tancik, Ben Mildenhall, Terrance Wang, Divi\nSchmidt, Pratul P Srinivasan, Jonathan T Barron, and Ren\nNg. Learned initializations for optimizing coordinate-based\nneural representations. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\npages 2846–2855, 2021.\n[31] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky.\nDeep image prior. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition , pages 9446–9454,\n2018. 3\n[32] Peng Wang, Yuan Liu, Zhaoxi Chen, Lingjie Liu, Ziwei Liu,\nTaku Komura, Christian Theobalt, and Wenping Wang. F2-\nnerf: Fast neural radiance field training with free camera\ntrajectories. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , pages 4150–\n4159, 2023. 3\n[33] Tian Xia, Binbin Liao, and Yizhou Yu. Patch-based image\nvectorization with automatic curvilinear feature alignment.\nACM Transactions on Graphics (TOG) , 28(5):1–10, 2009. 2\n15886'}, 'dist': 0.9286905527114868}
Result 15: {'text': 'data', 'metadata': {'created_at': 1736932570, 'modified_at': 1736932570, 'owner': 'sayandeep', 'path': 'RAG_CONTENT/conferences/cvpr_ocr/Bastico_Coupled_Laplacian_Eigenmaps_for_Locally-Aware_3D_Rigid_Point_Cloud_Matching_CVPR_2024_paper.txt', 'size': 58176, 'seen_at': 1737191136, 'data': 'Coupled Laplacian Eigenmaps for Locally-Aware 3D Rigid Point Cloud\r\nMatching\r\nMatteo Bastico1⇤, Etienne Decenci `ere2, Laurent Cort ´e1, Yannick Tillier3, David Ryckelynck1\r\nMines Paris, Universit ´e PSL\r\n1Centre des Mat ´eriaux (MAT), UMR7633 CNRS, 91003 Evry, France\r\n2Centre de Morphologie Math ´ematique (CMM), 77300 Fontainebleau, France\r\n3Centre de Mise en Forme des Mat ´eriaux (CEMEF), UMR7635 CNRS, 06904 Sophia Antipolis, France\r\nAbstract\r\nPoint cloud matching, a crucial technique in computer\r\nvision, medical and robotics ﬁelds, is primarily concerned\r\nwith ﬁnding correspondences between pairs of point clouds\r\nor voxels. In some practical scenarios, emphasizing lo-\r\ncal differences is crucial for accurately identifying a cor-\r\nrect match, thereby enhancing the overall robustness and\r\nreliability of the matching process. Commonly used shape\r\ndescriptors have several limitations and often fail to pro-\r\nvide meaningful local insights about the paired geome-\r\ntries. In this work, we propose a new technique, based\r\non graph Laplacian eigenmaps, to match point clouds by\r\ntaking into account ﬁne local structures. To deal with the\r\norder and sign ambiguity of Laplacian eigenmaps, we in-\r\ntroduce a new operator, called Coupled Laplacian1, that\r\nallows to easily generate aligned eigenspaces for multiple\r\nregistered geometries. We show that the similarity between\r\nthose aligned high-dimensional spaces provides a locally\r\nmeaningful score to match shapes. We ﬁrstly evaluate the\r\nperformance of the proposed technique in a point-wise man-\r\nner, focusing on the task of object anomaly localization on\r\nthe MVTec 3D-AD dataset. Additionally, we deﬁne a new\r\nmedical task, called automatic Bone Side Estimation (BSE),\r\nwhich we address through a global similarity score derived\r\nfrom coupled eigenspaces. In order to test it, we propose a\r\nbenchmark collecting bone surface structures from various\r\npublic datasets. Our matching technique, based on Cou-\r\npled Laplacian, outperforms other methods by reaching an\r\nimpressive accuracy on both tasks.\r\n1. Introduction\r\nPoint cloud matching, or more generally 3D shape match-\r\ning, is a fundamental task in computer vision. It involves\r\n⇤Corresponding author: matteo.bastico@minesparis.psl.eu\r\n1Code: https://github.com/matteo-bastico/CoupLapﬁnding the closest matching geometry to a target shape\r\nwithin a set of reference shapes [ 65]. In addition, if the\r\ntask involves ﬁnding rigid transformations that best align\r\nthe target shape with the reference, it is often part of a reg-\r\nistration process. In particular, point-set rigid registration\r\ndetermines the relative transformation needed to align two\r\npoint clouds without altering their internal structures [ 41].\r\nThis problem is essential for many practical computer vi-\r\nsion tasks, such as medical image analysis [ 3,32,48,61],\r\nintelligent vehicles [ 21,34], human pose estimation [ 22]\r\nand objects retrieval and tracking [ 46,64]. Traditional\r\n[8,19,57] and probabilistic registration and matching meth-\r\nods [ 15,20,30,45], while robust, often struggle to opti-\r\nmally align complex geometries, especially in cases with\r\nintricate local structures or slight deformations.\r\nOver the years, several methods have been proposed\r\nto tackle the challenge of accurate and efﬁcient 3D shape\r\nmatching and retrieval [ 5,9,11,51,53,65,73]. Data-driven\r\n3D shape descriptors [ 54], capturing underlying properties\r\nof the shapes under study, are the common denominator of\r\nearly shape matching techniques. Global descriptors, such\r\nas volume and areas descriptors [ 78], describe the entirety\r\nof the shape in one compact representation, often failing\r\nto capture local ﬁne details of complex geometries. On the\r\nother hand, local descriptors [ 38,57] aim to tackle this issue\r\nbut they generally are sensitive to noise, based on landmarks\r\nand they might not capture semantic information [ 63]. More\r\nrecently, deep-learned shape descriptors [ 6,72] and neural\r\nnetworks for shape matching, based on auto-encoders [ 73]\r\nor transformers [ 59,68], have been proposed. Despite their\r\ngood performances, these methods require a huge amount\r\nof annotated data for training, which are hard to collect in\r\nﬁelds such as medical imaging [ 37]. Furthermore, non-rigid\r\npoint cloud matching and retrieval methods [ 35,36,71] are\r\ndesigned to handle shape deformations and, therefore, they\r\nmight be excessively ﬂexible ignoring ﬁne local details that\r\nare not due to deformations, such as anomalies.\r\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\r\nExcept for this watermark, it is identical to the accepted version;\r\nthe final published version of the proceedings is available on IEEE Xplore.\r\n3447\r\nIn this study, we introduce a novel method for rigid 3D\r\npoint cloud matching, based on spectral Laplacian eigen-\r\nmaps [ 4,23], which focus on local details. This technique\r\nis designed to overcome the limitations of shape descrip-\r\ntors and, although it can be seen as a linear graph neural\r\nnetwork, it does not require training. One of the main rea-\r\nsons that combinatorial and geometric Laplacians are of-\r\nten considered for spectral graph processing is that their\r\neigenvectors possess properties like the classical Fourier\r\nbasis functions [ 79]. We leverage these characteristics to\r\nperform locally-aware shape comparison of point clouds,\r\nequipped with k-nearest neighbor graph, without relying\r\non additional descriptors. Nevertheless, eigenspaces align-\r\nment, including both eigenvalue ordering and sign dis-\r\nambiguity of the eigendecomposition, is required to cor-\r\nrectly match shapes in the spectral space. Current State-\r\nof-The-Art methods for such alignment are based on Lapla-\r\ncian eigenfunctions matching [ 42,60], i.e. matching the\r\nhistograms of the eigenvectors. The latter is not robust\r\nand frequently fails in possible scenarios with highly sym-\r\nmetric geometries, such as bones, with small eigenvalue\r\nseparation. In this work, we introduce a novel operator,\r\ncalled Coupled Laplacian , designed to simultaneously pro-\r\nduce aligned eigenspaces for multiple registered geome-\r\ntries. Namely, we show that when two or more shapes, each\r\nwith their corresponding graphs, are merged into a single\r\ngraph using artiﬁcial cross-edges, the eigendecompositon\r\nof the Laplacian derived from such a combined graph yields\r\naligned spectral spaces for each individual component. Fur-\r\nthermore, this method is order-invariant, count-agnostic and\r\nlandmarks-free, enabling it to handle point clouds without\r\nbeing inﬂuenced by their speciﬁc arrangement or the num-\r\nber of points contained. Finally, we utilize the distance\r\nbetween these aligned higher-dimensional spectral spaces,\r\nwhich accounts for intricate local structures, as a global\r\nor point-wise score for different applications of 3D shape\r\nmatching, as shown in Fig. 1.\r\nOne natural application of the proposed technique, is 3D\r\nobject anomaly detection thought the identiﬁcation of lo-\r\ncal differences between two shapes. Hence, we tested our\r\nmethod on the MVTec 3D-AD dataset [ 7], recently pro-\r\nposed for unsupervised point cloud anomaly detection and\r\nlocalization. Furthermore, in the context of medical imag-\r\ning, correctly identifying the side of a bone (left or right) is\r\ncrucial for diagnosis, treatment planning, or skeletal anal-\r\nysis [ 12]. We refer to this task as Bone Side Estimation\r\n(BSE). Recent studies on bone landmark detection [ 18] and\r\nstatistical shape modeling [ 31] have highlighted the limita-\r\ntions of manual side identiﬁcation for ensuring proper func-\r\ntioning. To the best of our knowledge, manual bone mark-\r\nings [ 2] are currently the only technique used for this pur-\r\npose. Consequently, automatic BSE arises as an interesting,\r\nyet challenging, task to assist the development of fully auto-Global SimilarityPoint-Wise Similairty\r\nMatching\r\nCoupled Laplacian\r\nAligned Eigenmaps\r\nAnomaly Localization\r\nFigure 1. Overview of the proposed technique applied to different\r\ntasks. The global similarity between aligned eigenmaps of multi-\r\nple geometries, generated from the Coupled Laplacian, is used to\r\nmatch bones and predict their body side. While local similarity is\r\nexploited for accurate 3D anomaly detection.\r\nmated pipelines for medical image analysis, such as patient-\r\nspeciﬁc preoperative planning for Total Knee Arthroplasty\r\n(TKA) [ 33] or Anterior Cruciate Ligament Reconstruction\r\n(ACLR) [ 17,44]. Due to the bilateral symmetry of the an-\r\nimal and human body [ 27,67], i.e. right and left sides are\r\nmirror shapes of one another, the BSE can be deﬁned as\r\na non-trivial chiral shape matching problem. Then, given\r\na known reference or source bone, assessing the side of\r\na target bone involves ﬁnding the best match between the\r\nsource-target pair and its mirrored counterpart. The com-\r\nplexity of this task arises from subtle local differences in\r\nmirrored bones, which makes it a suitable application of\r\nthe proposed shape matching technique utilizing coupled\r\nLaplacian eigenmaps as local geometry descriptors. We\r\npropose a benchmark for human BSE by extracting sur-\r\nface point clouds of different bones, e.g. femur, hip and\r\ntibia, from public datasets [ 18,31,47]. We also discuss,\r\nin the Supplementary Material, a non-rigid correspondence\r\napplication of the proposed matching algorithm, the cross-\r\nspecies BSE (human to animal), on an internal dataset [ 40].\r\nOur contributions are summarized as follows:\r\n•We propose a new method to preform locally-aware 3D\r\nrigid point cloud matching, considering ﬁne local struc-\r\ntures, based on graph Laplacian eigenmaps.\r\n•We deﬁne the Coupled Laplacian operator for aligned\r\ngraphs to tackle the order and sign ambiguity issue of\r\nthe eigendecomposition.\r\n•Based on the proposed technique, we introduce a new\r\nmethod for automatic BSE to assist fully automated med-\r\nical pipelines and we propose a benchmark to test it.\r\n3448\r\n•We extensively evaluate our method on two tasks:\r\nanomaly detection and BSE, outperforming previously\r\nproposed techniques.\r\n2. Related Works\r\nEarly efforts in developing hand-crafted 3D local features\r\nfor point cloud matching, registration and retrieval have\r\ntypically drawn inspiration from 2D descriptors. Several\r\nmethods, such as Signature of Histograms of OrienTations\r\n(SHOT) [ 58,66], Rotational Projection Statistics (RoSP)\r\n[25] and Unique Shape Context (USC) [ 66] rely on the es-\r\ntimation of an unique Local Reference Frame (LRF). The\r\nlatter is usually based on the eigendecomposition of the\r\ncovariance matrix of the neighbours of a point of interest,\r\nwhich are then projected into the LRF to analyze their ge-\r\nometric properties. For instance, SHOT [ 58] captures lo-\r\ncal shape and orientation information by computing his-\r\ntograms of surface normals and point distribution around\r\nkeypoints. In contrast, LRF-free approaches [ 10,56,57]\r\ntry to rely just on features that are intrinsically invariant.\r\nThe most common LRF-free features are Fast Point Feature\r\nHisograms (FPFH) [ 57] which, similarly to SHOT, calcu-\r\nlates histograms of surface normals considering their rela-\r\ntionships in a local region around a keypoint and generat-\r\ning33-dimensional descriptors. Despite the progress made\r\nwith hand-crafted 3D local features, they encounter difﬁ-\r\nculties when it comes to deal with issues like point cloud\r\nresolutions, noisy data, and occlusions [ 26].\r\nOn the contrary, spectral-based point cloud descriptors\r\n[1,28,52,55,62,69,70] are a category of feature ex-\r\ntraction methods that leverage spectral analysis techniques\r\nfrom graph theory to capture the underlying structure and\r\nintrinsic geometric properties of point clouds. Among them,\r\nshape-DNA [ 52] is a surface descriptor based on eigenvalue\r\nanalysis of the Laplace-Beltrami operator. They propose to\r\nuse the sequence of eigenvalues (spectrum) of the Laplace\r\noperator as a ﬁngerprint characterizing the intrinsic geome-\r\ntry of 3D shapes represented as point clouds. This method\r\nhas been used for shape retrieval, classiﬁcation, and cor-\r\nrespondence. Weinmann et al. [70] also proposed to ex-\r\ntract a features set consisting of 8eigenvalues-based indices\r\nfor each 3D point of a cloud. Furthermore, Heat Kernel\r\nSignature (HKS) [ 62] and Wave Kernel Signatures (WKS)\r\n[1] are descriptors measuring how heat and wave propagate\r\nacross a shape, having the eigendecomposition as leading\r\nelement of the computation. Scaled eigenvectors evaluated\r\nat each point are instead directly exploited by the Global\r\nPoint Signature (GPS) [ 55] to represent a point cloud as\r\na set of inﬁnite-dimensional vectors, characterizing each\r\npoint within the global context of the surface it belongs.\r\nNevertheless, the vast majority of these works assumes that\r\nthe eigenvalues of a shape are distinct and, therefore, can be\r\nordered. Indeed, in practice, due to numerical approxima-tions, we cannot guarantee that the eigenvalues of the Lapla-\r\ncian are all distinct and, possible symmetries in the shapes\r\nmay cause some of them to have multiplicity grater than\r\none. As shown by Mateus et al. [42], when dealing with\r\nshape matching, an elegant way to overcome this problem\r\nis to use the Laplacian eigenmaps scheme [ 4], which can\r\nbe seen as a reduced GPS, and perform a-posteriori align-\r\nment of the resulting point cloud embeddings. Matching the\r\neigenfunctions histograms, i.e. their signatures, is the only\r\nreliable method for such embeddings alignment [ 42,60].\r\nRecently, Ma et. al. [39] proposed a canonization algorithm\r\nfor sign and basis invariance, called Maximal Axis Projec-\r\ntion (MAP), that adopts the permutation-invariant axis pro-\r\njection functions to determine the canonical directions. Un-\r\nfortunately, when the eigenvalues separation is small due to\r\nsymmetries in the geometries, these methods becomes un-\r\nstable and sensitive to noise. To overcome this issue, in\r\nthis work we propose the Coupled Laplacian operator to\r\nproduce a-priori aligned eigenmaps for several registered\r\nshapes, and perform locally-aware shape matching.\r\nDeep-learned point-cloud descriptors [ 6,24,72,77]\r\narose as an alternative to generate local features for 3D\r\nsufraces. We can distinguish three main categories de-\r\npending on the backbone architecture employed. Convo-\r\nlutional Neural Networks (CNNs) are often used on point\r\nclouds projected into 2D depth images [ 16,29] or directly\r\non 3D voxels [ 24,29,77]. Secondly, to work directly on\r\nraw point cloud data, PointNet and PointNet++ have been\r\nproposed aiming to learn rotation and permutation invari-\r\nant features [ 49,50]. Based on PointNet, several learned\r\ndescriptors have been introduced [ 13,14,75,76]. Among\r\nthem, PPFNet [ 14] and PPF-FoldNet [ 13] try to improve the\r\nfeature representation of PointNet by incorporating global\r\ncontext and point-pair features. Nevertheless, the lack of\r\nconvolutional layers in these models limits the learning of\r\nlocal geometries. Finally, transformers-based descriptors,\r\nsuch as Deep Closest Point [ 68], have been recently pro-\r\nposed trying to exploit the attention mechanisms to cap-\r\nture shapes and surfaces intrinsic characteristics. Arguably,\r\ntraining a deep-learning model, especially if based on trans-\r\nformers, it is not always feasible in terms of training sam-\r\nples required to achieve good performances. Furthermore,\r\nin supervised algorithms [ 13,14,24,77], paired 3D patches,\r\nsuch as in the 3DMatch dataset [ 77], are needed to train the\r\nmodels. For this reason, the 3D rigid point cloud matching\r\nmethod we propose does not need training and utilizes only\r\nthe similarity among properly aligned spectral embeddings\r\nto relate an unseen target shape to the given references.\r\n3. Method\r\nGraph Laplacian. A 3D point-cloud {xxxi}n\r\ni=1can be\r\ntreated as a connected undirected weighted graph G(V,E),\r\nwhere V={xxxi}n\r\ni=1is the nodes set and E={eeeij}is the\r\n3449\r\nTargetSource 1Source N\r\nRegistration\r\nCrossConnections\r\nRegistered Target-SourcesCoupled Laplacian\r\nAligned -dimensional  Embeddings \r\n12\r\nFigure 2. Proposed workﬂow of the Coupled Laplacian applied to proximal femur shapes. The Nsources are registered to the target\r\nusing a rigid or afﬁne registration. After that, cross-connection are added between each target-source pair (for simplicity, in the zoom the\r\nshapes are not overlapping) and the Coupled Laplacian is computed on the global graph. Its eigendecomposition leads to aligned spectral\r\nembeddings of the input geometries that can be used for shape matching.\r\nedge set. The latter is generally obtained though the con-\r\nstruction of a nearest neighbors graph [ 4]. To that purpose,\r\nin this work, we consider the k-Nearest Neighbour ( k-NN)\r\napproach, that is, node jis connected to node iifxxxiis\r\namong the knearest neighbours of xxxj. Hence, we can build\r\na weighted adjacency matrix, WWW={wij}, which stores the\r\nconnections between nodes. In particular, in spectral graph\r\ntheory, a Radial Basis Function (RBF) is commonly used as\r\nweight function for the edges between xxxiandxxxj, as\r\nwij=e x p✓\r\n\x00d2(xxxi,xxxj)\r\n\x002◆\r\n(1)\r\nwhere d2(·,·)is the Euclidean distance between two ver-\r\ntices and \x002a free parameter which, for simplicity, we\r\nset to the maximum distance between connected nodes,\r\nmax ijd2(xxxi,xxxj). The Laplacian matrix ,L2Rn⇥n, of\r\na graph constructed in such way, is deﬁned as L=D\x00W\r\nwhere Rn⇥n3DDD=diag([d1,···,dn]) is the degree ma-\r\ntrix with diagonal elements di=Pn\r\nj=1wij[43]. The\r\nLaplacian eigenvalues, {\x00i}n\r\ni=0, and eigenvectors, {\x00\x00\x00i}n\r\ni=0,\r\ncan be computed by solving the generalized eigenproblem\r\nL\x00\x00\x00i=\x00iBBB\x00\x00\x00i,i=0,···,n . (2)\r\nwhere BBB2Rn⇥nis generally set as DDD, and \x00i\uf8ff\x00i+1. Fi-\r\nnally, eigenmaps are simply eigenvectors sub-spaces, gener-\r\nated by leaving out \x00\x00\x000, corresponding to \x000=0, and using\r\nthe next meigenvectors for embedding graph nodes in an\r\nm-dimensional space, xxxi![\x00\x00\x001(i),···,\x00\x00\x00m(i)]\r\nCoupled Laplacian. The complete pipeline to gener-\r\nate aligned Laplacian embeddings through graph coupling\r\nis illustrated in Fig. 2. Let GTbe the graph of a target\r\n3D point cloud and GS1,···,GSNthe ones of Ndifferent\r\nsources. We construct a global graph, or coupled graph,\r\nGC, by adding cross-connections separately between thevertices of the source, VT={xxxT\r\ni}nT\r\ni=1, and each of the\r\nreference shapes, VSk={xxxSk\r\ni}nSk\r\ni=1with k=1,···,N.T o\r\ninclude meaningful cross-connections, we ﬁrst perform a\r\nrigid [ 19,20,45] or afﬁne [ 45] registration of the source ge-\r\nometries to align to the target. These methods are preferred\r\nto a non-rigid registration because, when dealing with the\r\nidentiﬁcation of ﬁne local variations, they do not change the\r\nrelative position of the points inside a point-set and, there-\r\nfore, the deformations are kept unchanged. After that, a\r\nsub-set of vertices, FT⇢VT, is stochastically extracted\r\nfrom the target geometry and their nearest correspondences\r\nare searched in each of the aligned reference point clouds as\r\nFSk={fffSk\r\nj: arg min\r\nxxxSk\r\ni2VSkd2(xxxSk\r\ni,fffT\r\nj)|fffT\r\nj2FT} (3)\r\nwith k=1,···,N. In this way, there is no constraint on the\r\noriginal number points comprising each shape nor on their\r\ninitial coordinates systems. Furthermore, the cardinality of\r\nthe target sub-set, is chosen as a fraction of its total points,\r\n|FT|=l·nT, where 0<l\uf8ff1. The cross-connection are\r\ntherefore added for each pair (fffSk\r\ni,fffT\r\ni),1\uf8ffi\uf8ff|FT|,\r\nbetween the target and the k-th source. Finally, we ar-\r\nrange the vertex indices of the coupled graph such that they\r\nare grouped for each individual geometry, and the whole\r\nweighted adjacency matrix can be computed as in Eq. ( 1).\r\nWe deﬁne the Laplacian matrix of a global graph, con-\r\nstructed as describe above, as the Coupled Laplacian, LLLC2\r\nRn⇥n, where n=nT+PN\r\nk=1nSk. Thanks to the vertex or-\r\ndering of the coupled graph, the solution of the generalized\r\neigenproblem of Eq. ( 2) applied to the Coupled Laplacian,\r\nwith BBBC=diag([DDDT,DDDS1,···,DDDSN]), yields to coupled\r\neigenvectors {\x00\x00\x00C\r\ni}n\r\ni=0. These eigenvectors can then be split\r\ninto each single component of the coupled graph as\r\n\x00\x00\x00C\r\ni=[\x00\x00\x00T\r\ni,\x00\x00\x00S1\r\ni,···,\x00\x00\x00SN\r\ni]T. (4)\r\n3450\r\nThe eigenspaces restricted to the single geometries, ob-\r\ntained thought the Coupled Laplacian, are intrinsically\r\naligned up to a certain component, depending of the fac-\r\ntorl, and the eigenvalues ordering issue is automatically\r\nsolved since the split eigenvectors are associated to the same\r\neigenvalues. The proof of this property for the ideal case of\r\nperfect match and more theoretical aspects of the Coupled\r\nLaplacian are reported in the Supplementary Material.\r\nShape Matching with Eigenmaps. The Coupled Lapla-\r\ncian allows the generation of m-dimensional embeddings\r\naligned for the graph vertices of the target and reference\r\nshapes, \x00\x00\x00T=( \x00\x00\x00T\r\n1,···,\x00\x00\x00T\r\nm)2RnT⇥mand\x00\x00\x00Sk=\r\n(\x00\x00\x00Sk\r\n1,···,\x00\x00\x00Skm)2RnSk⇥mfor1\uf8ffk\uf8ffN, respec-\r\ntively. Therefore, a comparison of multiple geometries in\r\nthis higher dimensional spectral space yields to a proper\r\nconsideration of local structures. We consider in the fol-\r\nlowing two possible applications of the coupled eigenmaps:\r\n(1) a global shape matching score and (2) local similarity\r\nscores. The ﬁrst one can be obtained by measuring the sim-\r\nilarity between the aligned eigenspaces through the Grass-\r\nmann distance, dG(·,·)[74]. Nevertheless, it cannot be\r\ncomputed directly, since we consider point clouds of ar-\r\nbitrary size and we do not have point-to-point correspon-\r\ndences. Hence, we restrict the distance computation only\r\nto the set of cross-connected vertices, FTfor the target and\r\nFSkfor the k-th reference. In order to get the reduced basis\r\nrestricted only to those coupled points, we perform a QR\r\ndecomposition of the restriction of the eigenmodes to the\r\ncross-points. The best matching source is then given by\r\narg min\r\nkdG(QQQT,QQQSk). (5)\r\nwhere QQQTandQQQSkare obtained from the QR factorization\r\nof\x00\x00\x00T(FT,:)and\x00\x00\x00Sk(FSk,:), respectively. This ﬁrst ap-\r\nproach is used in the following to solve the BSE task by\r\nusing two source shapes, i.e. the reference bone of known\r\nside and its contralateral mirrored version.\r\nOn the other hand, to obtain point-wise similarity scores\r\nwe propose to compare the m-dimensional embeddings of\r\nthe cross-connection vertices by using the cosine distance\r\nfunction. In this case, we interpret the distance value as\r\nthe probability of local structural difference, where 0 means\r\nthat the two compared points have the same local structure.\r\n4. Experiments\r\n4.1. Experiment settings\r\nBone Side Estimation. The proposed global shape match-\r\ning score is used to perform automatic BSE. Assuming gen-\r\neral unknown initial frames, we used the Principal Com-\r\nponent Analysis (PCA) to generate a mirrored version of\r\nthe reference surface, i.e. a synthetic contralateral bone.\r\nNamely, we identiﬁed that the bilateral symmetry of hu-\r\nman and animal bodies is equivalent to a mirroring aroundTable 1. Summary of the point cloud bone structures collected for\r\nthe BSE benchmark. L and R stands for Left and Right, respec-\r\ntively, and Sfor Sheep.\r\nDatasetFemur Hip Tibia Fibula\r\nLR LR LR LRHumanFisher et al. [18]18 19 20 20 -- --\r\nSSM-Tibia [ 31] -- -- - 30 - 30\r\nICL [ 47] 35 35 -- 35 35 35 35SInternal [ 40] 18 18 -- 18 18 --\r\nTotal 71 72 20 20 53 83 35 65\r\nthe second principal component on the vast majority of\r\nbones, such as the ones considered in this study. Hence,\r\ngiven the three point clouds, target, source and mirrored\r\nsource, the Coupled Laplacian of Fig. 2can be applied.\r\nMore speciﬁcally, we selected the Random Sample Con-\r\nsensus (RANSAC) [ 19], preceded by a spectral scaling, as\r\nrigid registration method to handle varying bone lengths\r\nwithout altering local structures. The scaling is performed\r\nusing only the information carried by the Fiedeler vector,\r\ni.e. the eigenvector \x00\x00\x001corresponding to the ﬁrst non-zero\r\neigenvalue \x001. A rough estimation of the bones length, in-\r\ndependently of the Euclidean frame, is given by the dis-\r\ntance between the points corresponding to minimum and\r\nmaximum values of the Fiedler vectors. Thanks to that,\r\none of the two bones can be scaled in order to match the\r\nlength of the other and improve the RANSAC registra-\r\ntion. The three aligned m-dimensional embeddings, derived\r\nfrom the eigendecomposition of the Coupled Laplacian, are\r\nthen used as in Eq. ( 5) to retrieve the best matching source\r\nand, consequently, the target side. Note that, the proposed\r\nBSE method is fully independent of the target and reference\r\nframes and therefore can be applied directly on the segmen-\r\ntation obtained from a medical image, without any previous\r\nknowledge, in a fully automated pipeline. In alternative,\r\none can using two different bone references, one left and\r\none right, avoiding the mirroring step, or apply the mirror-\r\ning on the target shape while letting the source unchanged.\r\nFull details of the described algorithm are provided in the\r\nSupplementary Material.\r\nWe generated a benchmark to test the BSE task by col-\r\nlecting several bone surface point cloud data from public\r\ndatasets [ 18,31,47]. Namely, from Fisher et al. [18] we\r\nextracted Femur and Hip structures, from SSM-Tibia [ 31]\r\nonly right Tibia and Fibula, and from the Imperial College\r\nLondon (ICL) [ 47] we collected Femur, Tibia and Fibula\r\npoint clouds. The detailed composition of the benchmark\r\nis reported in Tab. 1. Such a variety of shapes, which\r\nare acquired using different methods, makes the task more\r\nchallenging, including some intra-clinic variability. In our\r\nexperiments, for a given bone class, we performed cross-\r\ntesting by selecting each shape once as source and the others\r\n3451\r\nTable 2. Anomaly localization results. The area under the PRO curve is reported for an integration limit of 0.3for each evaluated method\r\nand dataset category. GAN, AE and VM results are provided by Bergmann et al. [7]. Moreover, we include the results obtained by\r\nrestricted GPS [ 55], eigenfunctions (Hist) [ 42,60] and Euclidean matching and MAP [ 39]. All the matching methods are applied after\r\nafﬁne CPD registration, when not speciﬁed, or CMM + CPD Non-Rigid ( NR) registration. The subsctipt on a method indicates the number\r\nof eigenmaps used. The overall best performing methods are highlighted in boldface, while the bests for each category are underlined.\r\nMethod BagelCable\r\nGlandCarrot Cookie Dowel Foam Peach Potato Rope Tire Mean "3D RGBVM 0.388 0.321 0.194 0.570 0.408 0.282 0.244 0.349 0.268 0.331 0.335\r\nGAN 0.421 0.422 0.778 0.696 0.494 0.252 0.285 0.362 0.402 0.631 0.474\r\nAE 0.432 0.158 0.808 0.491 0.841 0.406 0.262 0.216 0.716 0.478 0.4813D OnlyGAN 0.111 0.072 0.212 0.174 0.160 0.128 0.003 0.042 0.446 0.075 0.143\r\nAE 0.147 0.069 0.293 0.217 0.207 0.181 0.164 0.066 0.545 0.142 0.203\r\nVM 0.280 0.374 0.243 0.526 0.485 0.314 0.199 0.388 0.543 0.385 0.374\r\nEuclidean ( NR) 0.404 0.623 0.731 0.366 0.771 0.303 0.590 0.772 0.697 0.583 0.584\r\nGPS 100[55] 0.452 0.616 0.695 0.364 0.738 0.471 0.659 0.844 0.647 0.651 0.613\r\nGPS 200[55] 0.465 0.621 0.690 0.363 0.739 0.480 0.672 0.833 0.653 0.670 0.619\r\nHist 100[60] 0.476 0.629 0.703 0.365 0.744 0.473 0.661 0.840 0.647 0.693 0.623\r\nMAP 200[39] 0.481 0.630 0.694 0.399 0.742 0.497 0.653 0.832 0.649 0.675 0.625\r\nHist 200[60] 0.491 0.629 0.698 0.351 0.746 0.501 0.663 0.841 0.652 0.695 0.627\r\nEuclidean 0.655 0.631 0.743 0.615 0.803 0.528 0.726 0.875 0.762 0.695 0.703\r\nOurs 100 0.669 0.642 0.808 0.714 0.812 0.582 0.748 0.897 0.750 0.733 0.736\r\nOurs 200 0.702 0.630 0.728 0.735 0.812 0.701 0.780 0.914 0.767 0.713 0.748\r\nTable 3. Accuracy [%] of human BSE. All the matching methods\r\nare applied after RANSAC registration with spectral scaling, when\r\nnot speciﬁed, or CMM + CPD Non-Rigid ( NR) registration. The\r\noverall best performing methods are highlighted in boldface.\r\nMethod Femur Hip Fibula Tibia Mean "\r\nChamfer ( NR) 56.58 86.58 57.89 74.74 68.95\r\nHausdorff ( NR) 57.37 87.63 59.47 73.68 69.54\r\nHausdorff 73.52 97.95 59.30 72.52 75.82\r\nChamfer 71.64 98.65 64.62 74.47 77.35\r\nFPFH [ 57] 68.88 96.67 66.69 78.35 77.65\r\nMAP 20[39] 76.05 98.54 71.32 74.21 80.13\r\nHist 20[60] 77.63 98.42 69.47 77.37 80.72\r\nOurs 20 78.79 98.78 71.28 78.46 81.83\r\nOurs 10 79.68 97.76 73.47 78.66 82.39\r\nas targets on which the side has to be inferred. Therefore,\r\nthe average accuracy of all the experiments, achieved when\r\npredicting the correct body side, i.e. binary classiﬁcation,\r\ntakes into account the variability of the source shape. The\r\nrobustness of the method with respect to the reference is\r\ndiscussed in the Supplementary Material.\r\nAnomaly Localization. We tested the proposed ap-\r\nproach for the 3D anomaly detection task in the MVTec\r\n3D-AD dataset [ 7]. This recent dataset was designed for\r\nthe unsupervised detection of anomalies in point clouds of\r\nindustrially manufactured products. It contains over 4000\r\nhigh-resolution 3D scans of 10 object categories. In our ex-\r\nperiments, we chose one anomaly-free training sample for\r\neach class as source and we used the point-wise distances\r\nof the aligned eigenmaps between this shape and each tar-\r\nget in the test set as anomaly scores. We performed the pre-00.10.20.30.40.56065707580Eigenmaps5102030\r\nFraction of Cross-EdgesAccuracy [%]Figure 3. Average BSE accuracy with respect to the fraction of\r\ncross-edges used to build the Coupled Laplacian and the number\r\nof eigenmaps used for matching.\r\nregistration using afﬁne Coherent Point Drift (CPD) [ 45],\r\nwhich is more suited for objects like the ones included in\r\nthe MVTec 3D-AD, e.g. length and thickness variations are\r\nbetter captured by an afﬁne registration rather than rigid.\r\nIn order to speed-up the computation, we pre-processed\r\nthe point clouds by removing their ﬂat backgrounds with\r\na threshold on the z-axis after a 3-dimensional PCA and,\r\nwithout losing generality, we sub-sampled the number of\r\nforeground points to a maximum of 13000 . Finally, to com-\r\npare the obtained anomaly scores with the Ground Truth\r\n(GT) image, we projected back the points to the original 2D\r\nplane and, if any sub-sampling was performed, we applied\r\na dilation with a structuring element of size the inverse of\r\nthe sampling factor.\r\n3452\r\n = 100\r\n = 200  = 300 \r\nAnomaly (GT) Anomaly Score\r\nTarget\r\nSource Anomaly-Free\r\nHoleOpen\r\nCrack\r\nContaminationFigure 4. Graphical comparison of 3D anomaly localization using different numbers of aligned eigenmaps, m. Tuning the dimension of the\r\nembeddings computed with the Coupled Laplacian, it is possible to decide the extent and size of local surface differences that are detected.\r\n4.2. Experiment results\r\nBone Side Estimation. In Fig. 3, the average accuracy of\r\nBSE is depicted based on the fraction of artiﬁcial cross-\r\nedges added to create the coupled graph and the dimen-\r\nsion of the spectral embeddings, landm, respectively. We\r\ncan observe that the more coupled eigenmaps we aim to\r\nutilize for computing a global similarity score, the greater\r\nthe need for additional cross-connections to ensure the re-\r\nliability of the results. This occurs because, when graphs\r\nare weakly coupled with only a few cross-connections, the\r\nCoupled Laplacian captures limited intra-shape characteris-\r\ntics and predominantly emphasizes individual local geome-\r\ntries. In fact, when l=0and therefore the coupled graph is\r\nnot fully connected, the eigenmaps obtained using the Cou-\r\npled Laplacian are equivalent to the ones computed inde-\r\npendently on each single graph, if the eigenvalues are the\r\nsame (proof in the Supplmentary Material). On the other\r\nhand, not too many cross-edges, nor eigenmaps, are needed\r\nto obtain meaningful global matching scores and achieve\r\ngood performances in the BSE task. In Tab. 3a quantitative\r\ncomparison with other methods of human BSE accuracy is\r\nreported. For a fair comparison, all methods are performed\r\nafter RANSAC registration of the sources. Hausdorff and\r\nChamfer discrepancies are calculated on the Euclidean co-\r\nordinates of the points, for completeness, also in case of\r\n2-Step non-rigid GMM + CPD registration ( 2000 x slower\r\nthan RANSAC). Our method, using 10and20eigenmodes\r\nwith l=0.5fraction of cross-connections, outperforms the\r\nother techniques achieving higher accuracy both on the sin-\r\ngle bones and in average. Hence, the description provided\r\nby the aligned eigenmaps, obtained with Coupled Lapla-\r\ncian, is more aware of local details than the other methods.\r\nAnomaly Localization. Tab. 2lists quantitative resultsof each evaluated method for the localization of anoma-\r\nlies. For each category, the normalized area under the Per-\r\nRegion Overlap (PRO) curve with an upper integration limit\r\nof0.3[7], as well as, the mean performance, are reported.\r\nPerformance of Generative Adversarial Network (GAN),\r\nAutoencoder (AE) and Variation Model (VM) on the same\r\ntest set are provided by the dataset authors [ 7]. Further-\r\nmore, we tested restricted GPS [ 55], eigenmaps histogram\r\nmatching [ 42,60], MAP [ 39] and our method, all with the\r\nsame pre-processing and source shapes, using 100and200\r\neigenmaps. In order to obtain denser and more precise\r\nanomaly localization, the coupled graph is built using the\r\nwhole set of points as cross-connections, i.e. l=1. Fur-\r\nthermore, we include the result obtained using as anomaly\r\nscore for each target point the normalized Euclidean dis-\r\ntance of the nearest point in the source geometry, both using\r\nrigid and non-rigid registration. Our method, using only 3D\r\ninformation, outperforms all the other techniques. More-\r\nover, we obtained better results than deep-learning meth-\r\nods having RGB and Depth in combination as input. Inter-\r\nestingly, the point-wise similarity computed on eigenmaps\r\nnot properly aligned (GPS and Hist) is worst than just con-\r\nsidering Euclidean distances between points of registered\r\nshapes, making worthless the computation of spectral em-\r\nbeddings. In Fig. 4we compare the qualitative results ob-\r\ntained using different numbers of eigenmaps to score the\r\npoint-wise similarities. Using a smaller m, the proposed\r\ntechnique is prone to individuate only small regions with\r\nhighly dissimilar local geometries. By increasing the size\r\nof the spectral embeddings, we can identify larger and more\r\nsubtle surface differences, even if they are not necessarily\r\nclassiﬁed as anomalies in the GT. For instance, the potato\r\nand bagel surfaces have some natural irregularities, with re-\r\n3453\r\nEuclideanLaplacian\r\nTarget\r\nAnomaly (GT)Source  Anomaly-Free \r\nAnomaly Score\r\nCut\r\n0CombinedBent\r\nFigure 5. Comparison of the anomaly score obtained using the\r\nCoupled Laplacian technique, with m=2 0 0 andl=1, and\r\nEuclidean distance, both after afﬁne CPD alignment.\r\nspect to the source, that are not highlighted using 100and\r\n200-dimensional embeddings, but are instead detected with\r\n300eigenmaps. This concept is linked to the modal length ,\r\nwhich we deﬁne in the Supplementary Material. The trade-\r\noff between number of maps and extent of differences de-\r\ntected is interesting to tune the method to other tasks requir-\r\ning speciﬁc attention to identify surface dissimilarities.\r\nAblation Study. To further motivate the preference for\r\nCoupled Laplacian spectral space over the Euclidean one,\r\nin Fig. 5, we display comparisons of point-wise distances\r\n(anomaly scores) between source and target in both spaces,\r\nafter the same registration. In the ﬁrst and second row the\r\nbetter resistance to noise and outliers in the spectral space\r\nis highlighted. The bending of the cable gland, i.e. the\r\nanomaly, is barely detected in the Euclidean space because\r\nof noisy points on the back. Instead, with the aligned eigen-\r\nmaps comparison, the anomaly, as well as the noise, is cor-\r\nrectly localized. This is because both are local differences,\r\nwith respect to the reference, and, therefore, they can be\r\neasily captured with coupled eigenmaps. In addition, in the\r\nthird row, we show that our technique provides a sharper\r\nanomaly split of the cookie point cloud, which simpliﬁes\r\nthe anomaly detection task. Nevertheless, the usage of the\r\nproposed landmarks-free Coupled Laplacian is limited to\r\ntasks in which a correspondence between target and source\r\nshapes can be deﬁned by mean of a registration process.\r\nProblems including graph isomorphism, such as articulated\r\nshape matching [ 42,60], or non-rigid shape matching, can\r\nbe still solved using our method, without relying on the rigid\r\nalignment, if some cross-connection are deﬁned a-priori. In\r\nthis case, landmarks and their neighbouring points would be\r\nmost likely the easiest option to connect separate graphs and\r\nobtain aligned spectral embeddings for further processing.\r\nMoreover, a wrong registration of the shapes, due to ran-\r\ndomness and errors in the selected registration algorithm,\r\nmay affect the alignment of the m-dimensional embeddingsderived from the Coupled Laplacian. We report here some\r\nstatistics on registration failures in BSE (failure rate [%] /\r\nBSE accuracy on failures [%]), using different references,\r\nfor femur 6.84 ±7.82 / 96.67 ±11.30 (high symmetry)\r\nand hip 0.0 ±0.0 (low symmetry). Moreover, to test ro-\r\nbustness, we added white Gaussian rotation and translation\r\nnoise, i.i.d. in the 3 axis post-registration, obtaining (Rot.\r\nSD [\x00] / Trans. SD [mm] / Acc. Drop [%]): 2.5 / 5 / 8.16\r\n- 5 / 5 / 15.13 - 7.5 / 10 / 18.16. Given the low registration\r\nfailure rates and high robustness to noise, we can rely on the\r\nproposed coupling algorithm, making the method indepen-\r\ndent of the acquisition frames. Moreover, the method can\r\nwell generalize to any chiral shape matching problem and\r\nto tasks like identiﬁcation of topological noise in graphs and\r\npartial matching (see example in Supplementary Material).\r\n5. Conclusions\r\nWe presented a versatile method to perform 3D rigid point\r\ncloud matching, both globally and locally, by using aligned\r\neigenspaces for two or more similar shapes without prior\r\nknowledge on the local frames or specif markers on them.\r\nThe introduction of the Coupled Laplacian operator en-\r\nables the generation of aligned eigenmaps relying on cross-\r\nconnections added between graphs of registered geometries.\r\nWe introduced a novel task consisting in the automatic de-\r\ntection of a bone side, i.e. Bone Side Estimation (BSE), and\r\nwe proposed a benchmark to test it. Leveraging global sim-\r\nilarities of eigenmaps derived from the Coupled Laplacian,\r\nwe deﬁne a full pipeline to perform BSE on arbitrary bone\r\nsurfaces segmented from a generic medical image. More-\r\nover, we tested the ability of the proposed method in captur-\r\ning local surface differences by performing 3D anomaly de-\r\ntection on the MVTec 3D-AD dataset. The proposed tech-\r\nnique outperforms other methods on the two tasks, therefore\r\ncapturing better both global and local similarities, thanks\r\nto the matching performed on correctly aligned spectral\r\nspaces. Beside the applications showcased here, we believe\r\nthat our work can help several tasks in many ﬁelds in which\r\nthe localization of local difference is crucial for matching,\r\ndetection or retrieval. Future extensions could explore gen-\r\neralizations of the technique, not only for rigid matching\r\nbut also to address challenges such as graph isomorphism.\r\nThis approach opens new avenues for advancing the State-\r\nof-The-Art in 3D shape analysis and matching techniques.\r\n6. Acknowledgements\r\nThis project has received funding from the European\r\nUnion’s Horizon 2020 research and innovation programme\r\nunder the Marie Skłodowska-Curie grant agreement No\r\n945304-Cofund AI4theSciences hosted by PSL University.\r\nThis work was granted access to the HPC/AI resources of\r\nIDRIS under the allocation 2022-AD011013902 made by\r\nGENCI.\r\n3454\r\nReferences\r\n[1]Mathieu Aubry, Ulrich Schlickewei, and Daniel Cremers.\r\nThe wave kernel signature: A quantum mechanical approach\r\nto shape analysis. In 2011 IEEE International Conference\r\nonComputer Vision Workshops (ICCV Workshops), pages\r\n1626–1633, Barcelona, Spain, 2011. IEEE.\r\n[2]Ivan Bandovic, Matthew R. Holme, Asa C. Black, and Ben-\r\nnett Futterman. Anatomy, Bone Markings. In StatPearls.\r\nStatPearls Publishing, Treasure Island (FL), 2023.\r\n[3]Zachary M C Baum, Yipeng Hu, and Dean C Barratt. Real-\r\ntime multimodal image registration with partial intraoper-\r\native point-set data. Medical Image Analysis, 74:102231,\r\n2021.\r\n[4]Mikhail Belkin and Partha Niyogi. Laplacian Eigenmaps for\r\nDimensionality Reduction and Data Representation. Neural\r\nComputation, 15(6):1373–1396, 2003.\r\n[5]S. Belongie, J. Malik, and J. Puzicha. Shape matching and\r\nobject recognition using shape contexts. IEEE Transactions\r\nonPattern Analysis andMachine Intelligence, 24(4):509–\r\n522, 2002. Conference Name: IEEE Transactions on Pattern\r\nAnalysis and Machine Intelligence.\r\n[6]Paul Bergmann and David Sattlegger. Anomaly Detection in\r\n3D Point Clouds using Deep Geometric Descriptors. In 2023\r\nIEEE/CVF Winter Conference onApplications ofComputer\r\nVision (WACV), pages 2612–2622, Waikoloa, HI, USA,\r\n2023. IEEE.\r\n[7]Paul Bergmann, Xin Jin, David Sattlegger, and Carsten Ste-\r\nger. The MVTec 3D-AD Dataset for Unsupervised 3D\r\nAnomaly Detection and Localization. In Proceedings of\r\nthe17th International Joint Conference onComputer Vision,\r\nImaging andComputer Graphics Theory andApplications,\r\npages 202–213, 2022. arXiv:2112.09045 [cs].\r\n[8]P.J. Besl and Neil D. McKay. A method for registration\r\nof 3-D shapes. IEEE Transactions onPattern Analysis and\r\nMachine Intelligence, 14(2):239–256, 1992. Conference\r\nName: IEEE Transactions on Pattern Analysis and Machine\r\nIntelligence.\r\n[9]S. Bickel, B. Schleich, and S. Wartzack. A Novel Shape\r\nRetrieval Method for 3D Mechanical Components Based on\r\nObject Projection, Pre-Trained Deep Learning Models and\r\nAutoencoder. Computer-Aided Design, 154:103417, 2023.\r\n[10] Tolga Birdal and Slobodan Ilic. Point Pair Features Based\r\nObject Detection and Pose Estimation Revisited. In 2015\r\nInternational Conference on3DVision, pages 527–535,\r\n2015.\r\n[11] Michael M. Bronstein and Iasonas Kokkinos. Scale-invariant\r\nheat kernel signatures for non-rigid shape recognition. In\r\n2010 IEEE Computer Society Conference onComputer\r\nVision andPattern Recognition, pages 1704–1711, 2010.\r\nISSN: 1063-6919.\r\n[12] Michael C. Corballis. Bilaterally Symmetrical: To Be or Not\r\nto Be? Symmetry, 12(3):326, 2020. Number: 3 Publisher:\r\nMultidisciplinary Digital Publishing Institute.\r\n[13] Haowen Deng, Tolga Birdal, and Slobodan Ilic. PPF-\r\nFoldNet: Unsupervised Learning of Rotation Invariant 3D\r\nLocal Descriptors, 2018. arXiv:1808.10322 [cs].[14] Haowen Deng, Tolga Birdal, and Slobodan Ilic. PPFNet:\r\nGlobal Context Aware Local Features for Robust 3D Point\r\nMatching, 2018. arXiv:1802.02669 [cs].\r\n[15] Ben Eckart, Kihwan Kim, and Jan Kautz. Fast and Accurate\r\nPoint Cloud Registration using Trees of Gaussian Mixtures,\r\n2018. arXiv:1807.02587 [cs].\r\n[16] Gil Elbaz, Tamar Avraham, and Anath Fischer. 3D Point\r\nCloud Registration for Localization Using a Deep Neu-\r\nral Network Auto-Encoder. In 2017 IEEE Conference on\r\nComputer Vision andPattern Recognition (CVPR), pages\r\n2472–2481, Honolulu, HI, 2017. IEEE.\r\n[17] Francisco Figueroa, David Figueroa, Rodrigo Guiloff, Sven\r\nPutnis, Brett Fritsch, and Minerva Itriago. Navigation in\r\nanterior cruciate ligament reconstruction: State of the art.\r\nJournal ofISAKOS, 8(1):47–53, 2023.\r\n[18] Maximilian C. M. Fischer, Sonja A. G. A. Grothues, Ju-\r\nliana Habor, Mat ´ıas de la Fuente, and Klaus Radermacher. A\r\nrobust method for automatic identiﬁcation of femoral land-\r\nmarks, axes, planes and bone coordinate systems using sur-\r\nface models. Scientiﬁc Reports, 10(1):20859, 2020. Num-\r\nber: 1 Publisher: Nature Publishing Group.\r\n[19] Martin A. Fischler and Robert C. Bolles. Random sample\r\nconsensus: a paradigm for model ﬁtting with applications to\r\nimage analysis and automated cartography. Communications\r\noftheACM, 24(6):381–395, 1981.\r\n[20] Wei Gao and Russ Tedrake. FilterReg: Robust and Ef-\r\nﬁcient Probabilistic Point-Set Registration using Gaussian\r\nFilter and Twist Parameterization, 2019. arXiv:1811.10136\r\n[cs].\r\n[21] Yang Gao, Honglin Yuan, Tao Ku, Remco C. Veltkamp,\r\nGeorgios Zamanakos, Lazaros Tsochatzidis, Angelos Ama-\r\nnatiadis, Ioannis Pratikakis, Aliki Panou, Ioannis Romanelis,\r\nVlassis Fotis, Gerasimos Arvanitis, and Konstantinos Mous-\r\ntakas. SHREC 2023: Point cloud change detection for city\r\nscenes. Computers &Graphics, 115:35–42, 2023.\r\n[22] Song Ge and Guoliang Fan. Articulated Non-Rigid Point Set\r\nRegistration for Human Pose Estimation from 3D Sensors.\r\nSensors, 15(7):15218–15245, 2015. Number: 7 Publisher:\r\nMultidisciplinary Digital Publishing Institute.\r\n[23] Benyamin Ghojogh, Ali Ghodsi, Fakhri Karray, and Mark\r\nCrowley. Laplacian-Based Dimensionality Reduction In-\r\ncluding Spectral Clustering, Laplacian Eigenmap, Local-\r\nity Preserving Projection, Graph Embedding, and Diffusion\r\nMap: Tutorial and Survey, 2022. arXiv:2106.02154 [cs,\r\nstat].\r\n[24] Zan Gojcic, Caifa Zhou, Jan D. Wegner, and Andreas\r\nWieser. The Perfect Match: 3D Point Cloud Matching With\r\nSmoothed Densities. In 2019 IEEE/CVF Conference on\r\nComputer Vision andPattern Recognition (CVPR), pages\r\n5540–5549, Long Beach, CA, USA, 2019. IEEE.\r\n[25] Yulan Guo, Ferdous Sohel, Mohammed Bennamoun, Min\r\nLu, and Jianwei Wan. Rotational Projection Statistics\r\nfor 3D Local Surface Description and Object Recognition.\r\nInternational Journal ofComputer Vision, 105(1):63–86,\r\n2013. arXiv:1304.3192 [cs].\r\n[26] Yulan Guo, Mohammed Bennamoun, Ferdous Sohel, Min\r\nLu, Jianwei Wan, and Ngai Ming Kwok. A Comprehen-\r\n3455\r\nsive Performance Evaluation of 3D Local Feature Descrip-\r\ntors. International Journal ofComputer Vision, 116(1):66–\r\n89, 2016.\r\n[27] G´abor Holl ´o and Mih ´aly Nov ´ak. The manoeuvrability hy-\r\npothesis to explain the maintenance of bilateral symmetry in\r\nanimal evolution. Biology Direct, 7(1):22, 2012.\r\n[28] Jiaxi Hu and Jing Hua. Salient spectral geometric features\r\nfor shape matching and retrieval. TheVisual Computer, 25\r\n(5):667–675, 2009.\r\n[29] Haibin Huang, Evangelos Kalogerakis, Siddhartha Chaud-\r\nhuri, Duygu Ceylan, Vladimir G. Kim, and Ersin Yumer.\r\nLearning Local Shape Descriptors from Part Correspon-\r\ndences With Multi-view Convolutional Networks, 2017.\r\narXiv:1706.04496 [cs].\r\n[30] Bing Jian and Baba C. Vemuri. Robust Point Set Registra-\r\ntion Using Gaussian Mixture Models. IEEE Transactions\r\nonPattern Analysis andMachine Intelligence, 33(8):1633–\r\n1645, 2011. Conference Name: IEEE Transactions on Pat-\r\ntern Analysis and Machine Intelligence.\r\n[31] Meghan Keast, Jason Bonacci, and Aaron Fox. Geometric\r\nvariation of the human tibia-ﬁbula: a public dataset of tibia-\r\nﬁbula surface meshes and statistical shape model. PeerJ, 11:\r\ne14708, 2023. Publisher: PeerJ Inc.\r\n[32] Kazuma Kobayashi, Lin Gu, Ryuichiro Hataya, Takaaki\r\nMizuno, Mototaka Miyake, Hirokazu Watanabe, Masamichi\r\nTakahashi, Yasuyuki Takamizawa, Yukihiro Yoshida,\r\nSatoshi Nakamura, Nobuji Kouno, Amina Bolatkan, Yusuke\r\nKurose, Tatsuya Harada, and Ryuji Hamamoto. Sketch-\r\nbased Medical Image Retrieval, 2023. arXiv:2303.03633\r\n[cs].\r\n[33] Adriaan Lambrechts, Roel Wirix-Speetjens, Frederik Maes,\r\nand Sabine Van Huffel. Artiﬁcial Intelligence Based Patient-\r\nSpeciﬁc Preoperative Planning Algorithm for Total Knee\r\nArthroplasty. Frontiers inrobotics andAI, 9:840282, 2022.\r\n[34] Liang Li, Ming Yang, Chunxiang Wang, and Bing Wang.\r\nRigid Point Set Registration Based on Cubature Kalman\r\nFilter and Its Application in Intelligent Vehicles. IEEE\r\nTransactions onIntelligent Transportation Systems, 19(6):\r\n1754–1765, 2018. Conference Name: IEEE Transactions\r\non Intelligent Transportation Systems.\r\n[35] Zhouhui Lian, Afzal Godil, Benjamin Bustos, Mohamed\r\nDaoudi, Jeroen Hermans, Shun Kawamura, Yukinori Kurita,\r\nGuillaume Lavou ´e, Hien Van Nguyen, Ryutarou Ohbuchi,\r\nYuki Ohkita, Yuya Ohishi, Fatih Porikli, Martin Reuter, Ivan\r\nSipiran, Dirk Smeets, Paul Suetens, Hedi Tabia, and Dirk\r\nVandermeulen. A comparison of methods for non-rigid 3D\r\nshape retrieval. Pattern Recognition, 46(1):449–461, 2013.\r\n[36] Z. Lian, J. Zhang, S. Choi, H. ElNaghy, J. El-Sana, T. Fu-\r\nruya, A. Giachetti, R. A. Guler, L. Lai, C. Li, H. Li, F. A.\r\nLimberger, R. Martin, R. U. Nakanishi, A. P. Neto, L. G.\r\nNonato, R. Ohbuchi, K. Pevzner, D. Pickup, P. Rosin, A.\r\nSharf, L. Sun, X. Sun, S. Tari, G. Unal, and R. C. Wilson.\r\nNon-rigid 3D shape retrieval. In Proceedings ofthe2015\r\nEurographics Workshop on3DObject Retrieval, pages 107–\r\n120, Goslar, DEU, 2015. Eurographics Association.\r\n[37] Eyal Lotan, Charlotte Tschider, Daniel K. Sodickson,\r\nArthur L. Caplan, Mary Bruno, Ben Zhang, and Yvonne W.Lui. Medical Imaging and Privacy in the Era of Artiﬁcial\r\nIntelligence: Myth, Fallacy, and the Future. Journal ofthe\r\nAmerican College ofRadiology :JACR, 17(9):1159–1162,\r\n2020.\r\n[38] David G. Lowe. Distinctive Image Features from Scale-\r\nInvariant Keypoints. International Journal ofComputer\r\nVision, 60(2):91–110, 2004.\r\n[39] Jiangyan Ma, Yifei Wang, and Yisen Wang. Laplacian Can-\r\nonization: A Minimalist Approach to Sign and Basis Invari-\r\nant Spectral Embedding, 2024. arXiv:2310.18716 [cs].\r\n[40] Deyo Maeztu Redin, Julien Caroux, Pierre-Yves Rohan,\r\nH´el`ene Pillet, Alexia Cermolacce, Julien Trnka, Mathieu\r\nManassero, V ´eronique Viateau, and Laurent Cort ´e. A wear\r\nmodel to predict damage of reconstructed ACL. Journal\r\noftheMechanical Behavior ofBiomedical Materials, page\r\n105426, 2022.\r\n[41] Baraka Maiseli, Yanfeng Gu, and Huijun Gao. Recent devel-\r\nopments and trends in point set registration methods. Journal\r\nofVisual Communication andImage Representation, 46:95–\r\n106, 2017.\r\n[42] Diana Mateus, Radu Horaud, David Knossow, Fabio Cuz-\r\nzolin, and Edmond Boyer. Articulated Shape Matching Us-\r\ning Laplacian Eigenfunctions and Unsupervised Point Reg-\r\nistration. In 2008 IEEE Conference onComputer Vision and\r\nPattern Recognition, pages 1–8, 2008. arXiv:2012.07340\r\n[cs].\r\n[43] Russell Merris. Laplacian matrices of graphs: a sur-\r\nvey.Linear Algebra anditsApplications, 197-198:143–176,\r\n1994.\r\n[44] Kento Morita, Syoji Kobashi, Kaori Kashiwa, Hiroshi\r\nNakayama, Shunichiro Kambara, Masakazu Morimoto,\r\nShinichi Yoshiya, and Satoru Aikawa. Computer-aided Sur-\r\ngical Planning of Anterior Cruciate Ligament Reconstruc-\r\ntion in MR Images. Procedia Computer Science, 60:1659–\r\n1667, 2015.\r\n[45] Andriy Myronenko and Xubo Song. Point-Set Regis-\r\ntration: Coherent Point Drift. IEEE Transactions on\r\nPattern Analysis andMachine Intelligence, 32(12):2262–\r\n2275, 2010. arXiv:0905.2635 [cs].\r\n[46] Thao Nguyen, Nakul Gopalan, Roma Patel, Matt Cor-\r\nsaro, Ellie Pavlick, and Stefanie Tellex. Robot Object Re-\r\ntrieval with Contextual Natural Language Queries, 2020.\r\narXiv:2006.13253 [cs].\r\n[47] Daniel Nolte, Chui Kit Tsang, Kai Yu Zhang, Ziyun Ding,\r\nAngela E. Kedgley, and Anthony M. J. Bull. Non-linear\r\nscaling of a musculoskeletal model of the lower limb using\r\nstatistical shape models. Journal ofBiomechanics, 49(14):\r\n3576–3581, 2016.\r\n[48] Abdol Hamid Pilevar. CBMIR: Content-based Image Re-\r\ntrieval Algorithm for Medical Image Databases. Journal of\r\nMedical Signals andSensors, 1(1):12–18, 2011.\r\n[49] Charles R. Qi, Hao Su, Kaichun Mo, and Leonidas J. Guibas.\r\nPointNet: Deep Learning on Point Sets for 3D Classiﬁcation\r\nand Segmentation, 2017. arXiv:1612.00593 [cs].\r\n[50] Charles R. Qi, Li Yi, Hao Su, and Leonidas J. Guibas. Point-\r\nNet++: Deep Hierarchical Feature Learning on Point Sets in\r\na Metric Space, 2017. arXiv:1706.02413 [cs].\r\n3456\r\n[51] Martin Reuter, Franz-Erich Wolter, and Niklas Peinecke.\r\nLaplace-spectra as ﬁngerprints for shape matching. In\r\nProceedings ofthe2005 ACM symposium onSolid and\r\nphysical modeling, pages 101–106, New York, NY, USA,\r\n2005. Association for Computing Machinery.\r\n[52] Martin Reuter, Franz-Erich Wolter, and Niklas Peinecke.\r\nLaplace–Beltrami spectra as ‘Shape-DNA’ of surfaces and\r\nsolids. Computer-Aided Design, 38(4):342–366, 2006.\r\n[53] Martin Reuter, Silvia Biasotti, Daniela Giorgi, Giuseppe\r\nPatan `e, and Michela Spagnuolo. Discrete Laplace–Beltrami\r\noperators for shape analysis and segmentation. Computers\r\n&Graphics, 33(3):381–390, 2009.\r\n[54] R. Rostami, F. S. Bashiri, B. Rostami, and Z. Yu. A Sur-\r\nvey on Data-Driven 3D Shape Descriptors. Computer\r\nGraphics Forum, 38(1):356–393, 2019. eprint:\r\nhttps://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.13536.\r\n[55] Raif M. Rustamov. Laplace-Beltrami Eigenfunctions\r\nforDeformation Invariant Shape Representation. The\r\nEurographics Association, 2007. Accepted: 2014-01-\r\n29T09:43:15Z ISSN: 1727-8384.\r\n[56] Radu Bogdan Rusu, Nico Blodow, Zoltan Csaba Marton,\r\nand Michael Beetz. Aligning point cloud views using per-\r\nsistent feature histograms. In 2008 IEEE/RSJ International\r\nConference onIntelligent Robots andSystems, pages 3384–\r\n3391, 2008. ISSN: 2153-0866.\r\n[57] Radu Bogdan Rusu, Nico Blodow, and Michael Beetz.\r\nFast Point Feature Histograms (FPFH) for 3D registration.\r\nIn2009 IEEE International Conference onRobotics and\r\nAutomation, pages 3212–3217, Kobe, 2009. IEEE.\r\n[58] Samuele Salti, Federico Tombari, and Luigi Di Stefano.\r\nSHOT: Unique signatures of histograms for surface and tex-\r\nture description. Computer Vision andImage Understanding,\r\n125:251–264, 2014.\r\n[59] Dimple A. Shajahan, Mukund Varma T, and Ramanathan\r\nMuthuganapathy. Point Transformer for Shape Classiﬁca-\r\ntion and Retrieval of 3D and ALS Roof PointClouds, 2021.\r\narXiv:2011.03921 [cs].\r\n[60] Avinash Sharma, Radu Horaud, and Diana Mateus. 3D\r\nShape Registration Using Spectral Graph Embedding and\r\nProbabilistic Matching, 2021. arXiv:2106.11166 [cs, stat].\r\n[61] Martin Sinko, Patrik Kamencay, Robert Hudec, and Miroslav\r\nBenco. 3D registration of the point cloud data using ICP\r\nalgorithm in medical image analysis. In 2018 ELEKTRO,\r\npages 1–6, 2018.\r\n[62] Jian Sun, Maks Ovsjanikov, and Leonidas Guibas. A Con-\r\ncise and Provably Informative Multi-Scale Signature Based\r\non Heat Diffusion. Computer Graphics Forum, 28(5):1383–\r\n1392, 2009.\r\n[63] Sarah Tang and Afzal Godil. An evaluation of local shape\r\ndescriptors for 3D shape retrieval. In Three-Dimensional\r\nImage Processing (3DIP) andApplications II, pages 217–\r\n231. SPIE, 2012.\r\n[64] Te Tang and Masayoshi Tomizuka. Track deformable objects\r\nfrom point clouds with structure preserved registration. The\r\nInternational Journal ofRobotics Research, 41(6):599–614,\r\n2022. Publisher: SAGE Publications Ltd STM.[65] Johan W. H. Tangelder and Remco C. Veltkamp. A survey of\r\ncontent based 3D shape retrieval methods. Multimedia Tools\r\nandApplications, 39(3):441–471, 2008.\r\n[66] Federico Tombari, Samuele Salti, and Luigi Di Stefano.\r\nUnique shape context for 3d data description. In Proceedings\r\noftheACM workshop on3Dobject retrieval, pages 57–62,\r\nNew York, NY, USA, 2010. Association for Computing Ma-\r\nchinery.\r\n[67] Søren Toxvaerd. The Emergence of the Bilateral Symmetry\r\nin Animals: A Review and a New Hypothesis. Symmetry,\r\n13(2):261, 2021. Number: 2 Publisher: Multidisciplinary\r\nDigital Publishing Institute.\r\n[68] Yue Wang and Justin M. Solomon. Deep Closest Point:\r\nLearning Representations for Point Cloud Registration,\r\n2019. arXiv:1905.03304 [cs] version: 1.\r\n[69] Yiqun Wang, Jianwei Guo, Dong-Ming Yan, Kai Wang,\r\nand Xiaopeng Zhang. A Robust Local Spectral Descriptor\r\nfor Matching Non-Rigid Shapes With Incompatible Shape\r\nStructures. In 2019 IEEE/CVF Conference onComputer\r\nVision andPattern Recognition (CVPR), pages 6224–6233,\r\nLong Beach, CA, USA, 2019. IEEE.\r\n[70] Martin Weinmann, Boris Jutzi, and Cl ´ement Mallet. Seman-\r\ntic 3D scene interpretation: A framework combining optimal\r\nneighborhood size selection with relevant features. ISPRS\r\nAnnals ofthePhotogrammetry, Remote Sensing andSpatial\r\nInformation Sciences, II-3:181–188, 2014.\r\n[71] Hao Wu, Lincong Fang, Qian Yu, and Chengzhuan Yang.\r\nLearning Robust Point Representation for 3D Non-Rigid\r\nShape Retrieval. IEEE Transactions onMultimedia, pages\r\n1–15, 2023. Conference Name: IEEE Transactions on Mul-\r\ntimedia.\r\n[72] Jin Xie, Guoxian Dai, Fan Zhu, Edward K. Wong, and Yi\r\nFang. DeepShape: Deep-Learned Shape Descriptor for 3D\r\nShape Retrieval. IEEE Transactions onPattern Analysis and\r\nMachine Intelligence, 39(7):1335–1345, 2017. Conference\r\nName: IEEE Transactions on Pattern Analysis and Machine\r\nIntelligence.\r\n[73] Guoqing Xu and Weiwei Fang. Shape retrieval us-\r\ning deep autoencoder learning representation. In 2016\r\n13th International Computer Conference onWavelet\r\nActive Media Technology and Information Processing\r\n(ICCWAMTIP), pages 227–230, 2016.\r\n[74] Ke Ye and Lek-Heng Lim. Schubert Varieties and Dis-\r\ntances between Subspaces of Different Dimensions. SIAM\r\nJournal onMatrix Analysis andApplications, 37(3):1176–\r\n1197, 2016.\r\n[75] Zi Jian Yew and Gim Hee Lee. 3DFeat-Net: Weakly Su-\r\npervised Local 3D Features for Point Cloud Registration.\r\nInComputer Vision –ECCV 2018, pages 630–646, Cham,\r\n2018. Springer International Publishing.\r\n[76] Kwang Moo Yi, Eduard Trulls, Vincent Lepetit, and Pas-\r\ncal Fua. LIFT: Learned Invariant Feature Transform, 2016.\r\narXiv:1603.09114 [cs].\r\n[77] Andy Zeng, Shuran Song, Matthias Nießner, Matthew\r\nFisher, Jianxiong Xiao, and Thomas Funkhouser. 3DMatch:\r\nLearning Local Geometric Descriptors from RGB-D Recon-\r\nstructions, 2017. arXiv:1603.08182 [cs].\r\n3457\r\n[78] Cha Zhang and Tsuhan Chen. Efﬁcient feature extraction\r\nfor 2D/3D objects in mesh representation. In Proceedings\r\n2001 International Conference onImage Processing (Cat.\r\nNo.01CH37205), pages 935–938 vol.3, 2001.\r\n[79] H. Zhang, O. Van Kaick, and R. Dyer. Spectral Mesh\r\nProcessing. Computer Graphics Forum, 29(6):1865–1894,\r\n2010.\r\n3458'}, 'dist': 0.9286905527114868}
Result 16: {'text': 'data', 'metadata': {'created_at': 1736932570, 'modified_at': 1736932570, 'owner': 'sayandeep', 'path': 'RAG_CONTENT/conferences/cvpr_ocr/Bulat_FFF_Fixing_Flawed_Foundations_in_Contrastive_Pre-Training_Results_in_Very_CVPR_2024_paper.txt', 'size': 51757, 'seen_at': 1737191136, 'data': "FFF: Fixing Flawed Foundations in contrastive pre-training results in very\nstrong Vision-Language models\nAdrian Bulat1,2Yassine Ouali1Georgios Tzimiropoulos1,3\n1Samsung AI Center Cambridge, UK2Technical University of Ias ,i, Romania\n3Queen Mary University of London, UK\nAbstract\nDespite noise and caption quality having been acknowl-\nedged as important factors impacting vision-language con-\ntrastive pre-training, in this paper, we show that the full\npotential of improving the training process by addressing\nsuch issues is yet to be realized. Specifically, we firstly\nstudy and analyze two issues affecting training: incorrect\nassignment of negative pairs, and low caption quality and\ndiversity. Then, we devise effective solutions for address-\ning both problems, which essentially require training with\nmultiple true positive pairs. Finally, we propose training\nwith sigmoid loss to address such a requirement. We show\nvery large gains over the current state-of-the-art for both\nimage recognition ( ∼+6% on average over 11 datasets)\nand image retrieval ( ∼+19% on Flickr30k and ∼+15%\non MSCOCO).\n1. Introduction\nLarge-scale contrastive image-text pre-training has emerged\nas the prevalent method for vision-language representation\nlearning [14, 21, 27, 29, 38, 51, 52]. The majority of\ndatasets employed for pre-training are web-collected [4, 10,\n34, 40–43, 45]. They offer a varied data distribution and\nare sufficiently large to effectively train high-performing\nvision-language models. However, since the raw captions\nfor each image are typically extracted from associated tags\nor descriptions, they often exhibit low quality, being noisy\nand suboptimal for training purposes [21, 27]. Although\nsome attempts to fix such issues have been already de-\nscribed, to some extent, in literature (e.g. ALIP [50],\nBLIP [27]), in this work, we show that the full potential of\nimproving the quality of the training process is far from be-\ning fully realized. Specifically, by studying and addressing\nspecific issues related to noise and low data quality, in this\nwork, we show that our improved vision-language training\npipeline can achieve massive gains over the current state-\nof-the-art methods for both image recognition ( ∼+6% on\nFigure 1. Our approach, FFF, achieves state-of-the-art accuracy\nacross multiple datasets, largely outperforming prior methods.\naverage over 11 datasets) and image retrieval ( ∼+19% on\nFlickr30k [53] and ∼+15% on MSCOCO [30]).\nThe first issue we study is related to noise impacting con-\ntrastive learning: near-duplicate samples which are incor-\nrectly treated as negative pairs. Even within a batch, it is not\nuncommon to find images and/or captions that are seman-\ntically similar or even identical. Since standard contrastive\nlearning assumes one positive pair, this significantly hinders\nthe training process and the quality of the trained models.\nThe second issue we study is related to low caption qual-\nity and diversity. Captions can be short and lacking detail,\nnoisy, or even entirely irrelevant to the image. Moreover,\nsince the mapping process between image and text is one-\nto-many, more than one caption is needed to provide an ap-\nproximate description of the image.\nTo fix issue one, we propose an algorithm that mines new\npositive pairs based on image-text, image-image, and text-\ntext similarities, aiming to decrease the number of false neg-\natives in the training data arising due to semantically similar\nimages and/or captions.\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n14172\nWe fix issue two by firstly generating pseudo-captions\nfor each training image using a state-of-the-art image cap-\ntioning technique [28] that will act as new true positives for\na given image. Then, we propose batch text augmentation\nfor training with multiple pseudo-captions ( i.e. five captions\nper image selected via beam search) within the same batch\nto effectively increase caption diversity.\nImportantly, after applying the proposed solutions, we\nend up with a variable number of positive pairs per im-\nagei.e. newly mined positive pairs and multiple pseudo-\ncaptions per image. This implies that we need to train our\nmodel with a loss function that accommodates multiple pos-\nitives and is robust to potential errors in the mining process.\nUnfortunately, neither contrastive loss [38] nor supervised\ncontrastive loss [22] can be directly applied for this case.\nTo this end, we propose to use the sigmoid loss [54] which\nallows the number of positives to vary dynamically per sam-\nple and per batch at no extra cost and is also robust to noise.\nOverall, we make the following contributions :\n• We study and provide in-depth analyses of two important\nissues related to vision-language training process/data:\nfalse negative pairs due to semantic near-duplicates, and\nlow caption quality and diversity (Sec. 2).\n• We provide two simple algorithms for addressing the\naforementioned issues: The first one uses text-image,\nimage-image, and text-text similarities for eliminating in-\ncorrectly assigned negatives and mining new true posi-\ntives. The second uses the proposed batch text augmenta-\ntionfor training with multiple pseudo-captions per image\nwithin the same batch. Both solutions induce multiple\nnew positives per each training image. To address this,\nwe propose to use sigmoid loss for training the model.\nSee Sec. 4.\n• We show very large gains over the current state-of-the-art\nfor both image recognition ( ∼+6% on average over 11\ndatasets) and image retrieval ( ∼+19% on Flickr30k and\n∼+15% on MSCOCO) (Sec. 5). We further ablate the\nimpact of many important components of our method in\nSec. 6.\n2. Flaws of web-collected datasets & potential\nsolutions\nSeveral observations drawn by analyzing the flaws of a web-\ncollected dataset (CC3M dataset), motivating the proposed\napproach, are provided below:\nOriginal captions are noisy and repetitive: For example,\nas illustrated in Fig. 2 for the CC3M dataset, original (raw)\ncaptions contain a high number of generic captions that fre-\nquently reoccur across the dataset (Fig. 2 (c)), and are often\nsemantically similar (Fig. 2 (a)). Moreover, many raw cap-\ntions may be unrelated to their associated images and their\nvisual content, as indicated by low CLIP scores (Fig. 2 (b)).Re-captioning enhances quality and diversity: A poten-\ntial solution to this issue is the use of state-of-the-art image\ncaptioning models ( e.g. BLIP2 [28], OFA [46]) to generate\nsynthetic pseudo-captions, which can enhance the quality\nand descriptiveness of the captions. When comparing raw\nand pseudo-captions, it is evident that the latter are more di-\nverse and semantically relevant to their associated images,\nas shown in Fig. 2.\nMultiple pseudo-captions should reduce noise: State-of-\nthe-art image captioning models, despite being capable of\ngenerating fluent and diverse captions, are often trained\nand bootstrapped from the same web-collected data used in\ntraining vision-language models. Consequently, as shown\nin Fig. 4, in some instances, the generated pseudo-captions\ncan be ambiguous and contain hallucinations, errors, and\nstylistic biases similar to those found in the raw captions.\nAs a result, relying on a single pseudo-caption per image\ncan still introduce a high degree of noise and can hinder the\ntraining of an effective vision-language model.\nA potential solution to this issue is the use of multiple\npseudo-captions or multiple positives per image in the hope\nthat even if individual captions are incorrect, their ensem-\nble is of higher quality and better reflects the content of the\nassociated image. To probe for the possible positive effect\nof using multiple synthetic captions, in Fig. 3 (a), we show\nthe intra-cosine similarities of 5 pseudo-captions generated\nusing beam search and, respectively, in Fig. 3 (b), the aver-\nage image-text CLIP score between these synthetic captions\nand their associated images - contrasted with the score cor-\nresponding to a single caption. We observe that: 1) a simple\nmethod such as beam search can generate diverse synthetic\ncaptions, and, more crucially, 2) using multiple positives per\nimage results in an improved ensemble that better describes\nthe image and helps alleviate the problem of false positives\ndue to incorrect individual instances.\nMining of new positives: As shown in Fig. 3 (c), even for a\nrelatively small batch of 1 kimage-caption pairs, it is com-\nmon to find captions more similar to the image than the\nground-truth caption ( i.e. higher ranks), and, as displayed\nin Fig. 5, such high-ranking captions often contain true pos-\nitives, which are captions that can be considered ground-\ntruth descriptions for the associated image.\nA potential solution to this is the use of online mining of\nnew positives based on image and text feature cosine simi-\nlarities. However, as shown in Fig. 5, text-image pairs with\nhigh cosine similarity can still be false positives. To reduce\nthem, we propose to mine the positives based on image-text,\nimage-image, and text-text similarities, aiming to decrease\nthe number of false negatives in the training data arising due\nto semantically similar images and/or captions.\n14173\n(a)\n (b) (c)Figure 2. Semantic and lexical diversity of raw and synthetic pseudo-captions of CC3M: (a): Average cosine similarities of each\ncaption and its 100 most similar captions using CLIP ViT-L/14 features. (b): Cosine similarities between features of each image and its\nground-truth caption. (c): The frequencies of the top-100 most frequent raw and synthetic pseudo-captions (generated using BLIP2). We\nobserve that the raw captions are semantically similar to each other (a), often not well aligned with their associated ground-truth images\n(b), and contain a high number of basic and redundant captions (c). By swapping them with pseudo-captions, we observe an improved\ndiversity (a,c) and better image-text alignment (b).\n(b)\n(a)\n(c)\nFigure 3. Quality assessment of synthetic captions of CC3M: (a) Average intra-cosine similarities between 5 synthetic captions of\neach image. (b) Cosine similarities between the features of each image and either the features of a single synthetic pseudo-caption or\nthe averaged features of 5 pseudo-captions. In (a) and (b), we observe that using multiple synthetic positives that are diverse (a), possible\nerroneous captions can be corrected using an ensemble of pseudo-captions that better converge to the ground truth, resulting in text features\nmore aligned with their associated images (b). (c): The rankings of the ground-truth captions for each image in a batch of 1 kimage-caption\npairs. This shows that, even with relatively small batches, many negatives are well aligned with some images, and it is very likely that many\nof these negatives are potentially correct matches for a subset of images, i.e. false negatives. Features are computed using CLIP ViT-L/14.\nR a w  C a p t i o n : R a w  C a p t i o n :\nc o n s u m e r  p r o d u c t  o n  t h e  \nb a l c o n yf i s h  -  4 0 c m  s q u a r e  a c r y l i c  p a i n t i n g  \no f  a  c u t t e r  .\nR a w  C a p t i o n : R a w  C a p t i o n :\nS y n t h e t i c  C a p t i o n :S y n t h e t i c  C a p t i o n :S y n t h e t i c  C a p t i o n :S y n t h e t i c  C a p t i o n :\nw a l t e r  h a n k s  a r t  f i s h  s e a g u l l  -  1 9 3 9b e a c h f r o n t  c o n d o  -  c o n d o  -  r e n t a l  -  \nc a b o  d e l  c a b a l l o  -  -  3  k i n g\nt h e  c o v e r  o f  t h e  s o n gd r e a m y  m o v i e  ,  a n d  a c t o r  a s  t h e  l e a d i n g  \nm a n  .\nm a u r i c e  n s o ,  1 9 3 3 ,  j u g  w i t h  g o u r d s  \ni n  t h e  f o r e s t ,  h i c k o r y  h a n d l ev a s e  w i t h  r e d  t o p  m u s h r o o m s  i n  a  \nf o r e s t\nFigure 4. Qualitative samples of synthetic captions from\nCC3M: We show 4 examples featuring original raw and synthetic\n(BLIP2) pseudo-captions. These examples highlight typical limi-\ntations and challenges observed in synthetic captions which, while\nsuperior to raw captions, can still be considered noisy.\n3. Related work\nContrastive pretraining under noisy web-collected data:\nCurrent publicly available vision-language datasets are\nmined from the internet automatically [4, 40, 41, 45] with\nonly basic automatic filtering applied, which results in im-perfect annotations and duplicate or near-duplicate pairs.\nA series of papers [1, 11, 15, 48] attempt to alleviate the\nnoise present in annotations by switching from hard to soft\nlabels, akin to knowledge distillation (KD), using various\ncombinations of contrastive loss (i.e. InfoNCE) and KL di-\nvergence. The work in [48] constructs the soft labels using\nan online entropic optimal transport algorithm implemented\nvia the Sinkhorn-Knopp algorithm. The probabilities for\neach image add up to 1, with 0.5 on the diagonal and the\nrest distributed. This assumes that, within the batch, there\nare always some images that are somewhat similar. In our\ncase, we use hard labels, with multiple positives, perform-\ning reassignments only when the samples are sufficiently\nclose, instead of forcing a distribution in all cases. Fur-\nthermore, we do not require running an optimal transport\nmethod, nor rely on a contrastive loss. The work of [1] pro-\ngressively self-distills soft image-text alignments to more\nefficiently learned robust representations from noisy data.\nAt every iteration, a subset of labels are “soft” while the\nrest are kept hard. Similarly, the work of [15] relaxes the\n14174\nR a w :  v i e w  t h i s  i m a g e  o f  a u t o m o b i l e  m a k e\nS y n t h e t i c :  a  b l a c k  s c i o n  s t i  p a r k e d  i n  t h e  s u n  w i t h  c l o u d y  s k yR a w :  r a c e c a r  d r i v e r  s t e e r s  h i s  c a r  d u r i n g  v i d e o  g a m e  s u b j e c t  .\nS y n t h e t i c :  t h e  r e d  b u l l  r a c i n g  c a r  i s  d r i v i n g  d o w n  a  r a c e  t r a c kG r o u n d - t r u t h s :\nR a w :  l o o k i n g  e a s t  a l o n g  a  c i t y  f r o m  1 9 3 0 s\nS y n t h e t i c :  a n  o l d  p h o t o  o f  a  c i t y  s t r e e t\nR a w :  w a i t i n g  o n  a  c o w b o y  -  l i v e  m u s i c\nS y n t h e t i c :  t w o  p e o p l e  a r e  s i n g i n g  a n d  p l a y i n g  a n  a c o u s t i c  g u i t a r  i n  a n  e m p t y  w a r e h o u s e\nR a w :  l o o k i n g  w e s t  t h a t  ' s  t h e  o l d  b u i l d i n g  o n  t h e  f a r  r i g h t  .\nS y n t h e t i c :  a n  o l d  p h o t o  o f  t h e  s t r e e t  i n  t h e  c i t yG r o u n d - t r u t h s :\nG r o u n d - t r u t h s :R a w :  r a c e c a r  d r i v e r  o f  p e r s o n  s t e e r s  h i s  c a r  d u r i n g  e v e n t\nS y n t h e t i c :  a  f o r m u l a  r a c e r  i n  m o t i o n  d u r i n g  e v e n i n g  s e s s i o nR a w :  s e a t  a c h i e v e s  t h e  r e c o r d  o f  c a r s\nS y n t h e t i c :  a n  o l d e r  b l u e  v a n  i n  d o z e n s  o f  o t h e r  c a r sC a p t i o n s  w i t h  h i g h e r  r a n k s  t h a n  t h e  g r o u n d - t r u t h s :\nC a p t i o n s  w i t h  h i g h e r  r a n k s  t h a n  t h e  g r o u n d - t r u t h s :\nR a w :  c o u n t r y  a r t i s t  a n d  p o p  a r t i s t  p e r f o r m  o n  s t a g e  d u r i n g  e v e n t\nS y n t h e t i c :  t w o  p e o p l e  i n  m u s i c a l  c o s t u m e s  p e r f o r m  a  s o n g  i n  a  s t a g e\nR a w :  y o u n g  b o y  a n d  g i r l  s i n g i n g  a  s o n g  i n  a  c h o i r\nS y n t h e t i c :  t w o  p e o p l e  s i n g i n g  w i t h  a n  i n s t r u m e n t  -  s t o c k  v e c t o rC a p t i o n s  w i t h  h i g h e r  r a n k s  t h a n  t h e  g r o u n d - t r u t h s :Figure 5. Examples of high-ranking captions from CC3M: We\nshow 3 examples of raw and synthetic captions ranked higher than\nthe ground-truths from a batch of 1 kimage-caption pairs. In green,\nwe show potential false negatives that can be used as new positives\nfor improved training. However, possible false positives, as shown\nin red, can still occur. These can be handled by the robust sigmoid\nloss. Rankings are obtained using CLIP ViT-L/14.\nstrict one-to-one constraint, transitioning to a soft cross-\nmodal alignment by introducing a softened target, which is\ngenerated from the fine-grained intra-modal self-similarity.\nAdditionally, they disentangle the negatives in the distribu-\ntion to further boost the relation alignment, resulting in a\ncombination of InfoNCE loss performed with hard labels\nand KL divergence. However, they do not perform batched\ntext augmentations with multiple positives, as in our work,\nand still use a contrastive loss combined with KL, operat-\ning on soft scores. The works of [6, 19, 20] study the ef-\nfect of removing false negatives in the context of unimodal\ni.e. pure vision models, not considering the case of multi-\nmodal learning. The work of [20] flags (a very small num-\nber) of potential negatives using the aggregated score ob-\ntained from multiple support views per image, [6] uses a\nclustering based approach while [19] is based on ranked\npositives, requiring a known class hierarchy (i.e. a fully\nsupervised case) or known changes/relations (i.e. videos).\nThe works of [6, 20] derive from the Supervised Contrastive\nLoss, while [19] from InfoNCE. In contrast, our work oper-\nates on image-text data, takes into account multi-modal in-\nteractions (I2T, T2T, T2I), does not use additional support\nviews, known hierarchies etc. and is easily scalable.\nFollowing a different direction, BLIP [27] and their fol-\nlowup [28] version, use a bootstrapping approach in which\nthe noisy captions are filtered out using the initial model,\nwhich is then retrained on the new data. This interplay is\nperformed offline and requires training a multitask model.\nThe work of [39] presents a small-scale study showing\nthat random sampling of pseudo-captions improves CLIP,\nconcluding however that scaling up the number of image-caption pairs appears to be more effective. Finally, very\nrecently, ALIP [50] adds a synthetic pseudo-caption and a\nconsistency gating mechanism that weights the influence of\nthe samples and image-text pairs on the contrastive loss.\nDifferent from the aforementioned methods, we propose\nto fix incorrectly assigned negatives and mine for new true\npositives using text-image, image-image, and text-text simi-\nlarities. Moreover, to increase caption quality and diversity,\nwe further propose training with multiple pseudo-captions\nper image within the same batch. As our methods require\ntraining with multiple positives per image, we further pro-\npose to use the sigmoid loss [54] for training the model.\n4. Method\nThis section describes the proposed method, whose aim is to\nimprove vision-language training by denoising and improv-\ning the quality of the training process/data. Specifically,\nSec. 4.1 addresses the problem of false negative pairs in-\nherent to the noisy nature of large-scale image-text datasets\nby re-assigning them as true positives1. Sec. 4.2 proposes\ntext batch augmentation for training the model with multi-\nplepositives pairs. The effect of Secs. 4.1 and 4.2 is that, for\neach training image, a variable number of positive-negative\npairs is formed (Sec. 4.3). Sec. 4.4 proposes a natural way\nto train the model in this case by using the recently proposed\nsigmoid loss for vision-language pre-training.\n4.1. Fixing incorrect negatives\nLetDbe a dataset consisting of image-text pairs, with\nBa batch of randomly selected samples (xi, ti), i =\n1,2, . . . , N . In addition to the ground truth positives\npairs (xi, ti), we seek to identify and correct wrongly co-\noccurring negative pairs (xi, tj)on-the-fly . To achieve this,\nlet us first define the image-text, image-image, and text-text\ncosine similarity matrices Sit=Xf·TT\nf, Sii=Xf·XT\nf\nandStt=Tf·TT\nf, where Sit, Sii, Stt∈RN×Nand\nXf∈RN×dandTf∈RN×drepresent the image and text\nfeatures, respectively.\nGiven the similarity score matrices, we define the assign-\nment matrix M∈ {0,1}N×Nas follows:\nM= (Sit> p 1)∨(Sii> p 2)∨[(Stt> p 3)∧(Sit> p′\n1)],(1)\nwhere ∨is the logical orand∧the logical and operator,\nandp1, p′\n1, p2andp3are the thresholds above which a sam-\nple is marked as positive, with p′\n1< p 1. Note that we filter\nthe positives found with text-text matching using image-text\nsimilarities (using threshold p′\n1), as we observed a high por-\ntion of false positives within text-text matching, due to the\nfact that repeated samples often correlate with poor overall\n1It is possible that such cases can occur in clean datasets too, as multiple\ncaptions can describe an image and vice versa, multiple images can be\ndescribed by one caption.\n14175\nImages\nCaptions\nOriginal maskContrastive loss(a)Baseline [38] : Ground truth construction\nand training loss. Prior work does not take into\naccount that some pairs may be incorrect nega-\ntives and is limited to one positive per sample.\nImages\nCaptions\nOriginal maskSigmoid loss\nCorrect pairs by measuring\nT2I, T2T and I2I similarities\nCorrected mask(b)Fixing incorrect negatives: Our approach analyzes on the fly the image-text, image-image, and\ntext-text similarities, correcting wrong negative pairs. The model is trained using the sigmoid loss\n(see Sec. 4.4) instead of the standard contrastive loss which is unsuitable for an arbitrary number of\npositive samples.\nImages\nOriginal maskSigmoid loss\nCorrect pairs by measuring\nT2I, T2T and I2I similarities\nCorrected expanded maskImage\ncaptioner\n( )Automatically generate multiple\nsynthetic captions per image\nExpanded mask with synthetic captions\n(c)Overall approach combining fixing incorrect negatives (Sec. 4.1) with batch text argumentation (Sec. 4.2). The synthetic pseudo-captions are generated\noffline and packed as part of the dataset.\nFigure 6. Overview of our approach: Fixing incorrect negatives is shown in (b). In (c) we describe our combined approach (including\nbatch text augmentation) contrasted with the baseline of (a). Green squares denote positive pairs, while pink are negatives. Green squares\nwith a dashed border denote identified false negatives that are corrected to true positives.\nimage description fidelity. The choice of p1, p′\n1, p2andp3\nis empirical and generally depends on the characteristics of\nthe model. We ablate the dependency of the method on the\nthreshold values in Sec. 6 where we show little sensitivity.\nNote that Mre-assigns a variable number of positives to\neach image. Fig. 6b depicts the construction process of M\nat a high level.\nIn order to calculate the cosine similarity matrices\nSit, SiiandSttrequired for the construction of M, we\nuse a pre-trained model. This is akin to a form of auto-\nlabeling/auto-filtering, where the pretrained model provides\na signal for re-assessing the labeling of the samples. Al-\nthough one could opt to use an EMA teacher-student ap-\nproach, we found this simple approach to work sufficiently\nwell. Moreover, some possible errors in Mcan be handled\nby the robust sigmoid loss used for training (see Eq. 2).\n4.2. Batch text augmentation with multiple positives\nThe currently available image-text datasets [4, 41, 42] are\nnoisy, with high variability in the quality of the text de-\nscriptions among samples. To improve data quality, we use\nBLIP2 [28], an off-the-shelf image captioner, to generate\nmultiple pseudo-captions for each image in the training set\n(see supplementary material for visual examples). Inspired\nby [18], we propose to include all pseudo-captions as true\npositives within the same batch , which we call batch textaugmentation. Note that simultaneously training with mul-\ntiple pseudo-captions within the same batch has not been\nconsidered in previous work. We show that this approach\nenables the training of highly accurate models (see Sec. 5\nand our ablation in Sec. 6). In the next section, we also\nshow how batch text augmentation can be integrated with\nthe mask construction process defined in Sec. 4.1. Finally,\nwe note that while batch text augmentation improves the\noverall performance, it does not address the presence of se-\nmantic near duplicates (i.e. false negatives) within the same\ntraining batch.\n4.3. Combined approach\nOur approach for fixing incorrect negative pairs (Sec. 4.1)\nand batch text augmentation (Sec. 4.2) can be naturally\ncombined in order to define the total number of true pos-\nitives per image.\nTo this end, and without loss of generality, we assume k\ncaptions per image (original caption plus pseudo-captions),\nhence the total number of captions and images are related by\nNtxt=kNimg. Given this, the image-text similarity matrix\nhas now (by construction) size Sit∈RNimg×Ntxt. Hence,\nthe computation of SiiandSttneeds to be adjusted to re-\nflect this change. For the image-image case, and as Nimg<\nNtxt, to make the image-image similarity matrix Siihave\nthe same dimensions as Sit, i.e. Sii∈RNimg×Ntxt, we\n14176\nreplicate the scores ktimes. In other words, a given image\nxiwill share the score with each group of captions belong-\ning to image xj,∀i, j∈Nimg. For the text-text case, the\nsimilarity matrix is now of size Stt∈RNtxt×Ntxt. Anal-\nogously, to make the Stthave the same dimensions as Sit,\nwe take the average score between each caption of image xi\nand all kcaptions of image xj.\nOverall, we end up with similarity matrices of the same\ndimensions Sit, Sii, Stt∈RNimg×Ntextand hence the as-\nsignment matrix Mcan be again constructed by applying\nEq. 1. The overall process is depicted in Fig. 6c.\n4.4. Loss function\nThe symmetrical contrastive loss (i.e. text→image andim-\nage→text) used in CLIP [38] supports only one positive\npair per sample (see Fig. 6a), being in discordance with the\nrequirement of training with a variable number of positive\npairs per image set by the proposed methods in Secs. 4.1\nand 4.2. A solution to this problem could be given by the\nSupervised Contrastive Loss [22], originally introduced to\nenable multi-view training of supervised image recognition.\nHowever, this loss is prone to noise [2], with the harder pos-\nitive pairs dominating the signal and hindering, in part, the\neffect from the rest of the positive samples. This is espe-\ncially problematic in the context of web-collected datasets,\nwhich are notoriously noisy. Finally, it is memory intensive\nand computationally demanding. In practice, we observe a\n1.9×slowdown for a batch size of 8,096 samples.\nA natural alternative is the BCE loss, shown to outper-\nform cross-entropy for image classification [47], and also\nshown to be a viable alternative for image-text representa-\ntion learning [54]. Such formulation is particularly advanta-\ngeous for the proposed approach, as the BCE loss natively\nsupports an arbitrary number of positives per sample per\nbatch, with the ground truth being provided simply as a bi-\nnary mask. Moreover, the loss is more robust to noise in\ngeneral, and hence to false negatives and positives [54]. Fi-\nnally, the initial negative bias prevents the model from being\nforced to learn incorrect assignments early one. Hence, we\npropose to use the following loss:\nℓmp=−1\nNtxtNimgX\ni=1NtxtX\nj=1log1\n1 +exp(mij(−sij/τ+β)),\n(2)\nwhere mijis the i, jelement of M(−1for negative and 1\nfor positive pairs), and respectively, sijthei, jelement of\nthe similarity matrix Sit.\nAs the negative pairs considerably outnumber the pos-\nitive ones, to ensure that we start from a low initial loss\n(making the same observation as in [54]), we add a learn-\nable scalar β, set initially to a negative value. However, as\nthe number of positive pairs is dynamic and is typically tied\nto both the specifics of the dataset and the threshold used todefine a positive sample, different from [54], we propose to\nestimate βat the beginning of the training process. Specif-\nically, given the randomly initialized model, we sample b\nbatches out of the training set, and then compute and store\nthe cosine similarities. Then, given the scores and the cor-\nresponding labels, we search for βsuch that the initial loss\nis minimized (everything else is kept frozen). The value of\nβcan be found either by gradient descent or alternatively,\nby performing a grid search.\n5. Results\nPretraining Datasets: To allow for fair comparisons with\nprior work, we pre-train our approach on YFCC15M-\nv2 [26], a subset of YFCC100M [45] containing ap-\nproximately 15M image-text pairs. To cover different\ndataset sizes, we also conduct experiments on CC3M [42]\nand CC12M [4], and in the supplementary material, on\nOpen30M and Open70M datasets, further showcasing our\nmethod’s scalability with respect to the dataset size.\nImplementation details: Architecturally, we use the same\nmodel topology and setting as in CLIP [38], specifically,\nusing AdamW [31], learning rate of 1e−3and weight de-\ncay of 0.1, except for CC3M where we set the weight de-\ncay to 0.5, as in prior work [29]. In terms of augmenta-\ntions, we follow [29], randomly resizing and cropping the\nimage to 224×224px, applying random flipping, random\nGaussian blur (between 0.1 and 2.0) and color jittering (0.4,\n0.4, 0.4, 0.1). For text, the data is truncated to 77 tokens.\nNote, that the branch used to construct the assignment ma-\ntrixMuses no augmentations (i.e. resize to 256×256px,\nfollowed by center crop, resulting in a 224×224px im-\nage). The thresholds were set to p1= 0.27,p2= 0.92,\np3= 0.99,p′\n1= 0.24. Unless otherwise specified, the\nmodels are trained for 32 epochs with a batch size of 8,096\non 8 NVIDIA A100 GPUs. All of our models and training\ncode are implemented using PyTorch [37].\n5.1. Comparison with state-of-the-art\nFollowing recent work on vision-language pretraining [16,\n33, 50], we compare our method with state-of-the-art ap-\nproaches for zero-shot classification and zero-shot retrieval.\nSee supplementary material for linear probe evaluation.\nZero-shot classification: For zero-shot classification eval-\nuation, for the main setting, we select the common sub-\nset of datasets that facilitate a direct comparison with prior\nstate-of-the-art. In particular, we evaluate our approach on\nthe following datasets: CIFAR-10 [24], CIFAR-100 [24],\nFood101 [3], Pets [36], Flowers [35], SUN397 [49],\nStanford Cars [23], DTD [7], Caltech101 [12], FGVC-\nAircraft [32] and ImageNet [9]. The evaluation is per-\nformed using the same prompt templates and class names\nas in prior work [16, 33, 50].\n14177\nMethodPre-train\ndataset\nCIFAR10\nCIFAR100\nFood101\nPets\nFlowers\nSUN397\nCars\nDTD\nCaltech101\nAircraft\nImageNet\nAverage\nCLIP-ViT-B/32[38] YFCC15M 63.7 33.2 34.6 20.1 50.1 35.7 2.6 15.5 59.9 1.2 32.8 31.8\nSLIP-ViT-B/32 [33] YFCC15M 50.7 25.5 33.3 23.5 49.0 34.7 2.8 14.4 59.9 1.7 34.3 30.0\nFILIP-ViT-B/32 [51] YFCC15M 65.5 33.5 43.1 24.1 52.7 50.7 3.3 24.3 68.8 3.2 39.5 37.2\nDeCLIP-ViT-B/32 [29] YFCC15M 66.7 38.7 52.5 33.8 60.8 50.3 3.8 27.7 74.7 2.1 43.2 41.3\nDeFILIP-ViT-B/32 [8] YFCC15M 70.1 46.8 54.5 40.3 63.7 52.4 4.6 30.2 75.0 3.3 45.0 44.2\nHiCLIP-ViT-B/32 [16] YFCC15M 74.1 46.0 51.2 37.8 60.9 50.6 4.5 23.1 67.4 3.6 40.5 41.8\nHiDeCLIP-ViT-B/32 [16] YFCC15M 65.1 39.4 56.3 43.6 64.1 55.4 5.4 34.0 77.0 4.6 45.9 44.6\nALIP-ViT-B/32 [50] YFCC15M 83.8 51.9 45.4 30.7 54.8 47.8 3.4 23.2 74.1 2.7 40.3 41.7\nFFF-ViT-B/32 (Ours) YFCC15M 75.8 56.3 58.6 59.8 62.1 61.5 16.3 33.4 79.6 4.6 51.1 50.8\nTable 1. Zero-shot classification performance on 11 downstream datasets. All models were pre-trained on YFCC15M.\nText retrieval Image retrieval\nFlickr30k MSCOCO Flickr30k MSCOCO\nMethod R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10\nCLIP-ViT-B/32[38] 34.9 63.9 75.9 20.8 43.9 55.7 23.4 47.2 58.9 13.0 31.7 42.7\nSLIP-ViT-B/32 [33] 47.8 76.5 85.9 27.7 52.6 63.9 32.3 58.7 68.8 18.2 39.2 51.0\nDeCLIP-ViT-B/32 [29] 51.4 80.2 88.9 28.3 53.2 64.5 34.3 60.3 70.7 18.4 39.6 51.4\nUniCLIP-ViT-B/32 [25] 52.3 81.6 89.0 32.0 57.7 69.2 34.8 62.0 72.0 20.2 43.2 54.4\nHiCLIP-ViT-B/32 [16] - - - 34.2 60.3 70.9 - - - 20.6 43.8 55.3\nHiDeCLIP-ViT-B/32 [16] - - - 38.7 64.4 74.8 - - - 23.9 48.2 60.1\nALIP-ViT-B/32 [50] 70.5 91.9 95.7 46.8 72.4 81.8 48.9 75.1 82.9 29.3 54.4 65.4\nFFF-ViT-B/32 (Ours) 85.3 97.5 99.4 61.7 84.5 90.4 67.6 89.1 93.3 44.3 70.9 80.1\nTable 2. Zero-shot image-text retrieval on the test splits of Flickr30k and MSCOCO. All models were pre-trained on YFCC15M.\nAs the results from Tab. 1 show, our approach outper-\nforms all prior methods, improving by 6.2%in absolute\nterms on top of the previous best result of HiDeCLIP [16]\n(which benefits from a better architecture) when aggregated\nacross 11 datasets. Notably, we set a new state-of-the-art\nresult on ImageNet, too ( 51.1%). Finally, we significantly\nimprove upon ALIP [50], which also makes use of synthetic\ncaptions, outperforming it by 9.1%.\nMethod CC3M CC12M\nCLIP [38] 20.6 36.5\nProtoCLIP [5] 21.5 -\nCyCLIP [17] 22.1 -\nCLOOB [14] 24.0 -\nSoftCLIP [15] 24.2 43.2\nDeCLIP [29] 27.2 41.0\nCLIP-Rocket [13] 27.4 44.4\nBoW [44] 30.3 -\nFFF (Ours) 33.4 47.4\nTable 3. Zero-shot evaluation on Imagenet in terms of Top-1 (%)\naccuracy for a ResNet-50 model pre-trained on CC3M/CC12M.\nFor completeness, we also adhere to the protocol of pre-\ntraining a ResNet-50 on CC3M, and respectively, CC12Mand then evaluating it for zero-shot classification on Ima-\ngeNet. As the results from Tab. 3 show, the same conclu-\nsions hold. Our method outperforms the previous best result\nby 3.1% on CC3M (30.3% vs 33.4%) and 3.0% on CC12M\n(44.4% vs 47.4%). See supplementary material for results\non Open30M and Open70M.\nZero-shot retrieval: Consistent with prior work, we\nevaluate our approach for zero-shot retrieval on Flickr-\n30k [53] and MS-COCO [30] reporting results in terms\nof R@ {1,5,10}for both text and image retrieval. The re-\nsults are summarized in Tab. 2. As it can be observed,\nour approach offers significant gains across all metrics and\ndatasets used, improving on top of the prior state-of-the-\nart ALIP [50] by 14.8% and 18.7% in terms of R@1 on\nFlickr30k for text, and respectively, image retrieval. Simi-\nlarly, we outperform the previous best result by 14.9% and\n15.0% in terms of R@1 on MSCOCO for text and image\nretrieval. This highlights that our approach results in repre-\nsentations that can capture subtle and fine-grained details.\n6. Ablation studies\nFor our ablation studies, the results reported are produced\nusing a ViT-B/16 model pretrained on CC3M dataset.\n14178\nEffect of fixing incorrect negatives: herein, we analyze\nthe effectiveness of the proposed algorithm of Sec. 4.1. By\nanalyzing the result from Tab. 4, we can observe consis-\ntent gains for all 3 cases of interest: a) when using the\nweb-collected captions (+2.7% gain), b) when using one\npseudo-caption (+3.5% improvement) and c) when all avail-\nable pseudo-captions at once (+1.8%). Overall, compared\nto the baseline accuracy of 18.6%, our approach improves\nby +14.3% (top-1 accuracy of 32.9%). The results show that\nour approach provides gains across all options considered.\nFix incorrect negatives Num. captions Top-1 (%)\n✗ 0 18.6\n✓ 0 21.3\n✗ 1 23.3\n✓ 1 26.8\n✗ 5 31.1\n✓ 5 32.9\nTable 4. Effect of fixing incorrect negatives: Zero-shot evalua-\ntion on ImageNet in terms of Top-1 (%) accuracy.\nEffect of different components in Eq. 1: In Eq. 1, the\nconstructed assignment matrix Mis computed from three\nfeature similarity matrices Sit,SiiandStt. Herein, we eval-\nuate the impact of each of these components. As the results\nfrom Tab. 5 show, viewed independently, the Sitis the most\nimpactful, as it has a dual effect, both in terms of filter-\ning incorrect pairs and of adjusting for semantically similar\nsamples. Moreover, the results hold for both ground truth\ncaptions and pseudo-captions.\nAssign. Matrix M Num. captions Top-1 (%)\nNone 0 18.6\nStt> p 3 0 18.8\nSii> p 2 0 21.3\nSit> p 1 0 21.4\nEq. 1 (all) 0 22.0\nNone 1 23.1\nStt> p 3 1 23.6\nSii> p 2 1 24.6\nSit> p 1 1 26.0\nEq. 1 (all) 1 26.8\nTable 5. Effect of different components in Eq. 1: Zero-shot\nevaluation on ImageNet in terms of Top-1 (%) accuracy.\nEffect of batch text augmentation: Herein, we assess the\nimpact of training with multiple pseudo-captions within the\nsame batch, as described in Sec. 4.2. Tab. 6 shows accuracy\nvs number of pseudo-captions used during training. As we\ncan observe, increasing the number of captions increases\nthe accuracy of the model, inline with the expectations.As an additional baseline, we compare against a model\ntrained by randomly sampling 1 out of 5 captions (as op-\nposed to using them jointly as proposed in our work) on\nCC3M and YFCC-15M. On CC3M the performance drops\nby 1.5%, from 32.9% to 31.4%, while on YFCC-v2 from\n51.1% to 44.1%. This further highlights the importance of\nthe proposed batch text augmentation.\nEffect of image captioner: We also compare the effect\nof using two different state-of-the-art image captioners,\nOFA [46] and BLIP-2 [28]. As the results from Tab. 7 show,\nboth captioners lead to identical performance.\nNum. captions 0 1 3 5\nTop-1 (%) 18.6 23.3 30.2 31.1\nTable 6. Effect of batch text augmentation: Zero-shot evaluation\non ImageNet in terms of Top-1 (%) accuracy.\nImage captioner Top-1 (%)\nOFA [46] 32.9\nBLIP-2 [28] 32.9\nTable 7. Effect of the image captioner: Zero-shot evaluation on\nImageNet in terms of Top-1 (%) accuracy.\nComparison with the supervised contrastive loss: To fur-\nther validate the loss choice, we compare against a model\ntrained with the supervised contrastive loss [22]. For a fair\ncomparison, both models were trained using the same set-\ntings on CC3M. When evaluated for zero-shot classification\non ImageNet, the supervised contrastive model achieved\nonly 19.0% accuracy vs 21.3% achieved by our model.\nNote, that similar results are obtained using a InfoNCE\nbased loss. This result empirically solidifies the arguments\nmade in Sec. 4.4.\n7. Conclusions\nIn this work, we propose a new approach to vision-language\npretraining based on multi-positive sample pairing that fixes\nincorrect negatives and addresses low caption quality. The\nlatter is tackled by a newly introduced batch text augmen-\ntation strategy, in which multiple new positive pairs are\nconcomitantly added via synthetic recaptioning. Departing\nfrom the typical contrastive loss, to enable efficient training\nunder an arbitrary number of positives per sample, we pro-\npose to train the model with a sigmoid loss. In the process,\nwe highlight the crucial role of noise and caption quality\nin vision-language pre-training, offering an in-depth analy-\nsis. All in all, we show large improvements over the cur-\nrent state-of-the-art method for both zero-shot image recog-\nnition ( ∼+6% on average of 11 datasets) and retrieval\n(∼+19% on Flickr30k and ∼+15% on MSCOCO).\n14179\nReferences\n[1] Alex Andonian, Shixing Chen, and Raffay Hamid. Robust\ncross-modal representation learning with progressive self-\ndistillation. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , pages 16430–\n16441, 2022. 3\n[2] Lucas Beyer, Olivier J H ´enaff, Alexander Kolesnikov, Xi-\naohua Zhai, and A ¨aron van den Oord. Are we done with\nimagenet? arXiv preprint arXiv:2006.07159 , 2020. 6\n[3] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool.\nFood-101–mining discriminative components with random\nforests. In Computer Vision–ECCV 2014: 13th European\nConference, Zurich, Switzerland, September 6-12, 2014,\nProceedings, Part VI 13 , pages 446–461. Springer, 2014. 6\n[4] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu\nSoricut. Conceptual 12M: Pushing web-scale image-text\npre-training to recognize long-tail visual concepts. In CVPR ,\n2021. 1, 3, 5, 6\n[5] Delong Chen, Zhao Wu, Fan Liu, Zaiquan Yang, Yixi-\nang Huang, Yiping Bao, and Erjin Zhou. Prototypical\ncontrastive language image pretraining. arXiv preprint\narXiv:2206.10996 , 2022. 7\n[6] Tsai-Shien Chen, Wei-Chih Hung, Hung-Yu Tseng, Shao-\nYi Chien, and Ming-Hsuan Yang. Incremental false neg-\native detection for contrastive learning. arXiv preprint\narXiv:2106.03719 , 2021. 4\n[7] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy\nMohamed, and Andrea Vedaldi. Describing textures in the\nwild. In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pages 3606–3613, 2014. 6\n[8] Yufeng Cui, Lichen Zhao, Feng Liang, Yangguang Li, and\nJing Shao. Democratizing contrastive language-image pre-\ntraining: A clip benchmark of data, model, and supervision.\narXiv preprint arXiv:2203.05796 , 2022. 7\n[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE conference on computer vision and\npattern recognition , pages 248–255. Ieee, 2009. 6\n[10] Karan Desai, Gaurav Kaul, Zubin Aysola, and Justin John-\nson. Redcaps: Web-curated image-text data created by the\npeople, for the people. arXiv preprint arXiv:2111.11431 ,\n2021. 1\n[11] Lijie Fan, Dilip Krishnan, Phillip Isola, Dina Katabi, and\nYonglong Tian. Improving clip training with language\nrewrites. arXiv preprint arXiv:2305.20088 , 2023. 3\n[12] Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning gener-\native visual models from few training examples: An incre-\nmental bayesian approach tested on 101 object categories. In\n2004 conference on computer vision and pattern recognition\nworkshop , pages 178–178. IEEE, 2004. 6\n[13] Enrico Fini, Pietro Astolfi, Adriana Romero-Soriano,\nJakob Verbeek, and Michal Drozdzal. Improved base-\nlines for vision-language pre-training. arXiv preprint\narXiv:2305.08675 , 2023. 7\n[14] Andreas F ¨urst, Elisabeth Rumetshofer, Johannes Lehner,\nViet T Tran, Fei Tang, Hubert Ramsauer, David Kreil,\nMichael Kopp, G ¨unter Klambauer, Angela Bitto, et al.Cloob: Modern hopfield networks with infoloob outperform\nclip. Advances in neural information processing systems , 35:\n20450–20468, 2022. 1, 7\n[15] Yuting Gao, Jinfeng Liu, Zihan Xu, Tong Wu, Wei Liu,\nJie Yang, Ke Li, and Xing Sun. Softclip: Softer cross-\nmodal alignment makes clip stronger. arXiv preprint\narXiv:2303.17561 , 2023. 3, 7\n[16] Shijie Geng, Jianbo Yuan, Yu Tian, Yuxiao Chen, and\nYongfeng Zhang. Hiclip: Contrastive language-image pre-\ntraining with hierarchy-aware attention. arXiv preprint\narXiv:2303.02995 , 2023. 6, 7\n[17] Shashank Goel, Hritik Bansal, Sumit Bhatia, Ryan Rossi,\nVishwa Vinay, and Aditya Grover. Cyclip: Cyclic contrastive\nlanguage-image pretraining. Advances in Neural Informa-\ntion Processing Systems , 35:6704–6719, 2022. 7\n[18] Elad Hoffer, Tal Ben-Nun, Itay Hubara, Niv Giladi, Torsten\nHoefler, and Daniel Soudry. Augment your batch: Improving\ngeneralization through instance repetition. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 8129–8138, 2020. 5\n[19] David T Hoffmann, Nadine Behrmann, Juergen Gall,\nThomas Brox, and Mehdi Noroozi. Ranking info noise con-\ntrastive estimation: Boosting contrastive learning via ranked\npositives. In Proceedings of the AAAI Conference on Artifi-\ncial Intelligence , pages 897–905, 2022. 4\n[20] Tri Huynh, Simon Kornblith, Matthew R Walter, Michael\nMaire, and Maryam Khademi. Boosting contrastive self-\nsupervised learning with false negative cancellation. In Pro-\nceedings of the IEEE/CVF winter conference on applications\nof computer vision , pages 2785–2795, 2022. 4\n[21] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,\nHieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom\nDuerig. Scaling up visual and vision-language representa-\ntion learning with noisy text supervision. In International\nconference on machine learning , pages 4904–4916. PMLR,\n2021. 1\n[22] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna,\nYonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and\nDilip Krishnan. Supervised contrastive learning. Advances\nin neural information processing systems , 33:18661–18673,\n2020. 2, 6, 8\n[23] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei.\n3d object representations for fine-grained categorization. In\nProceedings of the IEEE international conference on com-\nputer vision workshops , pages 554–561, 2013. 6\n[24] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple\nlayers of features from tiny images. 2009. 6\n[25] Janghyeon Lee, Jongsuk Kim, Hyounguk Shon, Bumsoo\nKim, Seung Hwan Kim, Honglak Lee, and Junmo Kim. Uni-\nclip: Unified framework for contrastive language-image pre-\ntraining. Advances in Neural Information Processing Sys-\ntems, 35:1008–1019, 2022. 7\n[26] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare,\nShafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi.\nAlign before fuse: Vision and language representation learn-\ning with momentum distillation. Advances in neural infor-\nmation processing systems , 34:9694–9705, 2021. 6\n14180\n[27] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.\nBlip: Bootstrapping language-image pre-training for uni-\nfied vision-language understanding and generation. In In-\nternational Conference on Machine Learning , pages 12888–\n12900. PMLR, 2022. 1, 4\n[28] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2: Bootstrapping language-image pre-training with\nfrozen image encoders and large language models. arXiv\npreprint arXiv:2301.12597 , 2023. 2, 4, 5, 8\n[29] Yangguang Li, Feng Liang, Lichen Zhao, Yufeng Cui, Wanli\nOuyang, Jing Shao, Fengwei Yu, and Junjie Yan. Su-\npervision exists everywhere: A data efficient contrastive\nlanguage-image pre-training paradigm. arXiv preprint\narXiv:2110.05208 , 2021. 1, 6, 7\n[30] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll ´ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nComputer Vision–ECCV 2014: 13th European Conference,\nZurich, Switzerland, September 6-12, 2014, Proceedings,\nPart V 13 , pages 740–755. Springer, 2014. 1, 7\n[31] Ilya Loshchilov and Frank Hutter. Fixing weight decay reg-\nularization in adam. 2018. 6\n[32] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew\nBlaschko, and Andrea Vedaldi. Fine-grained visual classi-\nfication of aircraft. arXiv preprint arXiv:1306.5151 , 2013.\n6\n[33] Norman Mu, Alexander Kirillov, David Wagner, and Sain-\ning Xie. Slip: Self-supervision meets language-image pre-\ntraining. In European Conference on Computer Vision , pages\n529–544. Springer, 2022. 6, 7\n[34] Thao Nguyen, Gabriel Ilharco, Mitchell Wortsman, Se-\nwoong Oh, and Ludwig Schmidt. Quality not quantity:\nOn the interaction between dataset design and robustness of\nclip. Advances in Neural Information Processing Systems ,\n35:21455–21469, 2022. 1\n[35] Maria-Elena Nilsback and Andrew Zisserman. Automated\nflower classification over a large number of classes. In 2008\nSixth Indian conference on computer vision, graphics & im-\nage processing , pages 722–729. IEEE, 2008. 6\n[36] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and\nCV Jawahar. Cats and dogs. In 2012 IEEE conference on\ncomputer vision and pattern recognition , pages 3498–3505.\nIEEE, 2012. 6\n[37] Adam Paszke, Sam Gross, Soumith Chintala, Gregory\nChanan, Edward Yang, Zachary DeVito, Zeming Lin, Al-\nban Desmaison, Luca Antiga, and Adam Lerer. Automatic\ndifferentiation in pytorch. 2017. 6\n[38] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning , pages\n8748–8763. PMLR, 2021. 1, 2, 5, 6, 7\n[39] Shibani Santurkar, Yann Dubois, Rohan Taori, Percy Liang,\nand Tatsunori Hashimoto. Is a caption worth a thousand im-\nages? a study on representation learning. In The Eleventh In-\nternational Conference on Learning Representations , 2022.\n4[40] Christoph Schuhmann, Richard Vencu, Romain Beaumont,\nRobert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo\nCoombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m:\nOpen dataset of clip-filtered 400 million image-text pairs.\narXiv preprint arXiv:2111.02114 , 2021. 1, 3\n[41] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon, Ross Wightman, Mehdi Cherti, Theo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, et al. Laion-5b: An open large-scale dataset for training\nnext generation image-text models. Advances in Neural In-\nformation Processing Systems , 35:25278–25294, 2022. 3,\n5\n[42] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu\nSoricut. Conceptual captions: A cleaned, hypernymed, im-\nage alt-text dataset for automatic image captioning. In Pro-\nceedings of the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers) , pages\n2556–2565, 2018. 5, 6\n[43] Krishna Srinivasan, Karthik Raman, Jiecao Chen, Michael\nBendersky, and Marc Najork. Wit: Wikipedia-based image\ntext dataset for multimodal multilingual machine learning.\nInProceedings of the 44th International ACM SIGIR Confer-\nence on Research and Development in Information Retrieval ,\npages 2443–2449, 2021. 1\n[44] Ajinkya Tejankar, Maziar Sanjabi, Bichen Wu, Saining Xie,\nMadian Khabsa, Hamed Pirsiavash, and Hamed Firooz. A\nfistful of words: Learning transferable visual models from\nbag-of-words supervision. arXiv preprint arXiv:2112.13884 ,\n2021. 7\n[45] Bart Thomee, David A Shamma, Gerald Friedland, Ben-\njamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and\nLi-Jia Li. Yfcc100m: The new data in multimedia research.\nCommunications of the ACM , 59(2):64–73, 2016. 1, 3, 6\n[46] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai,\nZhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and\nHongxia Yang. Ofa: Unifying architectures, tasks, and\nmodalities through a simple sequence-to-sequence learning\nframework. In International Conference on Machine Learn-\ning, pages 23318–23340. PMLR, 2022. 2, 8\n[47] Ross Wightman, Hugo Touvron, and Herv ´e J´egou. Resnet\nstrikes back: An improved training procedure in timm. arXiv\npreprint arXiv:2110.00476 , 2021. 6\n[48] Bichen Wu, Ruizhe Cheng, Peizhao Zhang, Peter Vajda, and\nJoseph E Gonzalez. Data efficient language-supervised zero-\nshot recognition with optimal transport distillation. arXiv\npreprint arXiv:2112.09445 , 2021. 3\n[49] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva,\nand Antonio Torralba. Sun database: Large-scale scene\nrecognition from abbey to zoo. In 2010 IEEE computer so-\nciety conference on computer vision and pattern recognition ,\npages 3485–3492. IEEE, 2010. 6\n[50] Kaicheng Yang, Jiankang Deng, Xiang An, Jiawei Li, Ziy-\nong Feng, Jia Guo, Jing Yang, and Tongliang Liu. Alip:\nAdaptive language-image pre-training with synthetic cap-\ntion. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision , pages 2922–2931, 2023. 1, 4,\n6, 7\n14181\n[51] Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe\nNiu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, and\nChunjing Xu. Filip: Fine-grained interactive language-image\npre-training. arXiv preprint arXiv:2111.07783 , 2021. 1, 7\n[52] Haoxuan You, Luowei Zhou, Bin Xiao, Noel Codella, Yu\nCheng, Ruochen Xu, Shih-Fu Chang, and Lu Yuan. Learn-\ning visual representation from modality-shared contrastive\nlanguage-image pre-training. In European Conference on\nComputer Vision , pages 69–87. Springer, 2022. 1\n[53] Peter Young, Alice Lai, Micah Hodosh, and Julia Hocken-\nmaier. From image descriptions to visual denotations: New\nsimilarity metrics for semantic inference over event descrip-\ntions. Transactions of the Association for Computational\nLinguistics , 2:67–78, 2014. 1, 7\n[54] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and\nLucas Beyer. Sigmoid loss for language image pre-training.\narXiv preprint arXiv:2303.15343 , 2023. 2, 4, 6\n14182"}, 'dist': 0.9286905527114868}
Result 17: {'text': 'data', 'metadata': {'created_at': 1736932570, 'modified_at': 1736932570, 'owner': 'sayandeep', 'path': 'RAG_CONTENT/conferences/cvpr_ocr/Li_MLIP_Enhancing_Medical_Visual_Representation_with_Divergence_Encoder_and_Knowledge-guided_CVPR_2024_paper.txt', 'size': 50568, 'seen_at': 1737191136, 'data': 'MLIP: Enhancing Medical Visual Representation with Divergence Encoder and\nKnowledge-guided Contrastive Learning\nZhe Li1, Laurence T. Yang1,2,∗, Bocheng Ren1, Xin Nie1, Zhangyang Gao3, Cheng Tan3, Stan Z. Li3\n1Huazhong University of Science and Technology\n2Zhengzhou University\n3AI Lab, Research Center for Industries of the Future, Westlake University\n*keycharon0122@gmail.com, ltyang@ieee.org, bc.Revincent@gmail.com, niexin@hust.edu.cn,\n{gaozhangyang,tancheng,stan.zq.li }@westlake.edu.cn\nAbstract\nThe scarcity of annotated data has sparked signifi-\ncant interest in unsupervised pre-training methods that\nleverage medical reports as auxiliary signals for medi-\ncal visual representation learning. However, existing re-\nsearch overlooks the multi-granularity nature of medical\nvisual representation and lacks suitable contrastive learn-\ning techniques to improve the models’ generalizability\nacross different granularities, leading to the underutiliza-\ntion of image-text information. To address this, we pro-\npose MLIP , a novel framework leveraging domain-specific\nmedical knowledge as guiding signals to integrate lan-\nguage information into the visual domain through image-\ntext contrastive learning. Our model includes global con-\ntrastive learning with our designed divergence encoder, lo-\ncal token-knowledge-patch alignment contrastive learning,\nand knowledge-guided category-level contrastive learning\nwith expert knowledge. Experimental evaluations reveal\nthe efficacy of our model in enhancing transfer performance\nfor tasks such as image classification, object detection, and\nsemantic segmentation. Notably, MLIP surpasses state-of-\nthe-art methods even with limited annotated data, highlight-\ning the potential of multimodal pre-training in advancing\nmedical representation learning.1\n1. Introduction\nRepresentation learning for medical radiographs has gained\nsignificant attention recently, owing to the availability of\nabundant annotated data. Numerous approaches [20, 23,\n46, 48] have employed deep learning in a supervised man-\nner to learn representations for downstream tasks. How-\never, the acquisition of large-scale annotated data is time-\n*Corresponding Author.\n1Codes are available at https://github.com/gentlefress/MLIP\nThe lung volumes remain low. Signs of mild fluid \noverload and the extent of the known left pleural \neffusion have improved.Positive Negative Negative False Negative\nReport\nNegative\nNegativePositiveFalse \nNegative\nNegative\nNegativePositiveFalse \nNegative\nReport ReportFigure 1. Detailed illustration of false negatives in medi-\ncal image-text. Conventional approaches consider false negative\nsamples as negatives that are distant from positive samples in the\nlower left corner. In contrast, in the lower right corner, our pro-\nposed method distinguishes false negatives from negatives, effec-\ntively bringing them closer to positives.\nconsuming and costly. unsupervised pre-training methods\nhave emerged as a promising alternative. These methods,\nwhich do not rely on annotated data, harness medical re-\nports as ancillary signals that provide targeted supervision\nfor visual representation learning. By incorporating lan-\nguage information, these models can acquire more universal\nvisual representations that are transferable to downstream\ntasks and capable of domain transfer.\nThere are three mainstream paradigms in visual repre-\nsentation learning. Masked image modeling [29, 38, 60]\nfollows mask-and-predict paradigm, randomly masking\nsome patches and predicting missing information. Mul-\ntimodal contrastive learning [10, 32, 64, 65] conducts\nembed-and-compare proxy tasks to maximize the mutual\ninformation between medical images and reports through\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n11704\nimage-text contrastive learning. Multi-view self-supervised\nlearning [9, 12, 13, 28] adopts an augment-and-compare\nparadigm, where an input image is randomly transformed\ninto two augmented views and compare the two distinct\nviews in the representation space.\nHowever, the fact that pathological features only occupy\na small part of a radiograph means that a significant por-\ntion of the information may not be relevant for our analysis,\ndecreasing the utilization of medical image-text data. More-\nover, due to the unique nature of medical image-report com-\npared to general text-image pairs, different symptoms may\ncorrespond to the same disease, and traditional contrastive\nlearning will mistake samples that are not in the same batch\nas negative samples even if they are very close in the se-\nmantic space. In Fig 1, we purpose to differentiate between\nfalse negative and negative samples and further reduce the\ndistance between false negative and positive samples.\nDriven by the revelation from [33, 39, 56], we design\na knowledge-guided align-and-compare framework to cap-\nture multi-grained semantic information and to accurately\nalign each image’s pathology with the corresponding med-\nical term [33, 36, 37]. We introduce a knowledge-guided\nmedical multimodal pre-trained model, dubbed MLIP, to\nexplore the inherent multi-granularity cross-modal corre-\nspondence for enhancing the generalizability of visual rep-\nresentation. Specifically, we employ a combination of three\ndistinct image-text contrastive learning methods to embed\nlanguage into vision at different granularity and utilize two\nproxy tasks to establish the match between vision and lan-\nguage. Our model exploits multi-level correspondences be-\ntween medical radiographs and reports to enhance general-\nized medical visual representation with contrastive learning.\nOur approach demonstrates state-of-the-art performance in\nimage classification, object detection, and semantic seg-\nmentation, even when working with limited annotated data.\nThe key contributions are summarized as follows:\n• We introduce two dynamically updated divergence en-\ncoders for data augmentation, aiming to increase the\nnumber of samples and thus enhance the generalization\nability of the model.\n• We propose to leverage cross-modal attention-based\ntoken-knowledge-patch alignment and incorporate con-\ntrastive learning to facilitate the exploration of local rep-\nresentations.\n• We propose a knowledge-guided prototype clustering\ncontrastive learning approach, which focuses on conduct-\ning contrastive learning at the category level rather than\nthe individual samples.\n• We pre-train MLIP on the MIMIC-CXR dataset [35],\nevaluating the learned representations on seven down-\nstream datasets. Experimental results demonstrate the su-\nperiority of our model over state-of-the-art methods, even\nwith 1 %and 10 %training data.2. Related Work\n2.1. Text-guided Medical Visual Representations\nLearning\nMedical reports are pivotal in unsupervised medical vi-\nsual representation learning, with two primary methods\ndominating the field. The first method involves extracting\ndisease labels from radiology reports using manually de-\nsigned rules [34, 35], followed by pre-training image mod-\nels for downstream tasks. However, defining the rules re-\nquires considerable human effort and domain expertise. On\nthe other hand, the second method adopts image-text con-\ntrastive learning methods to integrate text and vision in an\nunsupervised manner [20, 32, 33, 56, 64]. These methods\nhave been shown remarkable performance in diverse down-\nstream tasks, including medical object detection [4], im-\nage classification [33, 64], and semantic segmentation [64].\nHowever, they have not effectively explored visual repre-\nsentations at different granularities and rely on partial se-\nmantic information.\nTo address these limitations, MGCA [56] proposes to\nleverage multiple visual features at different granularities\nduring the pre-training phase, enhancing the performance\nof models in downstream tasks. However, it overlooks the\nchallenging sample issue in medical radiology. In this work,\nwe propose a divergence encoder that manually updates its\nparameters based on the similarity between the output fea-\ntures and those of a common encoder. By increasing di-\nvergence between the two encoders, we enhance feature di-\nversity and train the model to discriminate among similar\nsamples effectively.\n2.2. Knowledge-guided Pre-training\nTo enhance the model’s knowledge and understanding abil-\nity by leveraging a broader background, numerous vision-\nand-language pre-training methods have been devised to\nincorporate domain-specific knowledge. These methods\ncan be categorized into four distinct knowledge-guided\nschemes: embedding combination [66], data structure com-\npatibility [26, 42], knowledge supervision [58], and neural-\nsymbolic methods [2]. For instance, ERNIE-ViL [62] in-\ntroduces a vision and language alignment technique by uti-\nlizing a scene graph extracted from the input text. Simi-\nlarly, KB-VLP [11] incorporates object tags from images\nand knowledge graph embeddings from texts to enhance\nthe acquisition of knowledge-aware representations. ARL\n[15] utilizes expert knowledge as an intermediate medium\nto align images and reports. Additionally, a recent study\n[45] proposes the automatic generation of visual and tex-\ntual prompts, injecting expert medical knowledge into the\nprompt for pre-training.\nIn contrast to existing works, we propose an alignment\nmethod that leverages domain-specific knowledge as an in-\n11705\ntermediate mediator for aligning texts and images, along\nwith a knowledge-guided prototype clustering contrastive\nlearning. This approach integrates expert domain knowl-\nedge derived from the Unified Medical Language System\n(UMLS) [6]. By incorporating UMLS knowledge into both\nvision and language modalities, our approach leverages\nknowledge as a medium to achieve improved alignment be-\ntween images and text, facilitating more effective cluster-\ning of image-text pairs. Importantly, our method effectively\nmitigates the influence of disease-level false negatives with-\nout relying on object detectors or scene graph parsers.\n3. Proposed Approach\nIn this section, we present our approach for learning effec-\ntive medical visual representations using medical reports.\nWe utilize a knowledge-guided align-and-compare scheme,\nas depicted in Figure 2, to match and align modalities and\ncompare them in the representation space. Our method\ncomprises four key components: 1) global image-text con-\ntrastive learning; 2) local token-knowledge-patch alignment\ncontrastive learning; 3) knowledge-guided category-level\ncontrastive learning; and 4) proxy tasks to ensure matching\nand prevent shortcut exploitation by the network. We dis-\ncuss each component in detail in the following subsections\nand provide an overview of the overall training objective.\n3.1. Problem Setup\nRecently, it has been demonstrated in [33, 56] that learn-\ning medical visual representation learning without labels\ncan achieve competitive performance. In this study, we fol-\nlow the setting in [56], given a training set of Nmedical\nimage-report pairs D={(xi, yi)}i=1,...,N , we use an im-\nage encoder fvand a text encoder ftencode Dto a global\nfeature set Eil={(vi, ti)|vi=fv(xi), ti=ft(yi)}i=1,...,N ,\nand a local feature set Etl={(Pi,Si)}i=1,...,N , where Si=\n{s1\ni, s2\ni, ..., sV\ni} ∈RV×dandPi={p1\ni, p2\ni, ..., pM2\ni} ∈\nRM2×d.Vdenotes the length of the sentence and M2de-\nnotes the number of image patches.\nFurthermore, we incorporate expert knowledge into our\nmodel by constructing an extracted knowledge graph, as\ndescribed in [15]. This knowledge graph is denoted as\nG={(hei, rei, tai)}NG\ni=1, where NGrepresents the number\nof graph triples, and hei,rei, andtaicorrespond to the head\nentity, relation, and tail entity, respectively. The inclusion of\nthis expert knowledge enhances the model’s understanding\nand reasoning capabilities, enabling more informed align-\nment and representation learning.\n3.2. Global Image-text Contrastive Learning\nTo pull correct samples closer and push random samples\napart in the latent space, we follow [31, 52], present a\ncomprehensive discussion on global image-text contrastivelearning by maximizing mutual information I(X, Y )be-\ntween the vision element Xand the language component\nY:\nI(X, Y )=X\ny∈YX\nx∈XP(x, y) logP(x|y)\nP(x).(1)\nEq.1 suggests that the fractionP(x|y)\nP(x)collapses to zero\nwhen xandyare incompatible with each other. Therefore,\nwe hypothesize thatP(x|y)\nP(x)is proportional to the similarity\nbetween xandy. Further, the maximization of mutual in-\nformation corresponds to the maximization of the similarity\nsim(x, y)between xandy, which can be represented as:\nI(v, t)∝ I(X, Y )∝sim(x, y)∝sim(v, t). (2)\nSpecifically, inspired by [12], we firstly utilize two pro-\njection layers hvandhtto map viandtiinto a normalized\nshared feature space, yielding v∗\ni∈Rdandt∗\ni∈Rd, re-\nspectively. Then, we apply the dot product to model the\nsimilarity between v∗\niandt∗\ni. To obtain more effective fea-\ntures, we perform Self-Attention [54] and LayerNorm [3]\non features:\nv∗\ni=LN(SA{hv(vi)}); (3a)\nt∗\ni=LN(SA{ht(ti)}), (3b)\nsim{v∗\ni, t∗\ni}=v∗\nitiT, (3c)\nwhere SA denotes Self-Attention module and LN denotes\nLayerNorm module.\nWe optimize this process via image-text contrastive loss\nbased on InfoNCE loss [53], which are designed to maxi-\nmize the mutual information between the correct image-text\npairs in the latent space:\nLil\nv2t(vi, ti) =−log(ϕil(vi, ti)PB\nk=1ϕil(vi, tk)), (4a)\nLil\nt2v(vi, ti) =−log(ϕil(vi, ti)PB\nk=1ϕil(vk, ti)), (4b)\nwhere ϕil(vi, ti) = exp(sim(v∗\ni,t∗\ni)\nτ1),Bis the batch size and\nτ1is the global temperature hyper-parameter.\nDirectly optimizing I(v, t)is a challenging task. As an\nalternative, [53] has proposed an alternative method to op-\ntimize the lower bound of mutual information:\nI(v, t)≥logN′− LNCE(v, t), (5)\nwhere N′is the number of negative samples. In Eq.5, min-\nimizing LNCE(v, t)is equivalent to maximizing the lower\nbound of the mutual information between the medical im-\nage and the corresponding report.\nTo increase the number of samples and enhance the fea-\nture diversity, we perform a divergence encoder to achieve\n11706\na\nThere is no focal \nconsolidation, \npleural effusion \nor \npneumothorax...\nImage \nEncoderText \nEncoder\nThere is no focal \nconsolidation, \npleural effusion \nor \npneumothorax...Image \nDivergence \nEncoderText \nDivergence\nEncoderLocal \nITACategory-\nlevel ITAGlobal \nITA\nGlobal \nITA\nGlobal \nITACross\nPredictionCross\nPrediction\nCross\nPrediction\nD\n…\ne\uf057\ntv\noTucker Fusion\ne\uf057e\uf057\ntv\notv\noCross \nAttention\nKeyQuery\nValue\uf051KnowledgeStop Gradient Stop Gradient\uf058\n\uf059rt\uf058\uf059\ntfvf\nvotoaugvaugtt v\nRandom-transformed \nImageCategory-level ITAGlobal ITA\n\uf051Cross AttentionKey QueryValue\nValue\nQuery Key\nAlignAlignLocal ITA\n\uf053\n\uf050Knowledge\nt vt\nvGlobal FeatureLocal Feature\nGlobal Feature Local FeatureFigure 2. The framework of our MLIP. Our model architecture employs global, local, and category-level image-text contrastive learning.\nGiven medical images and reports as inputs, we extract global features and local features for each modality using image and text encoders.\nWe leverage global features for global image-text contrastive learning, while the local features are aligned with domain-specific knowledge\nfrom UMLS to achieve fine-grained image-text alignment. Through tucker fusion and cross-modal attention mechanisms, we combine\nthe image, text, and knowledge representations, facilitating category-level prototype contrastive learning. Furthermore, to enhance feature\ndiversity, we introduce a divergence encoder as a data augmentation strategy, generating similar yet distinct features. This enables global\ncontrastive learning between images and augmented text, as well as between text and augmented images.\ndata augmentation and extend the gap between samples. We\ndefine image divergence encoder ovand text divergence en-\ncoder ot, initialized by fvandft, respectively. Then we\nobtain features incrementally differentiated from viandti:\nvaug\ni=ov(xrt\ni);taug\ni=ot(yi), (6)\nwhere xrt\nidenotes randomly transformed images. We man-\nually update divergence encoders’ parameters instead of re-\nlying on backpropagation:\nθot=st∗θft+ (1−st)∗θot, (7a)\nθov=sv∗θfv+ (1−sv)∗θov, (7b)\nwhere st=cosine (ti, taug\ni)andsv=cosine (vi, vaug\ni), and\nθot, θov, θft, θfvare the parameters of ot, ov, ft, fv, respec-\ntively. In this way, as the sv(st)increases, we aim to retain\nfewer parameters from ov(ot)and incorporate more param-\neters from fv(ft), in order to generate more diverse fea-\ntures. Then we use Eq.4a, 4b to compute Lil\nv2aandLil\navt.We compute the objective Lilas the average of the four\nloss values:\nLil=1\n2NXN\ni=1(Lil\nv2t(vi, ti) +Lil\nt2v(vi, ti))\n+λ0\n2NXN\ni=1(Lil\nv2a(vi, taug\ni) +Lil\navt(vaug\ni, ti)),(8)\nwhere Nis the total number of samples and λ0denotes the\nweight for augmented image-text contrastive learning.\n3.3. Local Token-knowledge-patch Alignment Con-\ntrastive Learning\nIn medical images, pathologies are often visually subtle and\noccupy a small fraction of the overall image, while only\na few disease-related tags in the associated report accu-\nrately depict the critical medical condition. Given this ob-\nservation, we employ a local image-text contrastive learn-\ning method to maximize the mutual information between\nlocal features and achieve cross-modal alignment between\n11707\nimages and texts, inspired by [18, 56].\nHowever, traditional token-patch alignment contrastive\nlearning is utilizing the local features of the image and\ntext to compute the attention matrix, and then perform con-\ntrastive learning after aligning the images and texts. Since\nmedical radiology is highly professional and there is a cer-\ntain bias between different datasets, we regard professional\nknowledge from the UMLS [6] as a medium between vision\nand language. To achieve more accurate token-patch align-\nment, we align the knowledge with radiographs and reports.\nSimilar to global feature, we apply Self-Attention and\nLayerNorm module on every features:\npi=LN(SA{hv(pi)});si=LN(SA{ht(si)}).(9)\nWe apply the knowledge representation learning algo-\nrithm TransE [7] to the knowledge graph Gto obtain en-\ntity embeddings. Subsequently, we utilize the Graph At-\ntention Network [55] to capture local information in the\ngraph neighborhood for each node. This allows us to obtain\nknowledge representations, denoted as {ei}Ne\ni=1∈RNe×de,\nwhere derepresents the feature dimension and Nedenotes\nthe number of entity.\nWe adopt cross-modal attention mechanism [14, 44] to\nexplore the matching between knowledge and image:\nattnvk\nj,k=softmax ((Qpj\ni)T(Kek\ni)√\nd), (10a)\nzvj\ni=XN\nk=1attnvk\nj,k(V ek\ni), (10b)\nwhere Q, K, V ∈Rd×dare trainable matrices. eiis mapped\ntoRM2×d.zvj\niis cross-modal knowledge embedding cor-\nresponding to pj\ni.\nLying in the purpose of maximizing the lower bound of\nmutual information, we leverage InfoNCE loss [53] to pull\npj\niandzvj\nicloser and push pj\niand other cross-modal knowl-\nedge embeddings apart. However, given that irrelevant in-\nformation only occupies a vast majority of medical images,\nwe employ wj\nito balance the weights of different patches.\nThe loss Ltl\nv2tis designed symmetrically as:\nLtl\nv2t=−1\n2NM2NX\ni=1M2X\nj=1wj\ni(logϕtl(pj\ni, zvj\ni)\nPM2\nk=1ϕtl(pj\ni, zvk\ni)\n+ logϕtl(zvj\ni, pj\ni)\nPM2\nk=1ϕtl(zvk\ni, pj\ni)),\n(11)\nwhere ϕtl(pj\ni, zvj\ni) = exp(sim(pj\ni,zvj\ni)\nτ2),τ2is the local tem-\nperature hyper-parameter. To establish the correlation be-\ntween the j-th visual patch and the [CLS] token, we assign\nthe weight wj\niusing the last-layer attention mechanism av-\neraged across multiple heads.Similarly, for the j-th text token, we calculate corre-\nsponding cross-modal knowledge embedding ztj\niand con-\nstruct local contrastive loss Ltl\nt2vto maximize the lower\nbound of mutual information between sj\niandztj\ni. The ob-\njective Ltlcan be defined as the average of these two losses:\nLtl=1\n2(Ltl\nv2t+Ltl\nt2v). (12)\n3.4. Knowledge-guided Category-level Contrastive\nLearning\nFor a given radiograph-report pair, traditional contrastive\nlearning approaches treat other radiograph-report pairs\nwithin the same batch as negative samples. However, in the\ncontext of category-level analysis, samples that belong to\ndifferent batches but exhibit highly similar semantics should\nbe considered positive samples. In our approach, we aim to\nselect representative samples in each iteration, emphasiz-\ning their ability to capture meaningful disease-related in-\nformation. In the medical domain, expert knowledge plays\na crucial role in representation learning. We purpose to\nbridge the gap between the vast knowledge learned from\ngeneral visual and textual data and its effective applica-\ntion in the intricate realm of medical radiology. There-\nfore, we incorporate expert knowledge from UMLS [6] as\nan auxiliary signal. Drawing inspiration from [8, 45], we\npropose a knowledge-guided clustering-based approach to\nimprove the efficacy of learned representations. We bring\ntogether highly similar samples with high-level semantics,\neven when originating from different batches, and ensure\ntheir proximity in the feature space, rather than increasing\ntheir distance from one another.\nMotivated by [41], we realize to filter out irrelevant in-\nformation and explore more fine-grained relations between\nimages and text. To achieve this, we employ a mechanism\nthat identifies the most relevant topic in a given context.\nSpecifically, we utilize v∗\nito find the most relevant topic\nint∗\ni, resulting in ˙ti. Then, we use ˙tito find the relevant\ntopic in v∗\ni, leading to ˙vi. The process is mathematically\ndefined as follows:\n˙ti=LN(softmax (v∗\niTt∗\ni√\nd)t∗\ni); ˙vi=LN(softmax (v∗\niTv∗\ni√\nd)˙ti),\n(13)\nthen we utilize tucker fusion [5] to seamlessly integrate vi-\nsual and textual features, further fuse with knowledge rep-\nresentations:\nQ= ((Tc×1˙vi)×2˙ti)×3Wo, (14)\nwhere Worepresents a mapping matrix which is trainable\nand maps fused features to a certain dimensional space, and\nTcdenotes the core tensor.\nTo further integrate knowledge with modality-specific\nfeatures, we employ a linear mapping layer to project the\n11708\nknowledge representation eiinto a d-dimensional space and\nincorporate it with fused features using cross-modal atten-\ntion, thereby facilitating the fusion of information across\nmodalities:\nvkti=SA(softmax (QTei\nτ3)·ei), (15)\nwhere τ3is the temperature hyper-parameter we set to scale\nthe attention.\nFor image-text features pair ( ˙vi,˙ti)and knowledge-\nfused features, we apply the iterative Sinkhorn-Knopp clus-\ntering algorithm [19] to generate a cluster assignment code\nuvkt,i∈RC, by assigning vktitoCclusters separately. To\nfacilitate this, we introduce a set J=j1, ..., j Cthat con-\ntains Ctrainable cross-modal prototypes, where each pro-\ntotype jc∈Rd. We calculate the visual softmax probabil-\nitypv,iby computing the cosine similarity between the vi-\nsual feature vector ˙viand all cross-modal prototypes in J.\nSimilarly, the textual softmax probability pt,iis obtained by\nmeasuring the cosine similarity between the textual feature\nvector ˙tiand all cross-modal prototypes in J:\npv,i\nc=exp( ˙viTjc/τ4)P\nlexp( ˙viTjl/τ4);pt,i\nc=exp( ˙tiTjc/τ4)\nP\nlexp( ˙tiTjl/τ4),\n(16)\nwhere τ4is a category-level temperature hyper-parameter\nandcdenotes the c-th element of the vector.\nTo enable knowledge-guided category-level contrastive\nlearning, we employ uvkt,ias the pseudo-label for training\n˙tiand˙vi. This allows the three features to interact in the\nlatent space and guide the shifting of positive and negative\nsamples with the assistance of domain-specific knowledge.\nThe objective loss Lclis formulated as follows:\nLcl=1\n2NNX\ni=1CX\nc=1(uvkt,i\nclogpv,i\nc+uvkt,i\nclogpt,i\nc).(17)\n3.5. Image-text Matching and Text Swapping\nIn order to identify the alignment between radiographs and\ntheir corresponding reports, we propose two pretext tasks\naimed at bridging the semantic divide between visual and\nlinguistic information within the feature space: 1) comput-\ning relevance scores between image patch and contextual-\nized sentence to evaluate the degree of correlation between\nthe image and text elements; 2) randomly substituting med-\nical reports corresponding to the image with a predeter-\nmined probability, improving the discriminative ability on\nmismatched samples of the model.\nWe assume that the text features tand image fea-\ntures vhave been normalized. Therefore, we construct\nthe similarity between the two modalities as a relevance\nscore: r(v, t) =vT·t, subsequently, we randomly select an-\nother image v′and obtain its corresponding relevance scorer(v′, t). To ensure that the difference between r(v, t)and\nr(v′, t)is greater than a pre-specified margin G, we utilize\nthe hinge loss function to compute image-text match loss:\nLitm= max(0 ,G −r(v, t) +r(v′, t)). (18)\nSimilarly, we propose a text swapping task, which in-\nvolves randomly replacing text with a predefined probabil-\nityγ. We employ a bidirectional similarity Hinge loss to pe-\nnalize the model for insufficient discriminative ability. This\ntask aims to enhance the model’s ability to distinguish be-\ntween different reports. We employ a cross-modal attention\nmechanism to fuse the text and image modalities, then com-\npute the relevance score by performing a weighted summa-\ntion of the similarity between the fused representation and\nthe original text-image pair. Our objective is to ensure that\nthis score exceeds the score obtained after replacing the text\nby a margin G′:\nrts(v, t) =vT·t+α·CA(v, t)T·CA(t, v),(19a)\nrts(v, t′) =vT·t+α·CA(v, t′)T·CA(t′, v),(19b)\nLts= max(0 ,G′−rts(v, t) +rts(v, t′)), (19c)\nwhere CA(x,y) =softmax (xT·y√\nd)·y. Through these two\ndesigned proxy tasks, we compute the image-text match-\ning loss Litmand the text swapping loss Lts. These losses\nquantify the model’s ability to accurately match radiographs\nto their appropriate reports, thereby providing a measurable\nobjective for the optimization process.\n3.6. Overall Objective\nOur training approach involves joint optimization of the\nfive losses, aiming to promote the acquisition of effective\nand generalizable medical image representations by the net-\nwork. The overall training objective can be expressed as\nfollows:\nL=λ1Lil+λ2Ltl+λ3Lcl+λ4Litm+λ5Lts,(20)\nwhere λ1,λ2,λ3,λ4andλ5are hyper-parameters employed\nto balance the weights associated with each respective loss.\n4. Experiments\n4.1. Pre-training Dataset and Implementation De-\ntails\nOur MLIP framework is initially pre-trained on the MIMIC-\nCXR 2.0.0 dataset [35], with data consistency ensured\nthrough preprocessing methods from [64]. Lateral views\nare excluded from the dataset as downstream datasets only\ninclude frontal-view chest images. Inspired by [56], we ex-\ntract impression and finding sections from free-text reports,\nproviding comprehensive descriptions of medical diseases.\n11709\nWe filter out empty or short reports, resulting in approxi-\nmately 217,000 image-text pairs. Details about our imple-\nmentation can be found in the supplementary 6.1.\n4.2. Downstream Tasks\nMedical Object Detection. We assess the capability of\nour pre-trained image encoder for medical object detection\non the RSNA Pneumonia dataset [50] (stage 2 version) and\ntheObject CXR dataset [30]. The detection performance\nis evaluated using the YOLOv3 [25] frozen setting, where\nthe pre-trained ResNet-50 [27] image encoder acts as a fixed\nbackbone for YOLOv3. In this configuration, only the clas-\nsification layers are fine-tuned. To evaluate the efficiency\nof data utilization, we conduct experiments in the zero-shot\nscenario and further fine-tune the model using 1%, 10%,\nand 100% of the available training data. Evaluation is per-\nformed using the Mean Average Precision (mAP) metric,\ncomputed with IOU thresholds ranging from 0.4 to 0.75.\nMethodRSNA (mAP) Object CXR (mAP)\nZero-shot 1% 10% 100% Zero-shot 1% 10% 100%\nRandom Init ∼ 1.0 4.0 8.9 ∼ ∼ ∼ 4.4\nImageNet Init ∼ 3.6 8.0 15.7 ∼ ∼ 8.6 15.9\nConVIRT [64] 3.7 8.2 15.6 17.9 ∼ ∼ 8.6 15.9\nGLoRIA-CheXpert [33] 4.4 9.8 14.8 18.8 ∼ ∼ 10.6 15.6\nGLoRIA-MIMIC [33] 6.2 10.3 15.6 23.1 ∼ ∼ 8.9 16.6\nMGCA [56] 7.8 12.9 16.8 24.9 ∼ ∼ 12.1 19.2\nM-FLAG [40] 8.6 13.7 17.5 25.4 ∼ ∼ 13.6 19.5\nPRIOR [16] 10.7 15.6 18.5 25.2 1.4 2.9 15.2 19.8\nMLIP (Ours) 12.3 17.2 19.1 25.8 2.7 4.6 17.4 20.2\nTable 1. Fine-tuned results (mAP [%]) of object detection with\n1%, 10%, and 100% of the available training data in RSNA\nand Object CXR. ∼means mAP is smaller than 1%.\nMedical Semantic Segmentation. We evaluate the per-\nformance of our model for medical semantic segmentation\non the SIIM Pneumothorax dataset [63] and the RSNA\nPneumonia dataset [50]. Following the methodology pre-\nsented in [33], we adopt the fine-tuning protocol of U-Net\n[48] to assess the segmentation task. Specifically, we utilize\nthe pre-trained ResNet-50 image encoder as a fixed back-\nbone for the U-Net architecture and train the decoder com-\nponent using varying proportions of the available training\ndata (1%, 10%, and 100%). We also evaluate our model in\nthe zero-shot scenario. To evaluate the quality of segmenta-\ntion, we compute Dice scores [59] as the chosen metric for\nperformance assessment.\nMedical Image Classification. We perform medical im-\nage classification on the RSNA Pneumonia dataset [50],\nCOVIDx dataset [57], and CheXpert dataset [34]. To eval-\nuate the transferability of our pre-trained image encoder,\nwe adopt the Linear Classification setting following the\nmethodology proposed in prior work [33, 56]. This involves\nfreezing the pre-trained ViT-B/16 [21] or ResNet-50 imageMethodRSNA (Dice) SIIM (Dice)\nZero-shot 1% 10% 100% Zero-shot 1% 10% 100%\nRandom Init 3.9 6.9 10.6 18.5 ∼ 9.0 28.6 54.3\nImageNet Init 17.6 34.8 39.9 64.0 2.2 10.2 35.5 63.5\nConVIRT [64] 23.3 55.0 67.4 67.5 11.7 25.0 43.2 59.9\nGLoRIA-CheXpert [33] 32.0 59.3 67.5 67.8 19.8 35.8 46.9 63.4\nGLoRIA-MIMIC [33] 34.6 60.8 68.2 67.6 21.0 37.6 56.4 64.0\nMGCA [56] 34.9 63.0 68.3 69.8 33.5 49.7 59.3 64.2\nM-FLAG [40] 40.7 64.6 69.7 70.5 37.2 52.5 61.2 64.8\nPRIOR [16] 41.8 66.4 68.3 72.7 38.6 51.2 59.7 66.3\nMLIP (Ours) 44.3 67.7 68.8 73.5 40.2 51.6 60.8 68.1\nTable 2. Semantic segmentation results (Dice [%]) achieved on\nthe SIIM and RSNA datasets. Each dataset is fine-tuned using\n1%, 10%, and 100% of the available training data. The best results\nobtained for each setting are highlighted in red, while the subopti-\nmal results are highlighted in blue.\nencoder and training only a linear classification head for the\ndownstream classification task. Additionally, to assess data\nefficiency, we conduct experiments in the zero-shot scenario\nand evaluate the model using 1%, 10%, and 100% of the\ntraining data for each classification dataset. The evaluation\nmetrics used are the area under the receiver operating char-\nacteristic (ROC) curve (AUROC) for RSNA and CheXpert,\nand accuracy (ACC) for COVIDx-v6, consistent with the\nevaluation criteria outlined in [64]. More details and exper-\niment can be found in the supplementary 6.2 and 6.3.\n4.3. Results\nResults on Medical Object Detection. We evaluate the\nResNet-50-YOLOv3 architecture on the RSNA and Object\nCXR datasets. Our results, presented in Table 1, demon-\nstrate a significant improvement over ConVIRT [64], GLo-\nRIA [33], MGCA [56], M-FLAG [40] and PRIOR [16].\nNotably, our method achieves superior performance using\nonly 1% of the data, surpassing alternative approaches that\nrequire 10% or even 100% of the data for fine-tuning.\nResults on Medical Semantic Segmentation. In Table\n2, we present the semantic segmentation results (Dice [%])\nachieved on the SIIM and RSNA datasets using the ResNet-\n50-U-Net architecture. MLIP leverages contrastive learning\nand category-level approaches to achieve remarkable per-\nformance improvements, consistently obtaining the best re-\nsults in various settings, as highlighted in red. Specifically,\nMLIP outperforms the MGCA [56] by 4.7% on the RSNA\ndataset and 1.9% on the SIIM dataset when fine-tuned with\nonly 1% of the training data. Moreover, MLIP achieved\nstate-of-the-art results in zero-shot scenarios.\nResults on Medical Image Classification. Table 3 shows\nthe medical linear classification results on RSNA and\nCOVIDx datasets. We divide existing pre-trained meth-\nods into two categories: pre-trained on CheXpert [34] and\npre-trained on MIMIC-CXR[35]. The results of other ap-\nproaches are from original papers, and we refer to [56],\n11710\nMethodCheXpert (AUC) RSNA (AUC) COVIDx (ACC)\nZero-shot 1% 10% 100% Zero-shot 1% 10% 100% Zero-shot 1% 10% 100%\nRandom Init - 56.1 62.6 65.7 - 58.9 69.4 74.1 - 50.5 60.3 70.0\nImageNet Init - 74.4 79.7 81.4 - 74.9 74.5 76.3 - 64.8 78.8 86.3\npre-trained on CheXpert\nDSVE [22] 26.6 50.1 51.0 51.5 18.7 49.7 52.1 57.8 - - - -\nVSE++ [24] 27.3 50.3 51.2 52.4 19.1 49.4 57.2 67.9 - - - -\nGLoRIA [33] 50.4 86.6 87.8 88.1 39.2 86.1 88.0 88.6 20.9 67.3 77.8 89.0\npre-trained on MIMIC-CXR\nCaption-Transformer [17] 42.2 77.2 82.6 83.9 - - - - - - - -\nCaption-LSTM [61] 45.6 85.2 85.3 86.2 - - - - - - - -\nContrastive-Binary [51] 46.8 84.5 85.6 85.8 - - - - - - - -\nConVIRT [64] 47.6 85.9 86.8 87.3 34.7 77.4 80.1 81.3 17.8 72.5 82.5 92.0\nGLoRIA-MIMIC [33] 51.7 87.1 88.7 88.0 40.6 86.6 89.2 90.4 22.1 67.3 81.5 88.6\nMGCA (ResNet-50) [56] 50.2 87.6 88.0 88.2 41.0 88.6 89.1 89.9 24.5 72.0 83.5 90.5\nM-FLAG (ResNet-50) [40] 55.9 87.8 88.4 88.6 41.8 88.8 89.4 90.2 25.4 72.2 84.1 90.7\nPRIOR (ResNet-50) [16] 56.3 87.6 88.6 88.8 42.4 88.9 89.5 90.5 25.9 72.3 84.7 91.0\nMLIP (Ours, ResNet-50) 56.9 87.8 88.7 88.9 42.9 88.8 89.6 90.6 26.3 73.0 85.0 90.8\nMGCA (ViT-B/16) [56] 50.0 88.8 89.1 89.7 39.2 89.1 89.9 90.8 33.2 74.8 84.8 92.3\nMLIP (Ours, ViT-B/16) 57.0 89.0 89.4 90.0 53.0 89.3 90.0 90.8 34.8 75.3 86.3 92.5\nTable 3. Image classification results in zero-shot scenarios and fine-tuning with 1%, 10%, and 100% of the training data in\nCheXpert, RSNA and COVIDx. The evaluation metric used is AUC [%] for CheXpert and RSNA, and ACC [%] for COVIDx. The best\nresults achieved for each setting are highlighted in red, while the suboptimal results are highlighted in blue.\npre-train GLoRIA with MIMIC-CXR datasets. We evalu-\nate these approaches in the zero-shot scenario and with 1%,\n10% and 100% of the data for fine-tuning, the results all out-\nperform the SOTA. For a fair comparison, we pre-train our\nmodel with ResNet-50 and ViT-B/16 architecture. Except\nfor the ViT-B/16 architecture, which yields comparable re-\nsults to MGCA when fine-tuning is conducted using 100%\nof the available data, all others achieve better performance\nthan the same architecture.\n4.4. Ablation Study\nTable 4 presents ablation results on semantic segmenta-\ntion for both RSNA and SIIM datasets. We observe that\nleveraging knowledge as an intermediate medium for align-\ning image-text pairs in contrastive learning substantially\nenhances the model’s performance. Moreover, category-\nlevel contrastive learning aids in mitigating false negatives,\nthereby improving the model’s generalization. Global con-\ntrastive learning acts as a performance lower bound, com-\nplementing local and category-level approaches and yield-\ning promising outcomes. Other ablation studies can be\nfound in the supplementary 6.4.\n4.5. Visualization\nTo further understand the inner workings of MLIP, we\npresent learned local correspondences between radiographs\nand medical reports in the form of heatmaps and showcaseTasks Setting RSNA (Dice) SIIM (Dice)\nGlobal ITA Local ITA Category-level ITA 1% 10% 100% 1% 10% 100%\n✓ ✓ 57.4 66.3 71.7 49.3 56.7 64.6\n✓ ✓ 60.6 68.1 70.4 47.0 48.8 66.4\n✓ ✓ 64.7 68.2 73.3 50.0 51.3 67.7\n✓ ✓ ✓ 67.7 68.8 73.5 51.6 60.8 68.1\nTable 4. Results of ablation study on proxy tasks for the se-\nmantic segmentation task. Global ITA’s pivotal role is evident,\nwhich can be attributed to the role of the divergence encoder.\nthe performance of MLIP on downstream tasks (semantic\nsegmentation and object detection) in the supplementary\n6.5. The visual evidence supports that MLIP excels in fine-\ngrained feature extraction, boosting accuracy.\n5. Conclusion\nIn this study, we propose MLIP, a novel medical visual\nrepresentation learning framework that integrates language\ninformation into the visual domain. By introducing a\ndivergence encoder to enhance representations and handle\ndifficult samples, along with a language-knowledge-\nimage alignment method guided by domain expertise,\nwe alleviate false negative issue and imprecise alignment\nissue in other models. Experimental results demon-\nstrate the effectiveness of MLIP on multiple datasets,\neven in zero-shot scenarios and with limited annotated\ndata. Our proposed divergence encoder and knowledge-\nassisted alignment approach have broader applicability.\n11711\nReferences\n[1] Emily Alsentzer, John R Murphy, Willie Boag, Wei-\nHung Weng, Di Jin, Tristan Naumann, WA Redmond, and\nMatthew BA McDermott. Publicly available clinical bert\nembeddings. NAACL HLT 2019 , page 72, 2019. 1\n[2] Saeed Amizadeh, Hamid Palangi, Alex Polozov, Yichen\nHuang, and Kazuhito Koishida. Neuro-symbolic visual rea-\nsoning: Disentangling. In ICML , pages 279–290. PMLR,\n2020. 2\n[3] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. Layer normalization. arXiv preprint arXiv:1607.06450 ,\n2016. 3\n[4] Michael Baumgartner, Paul F J ¨ager, Fabian Isensee, and\nKlaus H Maier-Hein. nndetection: a self-configuring method\nfor medical object detection. In MICCAI , pages 530–539.\nSpringer, 2021. 2\n[5] Hedi Ben-Younes, Rmi Cadene, Matthieu Cord, and Nicolas\nThome. Mutan: Multimodal tucker fusion for visual question\nanswering. In ICCV , pages 2612–2620, 2017. 5\n[6] Olivier Bodenreider. The unified medical language sys-\ntem (umls): integrating biomedical terminology. NAR, 32\n(suppl 1):D267–D270, 2004. 3, 5\n[7] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Ja-\nson Weston, and Oksana Yakhnenko. Translating embed-\ndings for modeling multi-relational data. Advances in neural\ninformation processing systems , 26, 2013. 5\n[8] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Pi-\notr Bojanowski, and Armand Joulin. Unsupervised learning\nof visual features by contrasting cluster assignments. NIPS ,\n33:9912–9924, 2020. 5\n[9] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv ´e J´egou,\nJulien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-\ning properties in self-supervised vision transformers. In\nICCV , pages 9650–9660, 2021. 2\n[10] Geeticka Chauhan, Ruizhi Liao, William Wells, Jacob An-\ndreas, Xin Wang, Seth Berkowitz, Steven Horng, Peter\nSzolovits, and Polina Golland. Joint modeling of chest radio-\ngraphs and radiology reports for pulmonary edema assess-\nment. In MICCAI , pages 529–539. Springer, 2020. 1\n[11] Kezhen Chen, Qiuyuan Huang, Yonatan Bisk, Daniel Mc-\nDuff, and Jianfeng Gao. Kb-vlp: Knowledge based vision\nand language pretraining. In ICML , page 2021, 2021. 2\n[12] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-\noffrey Hinton. A simple framework for contrastive learn-\ning of visual representations. In ICML , pages 1597–1607.\nPMLR, 2020. 2, 3\n[13] Xinlei Chen and Kaiming He. Exploring simple siamese rep-\nresentation learning. In CVPR , pages 15750–15758, 2021. 2\n[14] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy,\nFaisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter:\nUniversal image-text representation learning. In ECCV ,\npages 104–120. Springer, 2020. 5\n[15] Zhihong Chen, Guanbin Li, and Xiang Wan. Align, rea-\nson and learn: Enhancing medical vision-and-language pre-training with knowledge. In ACM MM , pages 5152–5161,\n2022. 2, 3\n[16] Pujin Cheng, Li Lin, Junyan Lyu, Yijin Huang, Wenhan Luo,\nand Xiaoying Tang. Prior: Prototype representation joint\nlearning from medical images and reports. In Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion, pages 21361–21371, 2023. 7, 8, 2\n[17] Marcella Cornia, Matteo Stefanini, Lorenzo Baraldi, and\nRita Cucchiara. Meshed-memory transformer for image cap-\ntioning. In CVPR , pages 10578–10587, 2020. 8\n[18] Wanyun Cui, Guangyu Zheng, and Wei Wang. Unsupervised\nnatural language inference via decoupled multimodal con-\ntrastive learning. In EMNLP , pages 5511–5520, 2020. 5\n[19] Marco Cuturi. Sinkhorn distances: Lightspeed computation\nof optimal transport. NIPS , 26, 2013. 6\n[20] Jeffrey De Fauw, Joseph R Ledsam, Bernardino Romera-\nParedes, Stanislav Nikolov, Nenad Tomasev, Sam Black-\nwell, Harry Askham, Xavier Glorot, Brendan O’Donoghue,\nDaniel Visentin, et al. Clinically applicable deep learning for\ndiagnosis and referral in retinal disease. Nature medicine , 24\n(9):1342–1350, 2018. 1, 2\n[21] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. arXiv preprint\narXiv:2010.11929 , 2020. 7, 1\n[22] Martin Engilberge, Louis Chevallier, Patrick P ´erez, and\nMatthieu Cord. Finding beans in burgers: Deep semantic-\nvisual embedding with localization. In CVPR , pages 3984–\n3993, 2018. 8\n[23] Andre Esteva, Brett Kuprel, Roberto A Novoa, Justin Ko,\nSusan M Swetter, Helen M Blau, and Sebastian Thrun.\nDermatologist-level classification of skin cancer with deep\nneural networks. nature , 542(7639):115–118, 2017. 1\n[24] Fartash Faghri, David J Fleet, Jamie Ryan Kiros, and Sanja\nFidler. Vse++: Improving visual-semantic embeddings with\nhard negatives. arXiv preprint arXiv:1707.05612 , 2017. 8\n[25] Ali Farhadi and Joseph Redmon. Yolov3: An incre-\nmental improvement. In CVPR , pages 1–6. Springer\nBerlin/Heidelberg, Germany, 2018. 7\n[26] Bin He, Di Zhou, Jinghui Xiao, Xin Jiang, Qun Liu,\nNicholas Jing Yuan, and Tong Xu. Bert-mk: Integrating\ngraph contextualized knowledge into pre-trained language\nmodels. In EMNLP , pages 2281–2290, 2020. 2\n[27] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In CVPR ,\npages 770–778, 2016. 7, 1\n[28] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross\nGirshick. Momentum contrast for unsupervised visual rep-\nresentation learning. In CVPR , pages 9729–9738, 2020. 2\n[29] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr\nDoll´ar, and Ross Girshick. Masked autoencoders are scalable\nvision learners. In CVPR , pages 16000–16009, 2022. 1\n[30] J Healthcare. Object-cxr-automatic detection of foreign ob-\njects on chest x-rays, 2020. 7, 2\n11712\n[31] R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon,\nKaran Grewal, Phil Bachman, Adam Trischler, and Yoshua\nBengio. Learning deep representations by mutual in-\nformation estimation and maximization. arXiv preprint\narXiv:1808.06670 , 2018. 3\n[32] Tzu-Ming Harry Hsu, Wei-Hung Weng, Willie Boag,\nMatthew McDermott, and Peter Szolovits. Unsupervised\nmultimodal representation learning across medical images\nand reports. arXiv e-prints , pages arXiv–1811, 2018. 1, 2\n[33] Shih-Cheng Huang, Liyue Shen, Matthew P Lungren, and\nSerena Yeung. Gloria: A multimodal global-local represen-\ntation learning framework for label-efficient medical image\nrecognition. In ICCV , pages 3942–3951, 2021. 2, 3, 7, 8, 1\n[34] Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Sil-\nviana Ciurea-Ilcus, Chris Chute, Henrik Marklund, Behzad\nHaghgoo, Robyn Ball, Katie Shpanskaya, et al. Chexpert:\nA large chest radiograph dataset with uncertainty labels and\nexpert comparison. In AAAI , pages 590–597, 2019. 2, 7, 1\n[35] Alistair EW Johnson, Tom J Pollard, Seth J Berkowitz,\nNathaniel R Greenbaum, Matthew P Lungren, Chih-ying\nDeng, Roger G Mark, and Steven Horng. Mimic-cxr, a de-\nidentified publicly available database of chest radiographs\nwith free-text reports. Scientific data , 6(1):317, 2019. 2,\n6, 7, 1\n[36] Kuang-Huei Lee, Xi Chen, Gang Hua, Houdong Hu, and Xi-\naodong He. Stacked cross attention for image-text matching.\nInECCV , pages 201–216, 2018. 2\n[37] Kunpeng Li, Yulun Zhang, Kai Li, Yuanyuan Li, and Yun\nFu. Visual semantic reasoning for image-text matching. In\nICCV , pages 4654–4662, 2019. 2\n[38] Zhe Li, Zhangyang Gao, Cheng Tan, Stan Z Li, and Lau-\nrence T Yang. General point model with autoencoding and\nautoregressive. arXiv preprint arXiv:2310.16861 , 2023. 1\n[39] Zhe Li, T. Yang Laurence, Xin Nie, BoCheng Ren, and Xian-\njun Deng. Enhancing sentence representation with visually-\nsupervised multimodal pre-training. In ACM MM’23 , 2023.\n2\n[40] Che Liu, Sibo Cheng, Chen Chen, Mengyun Qiao, Weitong\nZhang, Anand Shah, Wenjia Bai, and Rossella Arcucci. M-\nflag: Medical vision-language pre-training with frozen lan-\nguage models and latent space geometry optimization. In\nInternational Conference on Medical Image Computing and\nComputer-Assisted Intervention , pages 637–647. Springer,\n2023. 7, 8\n[41] Fenglin Liu, Xian Wu, Shen Ge, Wei Fan, and Yuexian Zou.\nExploring and distilling posterior and prior knowledge for\nradiology report generation. In CVPR , pages 13753–13762,\n2021. 5\n[42] Weijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Qi Ju,\nHaotang Deng, and Ping Wang. K-bert: Enabling language\nrepresentation with knowledge graph. In AAAI , pages 2901–\n2908, 2020. 2\n[43] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. arXiv preprint arXiv:1711.05101 , 2017. 1\n[44] Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh.\nHierarchical question-image co-attention for visual question\nanswering. NIPS , 29, 2016. 5[45] Ziyuan Qin, Huahui Yi, Qicheng Lao, and Kang Li.\nMedical image understanding with pretrained vision lan-\nguage models: A comprehensive study. arXiv preprint\narXiv:2209.15517 , 2022. 2, 5\n[46] Pranav Rajpurkar, Jeremy Irvin, Robyn L Ball, Kaylie Zhu,\nBrandon Yang, Hershel Mehta, Tony Duan, Daisy Ding,\nAarti Bagul, Curtis P Langlotz, et al. Deep learning for\nchest radiograph diagnosis: A retrospective comparison of\nthe chexnext algorithm to practicing radiologists. PLoS\nmedicine , 15(11):e1002686, 2018. 1\n[47] Joseph Redmon and Ali Farhadi. Yolov3: An incremental\nimprovement. arXiv preprint arXiv:1804.02767 , 2018. 2\n[48] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:\nConvolutional networks for biomedical image segmentation.\nInMICCAI , pages 234–241. Springer, 2015. 1, 7, 2\n[49] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\nAditya Khosla, Michael Bernstein, et al. Imagenet large\nscale visual recognition challenge. International journal of\ncomputer vision , 115:211–252, 2015. 1\n[50] George Shih, Carol C Wu, Safwan S Halabi, Marc D\nKohli, Luciano M Prevedello, Tessa S Cook, Arjun Sharma,\nJudith K Amorosa, Veronica Arteaga, Maya Galperin-\nAizenberg, et al. Augmenting the national institutes of health\nchest radiograph dataset with expert annotations of possi-\nble pneumonia. Radiology: Artificial Intelligence , 1(1):\ne180041, 2019. 7, 2\n[51] Hao Tan and Mohit Bansal. Lxmert: Learning cross-\nmodality encoder representations from transformers. In\nEMNLP-IJCNLP , pages 5100–5111, 2019. 8\n[52] Yonglong Tian, Dilip Krishnan, and Phillip Isola. Con-\ntrastive multiview coding. In ECCV , pages 776–794.\nSpringer, 2020. 3\n[53] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Repre-\nsentation learning with contrastive predictive coding. arXiv\ne-prints , pages arXiv–1807, 2018. 3, 5\n[54] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. NIPS , 30, 2017. 3\n[55] Petar Velickovic, Guillem Cucurull, Arantxa Casanova,\nAdriana Romero, Pietro Lio, Yoshua Bengio, et al. Graph\nattention networks. 1050(20):10–48550, 2017. 5\n[56] Fuying Wang, Yuyin Zhou, Shujun Wang, Varut Vardhanab-\nhuti, and Lequan Yu. Multi-granularity cross-modal align-\nment for generalized medical visual representation learning.\nInNIPS . 2, 3, 5, 6, 7, 8, 1\n[57] Linda Wang, Zhong Qiu Lin, and Alexander Wong. Covid-\nnet: A tailored deep convolutional neural network design for\ndetection of covid-19 cases from chest x-ray images. Scien-\ntific reports , 10(1):1–12, 2020. 7\n[58] Xiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhengyan\nZhang, Zhiyuan Liu, Juanzi Li, and Jian Tang. Kepler: A\nunified model for knowledge embedding and pre-trained lan-\nguage representation. TACL , 9:176–194, 2021. 2\n[59] Zhaobin Wang, E Wang, and Ying Zhu. Image segmenta-\ntion evaluation: a survey of methods. Artificial Intelligence\nReview , 53:5637–5674, 2020. 7\n11713\n[60] Chen Wei, Haoqi Fan, Saining Xie, Chao-Yuan Wu, Alan\nYuille, and Christoph Feichtenhofer. Masked feature predic-\ntion for self-supervised visual pre-training. In CVPR , pages\n14668–14678, 2022. 1\n[61] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron\nCourville, Ruslan Salakhudinov, Rich Zemel, and Yoshua\nBengio. Show, attend and tell: Neural image caption gen-\neration with visual attention. In ICML , pages 2048–2057.\nPMLR, 2015. 8\n[62] Fei Yu, Jiji Tang, Weichong Yin, Yu Sun, Hao Tian, Hua Wu,\nand Haifeng Wang. Ernie-vil: Knowledge enhanced vision-\nlanguage representations through scene graphs. In AAAI ,\npages 3208–3216, 2021. 2\n[63] Anna Zawacki, Carol Wu, George Shih, Julia Elliott, Mikhail\nFomitchev, Mohannad Hussain, Paras Lakhani, Phil Culli-\nton, and Shunxing Bao. Siim-acr pneumothorax segmenta-\ntion, 2019. 7, 2\n[64] Yuhao Zhang, Hang Jiang, Yasuhide Miura, Christopher D\nManning, and Curtis P Langlotz. Contrastive learning of\nmedical visual representations from paired images and text.\nInMLHC , pages 2–25. PMLR, 2022. 1, 2, 6, 7, 8\n[65] Zizhao Zhang, Pingjun Chen, Manish Sapkota, and Lin\nYang. Tandemnet: Distilling knowledge from medical im-\nages using diagnostic reports as optional semantic refer-\nences. In MICCAI , pages 320–328. Springer, 2017. 1\n[66] Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong\nSun, and Qun Liu. Ernie: Enhanced language representation\nwith informative entities. In ACL, pages 1441–1451, 2019.\n2\n[67] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu,\nZekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao\nXiang, Philip HS Torr, et al. Rethinking semantic segmen-\ntation from a sequence-to-sequence perspective with trans-\nformers. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition , pages 6881–6890,\n2021. 2\n11714'}, 'dist': 0.9286905527114868}
Result 18: {'text': 'data', 'metadata': {'created_at': 1736932570, 'modified_at': 1736932570, 'owner': 'sayandeep', 'path': 'RAG_CONTENT/conferences/cvpr_ocr/Chen_Fair_Federated_Learning_under_Domain_Skew_with_Local_Consistency_and_CVPR_2024_paper.txt', 'size': 49476, 'seen_at': 1737191136, 'data': 'Fair Federated Learning under Domain Skew with\r\nLocal Consistency and Domain Diversity\r\nYuhang Chen1∗Wenke Huang1∗Mang Ye1,2†\r\n1National Engineering Research Center for Multimedia Software,\r\nSchool of Computer Science, Wuhan University, Wuhan, China\r\n2Taikang Center for Life and Medical Sciences, Wuhan University, Wuhan, China\r\n{yhchen0,wenkehuang,yemang }@whu.edu.cn\r\nhttps://github.com/yuhangchen0/FedHEAL\r\nAbstract\r\nFederated learning (FL) has emerged as a new paradigm\r\nfor privacy-preserving collaborative training. Under do-\r\nmain skew, the current FL approaches are biased and face\r\ntwo fairness problems. 1) Parameter Update Conflict: data\r\ndisparity among clients leads to varying parameter impor-\r\ntance and inconsistent update directions. These two dis-\r\nparities cause important parameters to potentially be over-\r\nwhelmed by unimportant ones of dominant updates. It con-\r\nsequently results in significant performance decreases for\r\nlower-performing clients. 2) Model Aggregation Bias: ex-\r\nisting FL approaches introduce unfair weight allocation\r\nand neglect domain diversity. It leads to biased model\r\nconvergence objective and distinct performance among do-\r\nmains. We discover a pronounced directional update con-\r\nsistency in Federated Learning and propose a novel frame-\r\nwork to tackle above issues. First, leveraging the discovered\r\ncharacteristic, we selectively discard unimportant parame-\r\nter updates to prevent updates from clients with lower per-\r\nformance overwhelmed by unimportant parameters, result-\r\ning in fairer generalization performance. Second, we pro-\r\npose a fair aggregation objective to prevent global model\r\nbias towards some domains, ensuring that the global model\r\ncontinuously aligns with an unbiased model. The proposed\r\nmethod is generic and can be combined with other existing\r\nFL methods to enhance fairness. Comprehensive experi-\r\nments on Digits and Office-Caltech demonstrate the high\r\nfairness and performance of our method.\r\n1. Introduction\r\nFederated learning (FL) aims to collaboratively train a high-\r\nperformance model while maintaining data privacy [28, 36].\r\n*Equal contributions.†Corresponding author.\r\nAggregation Bias\r\n020406080100Global Model Client Client\r\nLow Decrease Significant DecreaseUpdate Conflict\r\ncounteractGlobal Model\r\nunimportant important\r\nWebcam+\r\nAmazon\r\nCaltech\r\nParameter 𝑖𝑖\r\nLocal Update Local Update\r\n30405060708090100FedAvg w/o HEAL FedAvg w HEAL\r\nUSPS                    MNIST                    SVHN                      SYNSTD:23.82 STD:22.08Figure 1. Problem illustration of Federated Learning under do-\r\nmain skew. Conventional FL methods ( ) exhibit potential perfor-\r\nmance disparities due to Parameter Update Conflicts and Model\r\nAggregation Bias. The former indicates that varying parameter\r\nimportance and inconsistent update directions lead to an un-\r\nfair decline in aggregated performance. The latter suggests biased\r\nconvergence objective, resulting in performance disparities . Our\r\nmethod ( ) achieves more equitable performance across different\r\ndomains while enhancing overall performance.\r\nThe foundational method, FedAvg [36], allows numerous\r\nparticipants to send their models to the server instead of\r\ndata. Then, the server aggregates these models into a\r\nglobal model and sends it back for further training. No-\r\ntably, a significant challenge in FL is data heterogeneity\r\n[20, 22, 28, 29, 55] , which means that client data appears in\r\na Non-IID (non-independently and identically distributed)\r\nmanner [32, 50, 54, 56, 59]. One particular heterogeneity\r\ntype, domain skew [12, 16, 31, 55], refers to the client data\r\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\r\nExcept for this watermark, it is identical to the accepted version;\r\nthe final published version of the proceedings is available on IEEE Xplore.\r\n12077\r\nbeing sampled from various domains , leading to different\r\nfeature distributions for each client.\r\nUnder domain skew, the local data are sampled from\r\nmultiple domains, resulting in a significant disparity in dis-\r\ntributed data. This disparity introduces challenges of feder-\r\nated convergent inconsistency. Meanwhile, federated learn-\r\ning aims to achieve a lower overall loss [36]. These two\r\nfactors lead to FL being biased toward domains with easier\r\nconvergence, further resulting in the neglect of other do-\r\nmains. This bias leads to distinct performance among do-\r\nmains. However, if some clients feel undervalued, their mo-\r\ntivation to participate in the federation will diminish, lead-\r\ning to a narrow scope of knowledge in the federation, hin-\r\ndering its growth and contradicting original intent of FL.\r\nThis issue gives rise to a pivotal challenge in FL: Perfor-\r\nmance Fairness [17, 45], which aims to ensure the uni-\r\nform performance across different clients without neglect-\r\ning clients with inferior performance. The fairness issue\r\nis highlighted in Fig. 1, where preliminary methods might\r\noverfit some domains, leading to poor performance in other\r\ndomains. We argue that two primary reasons underlie this\r\nfairness issue: I Parameter Update Conflict :The incon-\r\nsistent parameter update directions and varying parameter\r\nimportance lead to conflicts between important and unim-\r\nportant parameters, degrading the performance of clients\r\nwith poorer results. Due to domain skew, there can be in-\r\nconsistencies in parameter update directions among clients.\r\nFurthermore, some parameters of the neural network are\r\nmore important to specific data [24, 46, 53], meaning that\r\nchanges in these parameters have a larger impact on perfor-\r\nmance. Domain skew results in varying parameter impor-\r\ntance. So important updates from poor-performing client\r\nmay be potentially overwhelmed by unimportant aspects of\r\nothers. But the latter can not signally boost performance.\r\nSo it finally leads to performance disparity. II Model Ag-\r\ngregation Bias :The general weighting distribution method\r\nis biased and neglects domain diversity, resulting in un-\r\nfair convergence objective and performance disparity. In\r\nconventional FL methods [29, 36], the strategy of weight-\r\ning proportional to sample quantity [36] hinders the global\r\nmodel from adequately learning from domains with few\r\nsamples. Alternatively, equal weighting overly emphasizes\r\nclients with fewer samples. Both strategies introduce biases\r\nand amplifying performance diversity. This bias disregards\r\nthe data diversity among different domains.\r\nTo address these issues, we present a novel solu-\r\ntion, Federated Parameter- Harmonized and Aggregation-\r\nEquAlized Learning (FedHEAL). For problem I, we ob-\r\nserve notable consistency in parameters updating during\r\nthe local training. Specifically, due to the unique do-\r\nmain knowledge, some parameters are consistently pushed\r\nto the same direction ( i.e., increment and decrement) dur-\r\ning local training across consecutive rounds, as detailed inSec. 3.3.1. Parameters with strong consistency occur be-\r\ncause the global model fails to adapt to certain domains,\r\nleading to repeated adjustments in the same direction. Mo-\r\ntivated by this, we argue that parameters with strong con-\r\nsistency are more crucial for specific domain. To mitigate\r\nparameter update conflict, we aim to prevent unimportant\r\nparameters from nullifying the crucial updates of domains\r\nwith poor performance. By including only essential param-\r\neters, we mitigate important updates from being drowned\r\nout by less important ones, thus promoting Performance\r\nFairness across multiple domains and clients.\r\nFor problem II, we argue that more diverse domains re-\r\nsult in larger model changes during local training. To ac-\r\ncount for the data diversity, we propose an optimization\r\nobjective that minimizes the variance of distances between\r\nthe global model and all the local models. By reducing the\r\nvariance, the global model maintains uniform distances to\r\nall the local models, preventing bias towards any clients.\r\nHowever, the extensive parameters in neural networks and\r\nthe large number of clients in FL make it computationally\r\nintensive. Thus, we propose a simplified method approxi-\r\nmately aligned with this fairness objective. Details of our\r\nsimplified approach are elaborated in Sec. 3.3.2.\r\nIn this paper, FedHEAL consists of two components.\r\nFirst , by discarding unimportant parameters, we mitigate\r\nconflict among parameter updates. Second , we present a\r\nfair aggregation objective and a simplified implementation\r\nto prevent global model bias towards some domains. Fed-\r\nHEAL ultimately achieves Performance Fairness under do-\r\nmain skew. Since our approach only focuses on aggrega-\r\ntion, it can be easily integrated with most existing FL meth-\r\nods. Our main contributions are summarized as follows:\r\n• We identify the parameter update consistency in FL and\r\nintroduce a partial parameter update method to update\r\nonly parameters significant to the local domain, enhanc-\r\ning fair performance across domains.\r\n• We propose a new fair federated aggregation objective\r\nand a practical approach to consider the domain diversity\r\nto improve Performance Fairness.\r\n• We conduct comprehensive experiments on the Digits\r\n[18, 25, 40, 43] and Office-Caltech [9] datasets, providing\r\nevidence of effectiveness of our method through ablation\r\nstudies and integrations with existing methods.\r\n2. Related Work\r\n2.1. Federated Learning with Data Heterogeneity\r\nFederated learning aims to collaboratively train models\r\nwithout centralizing data to protect privacy. The pioneer-\r\ning work, FedAvg [36], trains a global model by aggregat-\r\ning participants’ local model parameter. However, FedAvg\r\nis primarily designed for homogeneous data, and its perfor-\r\nmance degrades under data heterogeneity. Numerous meth-\r\n12078\r\nNotation Description Notation Description\r\nm Client Index pm Client Weight\r\ni Parameter Index qm,i Parameter Weight\r\nM Client V olume lm,i Increment Proportion\r\nG Parameter V olume cm,i PUC of Parameter\r\nNm Sample Size ∆wt\r\nm Model Update\r\nWtGlobal Model ∆wt\r\nm,i Parameter Update\r\nwt\r\nm Client Model dm Model Distance\r\nτ Importance Threshold β Update Momentum\r\nTable 1. Notation table of this paper.\r\nods based on FedAvg have emerged to address data hetero-\r\ngeneity [7, 8, 13, 14, 34, 44]. FedProx [29], SCAFFOLD\r\n[22] and FedDyn [1] constrain local updates by adding\r\npenalty terms. FedProto [48] and MOON [26] enhance the\r\nalignment between client-side training at the feature level.\r\nHowever, these methods overlook the issue of domain skew,\r\nleading to diminished performance in multi-domain scenar-\r\nios. Some methods have now been developed to address do-\r\nmain skew, such as FedBN [31] and FCCL [12, 15]. How-\r\never, these methods focus on personalized models rather\r\nthan shared models, the latter of which requires additional\r\npublic datasets. FPL [16] focuses on addressing domain\r\nskew but requires each client to upload high-level feature\r\ninformation, contradicting the privacy-preserving nature of\r\nFL. FedGA [58] and FedDG [33] focus on the problem of\r\nunseen domain generalization, but the former requires an\r\nadditional validation set, and the latter involves transmit-\r\nting data information among multiple clients, posing pri-\r\nvacy leakage concerns. In this paper, FedHEAL does not\r\nrequire any additional datasets or the transmission of addi-\r\ntional signal information. We solely focus on the most fun-\r\ndamental transmitted information in FL: the model updates\r\nthemselves, to extract the necessary information for enhanc-\r\ning Performance Fairness in multi-domain scenarios.\r\n2.2. Fair Federated Learning\r\nThe fairness in FL is currently of widespread interest\r\n[4, 11, 41]. The mainstream categorizations of federated\r\nfairness fall into three classes [19, 45]: Performance Fair-\r\nness [19, 27, 30, 38], Individual/Group Fairness [5, 6, 57],\r\nand Collaborative Fairness [19, 35, 42, 52, 60]. Perfor-\r\nmance fairness ensures that all participants experience sim-\r\nilar and equitable performance improvements. Individ-\r\nual/Group Fairness aims to minimize model bias towards\r\nspecific attributes( e.g., gender). Collaborative Fairness en-\r\nsures that participants are rewarded in proportion to contri-\r\nbutions. This paper primarily addresses the issue of Perfor-\r\nmance Fairness. AFL [38] utilizes a min-max optimization\r\nto boost the performance of the worst-performing clients.\r\nq-Fedavg [27] recalibrates the aggregate loss by assigning\r\nhigher weights to devices with higher losses to enhance per-\r\nformance fairness. FedFV [49] uses the cosine similarity\r\nto detect and eliminate gradient conflicts to achieve Per-\r\nformance Fairness. But they are tailored for label skewNon-IID data and do not consider domain skew. Ditto [30]\r\nenhances Performance Fairness by incorporating a penalty\r\nterm but employs a personalized model instead of a shared\r\nmodel. FedCE [19] addresses both Performance Fairness\r\nand Collaborative Fairness but necessitates an additional\r\nvalidation set, a requirement that is challenging to meet\r\ngiven the scarcity of client data in FL. Our method is tai-\r\nlored for Performance Fairness under domain skew and con-\r\nsider both the domain diversity. It can be easily integrated\r\nwith existing methods to enhance their fairness.\r\n3. Methodology\r\n3.1. Preliminaries\r\nFederated Learning . Following typical Federated Learn-\r\ning setup [29, 36, 37], we consider there are Mclients\r\n(indexed by m). Each client holds private data Dm=\r\n{xi, yi}Nm\r\ni=1, where Nmrepresents the data size of client m.\r\nThe optimization objective of FL is to minimize global loss:\r\nmin\r\nwF(w) =MX\r\nm=1pmfm(w), (1)\r\nwhere fm(w) =1\r\nNmPNm\r\ni=1L(xi, yi;w),pmis the weight\r\nof client. L(xi, yi;w)is the loss of data (xi, yi)with model\r\nparameters w. Each client updates its model locally, and the\r\nserver then aggregates model updates from all clients.\r\nDomain Skew . In heterogeneous federated learning, do-\r\nmain skew among private data occurs when the marginal\r\ndistribution of labels P(y)is consistent across clients, but\r\nthe conditional distribution of features given labels P(x|y)\r\nvaries among different clients [3, 12, 16, 21, 31, 51]:\r\nPm(x|y)̸=Pn(x|y)while Pm(y) =Pn(y). (2)\r\n3.2. Motivation\r\nObservation of Parameter Update Consistency . In het-\r\nerogeneous federated learning, grasping the characteristic\r\nof model updates across clients is crucial. This study intro-\r\nduces a novel observation, termed as Parameter Update\r\nConsistency (PUC), observed during local training phases.\r\nThrough a toy experiment involving 4 clients, each sam-\r\npling from distinct domains and training with a ResNet-10\r\nnetwork, we observe a significant Parameter Update Con-\r\nsistency during local training. As shown in Fig. 2, our\r\nfindings indicate that a substantial proportion of parame-\r\nters maintain consistent update directions in consecutive\r\nrounds of training. Specifically, most parameters demon-\r\nstrate significant update consistency. This consistency is no-\r\nticeable in shorter consecutive rounds (10 rounds) but per-\r\nsists even in the longer term (100 rounds), underscoring its\r\nenduring nature. Furthermore, the consistency observed in\r\nthe last 10 rounds reveals that PUC remains prominent as\r\nthe global model converges, indicating that the converged\r\nglobal model has not adapted to specific domains . These\r\n12079\r\n00.10.20.30.40.50.60.7\r\n0% 10% 20% 30% 40% 50% 60% 70% 80% 90%\r\nConsistencyLast 10 roundsLocal Update00.10.20.30.40.50.6\r\n0% 10% 20% 30% 40% 50% 60% 70% 80% 90%All 100 rounds\r\nround 0\r\nround 1\r\nround 𝑡𝑡Other Client\r\nParameter 𝑖𝑖Client 𝑚𝑚\r\nParameter 𝑖𝑖\r\nConsistent Update DirectionAggregateIncrement Decrement\r\nImportantParameter ProportionFigure 2. Illustration of Parameter Update Consistency . The\r\nconsistency of parameter updates is displayed over 10 and 100\r\nrounds for a randomly selected layer of the client model update.\r\nA significant proportion of parameters maintain a consistent up-\r\ndate direction, i.e., almost half of the parameters show the same\r\ndirection for over 90 of the 100 rounds, indicating a persistent ten-\r\ndency to steer the global model in a fixed direction.\r\nobservations suggest a potential global model bias towards\r\nother domains, emphasizing the necessity for strategies that\r\nmitigate such biases and ensure fairer model aggregation.\r\nInspired by our observations and insights, to tackle Prob-\r\nlemI, we categorize parameters into two distinct classes:\r\nimportant and unimportant. Important parameters are char-\r\nacterized by stable update directions in consecutive rounds,\r\nindicating consistent learning behavior within specific do-\r\nmains and inadequacy of the global model in fitting those\r\ndomains. These parameters are deemed crucial for offer-\r\ning greater contribution in performance improvement. Con-\r\nversely, unimportant parameters exhibit distinct directional\r\nchanges across rounds which provide little contribution in\r\nperformance improvement. We argue that involving unim-\r\nportant parameters in the updates exacerbates the parameter\r\nupdate conflict, leading to a decline in the performance of\r\nsome domains and resulting in unfair performance. There-\r\nfore, to mitigate this, we aim to minimize the impact of\r\nunimportant updates, enhancing the performance of poorly\r\nperforming domains. During parameter aggregation, we\r\ndiscard unimportant parameters by setting their weights to\r\nzero, which prevents them from participating in aggregation\r\nand mitigates the parameter update conflict. Then we nor-\r\nmalize all weights of a local model parameter. Those with\r\na weight of zero remain at zero but the weights of impor-\r\ntant parameters increase, amplifying their influence on the\r\nglobal model and improving its adaptation to underperform-\r\ning domains. The proposed method is detailed in Sec. 3.3.1.\r\nTo address Problem II, we employ domain diversity as\r\nthe guiding metric for allocating weights. Our intuition\r\nis that more diverse data will undergo larger parameter\r\nchanges for the local model to adapt to that domain. The pa-\r\nrameter changes exhibit the fitting gap between the globalmodel and local model, implying the potential for perfor-\r\nmance improvement. A larger model changes suggests a\r\ndomain is overlooked and has greater space for performance\r\nenhancement, which can also be reflected in the magnitude\r\nof model parameter updates. The distance between model\r\niterations can represent the extent of parameter updates.\r\nThus, this issue can be addressed through an optimization\r\nobjective that minimizes the variance of distances between\r\nthe global model and each client model. By reducing the\r\nvariance, the global model maintains a more uniform dis-\r\ntance to each client. However, this optimization problem\r\nis computationally intensive, so we introduce a simplified\r\napproach. We modify the weight allocation strategy with a\r\nmomentum update [23, 47], assign greater weight to clients\r\nthat induce larger parameter changes. This method approx-\r\nimately aligns with our objective and draws the new global\r\nmodel closer to neglected clients, thereby reducing the vari-\r\nance. We elaborate the proposed method in Sec. 3.3.2.\r\n3.3. Proposed Method\r\n3.3.1 Federated Parameter-Harmonized Learning\r\nIn a Federated Learning system with Mclients, the global\r\nmodel in round tis denoted as Wt. In round t, each client\r\ntrains their model on private data Dmto obtain local model\r\nwt\r\nm. The local model change is defined as ∆wt\r\nm=wt\r\nm−\r\nWt. The global model of next round, t+ 1, is then updated\r\nby aggregating the model updates of round t:\r\nWt+1=Wt+MX\r\nm=1pm∆wt\r\nm, (3)\r\nwhere pmis the aggregation weight of client m, typically\r\nproportion to local sample size( i.e.,pm=NmPM\r\nj=1Nj). In\r\nneural networks, wt\r\nmis potentially a large vector of param-\r\neters, and ∆wt\r\nmis a vector of parameter changes with the\r\nsame dimension. For simplicity, we disregard the internal\r\nstructure of the model and represent ∆wt\r\nmas\r\n∆wt\r\nm= [∆wt\r\nm,1,∆wt\r\nm,2, . . . , ∆wt\r\nm,G], (4)\r\nwhere Gdenotes the total number of parameters, and\r\n∆wt\r\nm,irepresents the change in the ithparameter of the\r\nmodel wt\r\nm. We aim to compute the PUC for each param-\r\neter across all clients. This computation characterizes how\r\nsignificantly the parameters of the global model are pushed\r\ntowards a consistent direction to fit specific domains. We\r\nmaintain a list of proportions for parameters that are in-\r\ncreasing for each client and then employ dynamic program-\r\nming [2] to update this list in each round. The list of incre-\r\nment proportions for client mis denoted as\r\nLm= [lm,1, lm,2, . . . , l m,G],\r\nlm,i=Pt−1\r\nj=0I(∆wj\r\nm,i≥0)\r\nt,(5)\r\nwhere I(·)is the indicator function and lm,iis the propor-\r\ntion of increasing parameter before round t. Applying Dy-\r\n12080\r\nConsistency Table Model Update Δ𝑤𝑤𝑚𝑚𝑡𝑡\r\nDiscard for 𝑐𝑐𝑚𝑚,𝑖𝑖<𝜏𝜏Remain for 𝑐𝑐𝑚𝑚,𝑖𝑖≥𝜏𝜏\r\n𝑞𝑞𝑚𝑚,𝑖𝑖=𝐼𝐼𝑐𝑐𝑚𝑚,𝑖𝑖≥𝜏𝜏𝑝𝑝𝑚𝑚\r\n∑𝑗𝑗=1𝑀𝑀𝐼𝐼𝑐𝑐𝑚𝑚,𝑖𝑖≥𝜏𝜏𝑝𝑝𝑗𝑗Increment Proportion\r\nServer Side\r\n𝑐𝑐𝑚𝑚𝑡𝑡\r\nΔ𝑤𝑤𝑚𝑚0Δ𝑤𝑤𝑚𝑚𝑡𝑡−1+\r\n-+-\r\nUpdate History𝑙𝑙𝑚𝑚,𝑖𝑖𝐿𝐿𝑚𝑚𝑡𝑡−1𝐿𝐿𝑚𝑚𝑡𝑡Server Side\r\n+\r\n+\r\n+-\r\nServer Receive\r\n𝑐𝑐𝑚𝑚,𝑖𝑖: Consistency of    \r\nParameter 𝑖𝑖\r\n𝑞𝑞𝑚𝑚Δ𝑤𝑤𝑚𝑚𝑡𝑡𝑞𝑞𝑛𝑛Δ𝑤𝑤𝑛𝑛𝑡𝑡\r\n𝑞𝑞𝑐𝑐Δ𝑤𝑤𝑐𝑐𝑡𝑡𝑝𝑝𝑚𝑚↓𝑝𝑝𝑛𝑛↑\r\n𝑝𝑝c↑𝑑𝑑𝑚𝑚Eq. (9)\r\nEq. (5)Eq. (6) Eq. (7)\r\nObtain dmby Eq. (11)Update pmby Eq. (12)\r\n(a) Federated Parameter-Harmonized Learning (b) Federated Aggregation-Equalized Learning\r\nFigure 3. Architecture illustration of FedHEAL. Clients send local model updates to the server. In FPHL (Sec. 3.3.1), the server maintains\r\na consistency table, computes the consistency of current updates with past directions and discards updates with low consistency. Then in\r\nFAEL (Sec. 3.3.2), server minimized the variance of distance between global model and client model to mitigate model aggregation bias.\r\nnamic Programming [2], the lm,iin the round tcan be cal-\r\nculated from the previous round’s result:\r\nlm,i←lm,i∗(t−1) +I(∆wt\r\nm,i≥0)\r\nt. (6)\r\nThen the PUC of ithparameter can be expressed as\r\ncm,i=PUC (wm,i) =(\r\nlm,i if∆wt\r\nm,i≥0,\r\n1−lm,i otherwise.(7)\r\nTo alleviate parameter update conflicts and mitigate the\r\nglobal model from experiencing unfair performance, we se-\r\nlect parameters with a strong PUC for retention and discard\r\nothers. We introduce a hyperparameter τfor this purpose.\r\nWe categorize the ithparameter as important if cm,i≥τ\r\nand as unimportant otherwise. Given the model aggrega-\r\ntion process in Eq. (3), the aggregation of ithparameter is\r\nWt+1\r\ni=Wt\r\ni+MX\r\nm=1pm∆wt\r\nm,i, (8)\r\nwhere Wt\r\niis the ithparameter of global model Wt. By\r\napplying our method, Eq. (8) can be reformulated as\r\nWt+1\r\ni=Wt\r\ni+MX\r\nm=1qt\r\nm,i∆wt\r\nm,i,\r\nqt\r\nm,i=I(cm,i≥τ)pmPM\r\nj=1I(cj,i≥τ)pj,(9)\r\nwhere pmis the aggregation weight of client mand all its\r\nparameters, qt\r\nm,izeros out the weights of insignificant pa-\r\nrameters and then further normalizes them to ensure the ag-\r\ngregation weights sum up to 1. Consequently, only impor-\r\ntant parameters will participate and unimportant updates no\r\nlonger impact important updates. By normalizing qt\r\nm,i, the\r\nproportion of important parameter updates is further ampli-fied, enhancing their contribution during aggregation.\r\n3.3.2 Federated Aggregation-Equalized Learning\r\nWe first define the distance between the global model and\r\nthe local model of client min round tasdm=∥U−wt\r\nm∥2\r\n2,\r\nwhere Uis the new global model and ∥·∥2\r\n2is the square of\r\nthe euclidean distance. To minimize the variance of dis-\r\ntances between the global model and each client model, we\r\nintroduce the following optimization objective:\r\nU∗\r\nt= arg min\r\nUVar({dm}M\r\nm=1)\r\n= arg min\r\nUVar({\r\rU−wt\r\nm\r\r2\r\n2}M\r\nm=1)\r\ns.t.U=MX\r\nm=1pmwt\r\nm,MX\r\nm=1pm= 1,and∀m, pm≥0,(10)\r\nwhere U∗\r\ntrepresents the unbiased global model. The time\r\ncomplexity of this optimization is O(qMG ), where qis\r\nthe number of iterations needed for convergence. However,\r\ncomputational resources are often limited in FL. Thus, we\r\npropose a simplified approach that reduces the time com-\r\nplexity to O(MG), requiring only a single distance calcula-\r\ntion for each client. Specifically, in round t, we measure the\r\ndistance between the trained client model and global model\r\nWtasdm=||∆wt\r\nm||2\r\n2. Notably, if we combine it with\r\nFPHL, dmcan be rewritten as\r\ndm=||GX\r\ni=1I(cm,i≥τ)·∆wt\r\nm,i||2\r\n2, (11)\r\nimplying that we only compute the important parameter\r\nchange distance which better reflects the alterations made\r\nfor specific domain. We then apply a momentum update\r\n12081\r\nstrategy [23, 47] to update the weight for each client:\r\n∆pt\r\nm= (1−β)∆pt−1\r\nm+βdmPM\r\nj=1dj,\r\npt\r\nm=pt−1\r\nm+ ∆pt\r\nm, pt\r\nm=pt\r\nmPM\r\nj=1pt\r\nj.(12)\r\nwhere βis a hyper-parameter, with larger values indicating\r\na more pronounced influence of the distance on pm. When\r\nβ= 0, the method degenerates to the FedAvg. At β= 1,\r\nthe weights are assigned purely based on the distance set of\r\ncurrent round. The simplified method strives to minimize\r\nthe variance, aligning with the unbiased global model.\r\n3.4. Discussion and Limitation\r\nThe key notations are summarized in Tab. 1 and the pseudo-\r\ncode of FedHEAL is presented in Algorithm 1.\r\nComparison with Analogous Methods . q-FFL [27] and\r\nFedCE [19] increase weights for poor-performing clients\r\nbased on single loss or accuracy metrics. However, in-\r\ncreasing the weights of these clients does not guarantee sig-\r\nnificant performance improvement. FAEL adjusts weights\r\nbased on the domain diversity, which implies fitting gap be-\r\ntween the global model and local models. It is a better way\r\nis to infer the effectiveness of weight modification based on\r\nthe potential space for performance improvement. Similar\r\nwork FedFV [49] alleviates model gradient conflicts. but it\r\nmodifies gradients based on cosine similarity and gradient\r\nprojection of the model, still allows less significant gradi-\r\nents to influence crucial ones. In contrast, FPHL, grounded\r\nin the observed PUC characteristics, selectively discards\r\nunimportant updates to safeguard the important ones. It\r\ndemonstrates targeted conflict resolution, leading to better\r\nfairness and higher overall performance.\r\nDiscussion on FPHL . FPHL selectively discards unimpor-\r\ntant parameters to reduce update conflicts, meaning it is\r\nparticularly effective in large-scale FL systems where such\r\nconflicts are more pronounced. Similar with other meth-\r\nods aimed at Performance Fairness [27, 38], FPHL can\r\nincrease the influence of clients with poorer performance,\r\nwhich may sometimes reduce the relative weight and per-\r\nformance of other clients. Yet, by discarding unimportant\r\nparameters uniformly, FPHL can diminish the adverse im-\r\npact of both poorly performing client updates and better-\r\nperforming clients to some extent. Consequently, while it\r\nsignificantly boosts the performance of the former, it may\r\nalso help to prevent performance decline in the latter. So it\r\nachieves higher average accuracy across domains.\r\nLimitation . Our method leverages the parameter update\r\nconsistency and the fitting gap between the global model\r\nand local models to guide parameter aggregation and client\r\naggregation weights. However, our method’s performance\r\nis sensitive to the selected hyperparameters. When a hyper-\r\nparameter is not selected properly, our method may become\r\nunstable. Additionally, our method is designed for scenar-Algorithm 1: FedHEAL\r\nInput: Communication rounds T, local epochs K,\r\nnumber of participants M,mthparticipant private\r\ndataDm, private model wm\r\nOutput: The final global model wT\r\nServer : initialize the global model w0and\r\nL0\r\nm= [0,0, . . . , 0]G\r\nfort= 0,1,2, ..., T −1do\r\nClient :\r\nform= 1,2, ..., M in parallel do\r\nwt\r\nm← Wt\r\nfork= 1,2, ...,Kdo\r\nwt\r\nm←wt\r\nm−η∇CE(wt\r\nm, Dm)\r\n∆wt\r\nm←wt\r\nm− Wt\r\nServer :\r\nqt, Lt←FedHEAL (Lt−1)\r\nfori= 1,2, . . . , G do\r\nWt+1\r\ni=Wt\r\ni+PM\r\nm=1qt\r\nm,i∆wt\r\nm,i\r\nFedHEAL (Lt−1):\r\nform= 1,2, . . . , M do\r\nfori= 1,2, . . . , G do\r\nlm,i←(lm,i,∆wt\r\nm,i)in Eq. (6)\r\ncm,i←(lm,i,∆wt\r\nm,i)in Eq. (7)\r\ndm←(cm,i,∆wt\r\nm,i)in Eq. (11)\r\nform= 1,2, . . . , M do\r\npt\r\nm,∆pt\r\nm←(pt−1\r\nm,∆pt−1\r\nm,∆wt\r\nm,i, Dt, β)in\r\nEq. (12)\r\nfori= 1,2, . . . , G do\r\nqt\r\nm,i←(pt\r\nm, ct\r\nm,i)in Eq. (9)\r\nreturn qt,Lt\r\nios where all clients share the same network architecture, so\r\nit may fail in cases where clients have different architectures\r\nand parameter update consistency cannot be assumed.\r\n4. Experiments\r\n4.1. Experiment Details\r\nDatasets . We evaluate our methods on two multi-domain\r\nimage classification tasks.\r\n• Digits [18, 25, 40, 43] includes four domains: MNIST,\r\nUSPS, SVHN and SYN, each with 10 categories.\r\n• Office-Caltech [9] includes four domains: Caltech, Ama-\r\nzon, Webcam, and DSLR, each with 10 categories.\r\nWe allocate 20 clients for each task and distribute an equal\r\nnumber of clients to each domain. We randomly sample a\r\ncertain proportion for each client from their datasets, based\r\non task difficulty and task size. Specifically, we sample 1%\r\nfor Digits and 10% for Office-Caltech. We fix the seed to\r\nensure reproduction of results. The example cases in each\r\ndomain are presented in Fig. 4.\r\nModel . For both classification tasks, we use ResNet-10 [10]\r\nas the shared model architecture for training.\r\n12082\r\nDigits Office-CaltechMethodsMNIST USPS SVHN SYN A VG↑↑↑STD↓↓↓ Amazon DSLR Caltech Webcam A VG↑↑↑STD↓↓↓\r\nFedAvg [36] 89.84 93.25 79.54 41.35 76.00 23.82 72.63 56.67 58.57 45.52 58.35 11.13\r\n+AFL [38] 90.59 95.83 75.13 44.42 76.49 23.12 64.21 65.37 57.50 48.28 58.83 7.84\r\n+q-FFL [27] 91.44 94.10 76.33 44.48 76.59 22.79 60.00 64.01 53.39 51.72 57.28 5.73\r\n+FedHEAL 90.27 95.69 79.94 46.45 78.09 22.08 67.90 66.00 59.28 66.21 64.85 3.80\r\nFedProx [29] 90.27 93.93 80.04 42.82 76.76 23.38 69.90 58.00 60.27 45.52 58.42 10.03\r\n+AFL [38] 92.86 96.17 74.47 42.22 76.43 24.72 68.10 62.67 59.29 52.41 60.62 6.57\r\n+q-FFL [27] 88.58 93.49 75.58 44.23 75.47 22.15 61.37 72.66 54.91 55.52 61.11 8.23\r\n+FedHEAL 89.06 95.52 79.44 46.67 77.67 21.70 66.11 72.67 57.50 67.59 65.97 6.30\r\nScaffold [22] 94.15 94.44 76.87 44.22 77.42 23.61 69.37 59.33 59.55 46.21 58.62 9.50\r\n+AFL [38] 91.77 96.05 78.60 46.39 78.20 22.47 66.42 63.33 59.11 49.31 59.54 7.45\r\n+q-FFL [27] 87.73 94.59 74.00 43.76 75.02 22.53 61.79 73.33 55.18 55.86 61.54 8.40\r\n+FedHEAL 92.68 96.25 78.54 47.72 78.80 22.08 64.11 67.99 55.18 62.41 62.42 5.37\r\nMOON [26] 90.46 92.65 80.48 40.58 76.04 24.23 74.00 59.33 60.63 46.90 60.21 11.08\r\n+AFL [38] 91.25 96.03 75.31 44.34 76.73 23.34 66.74 67.33 60.80 55.17 62.51 5.71\r\n+q-FFL [27] 90.43 94.84 76.48 43.95 76.42 23.02 64.32 65.33 54.28 61.03 61.24 4.99\r\n+FedHEAL 91.34 94.94 81.32 44.96 78.14 22.86 67.68 65.33 59.11 64.14 64.07 3.62\r\nFedDyn [1] 91.23 92.36 80.15 41.55 76.32 23.83 71.16 62.00 59.20 48.62 60.24 9.28\r\n+AFL [38] 92.11 96.10 71.46 41.52 75.30 24.97 70.10 58.67 59.82 51.03 59.91 7.84\r\n+q-FFL [27] 92.53 95.17 76.37 44.75 77.20 23.18 62.10 67.33 54.82 56.21 60.12 5.76\r\n+FedHEAL 89.87 95.00 80.18 44.23 77.32 22.90 67.47 60.66 59.02 54.83 60.50 5.26\r\nFedProc [39] 91.86 91.16 78.54 39.87 75.36 24.44 60.21 46.00 55.98 46.90 52.27 6.95\r\n+AFL [38] 87.85 94.28 78.52 41.54 75.55 23.58 52.63 52.67 55.09 43.45 50.96 5.14\r\n+q-FFL [27] 92.09 92.09 74.97 45.21 76.15 22.17 65.79 42.01 55.80 50.69 53.57 9.94\r\n+FedHEAL 94.23 92.93 81.43 48.67 79.31 21.22 67.58 66.00 56.79 61.38 62.94 4.87\r\nFedProto [48] 89.99 92.90 81.09 40.93 76.23 24.06 71.48 42.67 62.23 60.34 59.18 12.04\r\n+AFL [38] 85.27 92.90 67.16 42.36 71.92 22.47 70.74 56.67 57.77 79.65 66.21 11.01\r\n+q-FFL [27] 93.35 94.92 77.08 46.31 77.91 22.56 72.74 54.67 64.20 82.76 68.59 11.99\r\n+FedHEAL 88.49 94.62 81.39 48.46 78.24 20.58 75.68 76.00 65.18 80.34 74.30 6.44\r\nTable 2. Comparison of Average Accuracy( A VG ) and Standard Deviation( STD ) with AFL [38] and q-FFL [27]. See details in Sec. 4.3.\r\nMNIST(M) USPS(U)\r\nSVHN(SV) SYN(SY)\r\n(a) Digits\r\nAmazon(A) Caltech(C)\r\nWebcam(W) Dslr(D)\r\n (b) Office-Caltech\r\nFigure 4. Example cases in Digits [18, 25, 40, 43], Office-Caltech\r\n[9] tasks. Please see details in Sec. 4.1.\r\nComparison Methods . We compare FedHEAL with FL\r\nbaseline FedAvg [36] and existing solutions for Perfor-\r\nmance Fairness: AFL [38], q-FFL [27] (both integrable),\r\nFedFV [49], Ditto [30] (two independent methods, with per-\r\nsonalized models aggregated into global model for Ditto).\r\nImplementation Details . All methods are implemented\r\nwith the same settings. We set the communication rounds to\r\n200 and the local epoch to 10. We use SGD as the optimizer\r\nwith a learning rate of 0.001. Its weight decay is 1e−5and\r\nmomentum is 0.9. The training batch size is 64for Digits\r\nand16for Office-Caltech. The hyper-parameter setting for\r\nFedHEAL presents in the Sec. 4.2.\r\nEvaluation Metrics . Following [27], we utilize the Top-\r\n1 accuracy and the standard deviation of accuracy across\r\nmulti-domains as evaluation metrics. A smaller standarddeviation indicates better Performance Fairness across dif-\r\nferent domains. We use the average results from the last five\r\nrounds accuracy and variance as the final performance.\r\nDigitsMethodsMNIST USPS SVHN SYN A VG↑↑↑STD↓↓↓\r\nDitto [30] 90.59 92.98 79.20 41.89 76.16 23.62\r\nFedFV [49] 91.76 94.70 77.26 44.14 76.97 23.17\r\nFedHEAL 90.27 95.69 79.94 46.45 78.09 22.08\r\nOffice-CaltechMethodsAmazon DSLR Caltech Webcam A VG↑↑↑STD↓↓↓\r\nDitto [30] 58.00 70.00 56.25 63.45 61.92 6.20\r\nFedFV [49] 62.95 71.33 55.36 60.00 62.41 6.72\r\nFedHEAL 67.90 66.00 59.28 66.21 64.85 3.80\r\nTable 3. Comparison of Average Accuracy( A VG ) and Standard\r\nDeviation( STD ) with Ditto [30] and FedFV [49]. Please refer to\r\nSec. 4.3 for detailed discussion.\r\n4.2. Diagnostic Analysis\r\nHyper-parameter Study . We show the impact of the\r\nhyper-parameters τ(Eq. (9)) and β(Eq. (12)) on the perfor-\r\nmance in Tab. 4 and Fig. 6, in Digits, optimal performance\r\nis achieved when β= 0.4andτ= 0.3. Similar experi-\r\nments on Office-Caltech yields β= 0.4andτ= 0.4as the\r\nbest settings. We use these hyper-parameters by default in\r\nsubsequent experiments.\r\n12083\r\n404550556065707580\r\n0 50 100 150Digits\r\nFedAvg\r\nAFL\r\nq-FFL\r\nFedFV\r\nDitto\r\nOursFigure 5. Comparison of convergence of average accuracy with\r\ncounterparts on Digits. Please see details in Sec. 4.3.\r\nβ 0.0 0.2 0.4 0.6 0.8 1.0\r\nA VG↑↑↑ 76.00 76.88 77.20 76.83 76.96 77.13\r\nSTD↓↓↓ 23.82 22.70 22.64 22.83 22.71 22.70\r\nTable 4. Hyper-parameter study with different β(Eq. (12)) on\r\nDigits datasets. See details in Sec. 4.2.\r\n78.02\r\n22.08\r\n20222426\r\n707274767880AccuracyAVG(↑) AVG Baseline\r\nSTD(↓) STD Baseline\r\n0.1             0.2             0.3 0.4             0.5\r\n(a) Digits\r\n64.85\r\n3.8\r\n0481216\r\n40455055606570\r\nStandard Deviation\r\n0.1             0.2             0.3             0.4 0.5 (b) Office-Caltech\r\nFigure 6. Hyper-parameter study with variant τ(Eq. (5)) when\r\nfixβ= 0.4. Please see details in Sec. 4.2.\r\nAblation Study . To provide a comprehensive analysis of\r\nthe effectiveness of FPHL and FAEL, we carried out an ab-\r\nlation study on both the Digits and Office-Caltech in Tab. 5.\r\nThey contribute positively to the performance enhancement\r\nand their combination results in optimal performance.\r\nCompatibility Study . To validate the compatibility of\r\nFedHEAL, we compared the results of several widely-\r\nadopted FL methods, FedAvg [36], FedProx [29], Scaffold\r\n[22], MOON [26], FedDyn [1], FedProc [39], FedProto\r\n[48] without and with FedHEAL. The results are shown in\r\nTab. 2. They reveal tangible benefits offered by our system,\r\ni.e., FedProto [48] with FedHEAL achieves 5.60% reduc-\r\ntion in STD and 15.12% increase in A VG on Office-Caltech.\r\nWe plot the differences in convergence between the bench-\r\nmark without and with FedHEAL in Fig. 7, demonstrating\r\nfaster convergence and higher accuracy of FedHEAL.\r\n4.3. Comparison to State-of-the-Arts\r\nThe Tab. 2 and Tab. 3 shows the accuracy and standard de-\r\nviation at the end of communication with SOTAs that ad-\r\ndress Performance Fairness in FL. The results depict that\r\nour method outperforms counterparts in both standard devi-\r\nation and mean accuracy. This demonstrates that FedHEAL\r\n404550556065707580\r\n0 50 100 150Digits\r\nFedAvg\r\nFedAvg+HEAL\r\nFedProx\r\nFedProx+HEAL\r\nFedDyn\r\nFedDyn+HEAL\r\nFedProc\r\nFedProc+HEALFigure 7. Comparison of convergence of average accuracy with\r\nand without the integration of FedHEAL, across selected FL meth-\r\nods. Please see details in Sec. 4.2.\r\nDigitsFPHL FAELMNIST USPS SVHN SYN A VG↑↑↑STD↓↓↓\r\n89.84 93.25 79.54 41.35 76.00 23.82\r\n✓ 92.19 95.32 76.32 44.37 77.05 23.32\r\n✓ 90.05 95.16 78.76 44.83 77.20 22.64\r\n✓ ✓ 90.27 95.69 79.94 46.45 78.09 22.08\r\nOffice-CaltechFPHL FAELAmazon DSLR Caltech Webcam A VG↑↑↑STD↓↓↓\r\n72.63 56.67 58.57 45.52 58.35 11.13\r\n✓ 68.42 66.00 57.95 66.55 64.73 4.64\r\n✓ 66.73 63.33 57.59 53.10 60.19 6.05\r\n✓ ✓ 67.90 66.00 59.28 66.21 64.85 3.80\r\nTable 5. Ablation study on Digits and Office-Caltech. Pleaese\r\nrefer to Sec. 4.2 for detailed discussion.\r\nachieves better Performance Fairness and further improves\r\naccuracy across multiple domains. We plot the average ac-\r\ncuracy at each epoch in Fig. 5, which illustrates the faster\r\nconvergence of FedHEAL.\r\n5. Conclusion\r\nIn this paper, we address Performance Fairness in federated\r\nlearning with domain skew by tackling parameter update\r\nconflicts and model aggregation bias. We discover a prop-\r\nerty in federated learning which we term Parameter Update\r\nConsistency . Leveraging this characteristic, we propose a\r\nsimple yet effective approach. By discarding unimportant\r\nparameters, FedHEAL alleviates parameter update conflicts\r\nfor poor-performing clients. Moreover, considering domain\r\ndiversity, we reduce the variance of distances between the\r\nglobal model and local models, addressing model aggrega-\r\ntion bias. Extensive experiments demonstrate the effective-\r\nness and compatibility of FedHEAL. We believe that this\r\nnewly discovered property and our work will offer fresh re-\r\nsearch directions and insights for the community.\r\nAcknowledgement. This work is supported by Na-\r\ntional Natural Science Foundation of China under Grant\r\n(62361166629, 62176188, 62272354).\r\n12084\r\nReferences\r\n[1] Durmus Alp Emre Acar, Yue Zhao, Ramon Matas, Matthew\r\nMattina, Paul Whatmough, and Venkatesh Saligrama. Fed-\r\nerated learning based on dynamic regularization. In ICLR ,\r\n2021. 3, 7, 8\r\n[2] Richard Bellman. Dynamic Programming . Princeton Uni-\r\nversity Press, Princeton, NJ, 1957. 4, 5\r\n[3] Debora Caldarola, Massimiliano Mancini, Fabio Galasso,\r\nMarco Ciccone, Emanuele Rodol `a, and Barbara Caputo.\r\nCluster-driven graph federated learning over multiple do-\r\nmains. In CVPRW , pages 2749–2758, 2021. 3\r\n[4] Huiqiang Chen, Tianqing Zhu, Tao Zhang, Wanlei Zhou, and\r\nPhilip S Yu. Privacy and fairness in federated learning: on\r\nthe perspective of trade-off. ACM Computing Surveys , 2023.\r\n3\r\n[5] Sen Cui, Weishen Pan, Jian Liang, Changshui Zhang, and Fei\r\nWang. Addressing algorithmic disparity and performance\r\ninconsistency in federated learning. NeurIPS , 34:26091–\r\n26102, 2021. 3\r\n[6] Yahya H Ezzeldin, Shen Yan, Chaoyang He, Emilio Ferrara,\r\nand A Salman Avestimehr. Fairfed: Enabling group fairness\r\nin federated learning. In AAAI , pages 7494–7502, 2023. 3\r\n[7] Xiuwen Fang and Mang Ye. Robust federated learning with\r\nnoisy and heterogeneous clients. In CVPR , pages 10072–\r\n10081, 2022. 3\r\n[8] Xiuwen Fang, Mang Ye, and Xiyuan Yang. Robust hetero-\r\ngeneous federated learning under data corruption. In ICCV ,\r\npages 5020–5030, 2023. 3\r\n[9] Boqing Gong, Yuan Shi, Fei Sha, and Kristen Grauman.\r\nGeodesic flow kernel for unsupervised domain adaptation.\r\nInCVPR , pages 2066–2073. IEEE, 2012. 2, 6, 7\r\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\r\nDeep residual learning for image recognition. In CVPR ,\r\npages 770–778, 2016. 6\r\n[11] Samuel Horvath, Stefanos Laskaridis, Mario Almeida, Ilias\r\nLeontiadis, Stylianos Venieris, and Nicholas Lane. Fjord:\r\nFair and accurate federated learning under heterogeneous tar-\r\ngets with ordered dropout. NeurIPS , 34:12876–12889, 2021.\r\n3\r\n[12] Wenke Huang, Mang Ye, and Bo Du. Learn from others and\r\nbe yourself in heterogeneous federated learning. In CVPR ,\r\npages 10143–10153, 2022. 1, 3\r\n[13] Wenke Huang, Mang Ye, Bo Du, and Xiang Gao. Few-\r\nshot model agnostic federated learning. In ACM MM , pages\r\n7309–7316, 2022. 3\r\n[14] Wenke Huang, Guancheng Wan, Mang Ye, and Bo Du. Fed-\r\nerated graph semantic and structural learning. In IJCAI ,\r\npages 139–143, 2023. 3\r\n[15] Wenke Huang, Mang Ye, Zekun Shi, and Bo Du. Generaliz-\r\nable heterogeneous federated cross-correlation and instance\r\nsimilarity learning. IEEE PAMI , 2023. 3\r\n[16] Wenke Huang, Mang Ye, Zekun Shi, He Li, and Bo Du. Re-\r\nthinking federated learning with domain shift: A prototype\r\nview. In CVPR , pages 16312–16322, 2023. 1, 3\r\n[17] Wenke Huang, Mang Ye, Zekun Shi, Guancheng Wan, He\r\nLi, Bo Du, and Qiang Yang. A federated learning for gen-eralization, robustness, fairness: A survey and benchmark.\r\narXiv , 2023. 2\r\n[18] Jonathan J. Hull. A database for handwritten text recognition\r\nresearch. IEEE PAMI , 16(5):550–554, 1994. 2, 6, 7\r\n[19] Meirui Jiang, Holger R Roth, Wenqi Li, Dong Yang, Can\r\nZhao, Vishwesh Nath, Daguang Xu, Qi Dou, and Ziyue Xu.\r\nFair federated medical image segmentation via client contri-\r\nbution estimation. In CVPR , pages 16302–16311, 2023. 3,\r\n6\r\n[20] Xuefeng Jiang, Sheng Sun, Yuwei Wang, and Min Liu. To-\r\nwards federated learning against noisy labels via local self-\r\nregularization. In CIKM , pages 862–873, 2022. 1\r\n[21] Peter Kairouz, H Brendan McMahan, Brendan Avent,\r\nAur´elien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista\r\nBonawitz, Zachary Charles, Graham Cormode, Rachel Cum-\r\nmings, et al. Advances and open problems in federated learn-\r\ning. Foundations and Trends® in Machine Learning , 14(1–\r\n2):1–210, 2021. 3\r\n[22] Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri,\r\nSashank Reddi, Sebastian Stich, and Ananda Theertha\r\nSuresh. Scaffold: Stochastic controlled averaging for fed-\r\nerated learning. In ICML , pages 5132–5143. PMLR, 2020.\r\n1, 3, 7, 8\r\n[23] Diederik P Kingma and Jimmy Ba. Adam: A method for\r\nstochastic optimization. In ICLR , 2014. 4, 6\r\n[24] Yann LeCun, John Denker, and Sara Solla. Optimal brain\r\ndamage. NeurIPS , 2, 1989. 2\r\n[25] Yann LeCun, L ´eon Bottou, Yoshua Bengio, and Patrick\r\nHaffner. Gradient-based learning applied to document recog-\r\nnition. Proceedings of the IEEE , 86(11):2278–2324, 1998.\r\n2, 6, 7\r\n[26] Qinbin Li, Bingsheng He, and Dawn Song. Model-\r\ncontrastive federated learning. In CVPR , pages 10713–\r\n10722, 2021. 3, 7, 8\r\n[27] Tian Li, Maziar Sanjabi, Ahmad Beirami, and Virginia\r\nSmith. Fair resource allocation in federated learning. In\r\nICLR , 2019. 3, 6, 7\r\n[28] Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia\r\nSmith. Federated learning: Challenges, methods, and future\r\ndirections. IEEE signal processing magazine , 37(3):50–60,\r\n2020. 1\r\n[29] Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi,\r\nAmeet Talwalkar, and Virginia Smith. Federated optimiza-\r\ntion in heterogeneous networks. MLSys , 2:429–450, 2020.\r\n1, 2, 3, 7, 8\r\n[30] Tian Li, Shengyuan Hu, Ahmad Beirami, and Virginia\r\nSmith. Ditto: Fair and robust federated learning through per-\r\nsonalization. In ICML , pages 6357–6368. PMLR, 2021. 3,\r\n7\r\n[31] Xiaoxiao Li, Meirui JIANG, Xiaofei Zhang, Michael Kamp,\r\nand Qi Dou. Fedbn: Federated learning on non-iid features\r\nvia local batch normalization. In ICLR , 2021. 1, 3\r\n[32] Zexi Li, Xinyi Shang, Rui He, Tao Lin, and Chao Wu. No\r\nfear of classifier biases: Neural collapse inspired federated\r\nlearning with synthetic and fixed classifier. In ICCV , pages\r\n5319–5329, 2023. 1\r\n12085\r\n[33] Quande Liu, Cheng Chen, Jing Qin, Qi Dou, and Pheng-Ann\r\nHeng. Feddg: Federated domain generalization on medical\r\nimage segmentation via episodic learning in continuous fre-\r\nquency space. In CVPR , pages 1013–1023, 2021. 3\r\n[34] Kangyang Luo, Xiang Li, Yunshi Lan, and Ming Gao.\r\nGradma: A gradient-memory-based accelerated federated\r\nlearning with alleviated catastrophic forgetting. In CVPR ,\r\npages 3708–3717, 2023. 3\r\n[35] Lingjuan Lyu, Xinyi Xu, Qian Wang, and Han Yu. Collab-\r\norative fairness in federated learning. Federated Learning:\r\nPrivacy and Incentive , pages 189–204, 2020. 3\r\n[36] Brendan McMahan, Eider Moore, Daniel Ramage, Seth\r\nHampson, and Blaise Aguera y Arcas. Communication-\r\nefficient learning of deep networks from decentralized data.\r\nInAISTATS , pages 1273–1282, 2017. 1, 2, 3, 7, 8\r\n[37] Jiaxu Miao, Zongxin Yang, Leilei Fan, and Yi Yang. Fed-\r\nseg: Class-heterogeneous federated learning for semantic\r\nsegmentation. In CVPR , pages 8042–8052, 2023. 3\r\n[38] Mehryar Mohri, Gary Sivek, and Ananda Theertha Suresh.\r\nAgnostic federated learning. In ICLR , pages 4615–4625.\r\nPMLR, 2019. 3, 6, 7\r\n[39] Xutong Mu, Yulong Shen, Ke Cheng, Xueli Geng, Jiaxuan\r\nFu, Tao Zhang, and Zhiwei Zhang. Fedproc: Prototypical\r\ncontrastive federated learning on non-iid data. arXiv preprint\r\narXiv:2109.12273 , 2021. 7, 8\r\n[40] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bis-\r\nsacco, Bo Wu, and Andrew Y Ng. Reading digits in natural\r\nimages with unsupervised feature learning. 2011. 2, 6, 7\r\n[41] Tao Qi, Fangzhao Wu, Chuhan Wu, Lingjuan Lyu, Tong Xu,\r\nHao Liao, Zhongliang Yang, Yongfeng Huang, and Xing\r\nXie. Fairvfl: A fair vertical federated learning framework\r\nwith contrastive adversarial learning. NeurIPS , 35:7852–\r\n7865, 2022. 3\r\n[42] Bhaskar Ray Chaudhury, Linyi Li, Mintong Kang, Bo Li,\r\nand Ruta Mehta. Fairness in federated learning via core-\r\nstability. NeurIPS , 35:5738–5750, 2022. 3\r\n[43] Prasun Roy, Subhankar Ghosh, Saumik Bhattacharya, and\r\nUmapada Pal. Effects of degradations on deep neural net-\r\nwork architectures. arXiv preprint arXiv:1807.10108 , 2018.\r\n2, 6, 7\r\n[44] Jiangming Shi, Shanshan Zheng, Xiangbo Yin, Yang Lu,\r\nYuan Xie, and Yanyun Qu. Clip-guided federated learn-\r\ning on heterogeneous and long-tailed data. arXiv preprint\r\narXiv:2312.08648 , 2023. 3\r\n[45] Yuxin Shi, Han Yu, and Cyril Leung. Towards fairness-aware\r\nfederated learning. IEEE TNNLS , 2023. 2, 3\r\n[46] Neta Shoham, Tomer Avidor, Aviv Keren, Nadav Israel,\r\nDaniel Benditkis, Liron Mor-Yosef, and Itai Zeitak. Over-\r\ncoming forgetting in federated learning on non-iid data.\r\narXiv preprint arXiv:1910.07796 , 2019. 2\r\n[47] Ilya Sutskever, James Martens, George Dahl, and Geoffrey\r\nHinton. On the importance of initialization and momentum\r\nin deep learning. In ICML , pages 1139–1147. PMLR, 2013.\r\n4, 6\r\n[48] Yue Tan, Guodong Long, Lu Liu, Tianyi Zhou, Qinghua Lu,\r\nJing Jiang, and Chengqi Zhang. Fedproto: Federated proto-\r\ntype learning across heterogeneous clients. In AAAI , pages\r\n8432–8440, 2022. 3, 7, 8[49] Zheng Wang, Xiaoliang Fan, Jianzhong Qi, Chenglu Wen,\r\nCheng Wang, and Rongshan Yu. Federated learning with\r\nfair averaging. IJCAI , 2021. 3, 6, 7\r\n[50] Nannan Wu, Li Yu, Xin Yang, Kwang-Ting Cheng, and\r\nZengqiang Yan. Fediic: Towards robust federated learning\r\nfor class-imbalanced medical image classification. In MIC-\r\nCAI, pages 692–702. Springer, 2023. 1\r\n[51] An Xu, Wenqi Li, Pengfei Guo, Dong Yang, Holger R Roth,\r\nAli Hatamizadeh, Can Zhao, Daguang Xu, Heng Huang, and\r\nZiyue Xu. Closing the generalization gap of cross-silo fed-\r\nerated medical image segmentation. In CVPR , pages 20866–\r\n20875, 2022. 3\r\n[52] Xinyi Xu, Lingjuan Lyu, Xingjun Ma, Chenglin Miao,\r\nChuan Sheng Foo, and Bryan Kian Hsiang Low. Gradient\r\ndriven rewards to guarantee fairness in collaborative machine\r\nlearning. NeurIPS , 34:16104–16117, 2021. 3\r\n[53] Xiyuan Yang, Wenke Huang, and Mang Ye. Dynamic per-\r\nsonalized federated learning with adaptive differential pri-\r\nvacy. In NeurIPS , 2023. 2\r\n[54] Xiyuan Yang, Wenke Huang, and Mang Ye. Fedas: Bridging\r\ninconsistency in personalized federated learning. In CVPR ,\r\n2024. 1\r\n[55] Mang Ye, Xiuwen Fang, Bo Du, Pong C Yuen, and Dacheng\r\nTao. Heterogeneous federated learning: State-of-the-art and\r\nresearch challenges. arXiv preprint arXiv:2307.10616 , 2023.\r\n1\r\n[56] Mang Ye, Wenke Huang, Zekun Shi, He Li, and Du Bo.\r\nRevisiting federated learning with label skew: An over-\r\nconfidence perspective. SCIS , 2024. 1\r\n[57] Yuchen Zeng, Hongxu Chen, and Kangwook Lee. Im-\r\nproving fairness via federated learning. arXiv preprint\r\narXiv:2110.15545 , 2021. 3\r\n[58] Ruipeng Zhang, Qinwei Xu, Jiangchao Yao, Ya Zhang, Qi\r\nTian, and Yanfeng Wang. Federated domain generalization\r\nwith generalization adjustment. In CVPR , pages 3954–3963,\r\n2023. 3\r\n[59] Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon\r\nCivin, and Vikas Chandra. Federated learning with non-iid\r\ndata. arXiv preprint arXiv:1806.00582 , 2018. 1\r\n[60] Zirui Zhou, Lingyang Chu, Changxin Liu, Lanjun Wang,\r\nJian Pei, and Yong Zhang. Towards fair federated learning.\r\nInACM SIGKDD , pages 4100–4101, 2021. 3\r\n12086'}, 'dist': 0.9286905527114868}
Result 19: {'text': 'data', 'metadata': {'created_at': 1736932571, 'modified_at': 1736932571, 'owner': 'sayandeep', 'path': 'RAG_CONTENT/conferences/cvpr_ocr/Li_UnionFormer_Unified-Learning_Transformer_with_Multi-View_Representation_for_Image_Manipulation_Detection_CVPR_2024_paper.txt', 'size': 53167, 'seen_at': 1737191136, 'data': "UnionFormer: Uniﬁed-Learning Transformer with Multi-View Representation for\nImage Manipulation Detection and Localization\nShuaibo Li1,2Wei Ma1†Jianwei Guo2Shibiao Xu3Benchong Li1Xiaopeng Zhang2\n1Beijing University of Technology\n2MAIS, Institute of Automation, Chinese Academy of Sciences\n3Beijing University of Posts and Telecommunications\nAbstract\nWe present UnionF ormer , a novel framework that inte-\ngrates tampering clues across three views by uniﬁed learning\nfor image manipulation detection and localization. Speciﬁ-\ncally, we construct a BSFI-Net to extract tampering features\nfrom RGB and noise views, achieving enhanced responsive-\nness to boundary artifacts while modulating spatial consis-\ntency at different scales. Additionally, to explore the incon-\nsistency between objects as a new view of clues, we combine\nobject consistency modeling with tampering detection and\nlocalization into a three-task uniﬁed learning process, allow-\ning them to promote and improve mutually. Therefore, we\nacquire a uniﬁed manipulation discriminative representation\nunder multi-scale supervision that consolidates information\nfrom three views. This integration facilitates highly effec-\ntive concurrent detection and localization of tampering. We\nperform extensive experiments on diverse datasets, and the\nresults show that the proposed approach outperforms state-\nof-the-art methods in tampering detection and localization.\n1. Introduction\nThe rapid progression of deep generative models, such\nas GANs [ 21,43,60], V AEs [ 31,50], and Diffusion Mod-\nels [ 10,45,53], has facilitated the widespread availability of\nArtiﬁcial Intelligence Generated Content (AIGC) tools [ 8].\nAt the same time, image editing tools have become exception-\nally user-friendly and powerful, capable of creating highly\nrealistic images and videos. This assists users in better ex-pressing their creativity but also intensiﬁes the malicioususe of editing techniques to tamper with multimedia con-tent, resulting in the proliferation of faked images on the\nInternet [ 57]. Therefore, developing a universally effective\nmethod to discern the authenticity of images and accurately\nlocate the modiﬁed regions has become crucial. Research on\nrelated algorithms has become a hot topic [ 3,28], and many\n†Corresponding author.state-of-the-art methods based on deep learning models have\nbeen proposed.\nDigital image tampering falls into three main cate-\ngories [ 19]: splicing, which involves copying regions from\none image to another; copy-move, entailing the copying or\nmoving of elements within the same image; and removal,\nthe process of erasing parts of an image and creating visual\nconsistent content to obscure the alterations. These manipu-\nlations leave traces between the tampered regions and their\nsurroundings, causing inconsistencies between the authentic\nand forgery regions. Unlike traditional detection or segmenta-\ntion tasks emphasizing high-level semantic information, im-\nage tampering detection prioritizes local semantic-agnostic\nclues that distinguish authenticity rather than semantic con-\ntent. Therefore, the critical challenge in tampering detec-\ntion is learning generalizable features that combine different\nlevel information and capture multiple scale inconsistencies\nbetween authentic and tampered areas. Previous methods pri-\nmarily utilized deep convolutional neural networks designed\nfor high-level visual tasks as feature encoders or directly con-nected features from different layers [\n23,27,40,71], which\ncould not adequately represent tampering traces. Inspiredby [\n9,12,67], we designed a Boundary Sensitive Feature\nInteraction Network (BSFI-Net) speciﬁcally for extracting\nforensics artifacts and integrated it as the feature encoder\nin our framework. BSFI-Net is a parallel CNN-Transformer\nstructure that can reinforce edge responses while effectively\ninteracting between local features and global representations\nto explore consistencies within images at different scales.\nOn the other hand, many tampering artifacts impercep-\ntible in the RGB view become distinctly noticeable in the\nnoise view. Employing ﬁxed [ 18] or learnable high-pass\nﬁlters [ 6,35,66] to convert RGB images into noise maps\ncan suppress content and highlight the low-level forgeryclues. Thus, developing a multi-view strategy that simulta-\nneously models the RGB and noise dimensions is essential\nto detect subtle tampering traces. Our framework adopts adual-stream architecture to independently construct repre-sentation for RGB and noise views, subsequently merging\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n12523\nthem to enhance discriminative capability and generalizabil-\nity. Furthermore, we incorporate contrastive supervision to\nimprove the collaboration between the two views.\nIn addition, to create spatially coherent and semantically\nconsistent images, tamper operations invariably alter entire\nobjects to conceal evidence, namely performing object-level\nmanipulation. Current advanced methods focus on pixel or\npatch-level consistencies, overlooking object-level informa-\ntion. Conversely, we argue that image manipulation detection\nshould extend beyond merely identifying out-of-distribution\npixels or patches to also capture the anomalies in object con-\nsistency and distribution resulting from manipulation. Due\nto hyper-realistic tampered images generated by diffusion\nmodels [ 4,5,20,30,44,65,69], leveraging object view\ninformation becomes particularly crucial. Diffusion-based\nmodels [ 4,30,44] repeatedly update initial noise across the\nimage, enhancing spatial continuity and leaving fewer RGB\nand noise traces. Moreover, unlike authentic image sources,\nauto-generated forgery portions guided by natural language\nprompts are more likely to exhibit object incongruities. Re-\ncent Diffusion models [ 20,29,55,64] have attempted to\nsolve this issue by employing object-centric approaches, un-\nderscoring the necessity and feasibility of object view clues\nfor tampering detection. However, creating and integrating\nsuch a novel view with others for tampering artifact rep-resentation presents a signiﬁcant challenge, requiring new\narchitectures and learning strategies.\nConsidering the above vital points, we introduce Union-\nFormer, a uniﬁed-learning transformer framework withmulti-view representation for image manipulation detec-\ntion and localization, as illustrated in Figure 1. Firstly, we\nuse BSFI-Net as the feature encoder to obtain the general-\nizable features under RGB and noise views and combinethem. Then, we utilize the fused features to conduct a uni-\nﬁed learning process, which includes three sub-tasks: object\nconsistency modeling, forgery detection, and localization. In\nuniﬁed learning, our model establishes the object view repre-\nsentation and integrates three view information into a uniﬁed\nmanipulation discriminative representation (UMDR) to si-\nmultaneously accomplish forgery detection and localization.\nTo summarize, our main contributions are as follows:\n•We propose UnionFormer, a novel image forensics\ntransformer framework. By employing uniﬁed learn-\ning with multi-scale supervision, the UnionFormer in-\ntegrates information from all three views to execute\nimage manipulation detection and localization simulta-\nneously.\n•We introduce BSFI-Net, a hybrid network structure\nfor superior artifact representation learning, which en-\nhances boundary response while revealing local incon-\nsistencies at different levels across domains.\n•With the uniﬁed learning of UMDR, we construct aninnovative object view representation capable of cap-\nturing the inconsistency among objects and aggregated\ninformation from three views for forgery detection.\n•We involve comprehensive experiments across vari-\nous benchmarks, demonstrating that our method attains\nstate-of-the-art results in both detection and localization\ntasks.\n2. Related Work\nForgery Artifacts Representation. Most early works\n[17,33,42] design hand-crafted features to characterize\ntampering traces, often detecting speciﬁc types of manip-ulation. However, in real-world scenarios, various editing\noperations are usually combined, and the types are unknown,\npromoting more work to focus on practical general tam-pering detection [\n13,23,27,59,62]. Achieving general\ndetection requires more generalizable and semantic-agnostic\nfeatures, so a series of works explore clues beyond the RGB\nview to capture a broader range of tampering traces. The\nmost common approach is to use ﬁxed [ 18] or learnable\n[6,34,66] ﬁlters to transform the image into the noise view\nto highlight weak low-artifacts. Some other works leverage\nfrequency-aware clues to provide a complementary view-point [\n49,54]. These low-level features are always com-\nbined with the high-level features from the RGB view for\nmore effective detection [ 23,27,34,36,62,70]. For in-\nstance, [ 13] employs dual attention to combine information\nfrom RGB and noise views. [ 59] extracts high-frequency\nfeatures of the images and combines them with RGB features\nas multimodal patch embedding. In contrast, we not only\ncombine tampering representations from both streams (RGBand noise views) but also facilitate their sufﬁcient interaction\nthrough contrastive supervision. Moreover, we incorporate a\nnovel view that models the inconsistencies between objects,\nproviding robust additional cues for manipulation detection.\nTransformer in Vision. Transformer [ 58] employs self-\nattention mechanisms to model long-range dependencies,and it has been widely successful in natural language pro-\ncessing (NLP). Some works are inspired to explore the use\nof transformer architecture for various computer vision tasks\nand showed superior performance. Speciﬁcally, ViT [ 16]\nreshapes images into patch sequences and feeds them intoa transformer encoder for image classiﬁcation. DETR [\n9]\nand Deformable DETR [ 72] implement end-to-end object\ndetection using a transformer encoder-decoder architecture\nwith learnable queries and bipartite matching. CMX [ 68]\nproposed a transformer framework for semantic segmenta-tion that integrates RGB and other modal information. Inthis work, we ﬁrst introduce a CNN-Transformer parallel\nencoder, BSFI-Net, for tampering feature extraction. Then,\nwe utilize a uniﬁed-learning transformer framework to in-\ntegrate multiple views information for image manipulation\n12524\nBSFI\nNet\nBSFI\nNetRPN\n…\n…Projection\nUMDR\nUnified Learning with Multi-Scale Supervision 8-(\x03<OK]5HPKIZ\x03<OK]\n4UOYK\x03<OK]\nMulti-head\nSelf-Attention\nFeed-forward\nNetworkPositional\nEncoding\nFeature Interaction Encoding Feature Contrastive Collaboration\nObject\nConsistency ClassifierReal\nFake\nLocalization \nMask\nTransformer Encoder\nFigure 1. An overview of UnionFormer. We achieve simultaneous tampering detection and localization by integrating tampering clues from\nthree view representations, with each view represented by a different color background. We obtain representations under the RGB and noise\nviews through BSFI-Net and construct the object view representation based on both in the uniﬁed learning. Meanwhile, information from all\nthree views is interactively fused into a uniﬁed manipulation discriminative representation (UMDR) for detection and localization.\ndetection and localization.\n3. Method\nIn this section, we ﬁrst provide an overview of Union-\nFormer and a detailed introduction to each component. We\naim to fully leverage rich artifacts from three views for simul-\ntaneous tampering detection and localization. We achieve\nthis through a uniﬁed learning process under multi-scalesupervision. As illustrated in Figure 1, input RGB image\nXis ﬁrstly transformed into a noise view representation\nN=C(X)using constrained CNN [ 7], which can reveal\nlow-level tampering. Then, both XandNare individually\nfed into the Boundary Sensitive Feature Interaction Networks\n(BSFI-Net) for feature encoding. High-frequency edge fea-\ntures (H) are incorporated with either XorNas inputs into\nthe BSFI-Net to boost edge responsiveness. This allows us\nto acquire generalizable and discriminative features under\nthe RGB and noise views, constructing two feature pyramids\nfr=E1(X,H),fn=E2(N,H). Subsequently, we use a\nRegion Proposal Network (RPN) [ 51] to obtain a set of\nRegions of Interest (RoIs), represented as pi, from the fea-\nturefr. RoI information is extracted from frandfn, then\nﬂattened to get embedding representations for proposals, de-\nnoted as ri,ni. The RGB feature riand noise feature ni\nfor each proposal are concatenated to generate the fusedproposal feature di, which is input into the\nItransformer\nEncoder layer.\nDuring the uniﬁed learning phase, we address three sub-\ntasks: modeling object consistencies, binary classiﬁcation\nof authenticity, and tampered region localization. After the\ntransformer encoder, the forgery-discriminative query em-\nbeddings DIare fed into the uniﬁed manipulation discrimi-\nnative representation part to generate three predictions forthree sub-tasks. As shown in Figure 1, we employ multi-scale supervision with a uniﬁed form for three sub-tasks,\nincluding Lcls,Locm , and Lloc.\n3.1. Feature Interaction Encoding\nRGB and Noise View Representation. We utilize a dual-\nstream structure to harness clues from both RGB and noise\nviews in the feature encoding stage. The RGB stream is de-\nsigned to capture visually apparent tampering artifacts, while\nthe noise stream aims to explore the distribution inconsisten-\ncies between tampered and genuine regions. We employ the\nlearnable constrained convolutional layer proposed in [ 7]t o\ntransform the RGB image into the noise view.\nAs noted in Section 2, the edges of tampered regions and\ntheir surroundings exhibit more prominent tampering clues.\nTherefore, we enhance high-frequency edge information in\nboth streams to concentrate the network’s response on tam-\npered regions. Speciﬁcally, we utilize the Discrete Cosine\nTransform (DCT) to convert the image data Xinto the fre-\nquency domain and then apply a high-pass ﬁlter to obtainthe high-frequency component. We then convert the high-\nfrequency component back to the spatial domain to facilitate\nfeature interaction and preserve local consistency. Thus, we\nget the edge-enhanced information Has follows:\nH=T−1\nd(Fh(Td(X),β)), (1)\nwhere Tdrepresents DCT, Fhrepresents the high-pass ﬁlter,\nandβis the threshold. We input XandNseparately into the\nBSFI-Net, along with Hfor feature encoding, as illustrated\nin Figure 2.\nBoundary Sensitive Feature Interaction Network. In\naddition to enhancing boundary responses, integrating lo-\ncal features and global representations is crucial for image\nforgery detection. This allows for a comprehensive analysis\n12525\nTransformer\nBlock\nConvolution\nBlockConvolution\nBlockTransformer\nBlockTransformer\nBlockTransformer\nBlockTransformer\nBlock\nConvolution\nBlockConvolution\nBlockConvolution\nBlockProjection\nFCU\nC2 C4\nFeature Pyramid \nNetworkC2\nC3\nC4\nC3Conv+BN\nSigmoid\nUp-Sample\nConv+BNFCU\nC5Conv+BN\nSigmoid\nUp-Sample\nConv+BN\nBOB BOB\n(9,/\x134KZ \x03(U[TJGX_\x039KTYOZO\\K\x03 ,KGZ[XK\x03/TZKXGIZOUT\x034KZ]UXQ\x03\nFigure 2. An overview of BSFI-Net. FCU represents the Feature Coupling Unit, and BOB represents the Boundary Oriented Block.\nFC\nMLP\nMLPGT MaskClass Label\nObject Annotations\nEncoding\n;3*8 \x03;TOLOKJ\x033GTOV[RGZOUT\x03*OYIXOSOTGZO\\K\x038KVXKYKTZGZOUT\x03\n\u0de0ࣦܲܿ ݏ݈ܿ\n\u0de0ܲ୭ ݉ܿ\u074bࣦ\n\u0de0ܿ\u074b݈ࣦ݉ܲ\nFigure 3. The learning of UMDR with multi-scale supervision.\nof inconsistencies within the image at various scales. In-\nspired by [ 48], we propose a CNN-Transformer concurrent\nnetwork called BSFI-Net, which maintains edge sensitivity\nwhile facilitating thorough interaction between features at\ndifferent scales in the two branches.\nAs shown in Figure 2, the CNN branch serves as the main\nbranch, taking an RGB or noise image as input to encode\nlocal information. The transformer branch, with input as\nedge enhancement information H, guides the CNN branch\nto focus on tampered regions and transmits long-distanceinconsistencies between image patches to it. We use theFeature Coupling Unit (FCU) proposed by [\n48] to elimi-\nnate the misalignment between feature maps from the CNN\nbranch and patch embeddings from the transformer branch.\nMoreover, we design a Boundary Oriented Block (BOB)to facilitate transmitting high-level patch consistency andboundary information from the transformer branch to the\nCNN branch, guiding the latter.\nThe CNN branch consists of ﬁve convolution blocks,\nsimilar to the ResNet construction [ 24]. Like [ 16,48],\nthe transformer branch consists of 5 repeated transformer\nblocks, consisting of a multi-head self-attention module and\nan MLP block. The same tokenization operation as ViT [ 16]\nis adopted. In FCU, 1×1 convolution and re-sampling are\nused to align channels and spatial dimensions before addingpatch embeddings and CNN features. In BOB, feature maps\nfrom the CNN branch are fed into a 1×1 convolution layer, abatch normalization layer, a sigmoid layer, and up-sampled\nto high resolution by bilinear interpolation. Then, the fea-\ntures from the CNN branch are subjected to an element-wise\nmultiplication with the long-distance discriminate weights.\nWe pre-train BSFI-Net as a feature encoder to generate RGB\nand noise view representation, and two feature pyramids fr,\nfnare produced by the Feature Pyramid Network [ 38] based\non the intermediate feature maps {C2,C3,C4,C5}. The\ntraining details are provided in Section 4.1.\n3.2. Feature Contrastive Collaboration\nIn the feature collaboration stage, inspired by [ 51,56],\nwe ﬁrst employ a Region Proposal Network (RPN) based on\nthe RGB feature pyramid frto generate a set of Regions of\nInterest (RoIs). Then, we utilize RoIAlign [ 25] to extract\nthe information of RoIs from the feature pyramids frand\nfnof two streams. In addition to feature concatenation, we\nemploy contrastive supervision to promote collaborationbetween two views. We treat the tampered proposals from\ndifferent streams as positive proposals, and the tampered\nproposals and authentic proposals are assigned as negative\npairs. Following the InfoNCE loss [ 47,67], the contrast loss\nis deﬁned as:\nLcon=−1\nN/summationdisplay\nilogexp(s0)\nexp(s0)+/summationtext\njexp(s1)\n−1\nN/summationdisplay\nilogexp(s0)\nexp(s0)+/summationtext\njexp(s2),(2)\nwheres0stands for the similarity between positive pairs, s1\ndenotes the similarity between RGB tampered embeddings\nand noise authentic embeddings, and s2signiﬁes the similar-\nity between RGB authentic embeddings and noise tampered\nembeddings. The contrastive loss Lcon is introduced into\nthe supervision of uniﬁed learning and will be discussed in\nSection 3.3.\n12526\n3.3. Uniﬁed Learning with Multi-Scale Supervision\nTransformer Encoder. Our Uniﬁed learning module is\nan encoder-only transformer architecture that processes the\nfused proposal embeddings di, along with their speciﬁc po-\nsitional encoding as input. Within each layer of the trans-\nformer encoder, self-attention mechanisms aggregate the in-\nformation across different proposal embeddings and capture\ntheir long-distance dependencies, implying object consisten-\ncies. In detail, we utilize a transformer decoder featuring\nsix layers, a width of 512, and eight attention heads. Thefeedforward network (FFN) within the transformer has a\nhidden size 2048. After the transformer encoder, we gener-\nate the discriminative query embeddings DI, fed into the\nuniﬁed manipulation discriminative representation (UMDR)\npart to generate predictions for three sub-tasks, viz. objectconsistency modeling, image manipulation detection, and\nlocalization.\nUniﬁed Manipulation Discriminative Representation. Af-\nter the transformer encoder, each tampering discriminative\nquery in DIrepresents the tampering clues across three\nviews of the corresponding proposal. Figure 3shows the\nlearning process of three sub-tasks. UMDR is learned under\nthe supervision of authenticity classiﬁcation, object consis-\ntency modeling, and manipulation localization branches. The\nsame as DETR [ 9] and SOLQ [ 12], the classiﬁcation branch\nis a fully connected (FC) layer to predict the authenticity\nconﬁdences ˆPc. The object consistency modeling branch is\na multi-layer perception (MLP) with a hidden size of 256\nto predict object spatial information ˆPo. The manipulation\nlocalization branch is also a multi-layer perception with a hid-\nden size of 1024 to predict localization mask vector ˆPm. The\nsupervision for the ﬁrst two branches is similar to DETR[ 9].\nIn the third branch, we employ the mask vector, obtained by\nencoding the ground truth mask, as the supervision informa-\ntion. During the inference process, the compressed encoding\nprocedure is applied to ˆPmfor reconstructing the localiza-\ntion mask. In the compression encoding, we utilize Principal\nComponent Analysis (PCA) to transform 2D spatial binary\nmasks into 1D mask vectors.\nLoss Function. The overall loss function for supervision of\nthe UnionFormer can be expressed as:\nLunion=λcls·Lcls+Locm+λloc·Lloc+β·Lcon,(3)\nwhere Lclsdenotes the focal loss [ 39] for classiﬁcation. Lloc\ndenotes the L1loss for localization mask vector supervision.\nLcon is the contrastive learning loss introduced in Section3.2.\nλcls,λloc, andβare the corresponding modulation coefﬁ-\ncients. The Locm is the loss for object consistency modeling,\nwhich is deﬁned as:\nLocm=λL1·LL1+λgious ·Lgious, (4)where LL1andLgious areL1loss and generalized IoU loss\n[52], which is the same as DETR. λL1andλgious are corre-\nsponding coefﬁcients. Following [ 12],Llocis not included\nin the bipartite matching process.\n4. Experiments\n4.1. Experimental Setup\nTraining. We used a large-scale training dataset including\nvarious types of tampered and authentic images. It is divided\ninto ﬁve sections: 1) CASIA v2 [ 14], 2) Fantastic Reality\n[32], 3) Tampered COCO, derived from COCO 2017 datasets\n[37], 4) Tampered RAISE, constructed based on the RAISE\ndataset [ 11], and 5) Pristine images selected from the COCO\n2017 and RAISE datasets. We randomly add Gaussian noise\nor apply JPEG compression to the synthetic data to simulate\nthe visual quality and tampering traces in realistic scenarios.\nDuring the training process, we sequentially train BSFI-Net,\nRPN, and the entire UnionFormer in three stages.\nTesting. To comprehensively evaluate and compare our\nmodel with various state-of-the-art methods, we utilized six\npublicly available testing datasets and one more dataset of\nhyper-realistic tampered images created by the Blended Dif-\nfusion model [ 4]. Speciﬁcally, we employed CASIA v1 [ 14],\nColumbia [ 26], Coverage [ 61], NIST16 [ 22], IMD20 [ 46]\nand CocoGlide [ 23]. Then, we construct BDNIE, includ-\ning 512 hyper-realistic fake images we generated from the\nadvanced blended Diffusion model for text-driven natural\nimage editing. The details of the training and testing data are\nprovided in the Supplementary.\nEvaluation Metric. We evaluated the performance of the\nproposed method in the task of image tampering detectionand localization. For the task of localizing image manipu-\nlations, we report the pixel-level Area Under Curve (AUC)\nand F1 score, using both the best and the ﬁxed 0.5 thresholds.\nFor the detection task following [ 23], we adopt image-level\nAUC and balanced accuracy, which considers both false\nalarms and missed detection, in which case the threshold is\nset to 0.5. To ensure fairness and accuracy in the compari-\nson, some result values for other methods are taken from the\nliterature [ 23,59].\nImplementation Details. The BSFI-Net is trained with\ncross-entropy loss for 100 epochs, employing the AdamW\noptimizer [ 41], with a batch size of 512 and a weight decay\nof 0.05. The initial learning rate is set to 0.001 and decays in\na cosine schedule.\nDuring the training of complete UnionFormer with\nLunion , inspired by [ 56,63], we adopt a 36-epoch ( 3×)\nschedule to train the Unionformer for 2.7×105iterations\nwith batch size 16. An AdamW optimizer is also utilized in\nthis stage. The learning rate is set to 10−4at the beginning\nand multiplied by 0.1 at 1.8×105and2.4×105iterations.\n12527\nMethodOptimal threshold Fixed threshold (0.5)\nColumbia Coverage CASIA v1 NIST16 CoCoGlide A VG Columbia Coverage CASIA v1 NIST16 CoCoGlide A VG\nManTra-Net [ 62] 0.650 0.486 0.320 0.225 0.673 0.471 0.508 0.317 0.180 0.172 0.516 0.339\nSPAN [ 27] 0.873 0.428 0.169 0.363 0.350 0.437 0.759 0.235 0.112 0.228 0.298 0.326\nMVSS-Net [ 13] 0.781 0.659 0.650 0.372 0.642 0.621 0.729 0.514 0.528 0.320 0.486 0.515\nPSCC-Net [ 40] 0.760 0.615 0.670 0.210 0.685 0.588 0.604 0.473 0.520 0.113 0.515 0.445\nCA T-Net v2 [ 34] 0.923 0.582 0.852 0.417 0.603 0.675 0.859 0.381 0.752 0.308 0.434 0.547\nTruFor [ 23] 0.914 0.735 0.822 0.470 0.720 0.732 0.859 0.600 0.737 0.399 0.523 0.624\nOurs 0.925 0.720 0.863 0.489 0.742 0.748 0.861 0.592 0.760 0.413 0.536 0.632\nTable 1. Performance of pixel-level F1 with optimal and ﬁxed threshold for image manipulation localization task.\n4.2. Comparision with state-of-the-art\nBaseline. To ensure a fair and accurate comparison, we only\nselected state-of-the-art methods for which authors provided\npre-trained models, released source code, or evaluated un-\nder a common criterion [ 27,40,59]. To reduce biases, we\nexclusively considered the methods or versions trained onthe datasets that do not overlap with the test datasets. In\ndetail, we included seven state-of-the-art methods: Mantra-\nNet [ 62], SPAN [ 27], PSCC-Net [ 40], MVSS-Net [ 13],\nCA T-Net v2 [ 34], ObjectFormer [ 59], and TruFor [ 23].\nLocalization Results. Table 2and Table 1present the re-\nsults of image tampering localization based on pixel-level\nAUC and F1 score metrics, respectively. The top-ranking\nmethod is denoted in bold, a horizontal line represents the\nsecond-ranking method, and the same annotation is applied\nin Table 4and Table 3. Our method demonstrates the best\nperformance across all datasets for pixel-level AUC evalu-ation. As for F1 evaluation, our method ranks the best orsecond best across all datasets. On average, we achieved anotable advantage, regardless of using an optimal or ﬁxed\nthreshold. In fact, on the relatively novel CocoGlide dataset,\nwhich includes diffusion-based local manipulations, we out-\nperform the second-placed TruFor by 2.2% and 1.3% on the\ntwo thresholds, respectively. This is due to UnionFormer con-\nstructing object view artifacts expression, which can reveal\ninconsistencies between regions generated with diffusion\nmodels and authentic areas. These comparisons indicate that\nour method possesses strong generalization and a superior\nability to capture tampering artifacts.\nDetection Results. Table 4indicates the comparative results\nfor tampering detection. Following [ 23], we use the maxi-\nmum value of the localization map as the detection statistic\nfor methods not explicitly designed for the detection task.\nUnionFormer achieves optimal performance on all datasets\nexcept Columbia and demonstrates marked superiority in\naverage results, whether measured by AUC or balanced ac-\ncuracy. As mentioned in [ 13,23], accuracy is sensitive to\nthreshold selection and challenging to determine without a\nwell-calibrated dataset. However, our method and the second-\nplaced TruFor have achieved commendable results in this\ndemanding scenario. We maintain a 2.5% and 2% lead inMethod Columbia Coverage CASIA v1 NIST16 IMD20 A VG\nManTra-Net [ 62] 0.824 0.819 0.817 0.795 0.748 0.801\nSPAN [ 27] 0.936 0.922 0.797 0.840 0.750 0.849\nPSCC-Net [ 40] 0.982 0.847 0.829 0.855 0.806 0.864\nObjectFormer [ 59] 0.955 0.928 0.843 0.872 0.821 0.884\nTruFor [ 23] 0.947 0.925 0.957 0.877 - 0.927\nOurs 0.989 0.945 0.972 0.881 0.860 0.929\nTable 2. Performance of pixel-level AUC for image manipulation\nlocalization task. The results of Trufor on IMD20 are not reported\nbecause IMD20 is included in its training datasets.\nDistortion SPAN PSCC-Net ObjectFormer Ours\nw/o distortion 0.8359 0.8547 0.8718 0.8813\nResize( 0.78×) 0.8324 0.8529 0.8717 0.8726\nResize( 0.25×) 0.8032 0.8501 0.8633 0.8719\nGSBr(k=3 ) 0.8310 0.8538 0.8597 0.8651\nGSB(k=1 5 ) 0.7915 0.7993 0.8026 0.8430\nGSN(σ=3 ) 0.7517 0.7842 0.7958 0.8285\nGSN(σ=1 5 ) 0.6728 0.7665 0.7815 0.8057\nJPEG(q= 100 ) 0.8359 0.8540 0.8637 0.8802\nJPEG(q=5 0 ) 0.8068 0.8537 0.8624 0.8797\nTable 3. AUC scores for the localization performance on the NIST\n16 dataset.\nthe average AUC and accuracy, respectively. This advan-\ntage is primarily attributed to the uniﬁed learning process\nof our framework. Uniﬁed learning typically facilitates the\nmutual enhancement of localization and detection tasks. The\nmodel’s performance is further enhanced as both sub-tasks\nare mastered through a uniﬁed manipulation discriminative\nrepresentation.\nRobustness Evaluation. We tested the robustness of Union-\nFormer by applying image distortion to NIST 16 dataset\nimages. Following [ 40,59], we included four types of dis-\ntortions: 1) changing the size of images to different scales;\n2) applying Gaussian blur with a kernel size k; 3) adding\nGaussian noise characterized by a standard deviation σ;4 )\napplying JPEG compression to the images, utilizing a qual-\nity factor q. We compare the pixel-level AUC performance\nwith other methods. Table 3show that our method exhibits\nrobustness to various distortion operations, outperforming\nothers.\n12528\nMethodImage-level AUC Accuracy\nColumbia Coverage CASIA v1 NIST16 CoCoGlide A VG Columbia Coverage CASIA v1 NIST16 CoCoGlide A VG\nManTra-Net [ 62] 0.810 0.760 0.644 0.624 0.778 0.723 0.500 0.500 0.500 0.500 0.500 0.500\nSPAN [ 27] 0.999 0.670 0.480 0.632 0.475 0.651 0.951 0.605 0.487 0.597 0.491 0.626\nMVSS-Net [ 13] 0.984 0.733 0.932 0.579 0.654 0.776 0.667 0.545 0.808 0.538 0.536 0.619\nPSCC-Net [ 40] 0.300 0.657 0.869 0.485 0.777 0.618 0.508 0.550 0.683 0.456 0.661 0.572\nCA T-Net v2 [ 34] 0.977 0.680 0.942 0.750 0.667 0.803 0.803 0.635 0.838 0.597 0.580 0.691\nTruFor [ 23] 0.996 0.770 0.916 0.760 0.752 0.839 0.984 0.680 0.813 0.662 0.639 0.756\nOurs 0.998 0.783 0.951 0.793 0.797 0.864 0.979 0.694 0.843 0.680 0.682 0.776\nTable 4. Performance of image-level AUC and balanced accuracy for image manipulation detection.\n&ŽƌŐĞĚ 'd\n DĂŶdƌĂͲEĞƚ ^W\x04E W^\x12\x12ͲEĞƚ Ds^^ͲEĞƚ dƌƵ&Žƌ \x12\x04dͲEĞƚ\x03sϮ KƵƌƐ\nFigure 4. Qualitative comparison results. The ﬁrst to fourth rows are respectively sourced from CASIA v1 [ 14], Columbia [ 26], Coverage\n[61], and IMD20 [ 46]. The last row is from the BDNIE dataset.\nVariant ModelsCASIA v1 NIST 16\nAUC F1 AUC F1\nRGB View (baseline) 0.778 0.701 0.724 0.423RGB+Noise Views 0.865 0.767 0.807 0.448RGB +Noise+Object Views (w/o L\ncon) 0.950 0.849 0.853 0.472\nUnionFormer (w/ ResNet) 0.895 0.786 0.826 0.453UnionFormer (Ours) 0.972 0.863 0.881 0.489\nTable 5. Ablation results on CASIA and NIST16 datasets.\n4.3. Visualization Results\nQualitative Comparison. Figure 4presents localization\nresults across various datasets. Our method can accurately\nlocate tampered regions, predicting more detailed and clear\nboundaries. This is due to our multi-view artifacts captureand BSFI-Net, where frequency information boosts edgeresponse, and the interactions between branches enhance\nthe generalization and discrimination of features. Thanks to\nthe modeling of object view clues and the uniﬁed learning\nframework, our method achieves satisfactory results on the\nchallenging BDNIE dataset, while other methods fail.\nVisualization of Different View Representation. In Figure\n5, we visualize noise features and the edge-guided features\nof the transformer branch in BSFI-Net. As shown in columnsLocm AUCλloc AUC nv AUC Type AUC\nw/ 0.881 0.5 0.802 144 0.824 Sparse 0.847\nw/o 0.796 1 0.881 256 0.881 DCT 0.860\n- - 2 0.836 400 0.813 PCA 0.881\nTable 6. Ablation results for the UMDR on the NIST 16 dataset,\nwhere “ nv” denotes the dimension of the mask vector, and “Type”\nindicates the type of compression coding used.\n1 to 4, some images may appear natural in the RGB view, but\ntheir tampered/authentic parts are readily distinguished in thefrequency domain or under noise view. Columns ﬁve and six\nshow the RGB features generated by a single CNN branch\nand the dual branch of BSFI-Net. Compared to using onlythe CNN branch, BSFI-Net more accurately activates the\ntampered regions, thanks to edge guidance and long-distance\nclues provided by the transformer branch.\nFurthermore, we quantitatively analyze the object view,\nas shown in Figure 6. We derive the afﬁnity matrix Aifrom\nthe transformer encoder during the uniﬁed learning phase.\nBased on Ai, we randomly select a subset of proposal em-\nbeddings and compute their average afﬁnity with other pro-\nposals, denoted as ei.eiis then normalized to the range [0,1]\nand used as a color coefﬁcient to visualize proposals, with\n12529\n&ŽƌŐĞĚ 'd \x18\x12d EŽŝƐĞ \x12EE \x11^&/ͲEĞƚ WƌĞĚŝĐƚŝŽŶ\nFigure 5. Visualization of diverse features. From left to right, we display the forged image, reference mask, edge-guided input of BSFI-Net,\nnoise view input, CAM of the feature maps from CNN and BSFI-Net, and the prediction mask of UnionFormer.\n01ŽďũĞĐƚ\x03ĂĨĨŝŶŝƚǇ\n&ŽƌŐĞĚ DĂƐŬ KďũĞĐƚ\x03\x04ĨĨŝŶŝƚǇ\nFigure 6. Visualization of object view representation. From left to\nright, we display the forged image, ground truth mask, and the\nvisualization of object afﬁnity.\nlighter colors indicating lower afﬁnity. The results show that\nproposals with forged objects have a lower average afﬁnity\nwith other regions, demonstrating UMDR’s ability to capture\ninconsistencies between real and fake objects.\n4.4. Ablation Study\nAblation studies were carried out to assess the impact\nof critical components within our approach. The quantita-tive results are listed in Table 5. We can observe that by\nadding noise stream on the ﬁrst baseline model, the AUCscores increase by 8.7% on CASIA v1 and 8.3% on NIST\n16, while further adding object view representation, the AUC\nscores continue to increase by 10.7% on CASIA v1 and 7.4%\non NIST 16. This demonstrates the effectiveness of noise\nand object view representations. Moreover, when contrastive\nsupervision is lacking, or BSFI-Net is replaced with ResNet-\n50 [ 24], the model’s performance experiences a signiﬁcant\ndecline. This highlights the efﬁcacy of the interaction be-\ntween the two streams and the exceptional capability of the\nBSFI-Net in characterizing forgery artifacts.\nThe BOB and FCU modules within BSFI-Net improve\nthe interaction between its two branches and can effectivelyeliminate feature misalignment between them. When BOB\nor FCU is removed individually, the overall model’s localiza-\ntion AUC scores on the NIST 16 dataset decrease by 4.8%and 6.3% respectively. We further conduct experiments toinvestigate the effect of several key factors in UMDR, viz.\nλloc,Locm , the mask vector dimension nv, and the type of\ncompression coding. We compare three compression coding\nmethods: Sparse Coding [ 15], Discrete Cosine Transform\n(DCT) [ 2], and Principal Component Analysis (PCA) [ 1].\nAs shown in Table 6, when equipped with contrastive loss,\nusing PCA as the encoding type, and setting λlocandLocm\nto 1 and 256 respectively, the model performs the best on the\nNIST 16 dataset.\n5. Conclusion\nIn this paper, we introduced UnionFormer, a uniﬁed-\nlearning transformer framework that leverages clues fromthree distinct views for image manipulation detection andlocalization. UnionFormer employs BSFI-Net as a feature\nencoder to extract highly discriminative features under RGB\nand noise views. Then, through a uniﬁed learning process\nwith three tasks, UnionFormer models the discontinuity be-\ntween objects, i.e., object view representation, and learns a\nuniﬁed discriminative representation. The uniﬁed represen-\ntation integrating information from three views has stronggeneralizability and discrimination. It can accurately iden-\ntify various image manipulations, whether traditional manual\nediting or natural language-driven tampering based on dif-fusion models. Moreover, the uniﬁed learning frameworkenables the mutual enhancement of sub-tasks, achievinghigh-precision detection and localization. Comprehensive\nexperiments conducted on various datasets demonstrate the\nefﬁcacy of the proposed method.\nAcknowledgements. We thank the anonymous reviewers for\ntheir valuable suggestions. This work is funded by the Na-\ntional Natural Science Foundation of China (Nos. 62176010,\n61771026, U21A20515, 62172416, 62376271), and the\nY outh Innovation Promotion Association of the Chinese\nAcademy of Sciences (2022131).\n12530\nReferences\n[1] Herv ´e Abdi and Lynne J Williams. Principal component\nanalysis. Chemometrics and Intelligent Laboratory Systems ,\n2(1):37–52, 1987. 8\n[2] N. Ahmed, T. Natarajan, and K.R. Rao. Discrete cosine\ntransform. IEEE Transactions on Computers , C-23(1):90–93,\n1974. 8\n[3] Saadaldeen Rashid Ahmed, Emrullah Sonu c¸, Mo-\nhammed Rashid Ahmed, and Adil Deniz Duru. Analysissurvey on deepfake detection and recognition with convo-lutional neural networks. In 2022 International Congress\non Human-Computer Interaction, Optimization and Robotic\nApplications (HORA) , pages 1–7. IEEE, 2022. 1\n[4] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended\ndiffusion for text-driven editing of natural images. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR) , pages 18208–18218, 2022.\n2,5\n[5] Omri Avrahami, Ohad Fried, and Dani Lischinski. Blended\nlatent diffusion. ACM Transactions on Graphics (TOG) ,4 2\n(4):1–11, 2023. 2\n[6] Belhassen Bayar and Matthew C Stamm. A deep learning\napproach to universal image manipulation detection usinga new convolutional layer. In Proceedings of the 4th ACM\nWorkshop on Information Hiding and Multimedia Security ,\npages 5–10, 2016. 1,2\n[7] Belhassen Bayar and Matthew C Stamm. Constrained con-\nvolutional neural networks: A new approach towards general\npurpose image manipulation detection. IEEE Transactions on\nInformation F orensics and Security , 13(11):2691–2706, 2018.\n3\n[8] Yihan Cao, Siyu Li, Yixin Liu, Zhiling Y an, Y utong Dai,\nPhilip S Y u, and Lichao Sun. A comprehensive survey ofai-generated content (aigc): A history of generative ai from\ngan to chatgpt. arXiv preprint arXiv:2303.04226 , 2023. 1\n[9] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In European confer-\nence on computer vision , pages 213–229. Springer, 2020. 1,\n2,5\n[10] Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu,\nand Mubarak Shah. Diffusion models in vision: A survey.\nIEEE Transactions on Pattern Analysis and Machine Intelli-\ngence , 2023. 1\n[11] Duc-Tien Dang-Nguyen, Cecilia Pasquini, V alentina Conotter,\nand Giulia Boato. Raise: A raw images dataset for digital\nimage forensics. In Proceedings of the 6th ACM Multimedia\nSystems Conference , page 219–224, New Y ork, NY , USA,\n2015. Association for Computing Machinery. 5\n[12] Bin Dong, Fangao Zeng, Tiancai Wang, Xiangyu Zhang, and\nYichen Wei. Solq: Segmenting objects by learning queries.\nAdvances in Neural Information Processing Systems , 34:\n21898–21909, 2021. 1,5\n[13] Chengbo Dong, Xinru Chen, Ruohan Hu, Juan Cao, and\nXirong Li. Mvss-net: Multi-view multi-scale supervised net-\nworks for image manipulation detection. IEEE Transactionson Pattern Analysis and Machine Intelligence , 45(3):3539–\n3553, 2022. 2,6,7\n[14] Jing Dong, Wei Wang, and Tieniu Tan. Casia image tam-\npering detection evaluation database. In 2013 IEEE China\nSummit and International Conference on Signal and Informa-\ntion Processing , pages 422–426, 2013. 5,7\n[15] D.L. Donoho. Compressed sensing. IEEE Transactions on\nInformation Theory , 52(4):1289–1306, 2006. 8\n[16] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. arXiv preprint\narXiv:2010.11929 , 2020. 2,4\n[17] Pasquale Ferrara, Tiziano Bianchi, Alessia De Rosa, and\nAlessandro Piva. Image forgery localization via ﬁne-grained\nanalysis of cfa artifacts. IEEE Transactions on Information\nF orensics and Security , 7(5):1566–1577, 2012. 2\n[18] Jessica Fridrich and Jan Kodovsky. Rich models for steganal-\nysis of digital images. IEEE Transactions on Information\nF orensics and Security , 7(3):868–882, 2012. 1,2\n[19] Oran Gafni and Lior Wolf. Wish you were here: Context-\naware human generation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\npages 7840–7849, 2020. 1\n[20] Vidit Goel, Elia Peruzzo, Yifan Jiang, Dejia Xu, Xingqian Xu,\nNicu Sebe, Trevor Darrell, Zhangyang Wang, and Humphrey\nShi. PAIR-Diffusion: A Comprehensive Multimodal Object-\nLevel Image Editor. arXiv e-prints , art. arXiv:2303.17546,\n2023. 2\n[21] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nY oshua Bengio. Generative adversarial nets. Advances in\nNeural Information Processing Systems , 27, 2014. 1\n[22] Haiying Guan, Mark Kozak, Eric Robertson, Y ooyoung Lee,\nAmy N. Y ates, Andrew Delgado, Daniel Zhou, Timothee\nKheyrkhah, Jeff Smith, and Jonathan Fiscus. Mfc datasets:\nLarge-scale benchmark datasets for media forensic challenge\nevaluation. In 2019 IEEE Winter Applications of Computer\nVision Workshops (WACVW) , pages 63–72, 2019. 5\n[23] Fabrizio Guillaro, Davide Cozzolino, Avneesh Sud, Nicholas\nDufour, and Luisa V erdoliva. Trufor: Leveraging all-round\nclues for trustworthy image forgery detection and localization.\nInProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 20606–20615, 2023. 1,\n2,5,6,7\n[24] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition , pages 770–778, 2016. 4,8\n[25] Kaiming He, Georgia Gkioxari, Piotr Doll ´ar, and Ross Gir-\nshick. Mask r-cnn. In Proceedings of the IEEE International\nConference on Computer Vision , pages 2961–2969, 2017. 4\n[26] Y u-feng Hsu and Shih-fu Chang. Detecting image splicing\nusing geometry invariants and camera characteristics consis-\ntency. In 2006 IEEE International Conference on Multimedia\nand Expo , pages 549–552, 2006. 5,7\n12531\n[27] Xuefeng Hu, Zhihan Zhang, Zhenye Jiang, Syomantak Chaud-\nhuri, Zhenheng Y ang, and Ram Nevatia. Span: Spatial pyra-\nmid attention network for image manipulation localization.\nInComputer Vision–ECCV 2020: 16th European Conference,\nGlasgow, UK, August 23–28, 2020, Proceedings, Part XXI 16 ,\npages 312–328. Springer, 2020. 1,2,6,7\n[28] Suhaib Wajahat Iqbal and Bhavna Arora. Machine learning\ntechniques for image manipulation detection: A review and\nanalysis. In The International Conference on Recent Innova-\ntions in Computing , pages 209–224. Springer, 2022. 1\n[29] Jindong Jiang, Fei Deng, Gautam Singh, and Sungjin\nAhn. Object-centric slot diffusion. arXiv preprint\narXiv:2303.10834 , 2023. 2\n[30] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen\nChang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic:\nText-based real image editing with diffusion models. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition , pages 6007–6017, 2023. 2\n[31] Diederik P Kingma and Max Welling. Auto-encoding varia-\ntional bayes. arXiv preprint arXiv:1312.6114 , 2013. 1\n[32] Vladimir V . Kniaz, Vladimir Knyaz, and Fabio Remondino.\nThe point where reality meets fantasy: Mixed adversarial\ngenerators for image splice detection. In Advances in Neu-\nral Information Processing Systems . Curran Associates, Inc.,\n2019. 5\n[33] Neal Krawetz and Hacker Factor Solutions. A picture’s worth.\nHacker Factor Solutions , 6(2):2, 2007. 2\n[34] Myung-Joon Kwon, Seung-Hun Nam, In-Jae Y u, Heung-Kyu\nLee, and Changick Kim. Learning jpeg compression artifacts\nfor image manipulation detection and localization. Interna-\ntional Journal of Computer Vision , 130(8):1875–1895, 2022.\n2,6,7\n[35] Haodong Li and Jiwu Huang. Localization of deep inpainting\nusing high-pass fully convolutional network. In proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion , pages 8301–8310, 2019. 1\n[36] Shuaibo Li, Shibiao Xu, Wei Ma, and Qiu Zong. Image ma-\nnipulation localization using attentional cross-domain cnn\nfeatures. IEEE Transactions on Neural Networks and Learn-\ning Systems , 34(9):5614–5628, 2023. 2\n[37] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll ´ar, and C. Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nComputer Vision – ECCV 2014 , pages 740–755, Cham, 2014.\nSpringer International Publishing. 5\n[38] Tsung-Yi Lin, Piotr Doll ´ar, Ross Girshick, Kaiming He,\nBharath Hariharan, and Serge Belongie. Feature pyramidnetworks for object detection. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition ,\npages 2117–2125, 2017. 4\n[39] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and\nPiotr Doll ´ar. Focal loss for dense object detection. In Pro-\nceedings of the IEEE International Conference on Computer\nVision , pages 2980–2988, 2017. 5\n[40] Xiaohong Liu, Y aojie Liu, Jun Chen, and Xiaoming Liu. Pscc-\nnet: Progressive spatio-channel correlation network for image\nmanipulation detection and localization. IEEE Transactionson Circuits and Systems for Video Technology , 32(11):7505–\n7517, 2022. 1,6,7\n[41] Ilya Loshchilov and Frank Hutter. Decoupled weight de-\ncay regularization. In International Conference on Learning\nRepresentations , 2018. 5\n[42] Babak Mahdian and Stanislav Saic. Using noise inconsisten-\ncies for blind image forensics. Image and vision computing ,\n27(10):1497–1503, 2009. 2\n[43] Mehdi Mirza and Simon Osindero. Conditional generative\nadversarial nets. arXiv preprint arXiv:1411.1784 , 2014. 1\n[44] Ron Mokady, Amir Hertz, Kﬁr Aberman, Y ael Pritch, and\nDaniel Cohen-Or. Null-text inversion for editing real im-ages using guided diffusion models. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 6038–6047, 2023. 2\n[45] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav\nShyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and\nMark Chen. Glide: Towards photorealistic image generation\nand editing with text-guided diffusion models. arXiv preprint\narXiv:2112.10741 , 2021. 1\n[46] Adam Novozamsky, Babak Mahdian, and Stanislav Saic.\nImd2020: A large-scale annotated dataset tailored for detect-\ning manipulated images. In Proceedings of the IEEE/CVF\nWinter Conference on Applications of Computer Vision\n(WACV) Workshops , 2020. 5,7\n[47] Aaron van den Oord, Y azhe Li, and Oriol Vinyals. Repre-\nsentation learning with contrastive predictive coding. arXiv\npreprint arXiv:1807.03748 , 2018. 4\n[48] Zhiliang Peng, Wei Huang, Shanzhi Gu, Lingxi Xie, Y aowei\nWang, Jianbin Jiao, and Qixiang Y e. Conformer: Local fea-\ntures coupling global representations for visual recognition.InProceedings of the IEEE/CVF International Conference\non Computer Vision (ICCV) , pages 367–376, 2021. 4\n[49] Y uyang Qian, Guojun Yin, Lu Sheng, Zixuan Chen, and Jing\nShao. Thinking in frequency: Face forgery detection by min-\ning frequency-aware clues. In European Conference on Com-\nputer Vision , pages 86–103. Springer, 2020. 2\n[50] Ali Razavi, Aaron V an den Oord, and Oriol Vinyals. Generat-\ning diverse high-ﬁdelity images with vq-vae-2. Advances in\nNeural Information Processing Systems , 32, 2019. 1\n[51] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\nFaster r-cnn: Towards real-time object detection with region\nproposal networks. Advances in Neural Information Process-\ning Systems , 28, 2015. 3,4\n[52] Hamid Rezatoﬁghi, Nathan Tsoi, JunY oung Gwak, Amir\nSadeghian, Ian Reid, and Silvio Savarese. Generalized in-\ntersection over union: A metric and a loss for bounding box\nregression. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 658–666,\n2019. 5\n[53] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj ¨orn Ommer. High-resolution image\nsynthesis with latent diffusion models. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 10684–10695, 2022. 1\n[54] Zenan Shi, Xuanjing Shen, Hui Kang, and Yingda Lv. Im-\nage manipulation detection and localization based on the\n12532\ndual-domain convolutional neural networks. IEEE Access ,6 :\n76437–76453, 2018. 2\n[55] Yizhi Song, Zhifei Zhang, Zhe Lin, Scott Cohen, Brian Price,\nJianming Zhang, Soo Y e Kim, and Daniel Aliaga. Object-\nstitch: Object compositing with diffusion model. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition , pages 18310–18319, 2023. 2\n[56] Zhiqing Sun, Shengcao Cao, Yiming Y ang, and Kris M Ki-\ntani. Rethinking transformer-based set prediction for object\ndetection. In Proceedings of the IEEE/CVF International\nConference on Computer Vision , pages 3611–3620, 2021. 4,\n5\n[57] Cristian V accari and Andrew Chadwick. Deepfakes and dis-\ninformation: Exploring the impact of synthetic political video\non deception, uncertainty, and trust in news. Social Media+\nSociety , 6(1):2056305120903408, 2020. 1\n[58] Ashish V aswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. Advances in Neural\nInformation Processing Systems , 30, 2017. 2\n[59] Junke Wang, Zuxuan Wu, Jingjing Chen, Xintong Han, Ab-\nhinav Shrivastava, Ser-Nam Lim, and Y u-Gang Jiang. Ob-\njectformer for image manipulation detection and localization.\nInProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 2364–2373, 2022. 2,5,\n6\n[60] Zhengwei Wang, Qi She, and Tomas E Ward. Generative\nadversarial networks in computer vision: A survey and tax-\nonomy. ACM Computing Surveys (CSUR) , 54(2):1–38, 2021.\n1\n[61] Bihan Wen, Y e Zhu, Ramanathan Subramanian, Tian-Tsong\nNg, Xuanjing Shen, and Stefan Winkler. Coverage — a novel\ndatabase for copy-move forgery detection. In 2016 IEEE\nInternational Conference on Image Processing (ICIP) , pages\n161–165, 2016. 5,7\n[62] Y ue Wu, Wael AbdAlmageed, and Premkumar Natarajan.\nMantra-net: Manipulation tracing network for detection and\nlocalization of image forgeries with anomalous features. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition , pages 9543–9552, 2019. 2,6,\n7\n[63] Y uxin Wu, Alexander Kirillov, Francisco Massa, Wan-Y en\nLo, and Ross Girshick. Detectron2. https://github.\ncom/facebookresearch/detectron2 , 2019. 5\n[64] Ziyi Wu, Jingyu Hu, Wuyue Lu, Igor Gilitschenski, and Ani-\nmesh Garg. Slotdiffusion: Object-centric generative model-\ning with diffusion models. arXiv preprint arXiv:2305.11281 ,\n2023. 2\n[65] Binxin Y ang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin\nChen, Xiaoyan Sun, Dong Chen, and Fang Wen. Paint by ex-\nample: Exemplar-based image editing with diffusion models.\nInProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 18381–18391, 2023. 2\n[66] Chao Y ang, Huizhou Li, Fangting Lin, Bin Jiang, and Hao\nZhao. Constrained r-cnn: A general image manipulation\ndetection model. In 2020 IEEE International Conference on\nMultimedia and Expo (ICME) , pages 1–6. IEEE, 2020. 1,2[67] Y uyuan Zeng, Bowen Zhao, Shanzhao Qiu, Tao Dai, and Shu-\nTao Xia. Towards effective image manipulation detection\nwith proposal contrastive learning. IEEE Transactions on\nCircuits and Systems for Video Technology , 2023. 1,4\n[68] Jiaming Zhang, Huayao Liu, Kailun Y ang, Xinxin Hu, Ruip-\ning Liu, and Rainer Stiefelhagen. Cmx: Cross-modal fusion\nfor rgb-x semantic segmentation with transformers. IEEE\nTransactions on Intelligent Transportation Systems , 2023. 2\n[69] Zhixing Zhang, Ligong Han, Arnab Ghosh, Dimitris N\nMetaxas, and Jian Ren. Sine: Single image editing with text-\nto-image diffusion models. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\npages 6027–6037, 2023. 2\n[70] Peng Zhou, Xintong Han, Vlad I. Morariu, and Larry S. Davis.\nLearning rich features for image manipulation detection. In\nProceedings of the IEEE Conference on Computer Vision and\nPattern Recognition (CVPR) , 2018. 2\n[71] Peng Zhou, Bor-Chun Chen, Xintong Han, Mahyar Najibi,\nAbhinav Shrivastava, Ser-Nam Lim, and Larry Davis. Gen-\nerate, segment, and reﬁne: Towards generic manipulationsegmentation. In Proceedings of the AAAI Conference on\nArtiﬁcial Intelligence , pages 13058–13065, 2020. 1\n[72] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang,\nand Jifeng Dai. Deformable detr: Deformable transformers for\nend-to-end object detection. arXiv preprint arXiv:2010.04159 ,\n2020. 2\n12533"}, 'dist': 0.9286905527114868}
Result 20: {'text': 'data', 'metadata': {'created_at': 1736932570, 'modified_at': 1736932570, 'owner': 'sayandeep', 'path': 'RAG_CONTENT/conferences/cvpr_ocr/Barquero_Seamless_Human_Motion_Composition_with_Blended_Positional_Encodings_CVPR_2024_paper.txt', 'size': 65599, 'seen_at': 1737191136, 'data': 'Seamless Human Motion Composition with Blended Positional Encodings\nGerman Barquero Sergio Escalera Cristina Palmero\nUniversitat de Barcelona and Computer Vision Center, Spain\n{germanbarquero, sescalera }@ub.edu ,crpalmec7@alumnes.ub.edu\nhttps://barquerogerman.github.io/FlowMDM/\n("forward kick", 2.5s) ("walk slowly", 3.2s) ("get down on ground", 3s) ("crawl", 3.3s) ("walk", 2s) ("walk", 2s) ("walk", 2s) ("walk", 2s) ("walk", 2s) ...\nFigure 1. We present FlowMDM, a diffusion-based approach capable of generating seamlessly continuous sequences of human motion\nfrom textual descriptions (left). The whole sequence is generated simultaneously and it does not require any postprocessing. FlowMDM\nalso makes strides in the challenging problem of extrapolating and controlling periodic motion such as walking, jumping, or waving (right).\nAbstract\nConditional human motion generation is an important\ntopic with many applications in virtual reality, gaming, and\nrobotics. While prior works have focused on generating mo-\ntion guided by text, music, or scenes, these typically result\nin isolated motions confined to short durations. Instead,\nwe address the generation of long, continuous sequences\nguided by a series of varying textual descriptions. In this\ncontext, we introduce FlowMDM, the first diffusion-based\nmodel that generates seamless Human Motion Composi-\ntions (HMC) without any postprocessing or redundant de-\nnoising steps. For this, we introduce the Blended Positional\nEncodings, a technique that leverages both absolute and\nrelative positional encodings in the denoising chain. More\nspecifically, global motion coherence is recovered at the ab-\nsolute stage, whereas smooth and realistic transitions are\nbuilt at the relative stage. As a result, we achieve state-of-\nthe-art results in terms of accuracy, realism, and smooth-\nness on the Babel and HumanML3D datasets. FlowMDM\nexcels when trained with only a single description per mo-\ntion sequence thanks to its Pose-Centric Cross-ATtention,\nwhich makes it robust against varying text descriptions at\ninference time. Finally, to address the limitations of existing\nHMC metrics, we propose two new metrics: the Peak Jerk\nand the Area Under the Jerk, to detect abrupt transitions.1. Introduction\nIn the field of computer vision, recent progress has been\nmade in developing photorealistic avatars [53] for appli-\ncations like virtual reality, gaming, and robotics [60, 76].\nAside from looking visually realistic, avatars must also\nmove in a convincing manner. This is challenging due to\nthe intricate nature of human motion, strongly influenced\nby factors such as the environment, interactions, and phys-\nical contact [14]. Furthermore, complexity increases when\nattempting to control these motions. Recent advances in-\nclude the generation of motion sequences from control sig-\nnals like text descriptions or actions [106]; however, such\nmethods only produce isolated, standalone motion. There-\nfore, these approaches fail to handle scenarios where a long\nmotion is driven by distinct control signals on different time\nslices. Such capability is needed to provide full control over\nthe sequence of desired actions and their duration. In these\nscenarios, the generated motion needs to feature seamless\nand realistic transitions between actions. In this work, we\ntackle this problem, which we refer to as generative Hu-\nman Motion Composition (HMC). In particular, we focus\non generating single-human motion from text (Fig. 1).\nOne of the primary obstacles in HMC is the lack of\ndatasets that offer long motion sequences with diverse tex-\ntual annotations. Existing datasets typically feature se-\nquences of limited duration, often lasting only up to 10 sec-\nonds, and with just a single control signal governing the en-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n457\ntire sequence [26, 62]. This limitation calls for innovative\nsolutions to address the inherent complexities of the task.\nPrior works have tackled this problem mostly with autore-\ngressive approaches [4, 44, 47, 64, 101]. These methods it-\neratively create compositions by using the current motion as\na basis to generate subsequent motions. However, they re-\nquire datasets with multiple consecutive annotated motions,\nand tend to degenerate in very long HMC scenarios due to\nerror accumulation [104]. Other recent works have lever-\naged the infilling capabilities of motion diffusion models\nto generate motion compositions [70, 100]. However, for\nthese, a substantial portion of each motion sequence is gen-\nerated independently from adjacent motions, and generating\ntransitions requires computing redundant denoising steps.\nIn this work, we propose a novel architecture designed to\naddress these specific challenges. Our contributions are:\n• We propose FlowMDM, the first diffusion-based model\nthat generates seamless human motion compositions\nwithout any postprocessing or extra denoising steps. To\naccomplish it, we introduce Blended Positional Encod-\nings (BPE), a new technique for diffusion Transformers\nthat combines the benefits of both absolute and relative\npositional encodings during sampling. In particular, the\ndenoising first exploits absolute information to recover\nthe global motion coherence, and then leverages relative\npositions to build smooth and realistic transitions between\nactions. As a result, FlowMDM achieves state-of-the-art\nresults in terms of accuracy, realism, and smoothness in\nthe HumanML3D [26] and Babel [63] datasets.\n• We introduce a new attention technique tailored for HMC:\nthe Pose-Centric Cross-ATtention (PCCAT). This layer\nensures each pose is denoised based on its own condition\nand its neighboring poses. Consequently, FlowMDM can\nbe trained on a dataset with only a single condition avail-\nable per motion sequence and still generate realistic tran-\nsitions when using multiple conditions at inference time.\n• We reveal the lack of sensitivity of current HMC metrics\nto identify discontinuous or sharp transitions, and intro-\nduce two new metrics that help to detect them: the Peak\nJerk (PJ) and the Area Under the Jerk (AUJ).\n2. Related work\nConditional human motion generation. Recent studies in\nmotion generation have shown notable progress in synthe-\nsizing movements conditioned on diverse modalities such\nas text [21, 26, 27, 35, 40, 61, 78, 79, 97–99], music [2,\n17, 46, 74, 81, 93, 107], scenes [15, 30, 84–86, 94], interac-\ntive objects [1, 18, 41, 89], and even other humans’ behav-\nior [9, 10, 28, 77, 90]. Traditionally, these approaches have\nbeen designed to generate motion sequences matching a sin-\ngle condition. The progress of this domain has been boosted\nby the release of big datasets including diverse modalities or\nmanual annotations [12, 26, 28, 46, 50, 58, 62, 63]. Re-search has also focused on problems like human motion\nprediction [3, 52, 56, 69, 75, 80, 83, 96] and motion infill-\ning [29, 36, 39, 48, 49, 57, 65, 66, 72, 105], which do not\nrely on extensive manual annotations but rather on motion\nitself. Both tasks share a common challenge with HMC:\nthe synthesized motion must not only be plausible but also\nintegrate seamlessly with the neighboring behavior, ensur-\ning fluidity and continuity. In this context, the utilization\nof human motion priors has been proven to be a successful\ntechnique to ensure any generated motion includes natural\ntransitions [8, 45, 88]. In line with these approaches, our\nmethod learns a motion prior specifically tailored for HMC.\nAutoregressive human motion composition. As in\nmany other sequence modeling tasks, HMC was also first\ntackled with autoregressive methods. The gold standard has\nbeen pairing variational autoencoders with autoregressive\ndecoders such as recurrent neural networks [101] or Trans-\nformers [4, 44, 47, 64]. Alternative approaches have intro-\nduced specialized reinforcement learning frameworks [51,\n92, 102]. Autoregressive models rely on the availability of\nannotated motion transitions, a requirement that constrains\nthe robustness of the models due to the scarcity of such\ndata. To mitigate this issue, some methods include addi-\ntional postprocessing steps like linear interpolations [4], or\naffine transformations [44]. However, these can distort the\nhuman motion dynamics and require a predetermined esti-\nmation of the transitions duration. Furthermore, autoregres-\nsive approaches generate motion solely based on the preced-\ning motion. We argue that an accurate model should mimic\nthe humans innate capacity to anticipate their next action\nand adapt their current behavior accordingly [24, 42].\nDiffusion-based human motion composition. Diffu-\nsion models excel at conditional generation [20, 32, 71].\nThey also possess great zero-shot capabilities for image in-\npainting [67], and its equivalence in motion: motion infill-\ning. DiffCollage [100], MultiDiffusion [7], and Double-\nTake [70] introduced diffusion sampling processes that si-\nmultaneously generate temporally superimposed motions,\nand combine the overlapped regions so that an infilled tran-\nsition emerges. DoubleTake extended such sampling with\na refinement step in which the emerged transition under-\ngoes further unconditional denoising steps. All these meth-\nods share two main limitations. First, they are constrained\nto modeling dependencies among neighboring motion se-\nquences. This becomes a limitation when three or more\nconsecutive actions share semantics and collectively repre-\nsent a more comprehensive action. In this case, dependen-\ncies may extend beyond contiguous actions. Second, they\nneed to set the number of frames that each transition takes\nbetween consecutive actions, leading to extra computations.\nOur work addresses these constraints with a solution able to\nmodel longer inter-sequence dynamics without extra com-\nputational burdens or predefined transition durations.\n458\n3. Methodology\nProblem definition. Our goal consists in generating a mo-\ntion sequence of Nframes, with the capability of condition-\ning the generated motion inside non-overlapping intervals\n[0, τ1),[τ1, τ2), ...,[τj, N), with 0<τ1<···<τj<N. We\nwill refer to the motion inside these intervals as motion sub-\nsequences , orSi={xτi, ..., x τi+1−1}, each driven by its\ncorresponding condition ci, and with a maximum length of\nL. It is essential that consecutive subsequences, influenced\nby different control signals, transition seamlessly and real-\nistically. In particular, we aim at the even more challeng-\ning case where motion sequences containing several pairs\nof(Si, ci)are not necessarily available in our dataset.\nIn this section, we present FlowMDM, an architecture\nwith strong inductive biases that promote the emergence of\narobust translation-invariant motion prior . Such mo-\ntion prior is learned with a diffusion model equipped with\na bidirectional (i.e., encoder-only) Transformer, similar to\nprior works [70, 79]. With it, we overcome the main lim-\nitations of autoregressive methods (Sec. 3.1). However,\nprevious works are constrained in terms of motion dura-\ntion. We could arguably provide extrapolation capabili-\nties to the diffusion model by replacing the absolute po-\nsitional encoding with a relative alternative, thus making\nthe denoising of each pose translation invariant . How-\never, this technique would fail to build complex composi-\ntional semantics that require knowledge about the start and\nend of each subsequence. For example, when generating\nthe motion composition Si→Si+1withci=‘walking’ and\nci+1=‘walk and sit down’ ,Si+1might only feature the ac-\ntion ‘ sit down ’ because, with only relative positional infor-\nmation, the Transformer cannot know if the partially de-\nnoised ‘ walking ’ motion preceding the beginning of Si+1\nbelongs to SiorSi+1. To combine the benefits of both rel-\native and absolute positional encodings, we introduce BPE\n(Sec. 3.2). This novel technique exploits the iterative nature\nof diffusion models to promote intra-subsequence global\ncoherence in earlier denoising stages, while making later\ndenoising stages translation invariant, ensuring that realis-\ntic and plausible transitions naturally emerge between sub-\nsequences. Still, during training, the condition remains un-\nchanged throughout all ground truth motion sequences. In\norder to make our denoising model robust to having mul-\ntiple conditions per sequence at inference, we introduce a\nnew attention paradigm called PCCAT (Fig. 3.3). As a\nresult, FlowMDM is able to simultaneously generate very\nlong compositions of human motion subsequences, all in\nharmony and fostering plausible transitions between them,\nwithout explicit supervision on transitions generation.\n3.1. Bidirectional diffusion\nThe cumulative nature of errors in autoregressive models\noften results in a decline in performance when generatinglong sequences [104]. This is exacerbated in HMC, where\ntransitions are scarce or even missing in the training corpus,\nand the model needs to deal with domain shifts at infer-\nence. Another limitation of autoregressive methods is that\nthe generated Sionly depends on {Sj}j<i. We discussed\nin Sec. 2 why this is a suboptimal solution for HMC. Thus,\nan appropriate model for HMC should also be able to an-\nticipate the following motion, Si+1, and possibly adapt Si\nso that the transition is feasible. We argue that the iterative\nparadigm of diffusion models provides very appropriate in-\nductive biases for naturally mimicking such ability: the par-\ntially denoised SiandSi+1are refined later in successive\ndenoising steps. By choosing a bidirectional Transformer\nas our denoising function [38], we enable the modeling of\nboth past and future dependencies. Therefore, we design\nour framework as a bidirectional motion diffusion model,\nsimilar to MDM [79]. We refer the reader to [91] for more\ndetails on the theoretical aspects of diffusion models.\n3.2. Blended positional encodings\nDiffusion models can learn strong motion priors that ensure\nany motion generated is realistic and plausible [70]. In fact,\nthey can also generate smooth transitions between subse-\nquences [7, 70, 100]. However, these capabilities stem from\ninference-time motion infilling techniques, which we argue\ndo not exploit the full potential of human motion priors.\nIn fact, building a prior that extrapolates well to sequences\nlonger than those observed during training is very challeng-\ning. The field of natural language processing has made\nprogress in sequence extrapolation techniques, notably by\nsubstituting absolute positional encoding (APE) with a rela-\ntive (RPE) counterpart [37]. By only providing information\nregarding how far tokens are between them, they achieve\nsequence-wise translation invariance and, therefore, can ex-\ntrapolate their modeling capabilities to longer sequences.\nYet, the absolute positions of poses within a motion, in-\ncluding their distances to the start and end of the action,\nare necessary to build the global semantics of the motion,\nas exemplified at the beginning of this section.\nHere, we propose BPE, a novel positional encoding\nscheme designed for diffusion models that enables motion\nextrapolation while preserving the global motion seman-\ntics. Our BPE is inspired by the observation that in mo-\ntion, high frequencies encompass local fine details, whereas\nlow frequencies capture global structures. Similar insights\nhave been drawn for images [59]. Diffusion models ex-\ncel at decomposing the generation process into recovering\nlower frequencies, and gradually transitioning to higher fre-\nquencies. Fig. 2 shows how at early denoising phases, mo-\ntion diffusion models prioritize global inter-frame depen-\ndencies, shifting towards local relative dependencies as the\nprocess unfolds. The proposed BPE harmonizes these dy-\nnamics during inference : at early denoising stages, our de-\n459\nstart of\nsequencecurrent frame\nKeys positions0.000.020.040.060.08Average attention scores for a single query\nDenoising steps\nLast800th600th400th200th100th\n80th60th40th20th10thFirst\nend of\nsequence\nFigure 2. Attention scores of a single query pose (current frame)\nas a function of the pose attended to (x-axis) in a diffusion-based\nmotion generation model with a sinusoidal absolute positional en-\ncoding. Curves show the scores at each denoising step. We ob-\nserve that, whereas early steps show strong global dependencies\n(blue), later denoising stages exhibit a clearly local behavior (red).\nnoising model is fed with an APE and, towards the conclu-\nsion, with an RPE. A scheduler guides this transition. As\na result, intra-subsequence global dependencies are recov-\nered at the beginning of the denoising, and intra- and inter-\nsubsequences motion smoothness and realism are promoted\nlater. To make the model understand APE and RPE at infer-\nence, we expose it to both encodings by randomly alternat-\ning them during training. As a result, the BPE schedule can\nbe tuned at inference time to balance the intra-subsequence\ncoherence and the inter-subsequence realism trade-off.\nRotary Position Encoding (RoPE). Our choice for RPE\nis rotary embeddings [73]. RoPE integrates a position em-\nbedding into the queries and keys, ensuring that after dot-\nproduct multiplication, the attention scores’ positional in-\nformation reflects only the relative pairwise distance be-\ntween queries and keys. Specifically, let WqandWkbe the\nprojection matrices into the d-dimensional spaces of queries\nand keys. Then, RoPE encodes the absolute positions mand\nnof a pair of query ( qm=Wqxm) and key ( kn=Wkxn), re-\nspectively, as d-dimensional rotations Rd\nm, Rd\nnover the pro-\njected poses xm, xn. The rotation angles are parameterized\nbymandnso that the attention formulation becomes:\nqT\nmkn= (Rd\nmWqxm)T(Rd\nnWkxn) =xT\nmWqRd\nn−mWkxn.(1)\nNote that the resulting rotation Rd\nn−monly depends on\nthe distance between nandm, and any absolute information\nabout normis removed. RoPE is a natural choice for our\nRPE due to its simplicity and convenient injection before\nthe attention takes place. As a result, RoPE is compatible\nwith faster attention techniques like FlashAttention [22, 23].\nSinusoidal Position Encoding. Our APE is the classic\nsinusoidal position encoding [82], which leverages sine and\ncosine functions to inject positional information. It is added\nto the queries, keys, and values of the attention layers.\nNote that for APE, attention is limited to each subse-\nquence, while for RPE, attention spans all frames up to the\nattention horizon H<L<N . Since Ldefines the maximum\nrange of motion dynamics learned during RPE training,\nthere is no advantage in setting H≥L(Tabs. D/E in supp.\nQ K V\nSitting down Stand upDenosing\ntimestepT ext encoderTime\nConditionPose-Centric Cross-ATtention\nNoisy poses\nT ext encoderBlended Positional Encodings\nDenseBidirectional Transformer\nTransformer with PCCAT + BPETransformer with PCCAT + BPE\nNoisy poses at timestep tTime\nFigure 3. Pose-centric cross-attention. Our attention minimizes\nthe entanglement between the control signal (e.g., text, objects)\nand the noisy motion by feeding the former only to the query.\nConsequently, our model denoises each frame’s noisy pose only\nleveraging its own condition, and the neighboring noisy poses.\nmaterial). Leveraging both APE and RPE constraints en-\nsures quadratic complexity over the maximum subsequence\nlength Lin both memory and computation [11]. As a re-\nsult, FlowMDM’s complexity is equivalent to that of other\nTransformer-based motion diffusion models [70, 100].\n3.3. Pose-centric cross-attention\nIn order to make motion generation with diffusion mod-\nels efficient, we would like to simultaneously generate very\nlong sequences. In motion Transformers, the generation is\nconditioned at a sequence level by injecting the condition as\na token [79], or as a sequence-wise transformation in inter-\nmediate layers [99]. Therefore, they cannot be conditioned\non multiple signals in different subsequences. For this rea-\nson, diffusion-based methods for HMC opted for individu-\nally generating sequences and then merging them [70, 100].\nTo enable such simultaneous heterogeneous conditioning\nwithout any extra postprocessing, we propose to inject the\ncondition at every frame. However, we still need to deal\nwith a challenge: the condition never varies at training\ntime. Therefore, at inference time, attention scores are com-\nputed with the embeddings Exm,cmandExn,cnof the pose-\ncondition pairs ( xm,cm) and ( xn,cn) as:\nqT\nmkn= (WqExm,cm)T(WkExn,cn) =ET\nxm,cmWT\nqWkExn,cn.\n(2)\nWhen cm̸=cn,qT\nmknwas never encountered during train-\ning. If instead of injecting the condition at every frame, we\nused cross-attention layers, distinct conditions would also\nbe temporally mixed, and we would face the same prob-\nlem. To reduce the presence and impact of such training-\ninference misalignment, we introduce PCCAT, see Fig. 3,\nwhich aims at minimizing the entanglement between condi-\ntions and noisy poses. Specifically, PCCAT combines every\nframe’s noisy pose and condition into queries, while using\nonly noisy poses as keys and values. Thus, Eq. 2 becomes:\nqT\nmkn= (WqExm,cm)T(WkExn) =ET\nxm,cmWT\nqWkExn.(3)\n460\nWith PCCAT, the attention output for pose mbecomes a\nweighted average of the value projections of its neighboring\nnoisy poses. A residual connection adds the PCCAT output\nto the noisy poses. With comprehensive coverage of the mo-\ntion spectrum in the training dataset, the network observes\nvarious poses preceding and following each pose, particu-\nlarly within its local neighborhood. Therefore, local rela-\ntionships do not suffer from unseen intermediate represen-\ntations. Still, there is an obstacle to address: long-range de-\npendencies. However, as discussed in Sec. 3.2, their impor-\ntance is mostly confined to the initial stages of denoising.\nThere, the network is exposed to very noisy motion data,\nthus becoming robust to such unseen combinations of poses.\nIn the latest denoising stages, when the network deals with\nalmost clean input sequences, global dependencies have al-\nready been developed and attention is short-ranged (Fig. 2).\n4. Experiments\n4.1. Experimental setup\nDatasets. Our experiments are conducted on the Babel [63]\nand HumanML3D [26] datasets, with their train and test\nsplits. HumanML3D features multiple textual descriptions\nof each motion sequence, but lacks explicit transition an-\nnotations, making supervised learning infeasible for transi-\ntion generation. Babel, on the other hand, provides finely-\ngrained textual descriptions at an atomic level, including\ntransitions, which facilitates more precise and dynamic mo-\ntion control but also presents a greater challenge due to\nfast and short transitions. To demonstrate the flexibility of\nFlowMDM, we employ the standard motion representations\nprovided with each dataset. HumanML3D utilizes a 263D\npose vector that includes joint coordinates, angles, veloci-\nties, and feet contact. By contrast, Babel uses the global po-\nsition and orientation and a 6D rotation representation [103]\nof the SMPL model joints [13], as in [61].\nEvaluation. Our evaluation uses the metrics established\nby [26], and later refined for this task in [47, 70, 92]. More\nspecifically, motion sequences are synthesized as compo-\nsitions of 32 pairs of textual descriptions and their dura-\ntions. The 32 subsequences and the 31 transitions between\nSi−1andSipairs are evaluated independently. In partic-\nular, each transition is defined as the set of consecutive\nposes {xτi−Ltr/2, . . . , x τi+Ltr/2−1}, sharingLtr\n2frames\nwithSi−1andSi. The transition duration Ltris set to 30/60\nframes for Babel/HumanML3D (1/3 seconds). The top-3\nR-precision (R-prec), and the multimodal distance (MM-\nDist) are used to evaluate how well the subsequences’ mo-\ntion matches their textual description [26]. The FID score\nand the average pairwise distance among all motion em-\nbeddings (diversity) assess the quality and variety of both\nsubsequences and transitions, respectively [26, 31].\nClosing the gap: the Jerk. Generative models are hardto evaluate [19, 68, 91]. The FID score [31] has proven to be\na very reliable metric in quantifying the similarity between\ndistributions of generated and real motion data while be-\ning sensitive to motion artifacts or noise [54]. Nevertheless,\nonly relying on perceptual metrics like FID for assessing\ntransition quality can be misleading due to their insensitiv-\nity to motion anomalies such as abrupt accelerations [8], or\nfoot skating [55]. To complement the FID, our work in-\ntroduces two novel metrics built upon the concept of jerk\n(i.e., time derivative of acceleration), which is indicative of\nmotion smoothness and proven sensitive to kinetic irregu-\nlarities [5, 6, 16, 25, 34, 43, 95]. Given that natural human\nmotion typically exhibits constrained jerk due to relatively\nconsistent acceleration patterns [25, 43], our metrics are tai-\nlored to highlight persistent deviations from this norm on\ntransitions. Firstly, we compute the Peak Jerk (PJ) as the\nmaximum jerk value throughout the transition motion over\nall joints. While this measure captures extreme fluctuations,\nit favors models that unnaturally smooth transitions across\nseveral wider peaks of jerk. To measure this undesirable\neffect, we introduce the Area Under the Jerk (AUJ), calcu-\nlated as the sum of L1-norm differences between a method’s\ninstantaneous jerk and the dataset’s average jerk value. This\nmeasure serves as an aggregate indicator of motion smooth-\nness, quantifying the cumulative deviation from natural hu-\nman movement across the entire transition. The PJ and AUJ\nof a transition are formally defined as follows:\nPJ= max\n1≤i≤K\n1≤τ≤Ltr|ji(τ)|1,AUJ =LtrX\nτ=1max\n1≤i≤K|ji(τ)−javg|1,\n(4)\nwhere ji(τ)is the jerk at time τfor joint i,Kis the number\nof joints, and javgis the average joints-wise maximum jerk\nacross the dataset.\nBaselines. We compare our method to publicly released\nrelated works that can generate sequential motions from\ntext: the autoregressive TEACH [4], and the diffusion sam-\npling techniques DoubleTake [70], DiffCollage [100], and\nMultiDiffusion [7]. Sampling techniques are evaluated with\nPCCAT and APE for a fairer comparison. Additionally, we\nevaluate TEACH with its spherical linear interpolation over\ntransitions turned off (TEACH B), and DoubleTake with\nMDM, as originally proposed (DoubleTake*). TEACH and\nTEACH B cannot be trained for HumanML3D due to the\nlack of pairs of consecutive actions and textual descriptions.\nImplementation details. We tune the hyperparameters\nof all models with grid search. The attention horizon for\nRPE, H, is set to 100/150 for Babel/HumanML3D. The\nnumber of diffusion steps is 1K for all experiments. Our\nmodel is trained with the x0parameterization [87], and min-\nimizes the L2 reconstruction loss. During training, RPE and\nAPE are alternated randomly at a frequency of 0.5. We use\nclassifier-free guidance with weights 1.5/2.5 [33]. We use\n461\nGT TEACH_B TEACH DoubleTake DiffCollage MultiDiffusion FlowMDM\nTransition0.00.20.40.60.81.01.2\nTransition jerk - Extrapolation on Babel\nTransition0.00.10.20.30.40.50.6\nTransition jerk - Extrapolation on HumanML3D\nTransition0.00.20.40.60.81.01.2Maximum jerk over joints\nTransition jerk - Composition on Babel\nTransition0.00.10.20.30.40.50.6\nTransition jerk - Composition on HumanML3DFigure 4. Transitions smoothness. Average maximum jerk over joints at each frame of the transitions for both motion composition\n(left) and extrapolation (right) tasks. While other methods show severe smoothness artifacts in the beginning and end of their transition\nrefinement processes, FlowMDM’s jerk curve has the shortest peak for composition, and an absence of peaks for extrapolation.\nSubsequence Transition\nR-prec ↑ FID↓ Div→ MM-Dist ↓ FID↓ Div→ PJ→ AUJ↓\nGT 0.715±0.0030.00±0.008.42±0.153.36±0.000.00±0.006.20±0.060.02±0.000.00±0.00\nTEACH B 0.703±0.0021.71±0.038.18±0.143.43±0.013.01±0.046.23±0.051.09±0.002.35±0.01\nTEACH 0.655±0.0021.82±0.027.96±0.113.72±0.013.27±0.046.14±0.060.07±0.000.44±0.00\nDoubleTake* 0.596±0.0053.16±0.067.53±0.114.17±0.023.33±0.066.16±0.050.28±0.001.04±0.01\nDoubleTake 0.668±0.0051.33±0.047.98±0.123.67±0.033.15±0.056.14±0.070.17±0.000.64±0.01\nMultiDiffusion 0.702±0.0051.74±0.048.37±0.133.43±0.026.56±0.125.72±0.070.18±0.000.68±0.00\nDiffCollage 0.671±0.0031.45±0.057.93±0.093.71±0.014.36±0.096.09±0.080.19±0.000.84±0.01\nFlowMDM 0.702±0.0040.99±0.048.36±0.133.45±0.022.61±0.066.47±0.050.06±0.000.13±0.00\nTable 1. Comparison of FlowMDM with the state of the art in Babel. Symbols ↑,↓, and→indicate that higher, lower, or values closer to\nthe ground truth (GT) are better, respectively. Evaluation is run 10 times and ±specifies the 95% confidence intervals.\nSubsequence Transition\nR-prec ↑ FID↓ Div→ MM-Dist ↓ FID↓ Div→ PJ→ AUJ↓\nGT 0.796±0.0040.00±0.009.34±0.082.97±0.010.00±0.009.54±0.150.04±0.000.07±0.00\nDoubleTake* 0.643±0.0050.80±0.029.20±0.113.92±0.011.71±0.058.82±0.130.52±0.012.10±0.03\nDoubleTake 0.628±0.0051.25±0.049.09±0.124.01±0.014.19±0.098.45±0.090.48±0.001.83±0.02\nMultiDiffusion 0.629±0.0021.19±0.039.38±0.084.02±0.014.31±0.068.37±0.100.17±0.001.06±0.01\nDiffCollage 0.615±0.0051.56±0.048.79±0.084.13±0.024.59±0.108.22±0.110.26±0.002.85±0.09\nFlowMDM 0.685±0.0040.29±0.019.58±0.123.61±0.011.38±0.058.79±0.090.06±0.000.51±0.01\nTable 2. Comparison of FlowMDM with the state of the art in HumanML3D.\na binary step function to guide the BPE sampling, yielding\n125/60 initial APE steps. The minimum/maximum lengths\nfor training subsequences are set to 30/200 and 70/200\nframes (i.e., 1/6.7s and 3.5/10s). For Babel, training sub-\nsequences include consecutive ground truth motions with\ndistinct textual descriptions in order to increase the motions\nvariability, and make the network explicitly robust to mul-\ntiple conditions. The ablation study includes two condi-\ntioning baselines: 1) concatenating each frame’s condition\nand noisy pose, and replacing the PCCAT with vanilla self-\nattention (SAT), and 2) injecting the condition with cross-\nattention layers (CAT). More details in supp. material A.\n4.2. Quantitative analysis\nComparison with the state of the art on HMC. Tables\n1 and 2 show the comparison of FlowMDM with current\nstate-of-the-art models in Babel and HumanML3D datasets,\nrespectively. In HumanML3D, our model outperforms by\na fair margin the other methods in terms of subsequence\naccuracy-wise metrics (R-prec and MM-Dist), and FID. InBabel, it matches the state of the art in accuracy and excels\nin FID score. FlowMDM produces transitions of higher\nquality and smoothness on both datasets, as indicated by\nFID, PJ, and AUJ metrics. The lack of correlation be-\ntween the FID score and the AUJ underscores the impor-\ntance of the latter as a complementary metric for assess-\ning smoothness. Fig. 4-left shows the average jerk values\nacross the generated transitions. We observe that state-of-\nthe-art methods exhibit severe smoothness artifacts. Dur-\ning TEACH’s spherical linear interpolation, the jerk quickly\nreaches values near zero. By contrast, DiffCollage leans to-\nward higher-than-average jerk values, while MultiDiffusion\nexhibits the opposite trend. DoubleTake shows three peaks,\ncaused by their two-stage noise estimation process. In com-\nparison, FlowMDM successfully minimizes peak jerk val-\nues, producing the smoothest transitions between subse-\nquences. See supp. material Sec. C for in-depth analyses.\nHuman motion extrapolation. In single text-to-motion,\nthe duration of the generated motion is limited to the longest\nsubsequence length Lavailable in the training set. Extrap-\n462\nSubsequence Transition\nCond. Train. PE Inf. PE R-prec ↑ FID↓ Div→ MM-Dist ↓ FID↓ Div→ PJ→ AUJ↓\nGT - - 0.715±0.0030.00±0.008.42±0.153.36±0.000.00±0.006.20±0.060.02±0.000.00±0.00\nPCCAT A A 0.699±0.0041.34±0.048.36±0.123.40±0.024.26±0.075.98±0.061.81±0.013.73±0.01\nPCCAT R R 0.635±0.0061.28±0.038.05±0.114.02±0.022.18±0.076.14±0.080.03±0.000.20±0.00\nPCCAT B A 0.716±0.0061.20±0.048.31±0.143.32±0.023.01±0.066.35±0.071.78±0.013.66±0.02\nPCCAT B R 0.635±0.0040.85±0.028.25±0.123.98±0.022.14±0.046.44±0.090.04±0.000.15±0.00\nSAT B B 0.681±0.0041.52±0.048.22±0.113.61±0.021.91±0.036.41±0.070.06±0.000.12±0.00\nCAT B B 0.719±0.0041.29±0.028.16±0.133.27±0.022.57±0.086.06±0.070.02±0.000.07±0.00\nPCCAT B B 0.702±0.0040.99±0.048.36±0.133.45±0.022.61±0.066.47±0.050.06±0.000.13±0.00\nTable 3. Ablation study in Babel. Cond. indicates the conditioning scheme, Train./Inf. PE specify the positional encodings (PE) used at\ntraining/inference time, and A, R, and B refer to absolute, relative, and blended PE, respectively. ↑,↓, and→indicate that higher, lower,\nor values closer to the ground truth (GT) are better, respectively. Evaluation is run 10 times and ±specifies the 95% confidence intervals.\nSubsequence Transition\nCond. Train. PE Inf. PE R-prec ↑ FID↓ Div→ MM-Dist ↓ FID↓ Div→ PJ→ AUJ↓\nGT - - 0.796±0.0040.00±0.009.34±0.082.97±0.010.00±0.009.54±0.150.04±0.000.07±0.00\nPCCAT A A 0.689±0.0050.66±0.029.73±0.123.63±0.023.90±0.128.29±0.081.50±0.013.40±0.02\nPCCAT R R 0.531±0.0051.75±0.078.71±0.104.80±0.032.53±0.128.62±0.080.03±0.000.58±0.01\nPCCAT B A 0.699±0.0050.61±0.029.76±0.103.54±0.022.42±0.098.39±0.091.40±0.013.29±0.02\nPCCAT B R 0.554±0.0071.06±0.069.02±0.114.54±0.021.12±0.049.00±0.100.05±0.000.53±0.01\nSAT B B 0.692±0.0040.49±0.029.08±0.093.51±0.013.19±0.088.09±0.110.04±0.000.36±0.02\nCAT B B 0.622±0.0051.27±0.048.86±0.154.10±0.013.93±0.148.23±0.100.04±0.000.49±0.02\nPCCAT B B 0.685±0.0040.29±0.019.58±0.123.61±0.011.38±0.058.79±0.090.06±0.000.51±0.01\nTable 4. Ablation study in HumanML3D.\nolating periodic actions into sequences longer than those\nin the ground truth presents a notable challenge. Achiev-\ning this through HMC requires the harmonization of pe-\nriodicity across adjacent subsequences. Common strate-\ngies that combine independently generated subsequences\noften disrupt the periodicity of the motion. To assess our\nmodel’s capabilities in addressing this issue, we construct\nan evaluation set comprising 32 consecutive repetitions of\n32 different extrapolatable actions such as ‘walk forward’,\n‘jumping’, or ‘playing the guitar’, extracted from the Babel\nand HumanML3D test sets (more details in supp. mate-\nrial Sec. B). Fig. 4-right displays the motion jerk across\ntransitions for all models on this task. We observe that,\nwhile other models exhibit smoothness anomalies similar to\nthose in HMC, FlowMDM closely mirrors the ground truth\njerk. This observation indicates that the jerk peak noted in\nFlowMDM for the composition task is likely attributed to\nsmoothness irregularities in more complex transitions.\nAblation study. The effectiveness of BPE and PCCAT is\npresented in Tables 3 and 4. Reasonably, the baseline model\ntrained solely with APE fails to generate smooth transitions.\nConversely, a model trained only with RPE, despite produc-\ning the smoothest transitions, struggles to model global mo-\ntion dependencies and accurately reflect the corresponding\ntextual descriptions. Interestingly, training with BPE im-\nproves the performance of both APE- and RPE-only sam-\nplings. Sampling with BPE combines the best of both\nworlds by preserving the excellent AUJ values of the RPE\nmodels and reaching the state-of-the-art accuracy and FID\nBabel HumanML3D\nSubsequences TransitionsFigure 5. BPE trade-offs. Increasing the number of APE steps\nundergone during BPE sampling improves the correspondence be-\ntween motion and textual description (R-prec), but reduces the\ntransition realism and smoothness (FID and AUJ). The best bal-\nance is reached around 10% of APE denoising steps.\nscores of the APE models. Fig. 5 illustrates this balance.\nSpecifically, increasing the number of APE steps enhances\nthe motion’s congruence with the textual description, at the\ncost of reducing the smoothness and realism of the tran-\nsitions. In HumanML3D, the SAT and CAT conditioning\nschemes lead to worse transitions in terms of FID and di-\nversity. This is caused by the coexistence of different con-\nditions in the local neighborhood of the transition at infer-\nence, which never happens during training. Our PCCAT\nconditioning technique effectively solves this problem. In\nBabel, such effect is not present because the training motion\nsequences include several subsequences, thus increasing the\nmodel’s robustness to transitions with varying conditions.\n463\nC)\nD)TEACH DoubleT ake DiﬀCollage MultiDiﬀusion FlowMDM (Ours)\nB)\nC)\nD)\nA)Figure 6. Qualitative analysis (Babel). A) and B) show compositions of 3 motions (‘walk straight’ − →‘side steps’ − →‘walk backward’, and\n‘walk’− →‘turn around’ − →‘sit on the bench’, respectively), and C) and D) illustrate extrapolations that repeat 6 times a static (‘t-pose’) and\na dynamic (‘step to the right’) action, respectively. Solid curves match the trajectories of the global position (blue) and left/right hands\n(purple/green). Darker colors indicate instantaneous jerk deviations from the median value, saturating at twice the jerk’s standard deviation\nin the dataset (black segments). Abrupt transitions manifest as black segments amidst lighter ones. FlowMDM exhibits the most fluid\nmotion and preserves the staticity or periodicity of extrapolated actions, in contrast to other methods that show spontaneous high jerk\nvalues and fail to keep the motion coherence in extrapolations.\nOn the efficiency of FlowMDM. Diffusion-based state-\nof-the-art methods such as MultiDiffusion and DiffCollage\ndenoise poses from the transition more than once in order to\nharmonize it with the adjacent motions. DoubleTake’s tran-\nsitions undergo an additional denoising process, which adds\ncomputational burden and can not be parallelized. Oppo-\nsitely, FlowMDM does not apply redundant denoising steps\nto any pose. In particular, our model goes through 47.1%,\n28.4%, and 16.5% less pose-wise denoising steps than Dou-\nbleTake, DiffCollage, and MultiDiffusion, respectively.\n4.3. Qualitative results\nFig. 6 illustrates how our quantitative findings translate into\nvisual outcomes on the human motion composition and ex-\ntrapolation tasks. First, as anticipated by Fig. 4, we con-\nfirm that state-of-the-art methods produce short intervals of\njerk peaks around transitions. These do not typically match\nlong-range motion scenarios, where such jerks might be\ncontextually appropriate. Contrarily, FlowMDM produces\nmotion that is realistic, accurate, and smooth. Particularly,\nwe notice that DiffCollage’s bias toward producing con-\nstantly high jerk values around transitions is perceived as\nan overall chaotic motion. Due to the independent gener-\nation of their subsequences, DoubleTake, DiffCollage, and\nMultiDiffusion are unable to maintain the static or periodic\nnature of actions when extrapolating them. Only TEACH\nand FlowMDM are able to successfully extrapolate a static\n‘t-pose’, and ours is the only one capable of extrapolating a\n’step to the right’ sequence realistically. Finally, FlowMDMalso inherits the trajectory control capabilities of motion dif-\nfusion models as shown in Fig. 1-right.\n5. Conclusion\nWe presented FlowMDM, the first approach that generates\nhuman motion compositions simultaneously, without un-\ndergoing postprocessing or redundant denoising diffusion\nsteps. We also introduced the blended positional encodings\nto combine the benefits of absolute and relative positional\nencodings during the denoising chain. Finally, we presented\nthe pose-centric cross-attention, a technique that improves\nthe generation of transitions when training with only a sin-\ngle condition per motion sequence.\nLimitations and future work. The absolute stage of\nBPE does not model relationships between subsequences.\nConsequently, their low-frequency spectrum is generated\nindependently. This limitation could be addressed in fu-\nture work by incorporating an intention planning module.\nFinally, our method learns a strong motion prior that gener-\nates transitions between combinations of actions never seen\nat training time. Such capability could theoretically be used\nwith different models leveraging different control signals,\nassuming they all are trained under the same framework.\nFuture work will experimentally validate this hypothesis.\nAcknowledgements. This work has been partially\nsupported by the Spanish projects PID2022-136436NB-I00,\nTED2021-131317B-I00, and PDC2022-133305-I00 and by\nICREA under the ICREA Academia programme.\n464\nReferences\n[1] Vida Adeli, Mahsa Ehsanpour, Ian Reid, Juan Car-\nlos Niebles, Silvio Savarese, Ehsan Adeli, and Hamid\nRezatofighi. Tripod: Human trajectory and pose dynamics\nforecasting in the wild. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision , pages 13390–\n13400, 2021. 2\n[2] Simon Alexanderson, Rajmund Nagy, Jonas Beskow, and\nGustav Eje Henter. Listen, denoise, action! audio-driven\nmotion synthesis with diffusion models. ACM Transactions\non Graphics (TOG) , 42(4):1–20, 2023. 2\n[3] Sadegh Aliakbarian, Microsoft Fatemeh Saleh ACRV ,\nStephen Gould ACRV , and Anu Mathieu Salzmann CVLab.\nContextually plausible and diverse 3d human motion pre-\ndiction. Proceedings of the IEEE/CVF International Con-\nference on Computer Vision (ICCV) , 2021. 2\n[4] Nikos Athanasiou, Mathis Petrovich, Michael J Black, and\nG¨ul Varol. Teach: Temporal action composition for 3d\nhumans. In 2022 International Conference on 3D Vision\n(3DV) , pages 414–423. IEEE, 2022. 2, 5\n[5] Sivakumar Balasubramanian, Alejandro Melendez-\nCalderon, and Etienne Burdet. A robust and sensitive\nmetric for quantifying movement smoothness. IEEE\ntransactions on biomedical engineering , 59(8):2126–2136,\n2011. 5\n[6] Sivakumar Balasubramanian, Alejandro Melendez-\nCalderon, Agnes Roby-Brami, and Etienne Burdet.\nOn the analysis of movement smoothness. Journal of\nneuroengineering and rehabilitation , 12(1):1–11, 2015. 5\n[7] Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel.\nMultidiffusion: Fusing diffusion paths for controlled image\ngeneration. In International Conference on Machine Learn-\ning, pages 1737–1752. PMLR, 2023. 2, 3, 5\n[8] German Barquero, Sergio Escalera, and Cristina Palmero.\nBelfusion: Latent diffusion for behavior-driven human mo-\ntion prediction. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision , pages 2317–2327,\n2023. 2, 5\n[9] German Barquero, Johnny N ´unez, Sergio Escalera, Zhen\nXu, Wei-Wei Tu, Isabelle Guyon, and Cristina Palmero.\nDidn’t see that coming: a survey on non-verbal social hu-\nman behavior forecasting. In Understanding Social Behav-\nior in Dyadic and Small Group Interactions , pages 139–\n178. PMLR, 2022. 2\n[10] German Barquero, Johnny N ´u˜nez, Zhen Xu, Sergio Es-\ncalera, Wei-Wei Tu, Isabelle Guyon, and Cristina Palmero.\nComparison of spatio-temporal models for human motion\nand pose forecasting in face-to-face interaction scenarios.\nInUnderstanding Social Behavior in Dyadic and Small\nGroup Interactions , pages 107–138. PMLR, 2022. 2\n[11] Iz Beltagy, Matthew E Peters, and Arman Cohan. Long-\nformer: The long-document transformer. arXiv preprint\narXiv:2004.05150 , 2020. 4\n[12] Bharat Lal Bhatnagar, Xianghui Xie, Ilya A Petrov, Cristian\nSminchisescu, Christian Theobalt, and Gerard Pons-Moll.\nBehave: Dataset and method for tracking human object in-\nteractions. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 15935–15946, 2022. 2\n[13] Federica Bogo, Angjoo Kanazawa, Christoph Lassner, Pe-\nter Gehler, Javier Romero, and Michael J Black. Keep it\nsmpl: Automatic estimation of 3d human pose and shape\nfrom a single image. In Computer Vision–ECCV 2016: 14th\nEuropean Conference, Amsterdam, The Netherlands, Octo-\nber 11-14, 2016, Proceedings, Part V 14 , pages 561–578.\nSpringer, 2016. 5\n[14] Paulo Vinicius Koerich Borges, Nicola Conci, and Andrea\nCavallaro. Video-based human behavior understanding: A\nsurvey. IEEE transactions on circuits and systems for video\ntechnology , 23(11):1993–2008, 2013. 1\n[15] Zhe Cao, Hang Gao, Karttikeya Mangalam, Qi-Zhi Cai,\nMinh V o, and Jitendra Malik. Long-term human mo-\ntion prediction with scene context. In Computer Vision–\nECCV 2020: 16th European Conference, Glasgow, UK, Au-\ngust 23–28, 2020, Proceedings, Part I 16 , pages 387–404.\nSpringer, 2020. 2\n[16] Angela Castillo, Maria Escobar, Guillaume Jeanneret, Al-\nbert Pumarola, Pablo Arbel ´aez, Ali Thabet, and Artsiom\nSanakoyeu. Bodiffusion: Diffusing sparse observations for\nfull-body human motion synthesis. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision ,\npages 4221–4231, 2023. 5\n[17] Kang Chen, Zhipeng Tan, Jin Lei, Song-Hai Zhang, Yuan-\nChen Guo, Weidong Zhang, and Shi-Min Hu. Choreomas-\nter: choreography-oriented music-driven dance synthesis.\nACM Transactions on Graphics (TOG) , 40(4):1–13, 2021.\n2\n[18] Enric Corona, Albert Pumarola, Guillem Alenya, and\nFrancesc Moreno-Noguer. Context-aware human motion\nprediction. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , pages 6992–\n7001, 2020. 2\n[19] Antonia Creswell, Tom White, Vincent Dumoulin, Kai\nArulkumaran, Biswa Sengupta, and Anil A Bharath. Gen-\nerative adversarial networks: An overview. IEEE signal\nprocessing magazine , 35(1):53–65, 2018. 5\n[20] Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu,\nand Mubarak Shah. Diffusion models in vision: A survey.\nIEEE Transactions on Pattern Analysis and Machine Intel-\nligence , 2023. 2\n[21] Rishabh Dabral, Muhammad Hamza Mughal, Vladislav\nGolyanik, and Christian Theobalt. Mofusion: A frame-\nwork for denoising-diffusion-based motion synthesis. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition , pages 9760–9770, 2023. 2\n[22] Tri Dao. Flashattention-2: Faster attention with better par-\nallelism and work partitioning. In The Twelfth International\nConference on Learning Representations , 2023. 4\n[23] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christo-\npher R ´e. Flashattention: Fast and memory-efficient exact\nattention with io-awareness. Advances in Neural Informa-\ntion Processing Systems , 35:16344–16359, 2022. 4\n[24] David A Engstr ¨om, JA Scott Kelso, and Tom Holroyd.\nReaction-anticipation transitions in human perception-\naction patterns. Human movement science , 15(6):809–832,\n1996. 2\n[25] Philipp Gulde and Joachim Hermsd ¨orfer. Smoothness met-\n465\nrics in complex movement tasks. Frontiers in neurology ,\n9:615, 2018. 5\n[26] Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji,\nXingyu Li, and Li Cheng. Generating diverse and natural 3d\nhuman motions from text. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\npages 5152–5161, 2022. 2, 5\n[27] Chuan Guo, Xinxin Zuo, Sen Wang, and Li Cheng. Tm2t:\nStochastic and tokenized modeling for the reciprocal gener-\nation of 3d human motions and texts. In European Confer-\nence on Computer Vision , pages 580–597. Springer, 2022.\n2\n[28] Wen Guo, Xiaoyu Bie, Xavier Alameda-Pineda, and\nFrancesc Moreno-Noguer. Multi-person extreme motion\nprediction. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 13053–\n13064, 2022. 2\n[29] F ´elix G Harvey, Mike Yurick, Derek Nowrouzezahrai, and\nChristopher Pal. Robust motion in-betweening. ACM\nTransactions on Graphics (TOG) , 39(4):60–1, 2020. 2\n[30] Mohamed Hassan, Duygu Ceylan, Ruben Villegas, Jun\nSaito, Jimei Yang, Yi Zhou, and Michael J Black. Stochas-\ntic scene-aware motion prediction. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision ,\npages 11374–11384, 2021. 2\n[31] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. Gans trained by a\ntwo time-scale update rule converge to a local nash equilib-\nrium. Advances in neural information processing systems ,\n30, 2017. 5\n[32] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. Advances in Neural Informa-\ntion Processing Systems , 33:6840–6851, 2020. 2\n[33] Jonathan Ho and Tim Salimans. Classifier-free diffusion\nguidance. In NeurIPS 2021 Workshop on Deep Generative\nModels and Downstream Applications , 2021. 5\n[34] Neville Hogan and Dagmar Sternad. Sensitivity of smooth-\nness measures to movement duration, amplitude, and ar-\nrests. Journal of motor behavior , 41(6):529–534, 2009. 5\n[35] Biao Jiang, Xin Chen, Wen Liu, Jingyi Yu, Gang Yu, and\nTao Chen. Motiongpt: Human motion as a foreign lan-\nguage. Advances in Neural Information Processing Sys-\ntems, 36, 2024. 2\n[36] Manuel Kaufmann, Emre Aksan, Jie Song, Fabrizio Pece,\nRemo Ziegler, and Otmar Hilliges. Convolutional autoen-\ncoders for human motion infilling. In 2020 International\nConference on 3D Vision (3DV) , pages 918–927. IEEE,\n2020. 2\n[37] Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan Nate-\nsan Ramamurthy, Payel Das, and Siva Reddy. The impact of\npositional encoding on length generalization in transform-\ners. Advances in Neural Information Processing Systems ,\n36, 2024. 3\n[38] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina\nToutanova. Bert: Pre-training of deep bidirectional trans-\nformers for language understanding. In Proceedings of\nNAACL-HLT , pages 4171–4186, 2019. 3\n[39] Jihoon Kim, Taehyun Byun, Seungyoun Shin, Jung-\ndam Won, and Sungjoon Choi. Conditional motion in-betweening. Pattern Recognition , 132:108894, 2022. 2\n[40] Jihoon Kim, Jiseob Kim, and Sungjoon Choi. Flame: Free-\nform language-based motion synthesis & editing. In Pro-\nceedings of the AAAI Conference on Artificial Intelligence ,\nvolume 37, pages 8255–8263, 2023. 2\n[41] Nilesh Kulkarni, Davis Rempe, Kyle Genova, Abhi-\njit Kundu, Justin Johnson, David Fouhey, and Leonidas\nGuibas. Nifty: Neural object interaction fields for guided\nhuman motion synthesis. arXiv preprint arXiv:2307.07511 ,\n2023. 2\n[42] Wilfried Kunde, Katrin Elsner, and Andrea Kiesel. No\nanticipation–no action: the role of anticipation in action and\nperception. Cognitive Processing , 8:71–78, 2007. 2\n[43] Caroline Larboulette and Sylvie Gibet. A review of com-\nputable expressive descriptors of human motion. In Pro-\nceedings of the 2nd International Workshop on Movement\nand Computing , pages 21–28, 2015. 5\n[44] Taeryung Lee, Gyeongsik Moon, and Kyoung Mu Lee.\nMultiact: Long-term 3d human motion generation from\nmultiple action labels. In Proceedings of the AAAI Con-\nference on Artificial Intelligence , volume 37, pages 1231–\n1239, 2023. 2\n[45] Jiaman Li, Ruben Villegas, Duygu Ceylan, Jimei Yang,\nZhengfei Kuang, Hao Li, and Yajie Zhao. Task-generic hi-\nerarchical human motion prior using vaes. In 2021 Inter-\nnational Conference on 3D Vision (3DV) , pages 771–781.\nIEEE, 2021. 2\n[46] Ruilong Li, Shan Yang, David A Ross, and Angjoo\nKanazawa. Ai choreographer: Music conditioned 3d dance\ngeneration with aist++. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision , pages 13401–\n13412, 2021. 2\n[47] Shuai Li, Sisi Zhuang, Wenfeng Song, Xinyu Zhang, Hejia\nChen, and Aimin Hao. Sequential texts driven cohesive mo-\ntions synthesis with natural transitions. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vi-\nsion, pages 9498–9508, 2023. 2, 5\n[48] Weiyu Li, Xuelin Chen, Peizhuo Li, Olga Sorkine-\nHornung, and Baoquan Chen. Example-based motion syn-\nthesis via generative motion matching. ACM Transactions\non Graphics (TOG) , 42(4):1–12, 2023. 2\n[49] Yunhao Li, Zhenbo Yu, Yucheng Zhu, Bingbing Ni, Guang-\ntao Zhai, and Wei Shen. Skeleton2humanoid: Animat-\ning simulated characters for physically-plausible motion in-\nbetweening. In Proceedings of the 30th ACM International\nConference on Multimedia , pages 1493–1502, 2022. 2\n[50] Jing Lin, Ailing Zeng, Shunlin Lu, Yuanhao Cai, Ruimao\nZhang, Haoqian Wang, and Lei Zhang. Motion-x: A large-\nscale 3d expressive whole-body human motion dataset. In\nThirty-seventh Conference on Neural Information Process-\ning Systems Datasets and Benchmarks Track , 2023. 2\n[51] Zhengyi Luo, Jinkun Cao, Kris Kitani, Weipeng Xu, et al.\nPerpetual humanoid control for real-time simulated avatars.\nInProceedings of the IEEE/CVF International Conference\non Computer Vision , pages 10895–10904, 2023. 2\n[52] Hengbo Ma, Jiachen Li, Ramtin Hosseini, Masayoshi\nTomizuka, and Chiho Choi. Multi-objective diverse human\nmotion prediction with knowledge distillation. Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\n466\ntern Recognition , 2022. 2\n[53] Shugao Ma, Tomas Simon, Jason Saragih, Dawei Wang,\nYuecheng Li, Fernando De La Torre, and Yaser Sheikh.\nPixel codec avatars. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition , pages\n64–73, 2021. 1\n[54] Antoine Maiorca, Youngwoo Yoon, and Thierry Dutoit.\nEvaluating the quality of a synthesized motion with the\nfr´echet motion distance. In ACM SIGGRAPH 2022 Posters ,\npages 1–2, 2022. 5\n[55] Antoine Maiorca, Youngwoo Yoon, and Thierry Dutoit.\nValidating objective evaluation metric: Is fr ´echet motion\ndistance able to capture foot skating artifacts? In Proceed-\nings of the 2023 ACM International Conference on Interac-\ntive Media Experiences , pages 242–247, 2023. 5\n[56] Wei Mao, Miaomiao Liu, and Mathieu Salzmann. Generat-\ning smooth pose sequences for diverse human motion pre-\ndiction. Proceedings of the IEEE/CVF International Con-\nference on Computer Vision (ICCV) , 2021. 2\n[57] Boris N Oreshkin, Antonios Valkanas, F ´elix G Harvey,\nLouis-Simon M ´enard, Florent Bocquelet, and Mark J\nCoates. Motion in-betweening via deep delta-interpolator.\nIEEE Transactions on Visualization and Computer Graph-\nics, 2023. 2\n[58] Cristina Palmero, German Barquero, Julio CS Jacques Ju-\nnior, Albert Clap ´es, Johnny N ´unez, David Curto, Sorina\nSmeureanu, Javier Selva, Zejian Zhang, David Saeteros,\net al. Chalearn lap challenges on self-reported personal-\nity recognition and non-verbal behavior forecasting during\nsocial dyadic interactions: Dataset, design, and results. In\nUnderstanding Social Behavior in Dyadic and Small Group\nInteractions , pages 4–52. PMLR, 2022. 2\n[59] Zizheng Pan, Jianfei Cai, and Bohan Zhuang. Fast vision\ntransformers with hilo attention. Advances in Neural Infor-\nmation Processing Systems , 35:14541–14554, 2022. 3\n[60] Sang-Min Park and Young-Gab Kim. A metaverse: Taxon-\nomy, components, applications, and open challenges. IEEE\naccess , 10:4209–4251, 2022. 1\n[61] Mathis Petrovich, Michael J Black, and G ¨ul Varol. Temos:\nGenerating diverse human motions from textual descrip-\ntions. In European Conference on Computer Vision , pages\n480–497. Springer, 2022. 2, 5\n[62] Matthias Plappert, Christian Mandery, and Tamim Asfour.\nThe kit motion-language dataset. Big data , 4(4):236–252,\n2016. 2\n[63] Abhinanda R Punnakkal, Arjun Chandrasekaran, Nikos\nAthanasiou, Alejandra Quiros-Ramirez, and Michael J\nBlack. Babel: Bodies, action and behavior with english\nlabels. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 722–731,\n2021. 2, 5\n[64] Yijun Qian, Jack Urbanek, Alexander G Hauptmann, and\nJungdam Won. Breaking the limits of text-conditioned 3d\nmotion synthesis with elaborative descriptions. In Proceed-\nings of the IEEE/CVF International Conference on Com-\nputer Vision , pages 2306–2316, 2023. 2\n[65] Jia Qin, Youyi Zheng, and Kun Zhou. Motion in-\nbetweening via two-stage transformers. ACM Transactions\non Graphics (TOG) , 41(6):1–16, 2022. 2[66] Tianxiang Ren, Jubo Yu, Shihui Guo, Ying Ma, Yutao\nOuyang, Zijiao Zeng, Yazhan Zhang, and Yipeng Qin. Di-\nverse motion in-betweening with dual posture stitching.\narXiv preprint arXiv:2303.14457 , 2023. 2\n[67] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj ¨orn Ommer. High-resolution image\nsynthesis with latent diffusion models. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition , pages 10684–10695, 2022. 2\n[68] Mehdi SM Sajjadi, Olivier Bachem, Mario Lucic, Olivier\nBousquet, and Sylvain Gelly. Assessing generative models\nvia precision and recall. Advances in neural information\nprocessing systems , 31, 2018. 5\n[69] Tim Salzmann, Marco Pavone, and Markus Ryll. Motron:\nMultimodal probabilistic human motion forecasting. Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition , 2022. 2\n[70] Yoni Shafir, Guy Tevet, Roy Kapon, and Amit Haim\nBermano. Human motion diffusion as a generative prior.\nInThe Twelfth International Conference on Learning Rep-\nresentations , 2023. 2, 3, 4, 5\n[71] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli. Deep unsupervised learning using\nnonequilibrium thermodynamics. In International Con-\nference on Machine Learning , pages 2256–2265. PMLR,\n2015. 2\n[72] Paul Starke, Sebastian Starke, Taku Komura, and Frank\nSteinicke. Motion in-betweening with phase manifolds.\nProceedings of the ACM on Computer Graphics and Inter-\nactive Techniques , 6(3):1–17, 2023. 2\n[73] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan,\nWen Bo, and Yunfeng Liu. Roformer: Enhanced trans-\nformer with rotary position embedding. Neurocomputing ,\n568:127063, 2024. 4\n[74] Guofei Sun, Yongkang Wong, Zhiyong Cheng, Mohan S\nKankanhalli, Weidong Geng, and Xiangdong Li. Deep-\ndance: music-to-dance motion choreography with adver-\nsarial learning. IEEE Transactions on Multimedia , 23:497–\n509, 2020. 2\n[75] Jiarui Sun and Girish Chowdhary. Towards globally con-\nsistent stochastic human motion prediction via motion dif-\nfusion. arXiv preprint arXiv:2305.12554 , 2023. 2\n[76] Ryo Suzuki, Adnan Karim, Tian Xia, Hooman Hedayati,\nand Nicolai Marquardt. Augmented reality and robotics: A\nsurvey and taxonomy for ar-enhanced human-robot inter-\naction and robotic interfaces. In Proceedings of the 2022\nCHI Conference on Human Factors in Computing Systems ,\npages 1–33, 2022. 1\n[77] Julian Tanke, Linguang Zhang, Amy Zhao, Chengcheng\nTang, Yujun Cai, Lezi Wang, Po-Chen Wu, Juergen Gall,\nand Cem Keskin. Social diffusion: Long-term multiple hu-\nman motion anticipation. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision , pages 9601–\n9611, 2023. 2\n[78] Guy Tevet, Brian Gordon, Amir Hertz, Amit H Bermano,\nand Daniel Cohen-Or. Motionclip: Exposing human mo-\ntion generation to clip space. In European Conference on\nComputer Vision , pages 358–374. Springer, 2022. 2\n[79] Guy Tevet, Sigal Raab, Brian Gordon, Yoni Shafir, Daniel\n467\nCohen-or, and Amit Haim Bermano. Human motion diffu-\nsion model. In The Eleventh International Conference on\nLearning Representations , 2022. 2, 3, 4\n[80] Sibo Tian, Minghui Zheng, and Xiao Liang. Transfu-\nsion: A practical and effective transformer-based diffusion\nmodel for 3d human motion prediction. arXiv preprint\narXiv:2307.16106 , 2023. 2\n[81] Jonathan Tseng, Rodrigo Castellon, and Karen Liu. Edge:\nEditable dance generation from music. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 448–458, 2023. 2\n[82] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. Advances\nin neural information processing systems , 30, 2017. 4\n[83] Jacob Walker, Kenneth Marino, Abhinav Gupta, and Mar-\ntial Hebert. The pose knows: Video forecasting by gener-\nating pose futures. Proceedings of the IEEE international\nconference on computer vision , 2017. 2\n[84] Jingbo Wang, Yu Rong, Jingyuan Liu, Sijie Yan, Dahua\nLin, and Bo Dai. Towards diverse and natural scene-\naware 3d human motion synthesis. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 20460–20469, 2022. 2\n[85] Jiashun Wang, Huazhe Xu, Jingwei Xu, Sifei Liu, and\nXiaolong Wang. Synthesizing long-term 3d human mo-\ntion and interaction in 3d scenes. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 9401–9411, 2021.\n[86] Jingbo Wang, Sijie Yan, Bo Dai, and Dahua Lin. Scene-\naware generative network for human motion synthesis. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition , pages 12206–12215, 2021.\n2\n[87] Zhisheng Xiao, Karsten Kreis, and Arash Vahdat. Tackling\nthe generative learning trilemma with denoising diffusion\nGANs. In International Conference on Learning Represen-\ntations (ICLR) , 2022. 5\n[88] Jiachen Xu, Min Wang, Jingyu Gong, Wentao Liu, Chen\nQian, Yuan Xie, and Lizhuang Ma. Exploring versatile\nprior for human motion via motion frequency guidance. In\n2021 International Conference on 3D Vision (3DV) , pages\n606–616. IEEE, 2021. 2\n[89] Sirui Xu, Zhengyuan Li, Yu-Xiong Wang, and Liang-Yan\nGui. Interdiff: Generating 3d human-object interactions\nwith physics-informed diffusion. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision ,\npages 14928–14940, 2023. 2\n[90] Sirui Xu, Yu-Xiong Wang, and Liangyan Gui. Stochastic\nmulti-person 3d motion forecasting. In The Eleventh Inter-\nnational Conference on Learning Representations , 2022. 2\n[91] Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Run-\nsheng Xu, Yue Zhao, Wentao Zhang, Bin Cui, and Ming-\nHsuan Yang. Diffusion models: A comprehensive survey of\nmethods and applications. ACM Computing Surveys , 2022.\n3, 5\n[92] Zhao Yang, Bing Su, and Ji-Rong Wen. Synthesizing long-\nterm human motions with diffusion models via coherent\nsampling. In Proceedings of the 31st ACM InternationalConference on Multimedia , pages 3954–3964, 2023. 2, 5\n[93] Zijie Ye, Haozhe Wu, Jia Jia, Yaohua Bu, Wei Chen, Fanbo\nMeng, and Yanfeng Wang. Choreonet: Towards music to\ndance synthesis with choreographic action unit. In Proceed-\nings of the 28th ACM International Conference on Multime-\ndia, pages 744–752, 2020. 2\n[94] Hongwei Yi, Chun-Hao P Huang, Shashank Tripathi, Lea\nHering, Justus Thies, and Michael J Black. Mime:\nHuman-aware 3d scene generation. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 12965–12976, 2023. 2\n[95] Xinyu Yi, Yuxiao Zhou, and Feng Xu. Transpose: Real-\ntime 3d human translation and pose estimation with six\ninertial sensors. ACM Transactions on Graphics (TOG) ,\n40(4):1–13, 2021. 5\n[96] Ye Yuan and Kris Kitani. Dlow: Diversifying latent flows\nfor diverse human motion prediction. In Computer Vision–\nECCV 2020: 16th European Conference, Glasgow, UK, Au-\ngust 23–28, 2020, Proceedings, Part IX 16 , pages 346–364.\nSpringer, 2020. 2\n[97] Ye Yuan, Jiaming Song, Umar Iqbal, Arash Vahdat, and Jan\nKautz. Physdiff: Physics-guided human motion diffusion\nmodel. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision , pages 16010–16021, 2023. 2\n[98] Jianrong Zhang, Yangsong Zhang, Xiaodong Cun, Yong\nZhang, Hongwei Zhao, Hongtao Lu, Xi Shen, and Ying\nShan. Generating human motion from textual descrip-\ntions with discrete representations. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 14730–14740, 2023.\n[99] Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou\nHong, Xinying Guo, Lei Yang, and Ziwei Liu. Motiondif-\nfuse: Text-driven human motion generation with diffusion\nmodel. IEEE Transactions on Pattern Analysis and Ma-\nchine Intelligence , 2024. 2, 4\n[100] Qinsheng Zhang, Jiaming Song, Xun Huang, Yongxin\nChen, and Ming-Yu Liu. Diffcollage: Parallel generation\nof large content with diffusion models. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 10188–10198, 2023. 2, 3, 4, 5\n[101] Yan Zhang, Michael J Black, and Siyu Tang. Perpetual mo-\ntion: Generating unbounded human motion. arXiv preprint\narXiv:2007.13886 , 2020. 2\n[102] Yan Zhang and Siyu Tang. The wanderings of odysseus in\n3d scenes. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 20481–\n20491, 2022. 2\n[103] Yi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, and\nHao Li. On the continuity of rotation representations in\nneural networks. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition , pages\n5745–5753, 2019. 5\n[104] Yi Zhou, Zimo Li, Shuangjiu Xiao, Chong He, Zeng\nHuang, and Hao Li. Auto-conditioned recurrent networks\nfor extended complex human motion synthesis. In Interna-\ntional Conference on Learning Representations , 2018. 2,\n3\n[105] Yi Zhou, Jingwan Lu, Connelly Barnes, Jimei Yang, Sitao\nXiang, et al. Generative tweening: Long-term inbetweening\n468\nof 3d human motions. arXiv preprint arXiv:2005.08891 ,\n2020. 2\n[106] Wentao Zhu, Xiaoxuan Ma, Dongwoo Ro, Hai Ci, Jinlu\nZhang, Jiaxin Shi, Feng Gao, Qi Tian, and Yizhou Wang.\nHuman motion generation: A survey. IEEE Transactions\non Pattern Analysis and Machine Intelligence , 2023. 1\n[107] Wenlin Zhuang, Congyi Wang, Jinxiang Chai, Yangang\nWang, Ming Shao, and Siyu Xia. Music2dance: Dancenet\nfor music-driven dance generation. ACM Transactions\non Multimedia Computing, Communications, and Applica-\ntions (TOMM) , 18(2):1–21, 2022. 2\n469'}, 'dist': 0.9286905527114868}
Result 21: {'text': 'data', 'metadata': {'created_at': 1736932570, 'modified_at': 1736932570, 'owner': 'sayandeep', 'path': 'RAG_CONTENT/conferences/cvpr_ocr/Bourouis_Open_Vocabulary_Semantic_Scene_Sketch_Understanding_CVPR_2024_paper.txt', 'size': 53365, 'seen_at': 1737191136, 'data': 'Open Vocabulary Semantic Scene Sketch Understanding\nAhmed Bourouis1Judith E. Fan2Yulia Gryaditskaya1\n1Surrey Institute for People-Centered AI and CVSSP, University of Surrey, UK\n2Department of Psychology, Stanford University, USA\nhttps://ahmedbourouis.github.io/Scene Sketch Segmentation/\nAbstract\nWe study the underexplored but fundamental problem of\nmachine understanding of abstract freehand scene sketches.\nWe introduce a sketch encoder that ensures a semantically-\naware feature space, which we evaluate by testing its per-\nformance on a semantic sketch segmentation task. To train\nour model, we rely only on bitmap sketches accompanied\nby brief captions, avoiding the need for pixel-level anno-\ntations. To generalize to a large set of sketches and cat-\negories, we build upon a vision transformer encoder pre-\ntrained with the CLIP model. We freeze the text encoder and\nperform visual-prompt tuning of the visual encoder branch\nwhile introducing a set of critical modifications. First, we\naugment the classical key-query (k-q) self-attention blocks\nwith value-value (v-v) self-attention blocks. Central to our\nmodel is a two-level hierarchical training that enables ef-\nficient semantic disentanglement: The first level ensures\nholistic scene sketch encoding, and the second level focuses\non individual categories. In the second level of the hierar-\nchy, we introduce cross-attention between the text and vi-\nsion branches. Our method outperforms zero-shot CLIP\nsegmentation results by 37 points, reaching a pixel accu-\nracy of 85.5%on the FS-COCO sketch dataset. Finally, we\nconduct a user study that allows us to identify further im-\nprovements needed over our method to reconcile machine\nand human understanding of freehand scene sketches.\n1. Introduction\nEven a quick sketch can convey rich information about what\nis relevant in a visual scene: what objects there are and how\nthey are arranged. However, little work has been devoted\nto the task of machine scene sketch understanding, largely\ndue to a lack of data. Understanding sketches with meth-\nods designed for images is challenging because sketches\nhave very different statistics from images – they are sparser\nand lack detailed color and texture information. Moreover,\nsketches contain abstraction at multiple levels: the holis-\nCLIP zero shot\nsegmentationCLIP zero shot\nsegmentationOur\nsegmentationOur\nsegmentationA giraffe and a zebra are \nstanding on the grass.A man with a kite and a tree \nin the background.Figure 1. Comparison of the segmentation result obtained with\nCLIP visual encoder features and features from our model.\ntic scene level and the object level. Here we explore the\npromise of two main ideas: (1) the use of language to guide\nthe learning of how to parse scene sketches and (2) a two-\nlevel training network design for holistic scene understand-\ning and individual categories recognition.\nFreehand sketches can be represented as a sequence or\ncloud of individual strokes, or as a bitmap image. As one\nof the first works on scene sketch understanding, we target\na general setting where we assume only the availability of\nbitmap representations. We also aim at the method that can\ngeneralize to a large number of scenes and object categories.\nTo this end, we build our sketch encoder on a Visual Trans-\nformer (ViT) encoder pre-trained with a popular CLIP [44]\nfoundation model (Fig. 1). We propose a two-level hierar-\nchical training of our network, where the two levels (“Holis-\ntic” and “Category-level”) share the weights of our visual\nencoder. The first level focuses on ensuring that our model\ncan capture holistic scene understanding (Fig. 2: I. Holis-\ntic), while the second level ensures that the encoder can effi-\nciently encode and distinguish individual categories (Fig. 2:\nII. Category-level). We avoid reliance on tedious user per-\npixel annotations by leveraging sketch-caption pairs from\nthe FS-COCO dataset [9], and aligning the visual tokens of\nsketch patches with textual tokens from the sketch captions,\nusing triplet loss training. We strengthen the alignment by\nintroducing sketch-text cross-attention in the second level\nof the network’s hierarchy (Fig. 2: g.). Additionally, we in-\ntroduce a modified self-attention computation to the visual\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n4176\ntransformer encoder used in both layers, inspired by recent\nwork by Li et al. [33].\nWe conduct a comprehensive evaluation of our method\ncomparing it with recent language-supervised image seg-\nmentation methods [33, 44, 58], fine-tuned on the FS-\nCOCO dataset. We show that our approach outperforms\nwith a large margin all existing methods on the task of free-\nhand sketch segmentation. We also compare with a pre-\nvious fully supervised work on scene sketch segmentation\n[19], trained on a semi-synthetic set of sketches composed\nof individual category sketches. We demonstrate that their\nwork does not generalize well to freehand scene sketches\n[9]. Our method demonstrates consistent performance and\nsimilarly outperforms [19] on a dataset of freehand sketches\nprovided by Ge et al. [19].\nFinally, our analysis reveals that although our model con-\nsistently produces robust segmentation results across the\nmajority of sketches, there are a few challenging sketching\nscenarios for our method. We select a subset of representa-\ntive sketches for each scenario and collect multi-user anno-\ntations. We then carefully assess our approach by compar-\ning its performance with that of human participants, draw-\ning insights to guide future work.\nIn summary, our contributions include: (1) a two-level\nhierarchical training approach, focusing on holistic scene\nsketch understanding and category disentanglement, (2)\nthe first language-supervised scene sketch segmentation\nmethod, (3) per pixel segmentation annotations of 975\nsketches from the FS-COCO dataset, and (4) multi-user an-\nnotations of a subset of distinct groups of sketches.\n2. Related Work\n2.1. Unsupervised and Weekly Supervised Image\nSemantic Segmentation\nThe need for pixel-wise segmentation limits the number of\ninstances that supervised segmentation models [1, 6, 7, 17,\n36, 64] can use for training, as such annotations are costly\nto collect. This in turn limits the generalization properties\nof models trained with pixel-level annotations. To avoid the\nneed for extensive annotations, unsupervised [8, 23, 26, 39,\n62], semi-supervised [40, 72] and weakly supervised [13,\n14, 24, 37, 38, 41, 56, 58, 69] methods were proposed.\nOur method belongs to the group of weakly supervised\nmethods based on text annotations only [5, 13, 14, 37, 38,\n58], such methods are not limited to a fixed set of categories\nand therefore are referred to as open vocabulary semantic\nsegmentation methods. Image methods typically rely on the\nspatial proximity of semantically similar pixels. This is less\napplicable in the sparse and largely monochromatic land-\nscape of freehand sketches. For example, recent GroupViT\n[58] and SegCLIP [38] use learnable group tokens and se-\nmantic group modules to aggregate low-layer pixel features.In our work, we propose a two-level training architecture\ntaking sketch sparsity and abstraction into account.\n2.2. Sketch Semantic Segmentation\nThe majority of works on semantic sketch segmentation fo-\ncus on single-category sketches. Some of these works treat\nsketch as a bitmap image [32, 70, 71], but most leverage\nstroke-level information directly [12, 21, 22, 28, 42, 43, 48,\n55, 57, 60, 66] or as a segmentation refinement step [32, 71].\nAll these works are fully supervised except for [42], which\nsegments sketches of a given category provided at least one\nsegmented reference sketch.\nSemantic scene sketch segmentation [51], and more\nbroadly scene sketch understanding, is underexplored, to\na large extent due to a lack of data. The lack of data is\ntypically addressed by introducing semi-synthetic sketch\ndatasets. The SketchyScene dataset [73] consists of 7,264\nsketch-image pairs, obtained by arranging clip-art indi-\nvidual category sketches in alignment with a reference\nimage. SketchyCOCO dataset [18] is generated from\nCOCO-Stuff [4] by semi-automatically arranging freehand\nsketches of individual categories. Ge et al . [19] intro-\nduced their own semi-synthetic scene sketch dataset and\nadopted a DeepLab-v2 [6] architecture to the scene sketch\nsegmentation task. SketchSeger [59] proposed an encoder-\ndecoder model based on hierarchical Transformers, trained\nwith a stroke-based cross-entropy loss on semi-synthetic\nscene sketches formed by combining sketches from the\nQuickDraw dataset [21]. Zhang et al . [63] proposed an\nRNN-GCN-based architecture trained on annotated free-\nhand scene sketches. However, neither the dataset nor the\ncode have been released. We do not require stroke-level in-\nformation or pixel-wise segmentation of the training data,\nand leverage the FS-COCO dataset [9] of freehand sketches\nwith their textual descriptions.\n2.3. ViT-CLIP and Sketch\nWe build our encoder on a ViT (Vision Transformer) en-\ncoder pre-trained with CLIP (Contrastive Language-Image\nPre-training) [44]. CLIP is a model trained on roughly\n400 million image-text pairs to embed images and text in\na shared space. It uses ViT as a visual branch (image) en-\ncoder. A ViT encoder pre-trained with CLIP (ViT-CLIP) is\nused in a range of sketch-related tasks: sketch and drawing\ngeneration [16, 49, 53, 54], 2D image retrieval [9, 46, 47],\nobject detection [10], 3D shape retrieval [2, 30, 31, 50, 61],\n3D shape generation [65].\nWhile some works use ViT-CLIP purely pre-trained on\nimages, many fine-tune the encoder on sketches for down-\nstream tasks. Some works fine-tune all weights of the en-\ncoder [2, 47], some fine-tune Layer Normalization layers\nonly [9], and some rely on prompt-learning [27, 68] or the\ncombination of the latter two [10, 46]. In our work, we also\n4177\nCST CST\n(f) Multiply and \nthreshold\n𝓛𝓛𝑇𝑇−𝑔𝑔𝑔𝑔𝑔𝑔𝑔𝑔 =( VST  , CST+, CST−)\n⋰A sketch of a bat.A boy  holding \na baseball bat, \nwith trees \nbehind.\nA sketch of trees\nA sketch of a boy.⋰(a) Input\n(b) Category prompts⋮\n𝑃𝑃𝐾𝐾\nVST𝑃𝑃2𝑃𝑃1\n⋮𝑉𝑉𝑆𝑆𝑉𝑉1\n⋮\nLearnable tokens\nV-V Transformer encoder 𝐻𝐻𝐾𝐾\nVST𝐻𝐻2𝐻𝐻1\n⋮Linear projection\n(c) Visual encoder\nCLIP text encoderCST\nCCT\nCCT\nCCT(I) Holistic (d) Patches -\ncategory\nsimilarity\n⋮⋮⋮⋯\n⋯\n⋯⋰\n⋰⋰Resize & Upscale(e) Pixel -\ncategory\nsimilarity\n⋮\nVisual \nencoder \nw. CA\n→𝑄𝑄7,𝑄𝑄10, 𝑄𝑄12VCT⋮VCT⋮VCT⋮\nCaption Category Token Caption Scene Token Vision Category Token Vision Scene Token CCT(II) Category -level→𝑄𝑄7,𝑄𝑄10, 𝑄𝑄12→𝑄𝑄7,𝑄𝑄10, 𝑄𝑄12\nCST VST VCTVCTVCT\nVisual \nencoder \nw. CAVisual \nencoder \nw. CA(g) Category features w. CA\n𝓛𝓛𝑇𝑇−𝑐𝑐𝑐𝑐𝑔𝑔𝑐𝑐=( VCT  , CCT+,CCT−)\n⋰\n𝓛𝓛𝑇𝑇−𝑐𝑐𝑐𝑐𝑔𝑔𝑐𝑐=( VCT  , CCT+,CCT−)Figure 2. Our framework consists of two levels: I. Holistic Scene Sketch Understanding and II. Targeting individual categories disentan-\nglement. Please refer to Sec. 3 for details.\nrely on fine-tuning with visual prompt learning and Layer\nNormalization layers updates. Unlike previous methods tar-\ngeting sketch inputs, we additionally leverage a two-path\nViT architecture, inspired by Li et al. [33].\n3. Method\nAs we mention in the introduction, we build a sketch en-\ncoder such that the semantic meaning of individual stroke\npixels can be inferred from its feature embeddings. Build-\ning on the ViT encoder, pre-trained CLIP [44] model, we\nfine-tune a modified encoder architecture with a network\nconsisting of two levels: Holistic scene understanding and\nindividual category recognition. We start by describing the\nfirst level of our network (Fig. 2 I.) and introducing the ar-\nchitecture of our visual encoder (Fig. 2 c.). We then de-\nscribe our strategy to improve the model’s ability to under-\nstand individual categories (Fig. 2 II.).\n3.1. Holistic Scene Sketch Understanding\nThe architecture in the first level (Fig. 2: I. Holistic) is sim-\nilar to the architecture of the CLIP model [33]. We freeze\nthe weights of the textual encoder and fine-tune the mod-\nified architecture of the vision encoder (Sec. 3.1.1). The\nCLIP model is trained with a contrastive loss, ensuring that\nthe embedding of images and corresponding captions are\ncloser in space than embeddings of images and captions of\nother images. While our training has a similar goal, we train\nwith a triplet loss with hard triplet mining, as we found it to\nbe more beneficial with the batch size we use:\nLNTglbl=1\nNTNTX\ni=1max{||VSTi−CST+\ni||\n− ||VSTi−CST−\nj||+m, 0}.(1)Here, a holistic visual scene sketch embedding VST (Vi-\nsual Scene Token) serves as an anchor. An encoding of\nthe matching sketch caption CST+(Caption Scene Token)\nserves as a positive sample, and an encoding of the most\ndissimilar scene caption serves as a negative sample CST−.\nWe set the margin mto a commonly used value of 0.3. The\nnumber of triplets NTis equal to the number of samples in\na batch.\n3.1.1 Visual encoder\nThe input scene sketch is divided into non-overlapping\npatches, which are flattened and linearly projected into the\nfeature space. Concatenating with positional encodings,\nwe obtain one token Pk∈R1×dper patch. Addition-\nally, we add a set of learnable tokens, Vs, referred to as\nvisual prompts [11]. Finally, these tokens are also aug-\nmented with a special token that encodes holistic sketch\nmeaning, VST (Visual Scene Token). Note that in the con-\ntext of classification, a CLS token has a similar role to\nourVST token. Therefore, the input to the vision encoder\nisX= [VST, P1, ..., P K, V1, ..., V S]∈RNX×d, where\nNX= 1 + K+S.\nAttention computation It was observed by Li et al. [33]\nthat CLIP-predicted similarity maps between image and text\nfeatures emphasize background regions rather than areas\nthat correspond to a category in the text embedding. To\naddress this issue, they proposed to use an instance of self-\nself attention called v-v attention, which does not require\ntraining or fine-tuning the original model. Li et al . [33],\nand later Bousselham et al. [3] demonstrated that this leads\nto improved performance in open vocabulary segmentation\ntasks: Self-self-attention reinforces the similarity of tokens\n4178\nskateboard mountain benchq-k attention v-v attention\ncar\n building\n-1.0\n-0.8\n-0.6\n-0.4\n-0.2Figure 3. Comparison of similarity maps obtained with classical\nattention computation ( q-k attention ) in the second row, with the\nones obtained from v-v attention , given by Eq. (2).\nalready close to each other ( e.g. representing the same ob-\nject), which leads to a clearer separation in the feature\nspace, thereby improving the segmentation quality.\nWe performed a similar experiment with CLIP features\nfor sketch inputs: The similarity maps in the second row\nof Fig. 3 show the poor ability of CLIP features to identify\ntarget categories. Therefore, we follow [33] and use their\ntwo-path configuration of the vision transformer. However,\nwe use it not only for inference but also incorporate this\ntwo-path configuration directly into our network training, as\nwe find it more beneficial. We provide a detailed analysis\nin Sec. 4.5.1.\nThe first path represents the original vision encoder\nwhere identical blocks are repeated Ltimes. Each block\nconsists of Layer Normalization (LN) , followed by Multi-\nHead Self Attention (MHSA) , another LNandFead Forward\nNetwork (FFD) .\nThe second path blocks contain a modified attention\ncomputation in MHSA , dubbed as v-v self-attention , where\nKeys andQueries are ignored, and self-attention is com-\nputed using only Values, V∈RNX×d:\ns-attn (V, V, V ) =softmax\x10\nV VT/√\nd\x11\nV. (2)\nIn addition, blocks in the second path do not include the\nsecond LNandFFN layers. Finally, in the second path, the\ninput to the v-v multi-head attention is always the features\nfrom the original path. We use the output from the second\npath during training and inference.\nAs shown in Fig. 3 third row, the v-v attention results\nin feature representations that accurately represent distinct\nsemantic entities present in the scene sketch.\n3.2. Categories Disentanglement\nGiven the sketch caption we automatically identify individ-\nual categories and generate a set of textual prompts of the\nform “A sketch of *” (Fig. 2b.). Each of these textual cat-\negory prompts is encoded with the CLIP text encoder intoCCT∈R1×d(Caption Category Token).\nWe then compute the per-patch cosine similarity Mc\nk\nbetween the class embeddings CCT and the scene sketch\npatch embeddings Hk, defined as:\nMc\nk=CCTc·HT\nk\n|CCTc||HT\nk|, (3)\nwhere k∈[1, K]is the patch index and c∈[1, Nc]is an\nindex of a category ( e.g.trees ). The resulting similarity ma-\ntrixMc∈RK×Ncrepresents the category label probabili-\nties for each individual patch (Fig. 2d.). To generate a pixel-\nlevel similarity map, we reshape each Mcand then upscale\nto the dimensions of the original scene sketch using bi-cubic\ninterpolation [52]. By multiplying these per category maps\nwith the input scene sketch, as shown in Fig. 2e., we obtain\ndisentanglement into individual sketch categories.\nThresholding with a learnable parameter Only pixels\nwith similarity scores above a certain threshold are retained\nat this step (Fig. 2f.). We make the threshold learnable,\neliminating the need for manual tuning. More importantly,\nthe threshold value increases over epochs as the model be-\ncomes more confident in its predictions, allowing the model\nto obtain strong disentanglement performance.\nEpoch 0 Epoch 5 Epoch 10 Epoch 20Bicycle\nFigure 4. Visualization of disentanglement over epochs.\nVisual encoder with Cross-Attention The features of in-\ndividual category sketches are extracted with the visual en-\ncoder identical to the one used in the holistic scene sketch\nlevel understanding of our network, described in Sec. 3.1.1,\nup to one difference.\nWe enhance the interplay between textual and visual do-\nmains through the introduction of cross-attention. Namely,\nin 7th, 10th, and 12th layers in the MHSA , we feed CCT to-\nken from the textual encoder representing a target category\nto the linear projection for the queries. This enables the\nmodel to leverage category token embedding from the tex-\ntual domain to update the sketch token embedding. This re-\nsults in a better text-to-sketch alignment for individual cate-\ngories and subsequently improves sketch semantic segmen-\ntation. Our ablation study in Tab. 4 underscores the efficacy\nof this cross-attention strategy.\nText-sketch category-level alignment We train with a\ntriplet loss, LTctgr, so that the category sketch embed-\nding, VCT (Vision Category Token), is used as an anchor,\nthe matching embedding of the category prompt is used as\na positive sample and the embedding of the prompt of the\n4179\nmost dissimilar category is used as negative. We use the\nVCT from multiple encoder layers: l7, l10, l12.\n3.3. Efficient CLIP fine-tuning\nThe two levels (holistic and category) are trained jointly,\nusing the total loss\nL=LTglbl+LTctgr. (4)\nWe leverage the generalization properties of the pre-\ntrained foundation model through careful fine-tuning. We\nfreeze all the weights apart from weights of LN, as was\nproposed in [15], and we use learnable visual prompts, as\nwas proposed in [27]. We introduced visual prompts in\nSec. 3.1.1. We also train linear layers which take part in\ncross-attention computation.\n3.4. Inference\nOur network design allows segmentation for different sets\nof categories. Given a desirable set of categories for a given\nsketch, we obtain sketch segmentation by applying all the\nsteps of our network up to the calculation of pixel-category\nsimilarities (Fig. 2e.), followed by upscaling of similarity\nmaps for each category, as discussed in Sec. 3.2. To as-\nsign segmentation results we assign to each pixel a label\nthat yields the highest similarity value across category sim-\nilarity maps Mc\ni, where iis an index of a category.\nIf we want to isolate just a few categories in the sketch,\nwe can use the thresholding strategy that we use during\ntraining to isolate the pixels of a given category (Fig. 2f.).\nWe used this strategy to obtain visualizations in Fig. 1, with\na threshold value of 0.71that we found to be optimal on the\ntest set of sketches. We do not use the learned value from\nthe training, as during training the model does not have to\nselect all the pixels of the given category, but only those that\nare sufficient to confidently predict the category label. We\nprovide an in-depth discussion in the supplemental.\n4. Experiments\n4.1. Training and Test Data\nFor training and testing, we use the sketch-caption pairs\nfrom the FS-COCO [9] dataset. The dataset comprises\n10,000 sketch-caption pairs, associated with reference im-\nages from the MS-COCO [34] dataset. The sketches are\ndrawn from memory by 100 non-expert participants. The\nreference image was shown for 60 seconds, followed by a\n3-minute sketching window.\nTraining/Validation/Test splits We first selected 500\nsketches with distinct styles from five participants. We then\nrandomly sample 5 sketches from each of the remaining 95\nparticipants for validation (a total of 475 sketches). We use\nthe remaining 9025 sketches for training.Annotations One of the co-authors manually annotated test\nand validation sketches, relying on reference images and\ncategory labels from the MS-COCO [34] dataset. We assign\neach stroke a unique category label. Candidate category la-\nbels are extracted from MS-COCO image captions rather\nthan sketch captions to obtain richer ‘ground-truth’ annota-\ntions. Our test set contains 185 different object classes, with\nan average of 3.54 objects per sketch.\n4.2. Evaluation Metrics\nWe use standard metrics, commonly used in sketch segmen-\ntation literature [25, 57, 63].\nMean Intersection over Union ( mIoU ):evaluates the av-\nerage of the ratios between the intersection and the union of\nground truth and predicted labels over all categories.\nPixel Accuracy ( Acc@P):measures the ratio of correctly\nlabeled pixels to the total pixel count in a sketch.\nStroke Accuracy ( Acc@S):evaluates the percentage of\ncorrectly classified strokes to total strokes per sketch. A\nstroke label is determined by its most frequent pixel label.\n4.3. Implementation Details\nWe implemented our method in PyTorch and trained on two\n24GB Nvidia RTX A 5000 GPUs. We built on CLIP [44]\nwith a ViT backbone using ViT-B/16 weights. The input\nsketch image size is set as 224×224. We use 3 learnable\nvisual prompts. We use AdamW optimizer with a learning\nrate of 10−6, and train the model for 20epochs with a batch\nsize of 16. We pick a checkpoint based on the mIoU perfor-\nmance on the validation set. We provide more discussion\non the checkpoint choice in the supplemental.\n4.4. Comparison against state-of-the-art\n4.4.1 Comparison with fully-supervised methods\nWe first compare with several recent methods for image\nsegmentation that similarly to us utilize either CLIP as a\nbackbone: DenseCLIP [45] and ZegCLIP [69], or more\nrecent foundational backbones Grounding-DINO [35] and\nSAM [29], used in Grounded-SAM [20]. These meth-\nods require pixel-level annotated examples, and therefore\ncan not be fine-tuned on our training data. We also com-\npare to a recent fully supervised method LDP (Local Detail\nPerception) [19] for scene sketch semantic segmentation,\nwhich is trained on a dataset of semi-synthetic sketches.\nSuch sketches are obtained as a superposition of freehand\ncategory-level sketches. Tab. 1 shows that neither of the\nthese methods generalizes well to freehand scene sketches.\n4.4.2 Comparison with language-supervised methods\nNext, we compare with several recent methods targeting se-\nmantic segmentation with ViT encoders and image-text su-\npervision: GroupViT [58] and SegCLIP [38]. Additionally,\n4180\nkite woman grass tree fence giraffe zebra tree building disc dog grass tree\ncloud fence grass mountain sheep ball bat person grass tree road traffic signal house grass treeGround-truth CLIP Surgery** Ours Ground-truth CLIP Surgery** Ours Ground-truth CLIP Surgery** Ours\n68.75 94.56 68.06 92.31 82.11 93.11\n75.86 94.12 79.96 92.79 68.98 82.04Figure 5. Visual comparison of our method with CLIP Surgery⋆⋆.CLIP Surgery⋆⋆represents the fine-tuned ViT from the CLIP model\nwith v-v self-attention introduced at both training and inference stages. The numbers show Acc@P values.\nMethods mIoU Acc @P Acc @S\nZegCLIP [69] 15.45 32 .48 35 .21\nDenseCLIP [67] 28.22 50 .62 50 .25\nGrounded-SAM [20] 32.21 50 .12 50 .02\nLDP [19] 33.04 56 .23 56 .71\nOurs 73.48 85.54 87.02\nTable 1. Comparison of our method against state-of-the-art fully\nsupervised sketch method and image segmentation methods, rely-\ning on the availability of pixel-level annotations, on our test set of\nfreehand sketches from the FS-COCO dataset.\nwe compare with CLIP [44], as well as CLIP Surgery [33]\nthat introduced the usage of v-v-attention at inference time.\nZero-shot In Tab. 2, we first compare the performance of\nour method with the zero-shot performance of these meth-\nods. It shows that image segmentation methods do not gen-\neralize well to freehand sketches.\nFine-tuning We fine-tune each of the methods on our train-\ning set, by updating all their weights. Since such fine-tuning\nmight be sensitive to a learning-rate choice, we perform sev-\neral runs with several settings of learning rate parameters.\nWe chose the setting for each method that results in the best\nperformance on our validation set. The fine-tuned methods\nare marked with stars.\nTab. 2 shows that our method outperforms all consid-\nered baselines, and surpasses the best-performing baseline\nCLIP Surgery⋆⋆by a substantial margin of 13.5,9.9and5.9\npoints in mIoU score, Acc@PandAcc@S, respectively. In\nSec. 4.5.1, we evaluate various elements of our architecture\nand their contribution to overall performance.\nFig. 5 shows the qualitative comparison between our\nmethod and the CLIP Surgery⋆⋆. We provide additional vi-\nsual comparisons in the supplemental.Methods mIoU Acc @P Acc @SZero-shotCLIP [44] 17.33 28 .82 27 .15\nGroupViT [58] 38.25 61 .39 60 .07\nSegCLIP [38] 38.14 61 .45 65 .56\nCLIP Surgery [33] 52.63 72 .47 75 .17Fine-tunedCLIP⋆22.86 33 .41 32 .64\nGroupViT⋆45.71 66 .21 66 .89\nSegCLIP⋆49.26 69 .87 73 .64\nCLIP Surgery⋆48.74 65 .38 68 .78\nCLIP Surgery⋆⋆59.98 78 .68 81 .11\nOurs 73.48 85.54 87.02\nTable 2. Comparison of our method against state-of-the-art lan-\nguage supervised image segmentation methods on our test set of\nsketches from the FS-COCO dataset. The fine-tuned methods\non our training set of freehand sketches are marked with stars.\nCLIP Surgery⋆represents the fine-tuned CLIP model with v-v\nself-attention introduced only at inference stages. CLIP Surgery⋆⋆\nrepresents the fine-tuned model with v-v self-attention introduced\nat both training and inference stages.\n4.4.3 Generalization ability of our method\nNext, we evaluate our method on an additional dataset of 50\nfreehand sketches provided and annotated by Ge et al. [19].\nTab. 3 shows that our model again demonstrates superior\nperformance on this dataset over the method [19], fully su-\npervised on semi-synthetic sketches. We do not compute\nAcc@S as sketches are only available as bitmap images.\nThis experiment highlights that short language captions can\nbe efficiently used for training, eliminating the need for ex-\npensive and time-consuming per-pixel annotations.\nMethod mIoU Acc @P\nLDP [19] 37.16 78 .84\nOurs 53.94 81.63\nTable 3. Comparison on the freehand sketches from [19].\n4181\nThe lower mIoU values on these sketches than on FS-\nCOCO sketches can be explained by (1) on larger av-\nerage number of categories in them ( 5.74categories per\nsketch) than in our FS-COCO test set ( 3.54categories\nper sketch); (2) domain gap. The sketches from [19]\ncontain symbolic representations of objects (see the inset\non the left) and look more like a superpo-\nsition of sketches that can be found in the\nQuickDraw [21] dataset rather than holis-\ntic scene sketches. We analyze challenging\nscenarios for our method in Sec. 5.1.\n4.5. Ablation Study\n4.5.1 Importance of individual components\nWe perform an ablation analysis to assess the importance of\neach component in our architecture. Tab. 4 shows the per-\nformance of the complete model with individual elements\nremoved. We discuss them in order of impact on overall\nperformance.\nv-v attention First, we show the importance of the v-v at-\ntention, by substituting our dual path v-v attention-based\nViT encoder with the original configuration used in the\nCLIP model ( w/o v-v attention ).\nTwo-level network architecture We keep only the first\nlevel of holistic scene understanding of the network (Fig. 2\nI.). This architecture is similar to CLIP Surgery⋆⋆, but is su-\npervised with the triplet loss and is fine-tuned using learn-\nable visual prompts and updates only LNlayers. Tab. 4\n(w/o category-level) confirms that two-level network archi-\ntecture, along v-v attention , is central to the superiority of\nour model.\nThresholding We perform an experiment where instead\nof thresholding we weight each pixel according to cosine\nsimilarity scores in Mcmaps (Tab. 4 (w/o thresholding) ).\nThe learnable threshold more efficiently filters out irrelevant\npixels, forcing the model to learn superior disentanglement\nof individual categories.\nHolistic scene encoding Removing the global loss, given\nby Eq. (1), similarly results in the performance drop (w/o\nGlobal Loss) . This shows the mutual importance of the two\nlevels of our network.\nCross-Attention Cross attention also substantially con-\ntributes to performance. If we use a ViT encoder at the sec-\nond level of the network (category level), identical to the\none used at the first level (holistic level) (Fig. 2c.), then the\nperformance drops by a noticeable 3.35points in the mIoU\nscore (Tab. 4 (w/o cross-attention) ).\nMulti-layer features in the triplet loss Tab. 4 (w/o cross-\nattention) shows that using features from multiple layers(l7, l10, l12) in the category-level triplet loss is beneficial\nover using only the features from the last layer ( l12).\nModel mIoU Acc @P Acc @S\nw/o v-v attention 43.55 58 .09 59 .03\nw/o category-level 65.03 79 .35 81 .82\nw/o thresholding 66.93 81 .06 82 .56\nw/o global Loss 69.06 81 .35 83 .68\nw/o cross-attention 70.13 82 .86 85 .26\nw/o multi-layer Loss 71.29 83 .04 86 .13\nOurs-full 73.48 85.54 87.02\nTable 4. Ablation of the role of individual components of our\nmodel. See Sec. 4.5.1 for details.\n4.5.2 Efficient fine-tuning\nFig. 6 shows the comparison of different fine-tuning strate-\ngies. We obtain the best results by combining fine-tuning\nofLN(Layer Normalization) layers and the addition of 3\nlearnable tokens. Adding more or less tokens degrades the\nperformance Fig. 6b.\n5Acc@P838485\nNumber of learnable VP tokens1 0 2 3 4\n(b.)82.8684.0485.2185.54\nLN VP Full-FT Our\nFine tuning strategyAcc@P\n(a.)\nFigure 6. Evaluation of alternative fine-tuning strategies (a.) and\nthe impact of the number of learnable tokens on segmentation ac-\ncuracy (b.). LNmeans that only LNlayers are fine-tuned; VP\nmeans that only learnable Visual Prompt tokens are used; Full-\nFTmeans that all weights of ViT are fine-tuned.\n5. Human-Model Alignment\nFig. 7 shows that for the majority of sketches in our test\nset from the FS-COCO dataset, our model correctly labels\nmore than 80% pixels.\nIn this section, we investigate (1) which sketches are\nlikely to get low segmentation accuracy and (2) how the pre-\ndiction of our model compares with human observers across\ndifferent groups of sketches.\n5.1. Sketch Groups\nWe identified four distinct sketch groups that are challeng-\ning for our model: (1) Ambiguous sketches : sketches\nwhere it might be hard even for a human observer to un-\nderstand an input sketch; (2) Interchangeable categories :\nsketches containing multiple objects with labels that can\ninterchange each other, like ‘tower’ and ‘building’ , or\n‘girl’ and‘man’ ; (3) Correlated categories : sketches with\n4182\n0%10%\n8%\n6%\n2%4%\n30 100 40 50 60 70 80 90Acc@PFigure 7. Histogram of Acc@P values for our method on 500\nsketches from our FS-COCO test set.\ncategories that typically co-occur in scenes, e.g.,‘train’-\n‘railway’ and ‘airplane’-‘runway’ ; and (4) Numerous-\ncategories : sketches with six or more categories.\nWe supplement these four groups with sketches where\nour model labels correctly more than 80% of pixels: (5)\nStrong performance .\n5.2. User Study Setting\nData We sample 5 sketches for each of the first 4 cate-\ngories and 10 sketches for the 5th category. We visualize\nselected sketches in the supplemental material.\nParticipants We recruited 25 participants ( 14male).\nEach participant was randomly assigned 6 sketches: 1 from\neach of the first 4 groups and 2 from the 5th group, such that\nevery sketch was annotated by five unique participants.\nStudy Procedure Participants were presented with one\nsketch and one object category at a time and were not able to\nsee their previous annotations. Sketch-category pairs were\ninterlaced, to reduce the effect of memorizing their previ-\nous annotation on a certain sketch. The annotation inter-\nface enabled precise pixel-level segmentation by allowing\nparticipants to “paint” over each sketch using a brush with\nan adjustable radius. Participants could also use the eraser\nto correct erroneous annotations. Once a participant has\nmoved to a new sketch-category pair, they were not able\nto change their previous annotations.\n5.3. User Study Analysis and Future Work\n‘Human’ segmentation For each sketch, we generate one\n‘human’ segmentation using a majority vote. For each pixel\nand each label, we computed the percentage of annotators\nthat assigned a given label. We then assigned to each pixel\nthe label that was provided most frequently to that pixel by\ndifferent annotators. In cases where there were multiple la-\nbels were provided equally often for a pixel, we randomly\nsampled one of these labels.\nAnalysis First, we observed that on sketches that did not\nfall into any of the challenging categories, our model almost\nreaches human-level performance, with a negligible gap of\n0.11points on average (Fig. 8 Strong).\n80.96\nOurs Human\n40 %-50% -60% -70% -80% -90% -\nNumerous Ambiguous CorrelatedSegCLIP*\nStrong60.47\n42.4962.13\n41.8980.01\n69.66\n50.5464.5184.74\n57.7182.43\n49.9860.4973.9486.9793.61 93.72\n63.7871.03CLIPSurgery**\nInter-\nchangeableFigure 8. Comparison of the percentage of correctly predicted pix-\nels (Acc@P ) by different models and human observers across five\ndistinct sketch categories, introduced in Sec. 5.1.\nFig. 8 Ambiguous shows that, given a label, humans can\ncorrectly identify sketch pixels even in the presence of am-\nbiguity. While none of the models currently match human\nperformance on ambiguous sketches , our model surpasses\nthe other methods by a noticeable margin, demonstrating\nthe effectiveness of our two-level training architecture.\nThe performance across semantically interchange-\nable categories is uniform amongst the three language-\nsupervised models. This potentiality can be alleviated by\nproposing solutions that assign labels jointly.\nOn sketches with correlated categories our model and\nClipSurgery⋆⋆perform similarly, highlighting the inherent\nlimitation of training using language supervision. For a\nfew such categories, one might need to further fine-tune the\nmodel relying on sketches of isolated categories.\nOur model represents a substantial improvement over\ncurrent alternatives, surpassing them by more than 10\npoints. Future work should seek to improve alignment with\nhuman sketch understanding, especially on sketches with\nmore than six categories (Fig. 8 Numerous).\n6. Conclusion\nWhile focusing on the task of sketch segmentation, we in-\ntroduced a strategy to train a ViT encoder that results in the\nfeature space with good semantic disentanglement. Such\nfeature spaces contribute towards improving machine un-\nderstanding of abstract freehand sketches and underpin a\nrange of downstream tasks such as communication and cre-\native pipelines. In light of the latter, it can enable more\npotent tools for conditional generation and retrieval. In psy-\nchology, sketches are used to analyze cognitive functions.\nThis can be facilitated by the availability of robust sketch\nunderstanding tools. Importantly, we for the first time\ndemonstrated how language supervision can be used for the\ntask of scene sketch segmentation. Finally, we conducted a\ncomprehensive analysis of our model’s performance, identi-\nfying research directions to further align the understanding\nof sketches by humans and machines.\n4183\nReferences\n[1] Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla.\nSegnet: A deep convolutional encoder-decoder architecture\nfor image segmentation. IEEE transactions on pattern anal-\nysis and machine intelligence , 39(12):2481–2495, 2017. 2\n[2] Gianluca Berardi and Yulia Gryaditskaya. Fine-tuned but\nzero-shot 3d shape sketch view similarity and retrieval. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision , pages 1775–1785, 2023. 2\n[3] Walid Bousselham, Felix Petersen, Vittorio Ferrari, and\nHilde Kuehne. Grounding everything: Emerging localization\nproperties in vision-language transformers. arXiv preprint\narXiv:2312.00878 , 2023. 3\n[4] Holger Caesar, Jasper Uijlings, and Vittorio Ferrari. Coco-\nstuff: Thing and stuff classes in context. In Proceedings of\nthe IEEE conference on computer vision and pattern recog-\nnition , pages 1209–1218, 2018. 2\n[5] Jun Chen, Deyao Zhu, Guocheng Qian, Bernard Ghanem,\nZhicheng Yan, Chenchen Zhu, Fanyi Xiao, Mohamed Elho-\nseiny, and Sean Chang Culatana. Exploring open-vocabulary\nsemantic segmentation without human labels. arXiv preprint\narXiv:2306.00450 , 2023. 2\n[6] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,\nKevin Murphy, and Alan L Yuille. Deeplab: Semantic image\nsegmentation with deep convolutional nets, atrous convolu-\ntion, and fully connected crfs. IEEE transactions on pattern\nanalysis and machine intelligence , 40(4):834–848, 2017. 2\n[7] Liang-Chieh Chen, George Papandreou, Florian Schroff, and\nHartwig Adam. Rethinking atrous convolution for seman-\ntic image segmentation. arXiv preprint arXiv:1706.05587 ,\n2017. 2\n[8] Jang Hyun Cho, Utkarsh Mall, Kavita Bala, and Bharath\nHariharan. Picie: Unsupervised semantic segmentation us-\ning invariance and equivariance in clustering. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition , pages 16794–16804, 2021. 2\n[9] Pinaki Nath Chowdhury, Aneeshan Sain, Ayan Kumar Bhu-\nnia, Tao Xiang, Yulia Gryaditskaya, and Yi-Zhe Song. Fs-\ncoco: towards understanding of freehand sketches of com-\nmon objects in context. In Computer Vision–ECCV 2022:\n17th European Conference, Tel Aviv, Israel, October 23–27,\n2022, Proceedings, Part VIII . Springer, 2022. 1, 2, 5\n[10] Pinaki Nath Chowdhury, Ayan Kumar Bhunia, Aneeshan\nSain, Subhadeep Koley, Tao Xiang, and Yi-Zhe Song. What\ncan human sketches do for object detection? In CVPR , 2023.\n2\n[11] Timoth ´ee Darcet, Maxime Oquab, Julien Mairal, and Pi-\notr Bojanowski. Vision transformers need registers. arXiv\npreprint arXiv:2309.16588 , 2023. 3\n[12] Micha ¨el Defferrard, Xavier Bresson, and Pierre Van-\ndergheynst. Convolutional neural networks on graphs with\nfast localized spectral filtering. Advances in neural informa-\ntion processing systems , 29, 2016. 2\n[13] Jian Ding, Nan Xue, Gui-Song Xia, and Dengxin Dai. De-\ncoupling zero-shot semantic segmentation. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 11583–11592, 2022. 2[14] Xiaoyi Dong, Jianmin Bao, Yinglin Zheng, Ting Zhang,\nDongdong Chen, Hao Yang, Ming Zeng, Weiming Zhang,\nLu Yuan, Dong Chen, et al. Maskclip: Masked self-\ndistillation advances contrastive language-image pretraining.\nInProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 10995–11005, 2023.\n2\n[15] Jonathan Frankle, David J Schwab, and Ari S Morcos.\nTraining batchnorm and only batchnorm: On the expres-\nsive power of random features in cnns. arXiv preprint\narXiv:2003.00152 , 2020. 5\n[16] Kevin Frans, Lisa Soros, and Olaf Witkowski. Clipdraw:\nExploring text-to-drawing synthesis through language-image\nencoders. Advances in Neural Information Processing Sys-\ntems, 35:5207–5218, 2022. 2\n[17] Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhiwei\nFang, and Hanqing Lu. Dual attention network for scene seg-\nmentation. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition , pages 3146–3154,\n2019. 2\n[18] Chengying Gao, Qi Liu, Qi Xu, Limin Wang, Jianzhuang\nLiu, and Changqing Zou. Sketchycoco: Image gener-\nation from freehand scene sketches. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition , pages 5174–5183, 2020. 2\n[19] Ce Ge, Haifeng Sun, Yi-Zhe Song, Zhanyu Ma, and Jianxin\nLiao. Exploring local detail perception for scene sketch se-\nmantic segmentation. IEEE Transactions on Image Process-\ning, 31, 2022. 2, 5, 6, 7\n[20] GroundedSAM. Grounded-Segment-Anything.\nhttps://github.com/IDEA-Research/Grounded-Segment-\nAnything, 2023. 5, 6\n[21] David Ha and Douglas Eck. A neural representation of\nsketch drawings. arXiv preprint arXiv:1704.03477 , 2017.\n2, 7\n[22] F H ¨ahnlein, Y Gryaditskaya, and A Bousseau. Bitmap\nor vector? a study on sketch representations for deep\nstroke segmentation. In Journ ´ees Francaises d’Informatique\nGraphique et de R ´ealit´e virtuelle , 2019. 2\n[23] Mark Hamilton, Zhoutong Zhang, Bharath Hariharan, Noah\nSnavely, and William T Freeman. Unsupervised semantic\nsegmentation by distilling feature correspondences. arXiv\npreprint arXiv:2203.08414 , 2022. 2\n[24] Wenbin He, Suphanut Jamonnak, Liang Gou, and Liu Ren.\nClip-s4: Language-guided self-supervised semantic segmen-\ntation, 2023. 2\n[25] Zhe Huang, Hongbo Fu, and Rynson WH Lau. Data-driven\nsegmentation and labeling of freehand sketches. ACM Trans-\nactions on Graphics (TOG) , 33(6):1–10, 2014. 5\n[26] Jyh-Jing Hwang, Stella X Yu, Jianbo Shi, Maxwell D\nCollins, Tien-Ju Yang, Xiao Zhang, and Liang-Chieh Chen.\nSegsort: Segmentation by discriminative sorting of seg-\nments. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision , pages 7334–7344, 2019. 2\n[27] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie,\nSerge Belongie, Bharath Hariharan, and Ser-Nam Lim. Vi-\nsual prompt tuning. In Computer Vision–ECCV 2022: 17th\n4184\nEuropean Conference, Tel Aviv, Israel, October 23–27, 2022,\nProceedings, Part XXXIII , pages 709–727. Springer, 2022. 2,\n5\n[28] Kurmanbek Kaiyrbekov and Metin Sezgin. Deep stroke-\nbased sketched symbol reconstruction and segmentation.\nIEEE computer graphics and applications , 40(1):112–126,\n2019. 2\n[29] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\nhead, Alexander C. Berg, Wan-Yen Lo, Piotr Doll ´ar, and\nRoss Girshick. Segment anything. arXiv:2304.02643 , 2023.\n5\n[30] Trung-Nghia Le, Tam V Nguyen, Minh-Quan Le, Trong-\nThuan Nguyen, Viet-Tham Huynh, Trong-Le Do, Khanh-\nDuy Le, Mai-Khiem Tran, Nhat Hoang-Xuan, Thang-Long\nNguyen-Ho, et al. Sketchanimar: Sketch-based 3d ani-\nmal fine-grained retrieval. arXiv preprint arXiv:2304.05731 ,\n2023. 2\n[31] Hyundo Lee, Inwoo Hwang, Hyunsung Go, Won-Seok\nChoi, Kibeom Kim, and Byoung-Tak Zhang. Learning\ngeometry-aware representations by sketching. arXiv preprint\narXiv:2304.08204 , 2023. 2\n[32] Lei Li, Hongbo Fu, and Chiew-Lan Tai. Fast sketch seg-\nmentation and labeling with deep learning. IEEE computer\ngraphics and applications , 39(2):38–51, 2018. 2\n[33] Yi Li, Hualiang Wang, Yiqun Duan, and Xiaomeng Li. Clip\nsurgery for better explainability with enhancement in open-\nvocabulary tasks, 2023. 2, 3, 4, 6\n[34] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll ´ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nComputer Vision–ECCV 2014: 13th European Conference,\nZurich, Switzerland, September 6-12, 2014, Proceedings,\nPart V 13 , pages 740–755. Springer, 2014. 5\n[35] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao\nZhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun\nZhu, et al. Grounding dino: Marrying dino with grounded\npre-training for open-set object detection. arXiv preprint\narXiv:2303.05499 , 2023. 5\n[36] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully\nconvolutional networks for semantic segmentation. In Pro-\nceedings of the IEEE conference on computer vision and pat-\ntern recognition , pages 3431–3440, 2015. 2\n[37] Timo L ¨uddecke and Alexander Ecker. Image segmenta-\ntion using text and image prompts. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 7086–7096, 2022. 2\n[38] Huaishao Luo, Junwei Bao, Youzheng Wu, Xiaodong He,\nand Tianrui Li. Segclip: Patch aggregation with learnable\ncenters for open-vocabulary semantic segmentation. arXiv\ne-prints , pages arXiv–2211, 2022. 2, 5, 6\n[39] Luke Melas-Kyriazi, Christian Rupprecht, Iro Laina, and\nAndrea Vedaldi. Deep spectral methods: A surprisingly\nstrong baseline for unsupervised semantic segmentation and\nlocalization. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , pages 8364–\n8375, 2022. 2[40] Sudhanshu Mittal, Maxim Tatarchenko, and Thomas Brox.\nSemi-supervised semantic segmentation with high-and low-\nlevel consistency. IEEE transactions on pattern analysis and\nmachine intelligence , 43(4):1369–1379, 2019. 2\n[41] Deepak Pathak, Evan Shelhamer, Jonathan Long, and Trevor\nDarrell. Fully convolutional multi-class multiple instance\nlearning. In ICLR Workshop , 2015. 2\n[42] Anran Qi, Yulia Gryaditskaya, Tao Xiang, and Yi-Zhe Song.\nOne sketch for all: One-shot personalized sketch segmenta-\ntion. IEEE transactions on image processing , 31:2673–2682,\n2022. 2\n[43] Yonggang Qi and Zheng-Hua Tan. Sketchsegnet+: An end-\nto-end learning of rnn for multi-class sketch semantic seg-\nmentation. Ieee Access , 7:102717–102726, 2019. 2\n[44] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning , pages\n8748–8763. PMLR, 2021. 1, 2, 3, 5, 6\n[45] Yongming Rao, Wenliang Zhao, Guangyi Chen, Yansong\nTang, Zheng Zhu, Guan Huang, Jie Zhou, and Jiwen Lu.\nDenseclip: Language-guided dense prediction with context-\naware prompting. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition , pages\n18082–18091, 2022. 5\n[46] Aneeshan Sain, Ayan Kumar Bhunia, Pinaki Nath Chowd-\nhury, Subhadeep Koley, Tao Xiang, and Yi-Zhe Song. Clip\nfor all things zero-shot sketch-based image retrieval, fine-\ngrained or not. arXiv preprint arXiv:2303.13440 , 2023. 2\n[47] Patsorn Sangkloy, Wittawat Jitkrittum, Diyi Yang, and James\nHays. A sketch is worth a thousand words: Image retrieval\nwith text and sketch. In Computer Vision–ECCV 2022: 17th\nEuropean Conference, Tel Aviv, Israel, October 23–27, 2022,\nProceedings, Part XXXVIII , pages 251–267. Springer, 2022.\n2\n[48] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Ha-\ngenbuchner, and Gabriele Monfardini. The graph neural net-\nwork model. IEEE transactions on neural networks , 20(1):\n61–80, 2008. 2\n[49] Peter Schaldenbrand, Zhixuan Liu, and Jean Oh. Styleclip-\ndraw: Coupling content and style in text-to-drawing synthe-\nsis.arXiv preprint arXiv:2111.03133 , 2021. 2\n[50] Kristofer Schlachter, Benjamin Ahlbrand, Zhu Wang, Ken\nPerlin, and Valerio Ortenzi. Zero-shot multi-modal artist-\ncontrolled retrieval and exploration of 3d object sets. In SIG-\nGRAPH Asia 2022 Technical Communications , pages 1–4.\n2022. 2\n[51] Zhenbang Sun, Changhu Wang, Liqing Zhang, and Lei\nZhang. Free hand-drawn sketch segmentation. In Computer\nVision–ECCV 2012: 12th European Conference on Com-\nputer Vision, Florence, Italy, October 7-13, 2012, Proceed-\nings, Part I 12 , pages 626–639. Springer, 2012. 2\n[52] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herv ´e J´egou. Training\ndata-efficient image transformers & distillation through at-\ntention. In International conference on machine learning ,\npages 10347–10357. PMLR, 2021. 4\n4185\n[53] Yael Vinker, Yuval Alaluf, Daniel Cohen-Or, and Ariel\nShamir. Clipascene: Scene sketching with different types\nand levels of abstraction. arXiv preprint arXiv:2211.17256 ,\n2022. 2\n[54] Yael Vinker, Ehsan Pajouheshgar, Jessica Y Bo, Ro-\nman Christian Bachmann, Amit Haim Bermano, Daniel\nCohen-Or, Amir Zamir, and Ariel Shamir. Clipasso:\nSemantically-aware object sketching. ACM Transactions on\nGraphics (TOG) , 41(4):1–11, 2022. 2\n[55] Fei Wang, Shujin Lin, Hanhui Li, Hefeng Wu, Tie Cai, Xi-\naonan Luo, and Ruomei Wang. Multi-column point-cnn for\nsketch segmentation. Neurocomputing , 392:50–59, 2020. 2\n[56] Yunchao Wei, Huaxin Xiao, Honghui Shi, Zequn Jie, Jiashi\nFeng, and Thomas S Huang. Revisiting dilated convolution:\nA simple approach for weakly-and semi-supervised seman-\ntic segmentation. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition , 2018. 2\n[57] Xingyuan Wu, Yonggang Qi, Jun Liu, and Jie Yang. Sketch-\nsegnet: A rnn model for labeling sketch strokes. In 2018\nIEEE 28th International Workshop on Machine Learning for\nSignal Processing (MLSP) , pages 1–6. IEEE, 2018. 2, 5\n[58] Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon,\nThomas Breuel, Jan Kautz, and Xiaolong Wang. Groupvit:\nSemantic segmentation emerges from text supervision. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition , pages 18134–18144, 2022. 2,\n5, 6\n[59] Jie Yang, Aihua Ke, Yaoxiang Yu, and Bo Cai. Scene\nsketch semantic segmentation with hierarchical transformer.\nKnowledge-Based Systems , page 110962, 2023. 2\n[60] Lumin Yang, Jiajie Zhuang, Hongbo Fu, Xiangzhi Wei, Kun\nZhou, and Youyi Zheng. Sketchgnn: Semantic sketch seg-\nmentation with graph neural networks. ACM Trans. Graph. ,\n40(3):1–13, 2021. 2\n[61] Ruichen Yao, Ziteng Cui, Xiaoxiao Li, and Lin Gu. Im-\nproving fairness in image classification via sketching. arXiv\npreprint arXiv:2211.00168 , 2022. 2\n[62] Andrii Zadaianchuk, Matthaeus Kleindessner, Yi Zhu,\nFrancesco Locatello, and Thomas Brox. Unsupervised se-\nmantic segmentation with self-supervised object-centric rep-\nresentations. arXiv preprint arXiv:2207.05027 , 2022. 2\n[63] Zhengming Zhang, Xiaoming Deng, Jinyao Li, Yukun Lai,\nCuixia Ma, Yongjin Liu, and Hongan Wang. Stroke-based\nsemantic segmentation for scene-level free-hand sketches.\nThe Visual Computer , pages 1–13, 2022. 2, 5\n[64] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang\nWang, and Jiaya Jia. Pyramid scene parsing network. In\nProceedings of the IEEE conference on computer vision and\npattern recognition , pages 2881–2890, 2017. 2\n[65] Xin-Yang Zheng, Hao Pan, Peng-Shuai Wang, Xin Tong,\nYang Liu, and Heung-Yeung Shum. Locally attentional sdf\ndiffusion for controllable 3d shape generation. ACM TOG,\nProc. SIGGRAPH , 2023. 2\n[66] Yixiao Zheng, Jiyang Xie, Aneeshan Sain, Yi-Zhe Song, and\nZhanyu Ma. Sketch-segformer: Transformer-based segmen-\ntation for figurative and creative sketches. IEEE Transactions\non Image Processing , 2023. 2[67] Chong Zhou, Chen Change Loy, and Bo Dai. Dense-\nclip: Extract free dense labels from clip. arXiv preprint\narXiv:2112.01071 , 2021. 6\n[68] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei\nLiu. Conditional prompt learning for vision-language mod-\nels. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition , 2022. 2\n[69] Ziqin Zhou, Yinjie Lei, Bowen Zhang, Lingqiao Liu, and\nYifan Liu. Zegclip: Towards adapting clip for zero-shot se-\nmantic segmentation. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition , pages\n11175–11185, 2023. 2, 5, 6\n[70] Xianyi Zhu, Yi Xiao, and Yan Zheng. Part-level sketch\nsegmentation and labeling using dual-cnn. In Neural Infor-\nmation Processing: 25th International Conference, ICONIP\n2018, Siem Reap, Cambodia, December 13-16, 2018, Pro-\nceedings, Part I 25 , pages 374–384. Springer, 2018. 2\n[71] Xianyi Zhu, Yi Xiao, and Yan Zheng. 2d freehand sketch\nlabeling using cnn and crf. Multimed. Tools. Appl. , 79(1),\n2020. 2\n[72] Y Zhu, Z Zhang, C Wu, Z Zhang, T He, H Zhang, R\nManmatha, M Li, and A Smola. Improving semantic seg-\nmentation via self-training. arxiv 2020. arXiv preprint\narXiv:2004.14960 , 2021. 2\n[73] Changqing Zou, Qian Yu, Ruofei Du, Haoran Mo, Yi-Zhe\nSong, Tao Xiang, Chengying Gao, Baoquan Chen, and Hao\nZhang. Sketchyscene: Richly-annotated scene sketches. In\nProceedings of the european conference on computer vision\n(ECCV) , pages 421–436, 2018. 2\n4186'}, 'dist': 0.9286905527114868}
Result 22: {'text': 'data', 'metadata': {'created_at': 1736932570, 'modified_at': 1736932570, 'owner': 'sayandeep', 'path': 'RAG_CONTENT/conferences/cvpr_ocr/Bono_Learning_to_Navigate_Efficiently_and_Precisely_in_Real_Environments_CVPR_2024_paper.txt', 'size': 50704, 'seen_at': 1737191136, 'data': 'Learning to navigate efficiently and precisely in real environments\nGuillaume Bono, Herv ´e Poirier, Leonid Antsfeld, Gianluca Monaci, Boris Chidlovskii, Christian Wolf\nNA VER LABS Europe, Meylan, France\n{firstname }.{lastname }@naverlabs.com\nAbstract\nIn the context of autonomous navigation of terrestrial robots,\nthe creation of realistic models for agent dynamics and sens-\ning is a widespread habit in the robotics literature and in\ncommercial applications, where they are used for model\nbased control and/or for localization and mapping. The\nmore recent Embodied AI literature, on the other hand, fo-\ncuses on modular or end-to-end agents trained in simulators\nlike Habitat or AI-Thor, where the emphasis is put on photo-\nrealistic rendering and scene diversity, but high-fidelity robot\nmotion is assigned a less privileged role. The resulting\nsim2real gap significantly impacts transfer of the trained\nmodels to real robotic platforms. In this work we explore\nend-to-end training of agents in simulation in settings which\nminimize the sim2real gap both, in sensing and in actuation.\nOur agent directly predicts (discretized) velocity commands,\nwhich are maintained through closed-loop control in the real\nrobot. The behavior of the real robot (including the under-\nlying low-level controller) is identified and simulated in a\nmodified Habitat simulator. Noise models for odometry and\nlocalization further contribute in lowering the sim2real gap.\nWe evaluate on real navigation scenarios, explore different\nlocalization and point goal calculation methods and report\nsignificant gains in performance and robustness compared\nto prior work.\n1. Introduction\nPoint goal navigation of terrestrial robots in indoor buildings\nhas traditionally been addressed in the robotics community\nwith mapping and planning [ 6,30,50], which led to solu-\ntions capable of operating on robots in real environments.\nThe field of computer vision and embodied AI has addressed\nthis problem through large-scale machine learning in sim-\nulated 3D environments from reward with RL [ 24,34] or\nwith imitation learning [ 14]. Learning from large-scale data\nallows the agent to pick up more complex regularities, to\nprocess more subtle cues and therefore (in principle) to be\nmore robust, to exploit data regularities to infer hidden and\noccluded information, and generally to learn more powerful\nTraining/sim (ours)Test/realxt1212xt+1\nTraining/sim (classical)xt12xt+1Policy: ∆πOnboardctrl∆φ333ms/3 Hz(Unknown)Policy: ∆πDyn. model∆φ333ms/3 Hz33ms/30 HzPolicy: ∆πconstant333ms/3 HzvelocitiesPiecewise\nFigure 1. Efficient navigation with policies end-to-end trained in\n3D photorealistic simulators requires closing the sim2real gap in\nsensing andactuation. Efficiency demands that the robot continues\nto move during decision taking (as opposed to stopping for each\nsensing operation), and this requires a realistic motion model in\nsimulation allowing the agent to internally anticipate its future state.\nThis requirement is exacerbated by the delay between sensing\n①and actuation ②caused by the computational complexity of\nhigh-capacity deep networks (visual encoders, policy). To model\nrealistic motion while training in simulation, we create a 2ndorder\ndynamical model running with higher frequency, which models\nthe robot and its low-level closed-loop controller. We identify the\nmodel from real data and add it to the Habitat [44] Simulator.\ndecision rules. In this context, the specific task of point goal\nnavigation (navigation to coordinates) is now sometimes\nconsidered “solved” in the literature [ 38], incorrectly, as we\nargue. While the machine learning and computer vision\ncommunity turns its attention towards the exciting goal of\nintegrating language models into navigation [ 15,23], we\nthink that further improvements are required to make trained\nagents perform reliably in real environments with sufficient\nspeed.\nExperiments and evaluations of trained models in real\nenvironments and the impact of the sim2real gap do exist in\nthe Embodied AI literature [ 11,20,43], but they are rare and\nwere performed in restricted environments. Known models\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n17837\nare trained in 3D simulators like Habitat [ 44] or AI-Thor\n[26], which are realistic in their perception aspects, but lack\nthe accurate motion models of physical agents which the\nrobotics community uses for articulated robots, for instance,\nwhere forward and inverse kinetics are often modeled and\nidentified. This results in a large sim2real gap in motion char-\nacteristics, which is either ignored or addressed by discrete\nmotion commands. The latter strategy teleports the agent\nin simulation for each discrete action and executes these\nactions on the physical robot by low-level control on posi-\ntions instead of velocities, stopping the robot between agent\ndecisions and leading to extremely inefficient navigation and\noverly long episodes [43].\nIn this work we go beyond current models for terrestrial\nnavigation by realistically modeling both actuation and sens-\ning of real agents in simulation, and we successfully transfer\ntrained models to a real robotic platform. On the actuation\nside, we take agent decisions as discretized linear and an-\ngular velocities and predict them asynchronously while the\nagent is in motion. The resulting delay between sensing\nand actuation (see Figure 1) requires the agent to anticipate\nmotion, which it learns in simulation from a realistic motion\nmodel. To this end, we create a second-order dynamical\nmodel, which approximates the physical properties of the\nreal robot together with its closed-loop low-level controller.\nWe identify this model from real robot rollouts and expert\nactions. The model is integrated into the Habitat simulator as\nan intermediate solution between the standard instantaneous\nmotion model and a full-blown computationally-expensive\nphysics simulation based on forces, masses, frictions etc.\nOn the sensing side, we combine visual and Lidar obser-\nvations with different ways of communicating the navigation\n(point) goal to the agent, and different estimates of robot\npose relative to a reference frame, combining two different\ntypes of localization: first, dead reckoning from wheel en-\ncoders, and secondly, external localization based on Monte\nCarlo methods from Lidar input. We perform extensive ex-\nperiments and ablations on a real navigation platform and\nquantify the impact of the motion model, action spaces, sens-\ning capabilities and the way we provide goal coordinates\non navigation performance, showing that end-to-end trained\nagents can robustly navigate from visual input when their\nmotion has been correctly modelled in simulation.\n2. Related Work\nVisual navigation — navigation has been classically solved\nin robotics using mapping and planning [ 7,31,32], which re-\nquires solutions for mapping and localization [ 6,28,50], for\nplanning [ 27,46] and for low-level control [ 19,42]. These\nmethods depend on accurate sensor models, filtering, dynam-\nical models and optimization. End-to-end trained models\ndirectly map input to actions and are typically trained with\nRL [ 24,35] or imitation learning [ 14]. They learn repre-sentations such as flat recurrent states, occupancy maps [ 9],\nsemantic maps [ 8], latent metric maps [ 4,21,37], topologi-\ncal maps [ 3,10,47], self-attention [ 12,16,18,41], implicit\nrepresentations [ 33] or maximizing navigability directly with\na blind agent optimizing a collected representation [ 5]. Our\nproposed method is end-to-end trained and features recurrent\nmemory, but benefits from an additional motion model in\nsimulation.\nSim2real transfer — The sim2real gap can compromise\nstrong performance achieved in simulation when agents are\ntested in the real-world [ 22,29], as perception and control\npolicies often do not generalize well to real robots due to in-\naccuracies in modelling, simplifications and biases. To close\nthe gap, domain randomization methods [ 39,49] treat the\ndiscrepancy between the domains as variability in the simu-\nlation parameters. Alternatively, domain adaptation methods\nlearn an invariant mapping for matching distributions be-\ntween the simulator and the robot environment. Examples\ninclude transfer of visuo-motor policies by adversarial learn-\ning [ 55], adapting dynamics in RL [ 17], lowering fidelity\nsimulation [ 52] and adapting object recognition to new do-\nmains [ 56]. Bi-directional adaptation is proposed in [ 51];\nRecently, Chattopadhyay et al. [11] benchmarked the robust-\nness of embodied agents to visual and dynamics corruptions.\nKadian et al. [25] investigated sim2real predictability of\nHabitat-Sim [ 44] for PointGoal navigation and proposed a\nnew metric to quantify it, called Sim-vs-Real Correlation Co-\nefficient (SRCC). The PointGoal task on real robots is also\nevaluated in [ 43]. To reach competitive performance with-\nout modelling any sim2real transfer, the agent is pre-trained\non a wide variety of environments and then fine-tuned on a\nsimulated version of the target environment.\nMotion models — are usually kept very simple when train-\ning visual navigation policies. Several works use a discrete\nhigh-level action space in simulation [ 8,9,20] and rely on\ndedicated controllers to translate these discrete actions into\nrobot controls when transferring to the real world. Alter-\nnative approaches predict continuous actions [ 13,54] or\nwaypoints [ 2,48], that again are subsequently mapped to\nrobot controls when executing the policies in real. Some-\nwhat closer to our approach, Yokoyama et al. [53] adopt a\ndiscretized linear and angular velocity space, and introduce\na new metric, SCT, that uses a simplified “unicycle-cart” dy-\nnamical model to estimate the “optimal” episode completion\ntime computed by rapidly exploring random trees (RRT).\nHowever the unicycle model is only used for evaluation and\nit is not deployed for training nor testing: as in previous\nworks, the commands are mapped to robot controls using\nhandcrafted heuristics. In contrast to previous approaches,\nwe include the dynamical model of the robot in the training\nloop to learn complex dependencies between the navigation\npolicy and the robot physical properties.\n17838\nvGRUdg0πatlˆgthtRGB imgStatic goal vectorˆprtˆpatIntegrated relative pose estimateAbsolute pose estimateLidar scanu\nItStgtLPPOAux supervision\nPrevious actionat−1eFigure 2. The agent uses a recurrent policy with a static point goal\ng0as input, i.e. the goal is constant and given wrt. to the initial\nreference frame. During training, the estimation of the dynamic\npoint goal ˆgtis supervised from privileged information.\n3. End-to-end training with realistic sensing\nWe propose a method for training agents which (in principle)\ncould address a variety of visual navigation problems, and\nwithout loss of generality, focusing on sim2real transfer\nand efficiency, we formalize the problem as a PointGoal\ntask: an agent receives sensor observations at each time step\ntand must take actions atto reach a goal position given\nas polar coordinates. The method is not restricted to any\nspecific observations, in our experiments the agent receives\nRGB images It∈R3×H×Wand a Lidar-like vector of ranges\nSt∈RK, where Kis the number of laser rays.\nThe proposed agent is fully trained and therefore does not\nrequire to localize itself on a map for analytical path planning.\nHowever, even in this very permissive setting the application\nitself requires to communicate the goal to the agent in some\nform of coordinate system, which requires localization at\nleast initially with respect to the goal (in ImageGoal settings\nthis would be replaced by the goal image). We can classically\ndifferentiate between three different forms:\nAbsolute PointGoal — the goal vector ga= [ga\nx,ga\ny]T\nis given in a pre-defined agent-independent absolute refer-\nence frame. This requires access to a global localization\nsystem.\nDynamic PointGoal — the goal vector gtis provided with\nrespect to the current agent pose and therefore updated at\neach time instant t. This requires an external localization\nsystem providing estimates which are then transformed\ninto the egocentric reference frame.\nStatic PointGoal — the goal vector is provided with re-\nspect to the starting position at t=0of the robot, and not\nupdated with agent motion (= g0). This requires the agent\nto learn this update itself, and therefore some form of in-\nternal localization or integration of odometry information.\nWe target the Static PointGoal case, which does not require\nlocalization beyond the initial relative vector. However, we\nalso argue that some form of localization and/or odometry\ninformation is useful, which will assist the dynamics infor-mation inferred from (visual) sensor readings. Furthermore,\nduring training it will allow the agent to learn a useful latent\nrepresentation in its hidden state and an associated dynamics\nin this latent space. We therefore provide the agent with two\ndifferent localization components:\nˆpa\nt— an estimate of agent pose with respect to the initial\nagent position, asynchronously delivered and with low\nfrequency, around 0.5-1 fps. At test, this signal is\ndelivered by external localization, in our case from a\nLidar-based AMCL module (see Section 5).\nˆpr\nt— an estimate coming from onboard odometry, which\nwe integrate over time to provide an estimate situated\nin the same reference frame as ˆpa\nt, i.e. with respect to\nthe initial agent position.\nThe characteristics of the two localization signals are differ-\nent: while ˆpr\ntcorresponds to a dead reckoning process and\nis subject to drift, its relative noise is lower. The signal ˆpa\ntis\nnot subject to drift but suffers from larger errors in certain\nregions, due to registration errors or absence of structure.\nThe agent policy is recurrent, based on a hidden memory\nvector htupdated over time from input of all sensor readings\nIt,St,pr\nt,ˆpa\nt, the goal g0and the previous action at−1,\nht=d(ht−1, v(It), u(St),g0,ˆpr\nt,ˆpa\nt, e(at−1)),(1)\nat=π(ht), (2)\nˆgt=l(ht), (3)\nwhere dis a GRU with hidden state ht,vis a visual encoder\nof the image It(in our case, a ResNet-18); uis a 1D-CNN\nencoding the scan St;eis a set of action embedding vectors;\nπis a linear policy. Other inputs go through dedicated MLPs,\nnot named to lighten notations. To help the agent deal with\nthe noisy relative and absolute pose estimates, we put an in-\nductive bias for localization on htthrough an auxiliary head\nlpredicting the dynamic goal compass ˆgt, supervised during\ntraining from the simulated agent’s ground truth pose, as\nshown in Figure 2. In the following paragraphs we describe\nthe two localization systems and the noise models we use to\nsimulate them during training in simulation. Section 4 then\nintroduces the dynamical model used during training.\nIntegrated relative localization — the pose estimate ˆpr\nt\ncan be obtained with any commercially available odometry\nsystem. In our experiments we integrate readings from wheel\nencoders. During simulation and training, ˆpr\ntis estimated by\nsampling planar noise [ϵx, ϵθ]⊺from a multivariate Gaussian\ndistribution N(\x140.01\n0\x15\n,\x1410−410−4\n10−410−3\x15\n)which gets added\nto the step-wise motion of the agent, leading to the accumu-\nlation of drift over time.\nAbsolute localization — the pose estimate ˆpa\ntis ob-\ntained with the standard ROS 2 package for Adaptive Monte\nCarlo Localization (AMCL) [ 50], a KLD-sampling ap-\nproach requiring a pre-computed, static map based on par-\nticle filtering. During simulation and training, ˆpr\ntis es-\n17839\nTraining in Simulation\nTest onReal robotsDeploy\ng0at+\nDiff. driveLeft and right motor speed set valuesnatvrtvltgoal\nSecond order dynamical systemat\nPhysical robot3D simulatorActuation noise\nFiltered + integrated odometry (IMU + encoders)Global localization Fig. 2ˆprtˆpatRGB imgLidar scan\nItStg0goalFig. 2\nItStRGB imgLidar scanSystem identificationfromrecordedtrajectoriesOdometry and absolute pose wrt. to starting position.Noisy privileged informationˆprtˆpat\n30 HzState update (3 Hz)\nt+2t(333ms/3 Hz)∆π(333ms/3 Hz)∆πt+1Onboard closed loop control (PID), unknown freq.Policy/decision taking loop:τ+1τ…Figure 3. Training visual navigation with realistic motion models: we train an end-to-end agent in simulation (top) subject to two different\nsimulation loops: a slower loop at 3Hz (indexed by t) renders visual observations and takes agent decisions, while a faster loop at 30 Hz\n(indexed by τ) simulates physics. Physics is approximated with a 2ndorder model identified from real robot rollouts (bottom) and includes\nthe robot physics as well as the behavior of the closed-loop control of the differential drive (neither onboard control algorithm nor control\nfrequency need to be known). Operations in the intervals are pipelined, eg. sensing occurs at each time step, as does agent forward pass etc.\nThe agent architecture is detailed in Figure 2.\ntimated by sampling planar noise from another Gaussian\nN(0,Diag [0.03,0.03,0.05]), but this time it gets directly\nadded to the ground truth pose of the robot, thus modeling\nimprecise localization (eg. due to Lidar registration failures)\nwithout drift accumulation. In addition, we model the po-\ntential unreliability of such absolute pose estimates by only\nproviding a new observation to the agent every few steps,\nat an irregular period governed by a uniform distribution\nU(8,12). This potentially allows to plug-in low-frequency\nvisual localization.\n4. A dynamical model of realistic robot motion\nOne of our main motivations is to be able to use a trained\nmodel to navigate efficiently , i.e. fast and reliably. Stopping\nthe robot between predicted actions is therefore not an option,\nand we train the model to predict linear and angular velocity\ncommands in parallel to its physical motion, as illustrated\nin Figure 1. This requires the agent to anticipate motion\nto predict an action atrelative to the next (yet unobserved)\nstate at t, exacerbated by the delay between sensing at time\ntand actuation at t+1, during which computation through\nthe high-capacity visual encoder and policy happens. This\nmotion depends on the physical characteristics of the robot,\nas shown in Figure 3 (bottom): the linear and angular veloc-ity commands predicted by the neural policy πare translated\ninto “set values” for left and right motor speeds of the differ-\nential drive system with a deterministic function and used as\ntargets by the closed loop control algorithm, typically a PID\n(Proportional Integral Derivative Controller ), attempting\nto reach and maintain the predicted speed between agent\ndecisions. The behavior depends on the control algorithm\nbut also on the physical properties of the robot, like its mass\nand resulting inertia, power and torque curves of the motors,\nfriction etc. We decrease the actuation sim2real gap as as\nmuch as possible by integrating a model of realistic robot\nbehavior into the Habitat [ 44] platform, shown in the top part\nof Figure 3. We approximate the robot’s motion behavior by\nmodeling the combined behavior of both the robot and the\ncontrol algorithm implemented on the real platform as an\nasymmetric second-order dynamic system. The parameters\nof this low-level loop are estimated from recordings of the\nreal robot platform with a system identification algorithm.\nClassically, a sequential decision process can be mod-\neled as a POMDP ⟨X,U, T,O, O, R⟩. In pure 2D navi-\ngation settings with a fixed goal in a single static scene,\nthe environment maintains a (hidden) state xt∈ X con-\nsisting in the pose of the agent, i.e. position and orienta-\ntion,xt= [xt, yt, θt]. In most settings in the literature\n17840\nFigure 5. Robustness: Top: The ROS2 baseline fails when obsta-\ncles are thin at the height of the single Lidar ray. The robot doesn’t\nsee the stool, collides, drags it away. The red circles\n show the\nposition of the moved stool. Bottom: our agent is surprisingly\nrobust and allows navigation very close to fine structures.\nbased on 3D photo-realistic simulators like Habitat [ 44],\nthe state update in simulation ignores physical properties\nof the robot (mass/inertia, friction, accelerations, etc.), as\nit is implemented through “teleportation” of the robot to a\nnew pose. In that case, the environment transition decom-\nposes into a discrete state update xt+1=T(xt, ut)and\nan observation ot+1=O(xt+1), The discrete action space\nUmotion ={FORWARD 25cm ,TURN LEFT 10◦,TURN RIGHT\n10◦andSTOP}frequently chosen in the navigation literature\nresults in a halting and jerky robot motion.\nAdding realism — To be able to smooth robot trajecto-\nries, we need to model accelerations. We extend the agent\nstate as:\nxt= [xt, yt, θt, vt, wt,˙vt,˙wt], (4)\nwhere xt, yt, θtare the absolute position and orientation of\nthe robot in the plane of the scene (in m, m and rad); vt, wt\nare the linear (forward) and angular velocities of the robot (in\nm/s and rad/s); ˙vt; ˙wtare the linear and angular accelerations\nof the robot (in m/s2and rad/s2).\nFigure 4. The action space :\n28 actions, 4 choices of lin-\near velocities ∈[0,1]m/s, 7\nchoices for angular vel. ∈\n[−3,3]rad/s. Arrows show\nthe effect on pose of action\nheld for2\n3sec.The proposed agent predicts\na pair⟨v∗\nt, w∗\nt⟩of velocity com-\nmands, chosen from a discrete ac-\ntion space Uvelresulting from the\ncartesian product of linear and\nangular spaces {0,0.3,0.6,1} ×\n{−3,−2,−1,0, . . . ,3}, as shown\nin Figure 4. To simplify no-\ntations, we will use ut=\n⟨v∗\nt, w∗\nt⟩ ∈ U .\nOne possibility to model real-\nistic robot behavior is to design a\nfull dynamical model of the robot\nin the form of a set of differen-\ntial equations, identify its param-\neters, discretize it, and simulate\nit in Habitat, together with run-\nning an exact copy of the control algorithm used on the\nFigure 6. Dynamic obstacles: our agent stops and circumvents\npeople, achieved through a highly negative collision reward term.\nphysical robot with exactly the same parameters and control\nfrequency, which is a stringent constraint. Instead, we opted\nfor a more flexible solution and we approximate the com-\nbined behavior of the physical robot and its control algorithm\nby a second order dynamical model [ 36]. We decompose in-\nteractions with the simulator into two different loops running\nat different frequencies:\n•Visual observations, simulator state updates and agent\ndecisions (Equations (1) to (3)) are produced in a slower\nloop, indexed by letter tin the subsequent notation, and\nrun at f∗= 3Hz in our experiments.\n•Between two subsequent steps of the slower loop, a faster\nloop models physical motion of the robot, without ren-\ndering observations. At the end of this loop, the simula-\ntor state is updated, and a visual observation is returned.\nSteps in this faster loop are indexed by a second index τ\nin the subsequent notation, xt,τ.\nIn-between two environment steps t−1andt, we simulate\ndynamics at frequency fϕ= 30 > f∗. The number of phys-\nical sub-time steps per environment time step is therefore\nKϕ=l\nfϕ\nf∗m\n= 10 , their duration is ∆ϕ=1\nfϕ= 33 ms. The\nphysical loop running between two environment steps can\nbe formalized as the following set of state update equations,\not=O(xt−1). Observation\nxt−1,0=xt−1 Init\n˙vt,τ+1= ˙vt,τ+ ∆ϕ(fv\nt,τ2δv\nt,τ−2ζv\nt,τfv\nt,τ˙vt,τ)Upd. acc.\n˙wt,τ+1= ˙wt,τ+ ∆ϕ(fw\nt,τ2δw\nt,τ−2ζw\nt,τfw\nt,τ˙wt,τ)\nvt,τ+1=vt,τ+ ∆ϕvt,τ+1 Upd. vel.\nwt,τ+1=wt,τ+ ∆ϕwt,τ+1\nθt,τ+1=θt,τ+ ∆ϕwt,τ+1 Upd. pose\nxt,τ+1=xt,τ+ ∆ϕvt,τ+1cosθt,τ+1\nyt,τ+1=yt,τ+ ∆ϕvt,τ+1sinθt,τ+1\nxt=xt−1,K Final state\n(5)\nIn these equations, f.are natural frequencies and damping\ncoefficients of asymmetric 2ndorder dynamic models for\nlinear and angular motion. At each step τ, they are chosen\nfrom identified values given command errors as follows:\nδv\nt,τ=v∗\nt−vt,τ\n⟨fv\nt,τ, ζv\nt,τ⟩=(\n⟨fv↑, ζv↑⟩ifδv\nt,τ·vt,τ>0(acceleration)\n⟨fv↓, ζv↓⟩otherwise (deceleration)\n(6)\n17841\nand similarly for angular model parameters ⟨fw, ζw⟩. Veloc-\nity and acceleration are also clipped to identified maximum\nvalues.\nSystem identification — the model has 8 parameters,\nfv↑, ζv↑, fv↓, ζv↓, fw↑,ζw↑, fw↓,ζw↓, which we iden-\ntify from trajectories of a real robot controlled with pre-\ncomputed trajectories exploring the command space, see the\nsupplementary material for more details.\nTraining — the model is trained with RL, in particular\nPPO [ 45], and with a reward inspired by [ 11],rt= R·\nIsuccess−∆Geo\nt−λ−C·Icollision , where R=2.5,∆Geo\ntis the\ngain in geodesic distance to the goal, a slack cost λ=0.01\nencourages efficiency, and a cost C= 0.1penalizes each\ncollision without ending the episode.\nRecovery behavior — similar to what is done in classi-\ncal analytical planning, we added a rudimentary recovery\nbehavior on top of the trained agent: if the agent is notified\nof an obstacle and does not move for five seconds, it will\nmove backward at 0.2 m/s for two seconds.\n5. Experimental Results\nExperimental setup — we trained the agent in the Habitat\nsimulator [ 44] on the 800 scenes of the HM3D Dataset [ 40]\nfor 200M env. steps. Real robot experiments have been per-\nformed on a Naver Rookie robot, which came equipped with\nvarious sensors. We added an additional front facing RGB\ncamera and capture images with a resolution of 1280×720\nresized to 256×144. In our experiments, the Lidar scan S\nis not taken from an onboard Lidar, but simulated from the\n4RealSense depth cameras which are installed close to the\nfloor and oriented in 4 different directions. We use multiple\nscan lines of the cameras and fuse them into a single ray,\ndetails are given in the supplementary material. We did,\nhowever, add a single plane Lidar to the robot (which did\nnot come equipped with one) and used it for localization\nonly (see section 3). All processing has been done onboard,\nthanks to a Nvidia Jetson AGX Orin we added, with an\nintegrated Nvidia Ampere GPU. It handles pre-processing\nof visual observations and network forward pass in around\n70ms. The exact network architecture of the policy is given\nin the supplementary material.\nEvaluation — evaluating sim2real transfer is inherently\ndifficult, as it would optimally require to evaluate all agent\nvariants and ablations on a real physical robot and on a high\nnumber of episodes. We opted for a three-way strategy, all\ntables are color-coded with numbers corresponding to one of\nthe three following settings: (i) “Real” experiments evalu-\nate the agent trained in simulation on the real Naver Rookie\nrobot. It is the only form of evaluation which correctly\nestimates navigation performance in a real world scenario,\nbut for practical reasons we limit it to a restricted number\nof 20 episodes in a large office environment shown in Fig.\n7.(ii) “Simulation (+dyn. model)”) is a setting in simula-tion (Habitat), which allows large-scale evaluation on a large\nnumber of unseen environments and episodes, the HM3D\nvalidation set. Most importantly, during evaluation the sim-\nulator uses the identified dynamical model and therefore\nrealistic motion, even for baselines which do not have access\nto one during training. Similar to the “Real” setting, this al-\nlows to evaluate the impact of not modeling realistic motion\nduring training. (iii) “ Simulation (train domain)” evalu-\nates in simulation with the same action space an agent variant\nuses during training, i.e. there is no sim2real gap at all. This\nsetting evaluates the difficulty of the simulated task, which\nmight be a severe approximation of the task in real condi-\ntions. High performance in this setting is not necessarily\nindicative of high performance in a real environment.\nWe evaluate on two different sets of episodes:\nHM3D/2.5k consists of 2500 episodes in the HM3D val-\nidation scenes, used in simulation only. Office/20 consists\nof 20 episodes in the targeted office building, Figure 7. They\nare used for evaluation in both, real world and simulation.\nMetrics — Navigation performance is evaluated by suc-\ncess rate (SR), i.e., fraction of episodes terminated within\na distance of <0.2m to the goal by the agent calling the\n⟨v=0, w=0⟩action enough time to cancel its velocity, and\nSPL [ 1], i.e., SR weighted by the optimality of the path,\nSPL=1\nNPN\ni=1Isuccessℓ∗\ni\nmax( ℓi,ℓ∗\ni),where ℓiis the agent path\nlength and ℓ∗\nithe GT path length.\nSPL is limited in its ability to properly evaluate agents\nwith complex dynamics. Success Weighted by Completion\nTime (SCT) [ 53] explicitly takes the agent’s dynamics model\ninto consideration, and aims to accurately capture how well\nthe agent has approximated the fastest navigation behavior.\nIt is defined as SCT=1\nNPN\ni=1Sit∗\ni\nmax( ci,t∗\ni),where ciis the\nagent’s completion time in episode i, and t∗\niis the shortest\npossible amount of time it takes to reach the goal point from\nthe start point while circumventing obstacles based on the\nagent’s dynamics. To simplify implementation, we use a\nlower bound on t∗taking into account linear dynamics along\nstraight shortest path segments.\nCheckpoints have been chosen as follows: performance in\nReal is given with checkpoints selected as the ones provid-\ning max SR in simulation . Performance in Simu lation\nis given on the last checkpoint.\n5.1. Results\nAgent behavior — the agent is surprisingly robust and does\nnot collide with the infrastructure even though it can quite\nclosely approach it navigating around it, even if the obstacles\nare thin and light. Examples are given in Figure 5 (top), but\nthey are best viewed through the video in the supplementary\nmaterial . This is in stark contrast to the ROS 2 based planner,\nwhich uses a 2D Map constructed with the onboard Lidar:\nthin and light structures do not show up on the map or a\nfiltered, often because the larger part of the obstacle is not\n17842\nFigure 7. The 20 trajectories ofOffice/20 are distributed over 5 plots and superimposed on the map, color coded. If an episode fails, the\nremaining distance is shown as a straight black line from the last position to the goal. The agent is variant βof Table 4.\nMethod +dyn Ref. Sim (+dyn.) HM3D/2.5k Sim (+dyn.) Office/20 Real Office/20\n& action space (train) SR(%) SPL(%) SCT(%) SR(%) SPL(%) SCT(%) SR(%) SPL(%) SCT(%)\n(a) Position, disc(4) ✗ [43] 58.2 48.2 16.8 35.0 30.3 13.2 15.0 11.8 2.4\n(b) Velocity, disc(28) ✗ [53] 0.8 0.1 0.1 0.0 0.0 0.0 20.0 8.7 2.6\n(c) Velocity, disc(28) ✓ (Ours) 97.4 82.2 51.0 100.0 78.8 59.9 40.0 28.9 10.1\nTable 1. Impact of training with a realistic dynamical model: we compare end-to-end trained models using position commands (a) as in\n[43] and, discretized velocity commands without a dynamical model (considering constant velocity) as in [ 53], and our proposed method (c).\nThe references [43, 53] are cited for their action space and motion handling, but they have different agent architectures.\nMethod Sim(train) HM3D/2.5k Sim(+dyn) HM3D/2.5k\nSR% SPL% SCT% SR% SPL% SCT%\n(a)Pos, disc(4) 92.7 81.7 30.3 58.2 48.2 16.8\n(b)Vel, disc(28) 98.0 74.2 58.9 0.8 0.1 0.1\n(c)Vel, disc(28) 97.4 82.2 51.0 97.4 82.2 51.0\nTable 2. Difficulty of tasks given action spaces and motion mod-\nels: we evaluate the baseline model variants in the same simulated\nenvironment in which they have been trained. High performance\ndoes not necessarily transfer to real. Letters are variants in Table 1.\nin the height of the single Lidar ray. Collisions are frequent,\nexamples are given in Figure 5 (bottom). The agent is also\ncapable of avoiding dynamic obstacles like people in spite\nof not having seen them in simulation (cf. Figure 6), which\nwe conjecture is due to a high collision avoidance weight in\nthe reward.\nSim2real gap and memory — one interesting finding we\nwould like to share is that the raw base agent sometimes tends\nto get discouraged from initially being blocked in a situation,\nfor instance if the passage through a door is not optimal and\nrequires correction. The initial variants of the agent seemed\nto abandon relatively quickly and started searching for a\ndifferent trajectory, circumventing the passage way obstacle\naltogether. We conjecture that this stems from the fact that\nsuch blockings are rarely seen in simulation and the agent is\ntrained to “write off” this area quickly, storing in its recurrent\nmemory that this path is blocked. All our real experiments\nare therefore performed with a version, which resets its re-\ncurrent state hi(eq. (1)) every 10 seconds, leading to less\nfrequent abandoning. Future work will address this problem\nmore directly, for instance by simulating blocking situations\nduring training.Influence of motion model — Table 1 compares results\nof different agents trained with different action spaces and\nwith or without realistic motion during simulation. We can\nsee that the impact of training the agent with the correct\nmotion model in simulation is tremendous. The agent in line\n(b) uses the same action space, but no dynamical model is\nused in simulation, which means that changes in velocity are\ninstantaneous and velocities are constant between decisions.\nThe behavior is unexploitable, the agent is disoriented and\nkeeps crashing into its environment. Line (a) corresponds\nto a position controlled agent with action space {FORWARD\n25cm ,TURN LEFT 10◦,TURN RIGHT 10◦,STOP}. After\ntraining, it is adapted to motion commands by calculating\nthe corresponding velocity commands given the decision\nfrequency of 3 Hz. It has somewhat acceptable (but low)\nperformance in simulation, although it does not dispose of\na motion model during training. This fact that rotating and\nlinear motion are separated and cannot occur in the same\ntime simplifies the problem and leads to some success in\nsimulation, but this behavior does not transfer well to the real\nrobot. The agent with the identified motion model achieves\nnear perfect SR in simulation, which shows that the model\ncan learn to anticipate motion and (internal latent) future\nstate prediction correctly. When transferred to the real robot,\nperformance is the best among all agents, but still leaves\nroom much room for improvement.\nDifficulty of the tasks — Table 2 compares the same\nthree agents also in simulation when validated with the\nsame setting they have been trained on (action space, motion\nmodel or absence of). While this comparison is not at all\nindicative of the performance of these agents on a real robot,\nit provides evidence of the difficulty of the task in simulation.\n17843\nPoint Goal ˆpr\ntˆpa\ntSim (+dyn) Office/20 Real Office/20\nSR% SPL% SCT% SR% SPL% SCT%\ngt ✗✗40.0 34.5 27.4 35.0 23.6 5.2\ng0 ✓✓70.0 51.9 36.9 50.0 37.5 11.4\ng0+ superv. ✓✓100.0 78.8 59.9 40.0 28.9 10.1\ng0+ superv. ✓✗70.0 51.9 36.9 25.0 16.7 3.0\nTable 3. Localization and PointGoal calculation : we compare\nthe impact of the presence of external localization to the agent,\nand the point goal sources: dynamical point goal (through external\nlocalization) with static point goal w/ and w/o supervision. All\nagents are variant (c) from Table 1.\nMethod Real Office/20 Real Office/20-alt\nSR% SPL% SCT% SR% SPL% SCT%\n(α)ROS 2 (fused) 80.0 69.5 26.2 90.0 70.5 27.0\n(β)Ours (finetuned) 55.0 40.4 11.2 60.0 42.0 9.7\nTable 4. Comparisons — we compare with the ROS 2 NavStack,\nwhich has access to the 2D occupancy map beforehand, localizes\nitself with the single ray Lidar scan and AMCL (Monte Carlo local-\nization), uses the fused laserscan for obstacle avoidance, followed\nby shortest path planning. Metrics do no measure collisions ,\nwhich are much higher for the ROS 2 planner. To be comparable,\nour method is finetuned with RL on the Matterport scan of the same\nbuilding. We also add an experiment on the same office building\nwith a different furniture arrangement, Office/20-alt .\nSurprisingly, the performances are very close: the additional\nburden the motion model puts on the learning problem itself\ncan be handled very well by the agent.\nGoal vector calculation — As explained in Section 3,\nour base agent receives the static point goal g0, ie. a vector\nwith respect to the initial reference frame at time t=0, which\nis not updated. Table 3 compares this agent with two other\nvariants. A version where supervision is removed during\ntraining, which surprisingly shows good performance. We\nconjecture that the additional supervision learns integration\nof odometry which might not transfer well enough from sim-\nulation, indicating insufficient noise modeling in simulation.\nThis will be addressed in future work. In another variant the\ndynamic point goal gtis provided at each time step in the\nagent’s egocentric frame. It is calculated from an external\nlocalization source, noisy in simulation. Letting the agent\nitself take care of the point goal integration seems to be the\nbetter solution.\nComparison with a map based planner — Table 4\ncompares the method with a ROS 2 based planner, which\nuses a 2D occupancy map constructed beforehand (not on\nthe fly). To be comparable, we finetuned our agent on a\nMatterport scan of the same building. While the ROS 2 based\nplanner is still more efficient in terms of SR and time (SCT),\nit requires many experiments to finetune its parameters. For\nexample, low values for the inflation radius will produce\nmany collisions with tables. But when this parameter is too\n1\n 0 1 2 3\nx (m)4\n3\n2\n1\n0y (m)\nReal\nSim(+dyn.)0 5 10 15 20 25\nt (s)0.00.20.40.6v (m/s)\n0 5 10 15 20 25\nt (s)1.0\n0.5\n0.00.51.0w (rad/s)Command\nReal\nSim(+dyn.)Figure 8. Dynamical sim2real gap: we compared recorded trajec-\ntories to simulated rollouts obtained with the same actions.\nhigh, the planner cannot find any path through doors and\ncorridors.\nRearrangement — we test the impact of rearranging\nfurniture significantly in the Office environment (see the\nsupp.mat. for pictures) and show the effect in Table 4, block\nOffice/20-alt . The differences are neglectable.\nDynamical sim2real gap — Figure 8 compares real robot\ntrajectories obtained by the agent on a small map of 4m×4m\nwith rollouts of the motion model in simulation with the\nsame actions, indicating very small drift.\nlin+3\nlin+2\nlin+1\nlin+0ang+3\nang+2\nang+1\nang+0\nang-1\nang-2\nang-30%10%20%30%40%50%\nFigure 9. Distribution of actions ,\nOffice/20 ,Sim, Real.Actions taken —\nFigure 9 shows his-\ntograms of the actions\ntaken by the action in\nsimulation and in real\nonOffice/20 . There\nis a clear preference\nfor certain combina-\ntions of linear and an-\ngular velocity. Dis-\ntributions in sim and\nreal mostly match, ex-\ncept for a tendency to\nturn in-place in real, which could be explained by obstacle\nmis-detections due to scan range noise.\n6. Conclusion\nWe have presented an end-to-end trained method for swift\nand precise navigation which can robustly avoid even thin\nand finely structured obstacles. This is achieved with train-\ning in simulation only by adding a realistic motion model\nidentified from recorded trajectories from a real robot. The\nmethod has been extensively evaluated in simulation as well\nas on a real robotic platform, where we assessed the im-\npact of the motion model, the action space, and the way\nhow a point goal is calculated and provided to the agent.\nThe method is robust, future work will focus on closed-loop\nadaptation of dynamics and sensor noise estimation.\n17844\nReferences\n[1]Peter Anderson, Angel X. Chang, Devendra Singh Chaplot,\nAlexey Dosovitskiy, Saurabh Gupta, Vladlen Koltun, Jana\nKosecka, Jitendra Malik, Roozbeh Mottaghi, Manolis Savva,\nand Amir Roshan Zamir. On evaluation of embodied naviga-\ntion agents. arXiv preprint , 2018. 6\n[2]Somil Bansal, Varun Tolani, Saurabh Gupta, Jitendra Malik,\nand Claire Tomlin. Combining optimal control and learning\nfor visual navigation in novel environments. In CoRL , 2020.\n2\n[3]Edward Beeching, Jilles Dibangoye, Olivier Simonin, and\nChristian Wolf. Learning to reason on uncertain topological\nmaps. In ECCV , 2020. 2\n[4]Edward Beeching, Jilles Dibangoye, Olivier Simonin, and\nChristian Wolf. Egomap: Projective mapping and structured\negocentric memory for deep RL. In ECML-PKDD , 2020. 2\n[5]Guillaume Bono, Leonid Antsfeld, Assem Sadek, Gianluca\nMonaci, and Christian Wolf. Learning with a Mole: Trans-\nferable latent spatial representations for navigation without\nreconstruction. In ICLR , 2024. 2\n[6]Guillaume Bresson, Zayed Alsayed, Li Yu, and S ´ebastien\nGlaser. Simultaneous localization and mapping: A survey of\ncurrent trends in autonomous driving. IEEE Transactions on\nIntelligent Vehicles , 2017. 1, 2\n[7]Wolfram Burgard, Armin B Cremers, Dieter Fox, Dirk\nH¨ahnel, Gerhard Lakemeyer, Dirk Schulz, Walter Steiner,\nand Sebastian Thrun. The interactive museum tour-guide\nrobot. In Aaai/iaai , pages 11–18, 1998. 2\n[8]Devendra Singh Chaplot, Dhiraj Gandhi, Abhinav Gupta, and\nRuslan Salakhutdinov. Object goal navigation using goal-\noriented semantic exploration. In NeurIPS , 2020. 2\n[9]Devendra Singh Chaplot, Dhiraj Gandhi, Saurabh Gupta, Ab-\nhinav Gupta, and Ruslan Salakhutdinov. Learning to explore\nusing active neural slam. In ICLR , 2020. 2\n[10] Devendra Singh Chaplot, Ruslan Salakhutdinov, Abhinav\nGupta, and Saurabh Gupta. Neural topological slam for visual\nnavigation. In CVPR , 2020. 2\n[11] Prithvijit Chattopadhyay, Judy Hoffman, Roozbeh Mottaghi,\nand Aniruddha Kembhavi. Robustnav: Towards benchmark-\ning robustness in embodied navigation. CoRR , 2106.04531,\n2021. 1, 2, 6\n[12] Shizhe Chen, Pierre-Louis Guhur, Makarand Tapaswi,\nCordelia Schmid, and Ivan Laptev. Think Global, Act Lo-\ncal: Dual-scale Graph Transformer for Vision-and-Language\nNavigation. arXiv:2202.11742 , 2022. 2\n[13] Jinyoung Choi, Kyungsik Park, Minsu Kim, and Sangok Seok.\nDeep reinforcement learning of navigation in a complex and\ncrowded environment with a limited field of view. In ICRA ,\n2019. 2\n[14] Yiming Ding, Carlos Florensa, Pieter Abbeel, and Mariano\nPhielipp. Goal-conditioned imitation learning. In NeurIPS ,\n2019. 1, 2\n[15] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch,\nAakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan\nTompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen\nChebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine,\nVincent Vanhoucke, Karol Hausman, Marc Toussaint, KlausGreff, Andy Zeng, Igor Mordatch, and Pete Florence. PALM-\nE: An Embodied Multimodal Language Model. In ICML ,\n2023. 1\n[16] Heming Du, Xin Yu, and Liang Zheng. Vtnet: Visual trans-\nformer network for object goal navigation. arXiv preprint\narXiv:2105.09447 , 2021. 2\n[17] Benjamin Eysenbach, Shreyas Chaudhari, Swapnil Asawa,\nSergey Levine, and Ruslan Salakhutdinov. Off-dynamics\nreinforcement learning: Training for transfer with domain\nclassifiers. In ICLR , 2021. 2\n[18] Kuan Fang, Alexander Toshev, Li Fei-Fei, and Silvio Savarese.\nScene memory transformer for embodied agents in long-\nhorizon tasks. In CVPR , 2019. 2\n[19] Dieter Fox, Wolfram Burgard, and Sebastian Thrun. The\ndynamic window approach to collision avoidance. IEEE\nRobotics & Automation Magazine , 4(1):23–33, 1997. 2\n[20] Theophile Gervet, Soumith Chintala, Dhruv Batra, Jitendra\nMalik, and Devendra Singh Chaplot. Navigating to objects in\nthe real world. Science Robotics , 8(79), 2023. 1, 2\n[21] Jo˜ao F. Henriques and Andrea Vedaldi. Mapnet: An allo-\ncentric spatial memory for mapping environments. In CVPR ,\n2018. 2\n[22] Sebastian H ¨ofer, Kostas E. Bekris, Ankur Handa, Juan\nCamilo Gamboa Higuera, Florian Golemo, Melissa Mozifian,\nChristopher G. Atkeson, Dieter Fox, Ken Goldberg, John\nLeonard, C. Karen Liu, Jan Peters, Shuran Song, Peter Welin-\nder, and Martha White. Perspectives on sim2real transfer for\nrobotics: A summary of the R: SS 2020 workshop, 2020. 2\n[23] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang,\nPete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch,\nYevgen Chebotar, Pierre Sermanet, Noah Brown, Tomas Jack-\nson, Linda Luu, Sergey Levine, Karol Hausman, and Brian\nIchter. Inner Monologue: Embodied Reasoning through Plan-\nning with Language Models. In CoRL , 2022. 1\n[24] Max Jaderberg, V olodymyr Mnih, Wojciech Marian Czar-\nnecki, Tom Schaul, Joel Z. Leibo, David Silver, and Koray\nKavukcuoglu. Reinforcement learning with unsupervised\nauxiliary tasks. In ICLR , 2017. 1, 2\n[25] Abhishek Kadian, Joanne Truong, Aaron Gokaslan, Alexan-\nder Clegg, Erik Wijmans, Stefan Lee, Manolis Savva, Sonia\nChernova, and Dhruv Batra. Sim2Real Predictivity: Does\nEvaluation in Simulation Predict Real-World Performance?\nIEEE Robotics and Automation Letters , 5(4):6670–6677,\n2020. 2\n[26] Eric Kolve, Roozbeh Mottaghi, Daniel Gordon, Yuke Zhu,\nAbhinav Gupta, and Ali Farhadi. AI2-THOR: An Interactive\n3D Environment for Visual AI. CoRR , 1712.05474, 2017. 2\n[27] Kurt Konolige. A gradient method for realtime robot control.\nInIROS , 2000. 2\n[28] Mathieu Labb ´e and Fran c ¸ois Michaud. RTAB-Map as an\nopen-source lidar and visual simultaneous localization and\nmapping library for large-scale and long-term online opera-\ntion. Journal of Field Robotics , 36(2):416–446, 2019. 2\n[29] Chen Liu, Kiran Lekkala, and Laurent Itti. World model\nbased sim2real transfer for visual navigation. In NeurIPS\nRobot Learning Workshop , 2023. 2\n17845\n[30] Iker Lluvia, Elena Lazkano, and Ander Ansuategi. Active\nMapping and Robot Exploration: A Survey. Sensors , 21(7):\n2445, 2021. 1\n[31] Steve Macenski, Francisco Mart ´ın, Ruffin White, and\nJonatan Gin ´es Clavero. The marathon 2: A navigation system.\nInIROS , 2020. 2\n[32] Eitan Marder-Eppstein, Eric Berger, Tully Foote, Brian\nGerkey, and Kurt Konolige. The office marathon: Robust\nnavigation in an indoor office environment. In ICRA , 2010. 2\n[33] Pierre Marza, Laetitia Matignon, Olivier Simonin, and Chris-\ntian Wolf. Multi-Object Navigation with dynamically learned\nneural implicit representations. In ICCV , 2023. 2\n[34] Piotr Mirowski, Razvan Pascanu, Fabio Viola, Hubert Soyer,\nAndy Ballard, Andrea Banino, Misha Denil, Ross Goroshin,\nLaurent Sifre, Koray Kavukcuoglu, Dharshan Kumaran, and\nRaia Hadsell. Learning to navigate in complex environments.\nInICLR , 2017. 1\n[35] Piotr Mirowski, Razvan Pascanu, Fabio Viola, Hubert Soyer,\nAndy Ballard, Andrea Banino, Misha Denil, Ross Goroshin,\nLaurent Sifre, Koray Kavukcuoglu, Dharshan Kumaran, and\nRaia Hadsell. Learning to navigate in complex environments.\nInICLR , 2017. 2\n[36] Katsuhiko Ogata. Modern Control Engineering . Prentice\nHall, 2010. 5\n[37] Emilio Parisotto and Ruslan Salakhutdinov. Neural map:\nStructured memory for deep reinforcement learning. In ICLR ,\n2018. 2\n[38] Ruslan Partsey, Erik Wijmans, Naoki Yokoyama, Oles Dobo-\nsevych, Dhruv Batra, and Oleksandr Maksymets. Is mapping\nnecessary for realistic pointgoal navigation? In CVPR , 2022.\n1\n[39] Xue Bin Peng, Marcin Andrychowicz, Wojciech Zaremba,\nand Pieter Abbeel. Sim-to-real transfer of robotic control with\ndynamics randomization. In ICRA , 2018. 2\n[40] Santhosh Kumar Ramakrishnan, Aaron Gokaslan, Erik Wi-\njmans, Oleksandr Maksymets, Alexander Clegg, John M\nTurner, Eric Undersander, Wojciech Galuba, Andrew West-\nbury, Angel X Chang, Manolis Savva, Yili Zhao, and Dhruv\nBatra. Habitat-matterport 3D dataset (HM3D): 1000 large-\nscale 3d environments for embodied AI. In NeurIPS Datasets\nand Benchmarks Track , 2021. 6\n[41] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez\nColmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai\nGimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springen-\nberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley Edwards,\nNicolas Heess, Yutian Chen, Raia Hadsell, Oriol Vinyals,\nMahyar Bordbar, and Nando de Freitas. A Generalist Agent.\narXiv:2205.06175 , 2022. arXiv: 2205.06175. 2\n[42] Christoph R ¨osmann, Frank Hoffmann, and Torsten Bertram.\nTimed-elastic-bands for time-optimal point-to-point nonlinear\nmodel predictive control. In European Control Conference\n(ECC) , 2015. 2\n[43] Assem Sadek, Guillaume Bono, Boris Chidlovskii, and Chris-\ntian Wolf. An in-depth experimental study of sensor usage\nand visual reasoning of robots navigating in real environments.\nInICRA , 2022. 1, 2, 7\n[44] Manolis Savva, Abhishek Kadian, Oleksandr Maksymets,\nYili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, JiaLiu, Vladlen Koltun, Jitendra Malik, Devi Parikh, and Dhruv\nBatra. Habitat: A platform for embodied ai research. In ICCV ,\n2019. 1, 2, 4, 5, 6\n[45] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Rad-\nford, and Oleg Klimov. Proximal policy optimization algo-\nrithms. arXiv preprint , 2017. 6\n[46] James A Sethian. A fast marching level set method for mono-\ntonically advancing fronts. PNAS , 93(4):1591–1595, 1996.\n2\n[47] Dhruv Shah and Sergey Levine. ViKiNG: Vision-\nBased Kilometer-Scale Navigation with Geographic Hints.\narXiv:2202.11271 , 2022. 2\n[48] Dhruv Shah, Ajay Sridhar, Nitish Dashora, Kyle Stachowicz,\nKevin Black, Noriaki Hirose, and Sergey Levine. ViNT: A\nfoundation model for visual navigation. In CoRL , 2023. 2\n[49] Jie Tan, Tingnan Zhang, Erwin Coumans, Atil Iscen, Yunfei\nBai, Danijar Hafner, Steven Bohez, and Vincent Vanhoucke.\nSim-to-real: Learning agile locomotion for quadruped robots.\nInRSS, 2018. 2\n[50] Sebastian Thrun, Wolfram Burgard, and Dieter Fox. Proba-\nbilistic robotics, 2005. 1, 2, 3\n[51] Joanne Truong, Sonia Chernova, and Dhruv Batra. Bi-\ndirectional Domain Adaptation for Sim2Real Transfer of Em-\nbodied Navigation Agents. IEEE Robotics and Automation\nLetters , 6(2), 2021. 2\n[52] Joanne Truong, Max Rudolph, Naoki Yokoyama, Sonia Cher-\nnova, Dhruv Batra, and Akshara Rai. Rethinking sim2real:\nLower fidelity simulation leads to higher sim2real transfer in\nnavigation. In CoRL , pages 859–870, 2022. 2\n[53] Naoki Yokoyama, Sehoon Ha, and Dhruv Batra. Success\nweighted by completion time: A dynamics-aware evaluation\ncriteria for embodied navigation. In IROS , 2021. 2, 6, 7\n[54] Naoki Yokoyama, Qian Luo, Dhruv Batra, and Sehoon Ha.\nLearning robust agents for visual navigation in dynamic envi-\nronments: The winning entry of igibson challenge 2021. In\nIROS , pages 77–83, 2022. 2\n[55] Fangyi Zhang, J ¨urgen Leitner, Zongyuan Ge, Michael Mil-\nford, and Peter Corke. Adversarial discriminative sim-to-real\ntransfer of visuo-motor policies. Int. J. Robotics Res. , 38\n(10-11), 2019. 2\n[56] Xinge Zhu, Jiangmiao Pang, Ceyuan Yang, Jianping Shi, and\nDahua Lin. Adapting object detectors via selective cross-\ndomain alignment. In CVPR , 2019. 2\n17846'}, 'dist': 0.9286905527114868}
Result 23: {'text': 'data', 'metadata': {'created_at': 1736932570, 'modified_at': 1736932570, 'owner': 'sayandeep', 'path': 'RAG_CONTENT/conferences/cvpr_ocr/Choi_OmniLocalRF_Omnidirectional_Local_Radiance_Fields_from_Dynamic_Videos_CVPR_2024_paper.txt', 'size': 49446, 'seen_at': 1737191136, 'data': "OmniLocalRF: Omnidirectional Local Radiance Fields from Dynamic Videos\r\nDongyoung Choi Hyeonjoong Jang Min H. Kim\r\nKAIST\r\nAbstract\r\nOmnidirectional cameras are extensively used in various\r\napplications to provide a wide field of vision. However,\r\nthey face a challenge in synthesizing novel views due to\r\nthe inevitable presence of dynamic objects, including the\r\nphotographer, in their wide field of view. In this paper,\r\nwe introduce a new approach called Omnidirectional Lo-\r\ncal Radiance Fields (OmniLocalRF) that can render static-\r\nonly scene views, removing and inpainting dynamic ob-\r\njects simultaneously. Our approach combines the princi-\r\nples of local radiance fields with the bidirectional optimiza-\r\ntion of omnidirectional rays. Our input is an omnidirec-\r\ntional video, and we evaluate the mutual observations of\r\nthe entire angle between the previous and current frames.\r\nTo reduce ghosting artifacts of dynamic objects and inpaint\r\nocclusions, we devise a multi-resolution motion mask pre-\r\ndiction module. Unlike existing methods that primarily sep-\r\narate dynamic components through the temporal domain,\r\nour method uses multi-resolution neural feature planes for\r\nprecise segmentation, which is more suitable for long 360◦\r\nvideos. Our experiments validate that OmniLocalRF out-\r\nperforms existing methods in both qualitative and quanti-\r\ntative metrics, especially in scenarios with complex real-\r\nworld scenes. In particular, our approach eliminates the\r\nneed for manual interaction, such as drawing motion masks\r\nby hand and additional pose estimation, making it a highly\r\neffective and efficient solution.\r\n1. Introduction\r\nOmnidirectional cameras such as Ricoh Theta or Insta360\r\nallow capturing panoramic 360◦views in a single shot.\r\nVarious applications with omnidirectional images such as\r\nspherical depth estimation [53, 58, 59], novel view syn-\r\nthesis [2, 3, 5–7, 11, 18, 31, 35] and geometry reconstruc-\r\ntion [3, 18] aiming at large-scale static scenes have recently\r\nbeen explored. In particular, synthesizing 360◦novel views\r\ncan provide continuous views from unobserved camera an-\r\ngles while maintaining its details.\r\nHowever, recent novel view synthesis methods struggle\r\nto apply to omnidirectional input for the following reasons.\r\n(b) OmniLocalRF (ours) (a) Conventional neural rendering\r\nInput 360° video\r\nFigure 1. We introduce omnidirectional local radiance fields for\r\nphotorealistic view synthesis of static scenery from 360◦videos.\r\nOur method effectively removes dynamic objects (including the\r\nphotographer) without manual interaction. Also, it achieves high-\r\nresolution details in the inpainted regions by means of bidirec-\r\ntional observations of omnidirectional local radiance fields. Refer\r\nto the supplemental video for more results.\r\nWhen capturing omnidirectional videos to record static en-\r\nvironments, dynamic objects are prone to be captured as\r\nan extension of the field of view, and capturing a photog-\r\nrapher is inevitable unless employing a dedicated hardware\r\nor remote controller. When synthesizing novel views, these\r\ncaptured objects are represented as ghosting artifacts onto\r\nthe rendered results [37]. Despite these problems, existing\r\nmethods have achieved 360◦view synthesis, relying on con-\r\nstrained capturing conditions, where it minimizes the ad-\r\nvent of dynamic objects [3, 11] or requiring dedicated hard-\r\nware [5–7, 31, 35], which are not suitable for casual 360◦\r\nphotography.\r\nLow-rank decomposition through robust principal com-\r\nponent analysis [15, 16, 57] and existing optimization-\r\nbased methods [13, 14] effectively eliminate dynamic ob-\r\njects on the image domain. However, their applicabil-\r\nity is limited to scenarios involving multiple images cap-\r\ntured from the same viewpoints. Recent view synthesis\r\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\r\nExcept for this watermark, it is identical to the accepted version;\r\nthe final published version of the proceedings is available on IEEE Xplore.\r\n6871\r\nworks [32, 33, 46, 54] detach dynamic objects during static\r\nview synthesis by modeling dynamic objects along the tem-\r\nporal domain. However, they are inappropriate for long\r\n360◦videos because of the neural model’s capacities.\r\nIn this paper, we propose omnidirectional local radi-\r\nance fields (OmniLocalRF) that can render novel views\r\nof static scene environments from casual dynamic 360◦\r\nvideos. We formulate local radiance fields (LocalRF) [26]\r\ninto a novel bidirectional training scheme designed explic-\r\nitly for omnidirectional video input. We develop a mod-\r\nule that uses multi-resolution neural feature planes to pre-\r\ndict motion masks of every frame and segment frame-\r\ndependent components. Our LocalRF-based approach en-\r\nables us to synthesize views and estimate camera poses\r\nfrom long videos. We automatically remove and inpaint dy-\r\nnamic objects while performing large-scale omnidirectional\r\nview synthesis. See Figure 1.\r\nIt is worth noting that different from conventional per-\r\nspective cameras, omnidirectional cameras capture con-\r\ntinuous scene information across multiple frames. This\r\nunique characteristic allows us to design a novel opti-\r\nmization approach that involves the bidirectional evalua-\r\ntion of samples taken from distant frames through an om-\r\nnidirectional contraction scheme. This method enables\r\nus to effectively remove and inpaint broken dynamic ob-\r\njects through backward refinement, demonstrating omnidi-\r\nrectional reconstruction and decoupling of dynamic objects\r\nacross complex real-world scenes. In summary, our contri-\r\nbutions are:\r\n• A view synthesis method based on bidirectional opti-\r\nmization through distant frames while conserving lo-\r\ncality across radiance fields,\r\n• A new motion mask prediction module that accurately\r\nsegments dynamic objects, even in 360◦videos with-\r\nout requiring a pretrained model, and\r\n• A camera pose estimation technique based on local\r\nview synthesis of 360◦videos.\r\nOur code is freely available for research purposes1.\r\n2. Related work\r\nOmnidirectional view synthesis. OmniPhotos [3] and\r\nJang et al. [18] reconstruct geometry from 360◦videos us-\r\ning mesh representation. Several approaches [5–7, 31, 35]\r\nuse a spherical camera rig to render images at unobserved\r\ncamera positions. MatryODShka [1] proposes a multi-\r\nsphere image representation for omnidirectional view syn-\r\nthesis. However, these methods do not work for 360◦videos\r\ncasually taken with dynamic objects.\r\nRecent advancements for rendering 360◦images,\r\ne.g., Mip-NeRF360 [2] and EgoNeRF [11], use vol-\r\nume rendering to create omnidirectional images. Mip-\r\n1https://vclab.kaist.ac.kr/cvpr2024p1/NeRF360 extends the capabilities of Neural Radiance\r\nFields (NeRF) [27] to unbounded scenes, while EgoNeRF\r\nuses the Yin-Yang grid to obtain a balanced polar represen-\r\ntation of neural features. However, these methods limit use\r\nin scenarios with dynamic objects in the training dataset,\r\nas they are designed for egocentric scenes captured using a\r\nselfie stick or by excluding dynamic objects with a motion\r\nmask.\r\nRemoving dynamic objects. Novel view synthesis meth-\r\nods generally aim to reconstruct static objects through the\r\nmulti-view stereo [2, 27, 43, 47]. In existing works, separat-\r\ning dynamic and static objects by pretrained model [9, 17]\r\non input images and modeling them respectively enables\r\nrendering static geometry if multi-view of dynamic objects\r\nare sufficiently provided in a dataset [21, 25, 32, 33]. In\r\n360◦videos that do not provide enough multi-view cues\r\nof dynamic objects, modeling them across the 3D space is\r\nchallenging due to geometric inconsistency over time.\r\nTo reduce ghosting artifacts caused by dynamic objects\r\nin view synthesis, several models [36, 43] exclude dy-\r\nnamic objects using an external segmentation model [9, 10].\r\nHowever, these models fail to mask out objects that are\r\nnot labeled, such as shadows. Recent works, such as\r\nD2NeRF [54] and Neuraldiff [46], segment dynamic objects\r\nby reconstructing them in 3D space across the temporal\r\ndomain, but require an extensive parameterization, making\r\nthem less scalable for long 360◦videos. OmnimatteRF [24]\r\nestimates delicate motion masks from rough masks pro-\r\nvided by Mask R-CNN [17] with optical flow [45], but still\r\nneeds a pretrained segmentation model. RobustNeRF [37]\r\ndiscriminates between inliers and outliers based on photo-\r\nmetric error and down-weights the outliers to decrease the\r\neffect of dynamic objects during training. Nevertheless,\r\nusing a limited number of ray samples driven by down-\r\nweighting outliers slows down the training procedure, and\r\nthese models are not scalable for long 360◦videos.\r\nPose estimation. Conventional view synthesis relies on\r\nStructure from Motion (SfM) methods, like COLMAP [38]\r\nand OpenMVG [29], for camera poses, but these can be\r\nchallenging to compute for large-scale data, leading to poor\r\nview synthesis quality. Several NeRF variants [4, 19, 23,\r\n51] optimize radiance fields and poses jointly, but struggle\r\nto estimate camera poses for 360◦videos with dynamic ob-\r\njects [26]. RoDynRF [25] uses Mask R-CNN to separate\r\ndynamic and static components, but it is not scalable and\r\nrequires a pretrained segmentation model. LocalRF [26]\r\nsucceeds in long trajectory pose calibration but is suscep-\r\ntible to artifacts caused by dynamic objects.\r\n3. Omnidirectional Local Radiance Fields\r\nOverview. Our goal is to generate photorealistic static\r\nscenes from unobserved viewpoints using long 360◦videos,\r\n6872\r\nincluding dynamic objects as input. We optimize multiple\r\nradiance fields with continuous local frames while sliding\r\nframe windows and produce high-quality view synthesis.\r\nHowever, training blocks solely with local frames can result\r\nin ghosting artifacts from dynamic objects. To address this,\r\nwe use an omnidirectional local radiance fields approach\r\nand a motion mask module to separate dynamic and static\r\nobjects. We also propose a novel bidirectional optimiza-\r\ntion method to enhance the stability of static structural rep-\r\nresentations and eliminate residual artifacts. Our method\r\nproduces superior rendering results than existing methods.\r\n3.1. Preliminaries\r\nWe render the color ˆC(r)by volume rendering samples xi\r\nwhose density and color are (σi,ci)along a ray rand train\r\nthe radiance fields RFΘto predict (σi,ci)from the L1 pho-\r\ntometric error between ˆC(r)and input image C(r)as it is\r\nmore robust against outliers than MSE:\r\nLrgb=\r\r\rˆC(r)−C(r)\r\r\r\r\n1. (1)\r\nTo extend NeRF to cover a large scale, we also use the con-\r\ntraction equation [2] over sample points:\r\ncontract (x) =(x ∥x∥ ≤1\x10\r\n2−1\r\n∥x∥\x11\x10\r\nx\r\n∥x∥\x11\r\n∥x∥>1, (2)\r\nUsing the contraction function in radiance fields, which\r\nmaps world coordinates onto a contracted space, aids in\r\nlarge-scale view synthesis by focusing nearby regions while\r\nrepresenting distant components [26, 44]. However, they\r\nface challenges in long camera trajectories due to static\r\nmodel allocation.\r\nOur approach uses multiple TensoRFs [8] to per-\r\nform view synthesis and camera registration from\r\nvideos, following LocalRF [26]. We allocate a NeRF\r\nblock RFΘm, where m∈ {1,···, M}, and insert in-\r\nput frames Ckwith the corresponding camera poses\r\n[R|t]k,k∈ {1,···, K}into a temporal window Wm=\x08\r\n(Cw(m,1),[R|t]w(m,1)),···,(Cw(m,N),[R|t]w(m,N))\t\r\n,\r\nwhere w(m, n)denotes the frame number of m-th win-\r\ndows’ n-th frame. If the distance between the camera\r\n[R|t]w(m,N)and the block center becomes too large, we\r\nstop inserting frames and optimize the block RFΘmwith\r\nthe camera poses [R|t]w(m,n)where n∈ {1,···, N}\r\nusingWm. After optimizing a RFΘmblock, we create\r\na new block RFΘm+1with its window Wm+1and insert\r\nframes until the end of the videos.\r\n3.2. Bidirectional Optimization by Distant Frames\r\nPrevious reconstruction approaches [2, 11, 37, 48] prede-\r\nfine the reconstruction range before optimization and train\r\n(a) Perspective camera (b) Omnidirectional camera()pPr ()pPr\r\ncΘRFpΘRF cΘRF\r\npΘRFFigure 2. In the perspective video of marching forward, optimized\r\nradiance blocks RFΘpmay not be visible in the frame used to\r\ntrain current radiance fields RFΘc. However, in omnidirectional\r\nvideo, every uncontracted space of the optimized blocks can be\r\nseen, enabling effective bidirectional optimization. The boundary\r\nindicates the radiance fields’ focusing region uncontracted.\r\na single NeRF-based module with registered poses. Lo-\r\ncalRF [26] progressively allocates NeRF modules and per-\r\nforms large-scale reconstruction in a local manner without\r\nrequiring camera priors. However, the vanilla LocalRF has\r\nlimitations in leveraging global information and is suscep-\r\ntible to slowly moving objects that appear stationary from\r\na local perspective. To overcome these issues, we propose\r\na novel solution that leverages the omnidirectional nature\r\nof 360◦view synthesis while taking advantage of locality.\r\nThis improves overall reconstruction quality.\r\nAs shown in Figure 2, in perspective videos of marching\r\nforward, distant frames contain only a small amount of in-\r\nformation needed to refine poses and radiance fields. This\r\nis inefficient and can lead to instability. However, in 360◦\r\nvideos, distant frames contain a substantial amount of data\r\nto refine radiance fields. As a result, we propose a bidirec-\r\ntional optimization by distant frames as part of progressive\r\noptimization, which can globally refine radiance fields and\r\nposes for omnidirectional view synthesis.\r\nForward step. As we progressively optimize multiple ra-\r\ndiance fields RFΘm, there are RFΘcthat OmniLocalRF\r\ncurrently optimizes and a group of RFΘp, p∈ {1,···, c−\r\n1}, which are already converged. In LocalRF, RFΘcis\r\ntrained only with its temporal window Wcusing the pho-\r\ntometric loss defined as:\r\nLfor\r\nrgb, s=X\r\nrsrc∈R\r\r\rˆCc(rsrc)−C(rsrc)\r\r\r\r\n1, (3)\r\nwhere rsrcrepresents a ray from the current window Wc,\r\nlimiting the use of global frames. Here we additionally re-\r\nfineRFΘcusing prior frames outside Wcand refer to it\r\nas a forward step since it proceeds in the same direction as\r\nwindow sliding.\r\nAs shown in Figure 3(a), to concentrate on the areas\r\nwhere RFΘcoccupies, we render a depth value through\r\nRFΘcat the pixel psrc, the origin of rsrc, onw(c, i)-th\r\nframe, w(c, i)∈ W c, and obtain projected pixels pdstto\r\nthew(p, j)-th frame which is a randomly selected member\r\ninWpbut not included in Wcby\r\npdst= Π\x10\r\n[R|t]w(c,i)→w(p,j)Π−1\x10\r\npsrc,ˆDc(rsrc)\x11\x11\r\n.(4)\r\n6873\r\ncΘRFpΘRF(,)wpjsrc()cPr\r\n(,)wci\r\nsrcrdstrdst()Crdst()CrBilinear\r\ninterpolationσ\r\ndstˆ()cCr\r\nray distance\r\n(a) Forward stepcΘRFpΘRF(,)wpj\r\n(,)wci\r\nsrc'rsrc(' )pPr\r\ndst'r\r\n(b) Backward stepσ\r\ndstˆ()pCr\r\nray distance\r\ndst(' )Crdst(' )CrBilinear\r\ninterpolation\r\nRender by \r\npΘRF\r\nRender by \r\ncΘRF\r\nProjectionFigure 3. Our bidirectional optimization for omnidirectional videos. (a) In the forward step, we project the point Pc(rsrc)rendered by\r\nRFΘcto the destination frame w(p, j), used to train the previous radiance block RFΘp. We then render the color and depth through\r\nRFΘcandRFΘp, respectively, and use the L1 photometric error between the fully rendered color ˆCc(rdst)and the bilinearly interpolated\r\ninput image ¯C(rdst)(Eq. (6)) to update RFΘcand a mask module. (b) In the backward step, we switch the source and destination frames\r\nand refine RFΘpthrough the valid rays from static areas that meet RP.\r\nHere ˆDcis a rendered depth value using RFΘc, and\r\n[R|t]w(c,i)→w(p,j)is the relative camera matrix from\r\nw(c, i)-th to w(p, j)-th frame. Π() denotes the equirect-\r\nangular projection operator, and Π−1()backprojects a pixel\r\non an equirectangular image to the world space.\r\nWe then render a depth value and a color value along rdst\r\nwhich casts from pdstthrough RFΘpandRFΘc, respec-\r\ntively. In order to exclusively use the reliable samples, we\r\nchoose valid rays which satisfy\r\nRC={r|(1− T)ˆDp(r)≤ˆDc(r)≤(1 +T)ˆDp(r),ˆDc(r)≤1},\r\n(5)\r\nwhere ˆDpis a rendered depth by RFΘp, andTis the valid\r\nmargin of photometric refinement, constant at 0.05. The re-\r\ngions satisfying Eq. (5) are co-visible from two RF blocks.\r\nIn cases where static geometry exists, causing occlusion, the\r\nrendering depths between the two RF blocks differ signif-\r\nicantly, preventing the execution of bidirectional optimiza-\r\ntion. We update RFΘcand a mask module from the con-\r\nventional photometric loss in Eq. (3) with\r\nLfor\r\nrgb, d=P\r\nrdst∈R C\r\r\r\x10\r\n1−ˆM(rdst)\x11\x10\r\nˆCc(rdst)−¯C(rdst)\x11\r\r\r\r\n1,(6)\r\nwhere ˆCcdenotes a rendered color by RFΘcwithin RC.\r\nWe bilinearly interpolate input color based on the projected\r\npixel’s coordinates to estimate the color ¯C.ˆM(r)is the\r\nestimated motion mask of rusing the global mask module,\r\ndetailed in Section 3.3. RFΘcadditionally uses the static\r\nregions of frames beyond the temporal window to maintain\r\nlocality and incorporate richer view information.\r\nBackward step. We illustrate the backward process in\r\nFigure 3(b). We cast a ray from r′\r\nsrconw(p, j)-th frames,\r\nwhich is used as the destination frame in the forward step,\r\nand render a depth value with a color value through RFΘp.\r\nWe then supervise the rendered color by the color of the\r\ninput frame as:\r\nLback\r\nrgb, s=P\r\nr′src∈R\r\r\r\x10\r\n1−ˆM(r′src)\x11\x10\r\nˆCp(r′src)−C(r′src)\x11\r\r\r\r\n1,(7)\r\nwhere ˆCpindicates a rendered color by RFΘp. Unless we\r\n(a) Test view backward (b) No \r\nstep\r\n(c) Backward step\r\nback\r\nrgb, s w/o \uf04c\r\n(d) Complete\r\noptimizationFigure 4. Ablation study on the impact of the backward step. (b)\r\nSolely employing the forward step results in a blurred image. (c)\r\nOmitting the utilization of Eq. (7) leads to overfitting on distant\r\nframes. (d) Our bidirectional optimization shows great quality in\r\nrepresenting details.\r\nuse Eq. (7), RFΘpoverfits to render distant frames while\r\ndistorting adjacent views as shown in Figure 4(c).\r\nWe get the projected pixel p′\r\ndstonw(c, i)-th frame as\r\nfollows:\r\np′\r\ndst= Π\x10\r\n[R|t]w(p,j)→w(c,i)Π−1\x10\r\np′\r\nsrc,ˆDp(r′\r\nsrc)\x11\x11\r\n,(8)\r\nwhere the source and destination frame are reversed in\r\nEq. (4). We render a color with a depth through RFΘp\r\nand also render a depth value using RFΘcatr′\r\ndst. The pho-\r\ntometric loss for backward refinement supervises the color\r\nrendered by RFΘp:\r\nLback\r\nrgb, d=P\r\nr′\r\ndst∈R P\r\r\r\x10\r\n1−ˆM(r′\r\ndst)\x11\x10\r\nˆCp(r′\r\ndst)−¯C(r′\r\ndst)\x11\r\r\r\r\n1.(9)\r\nRPdenotes the valid ray bundles where the terms ˆDc(r)\r\nandˆDp(r)are reversed in Eq. (5), and ¯C(r′\r\ndst)is a bilinear\r\ninterpolation of C(r′\r\ndst)based on p′\r\ndst, a pixel origin of r′\r\ndst.\r\nDuring the forward and backward steps, we use rays\r\nfrom frames close to the target RF block (source frame) and\r\nfrom frames far away from the block (destination frame).\r\nIn this manner, we utilize four photometric loss functions,\r\nEqs. (3), (6), (7), and (9). Then, Eq. (3) is used to train\r\n6874\r\ns(() , () )csrrray distanceV olumerenderingrgbLmaskrgbL\r\nstˆ()Crˆ()Mrdyˆ()CrrDFQˆ()CrDynamiccompositing1lk-Z1kZ0kZFeature plane interpolationConcatenationFeature decoding\r\n-th framekmaskregLShallow MLP(, )kuvz(,)uvFigure 5. For motion mask prediction, we cast a ray rfrom the k-th frame and render the static structure ˆCst(r)through volume rendering\r\nusing radiance fields RFΘ. We extract multi-resolution features of normalized (u, v)by traversing feature plane set Zkand concatenate\r\nthem into a single code zk\r\n(u,v). We estimate dynamic color ˆCdy(r)and motion mask ˆM(r), and render the final results ˆC(r)through\r\ndynamic compositing (Eq. (10)). We jointly update the mask module (Zt, FΘD)with radiance fields RFΘusing L1 photometric loss. We\r\nsupervise ˆCdy(r)by˜C(r)for unique factorization (Eq. (11)) and regularize the alpha of the mask (Eq. (12)).\r\nRFΘcthrough both dynamic and static regions of the close\r\nframe. In this process, geometrically inconsistent areas are\r\nmapped to the mask module as dynamic objects. Eq. (6) re-\r\nfinesRFΘcby far frame that have been used to train RFΘp.\r\nAs we already obtained the motion mask, we use only in-\r\nformation from static regions to minimize the intervention\r\nof dynamic objects while refining. We also apply this ap-\r\nproach in the backward step, exclusively updating static re-\r\ngions during additional refinement of RFΘp(Eqs. (7), (9)).\r\nThrough this way, the overlapped regions in RFΘcand\r\nRFΘpare refined concurrently using distant frames.\r\n3.3. Motion Mask Prediction\r\nWe use a set of neural feature planes for each frame to es-\r\ntimate a motion mask. To do so, we decode a feature code\r\nfrom a multi-resolution feature plane. Our mask module is\r\ncompatible with a pixel-wise ray marching NeRF setup and\r\ncan be optimized jointly while rendering static objects. Fig-\r\nure 5 provides an overview of our motion mask prediction.\r\nMask module architecture. We leverage a Bayesian\r\nlearning framework for segmenting dynamic components\r\ninspired by DynIBaR [22] that combines IBRNet [49]\r\nwith 2D CNN. We create a low-resolution equirectangu-\r\nlar feature plane set Zkfor each frame to handle del-\r\nicate frame-dependent components. The feature plane\r\nset comprises multi-resolution planes, denoted by Zk=\x08\r\nZ1\r\nk,Z2\r\nk,···,ZL\r\nk\t\r\n, where the height of each plane is hl\r\nk=\r\nh0\r\nk/2l−1, with l∈ {1,···, L}. This approach follows pre-\r\nvious works [12, 30, 42, 47] that have shown better spatial\r\ncontext-aware inference under a multi-resolution manner.\r\nWe use 4 feature channels, h0\r\nk= 128 , and L= 4 for all\r\nour experiments.\r\nTo render an omnidirectional image, a camera ray r\r\nis generated by multiplying the camera matrix with an\r\nequirectangular backprojected ray from pixel p(u, v)on the\r\nk-th frame. Before casting a ray from p, we interpolate\r\nmulti-resolution features at normalized (u, v)by traversingZkand concatenate Llevels of features into a single code.\r\nWe use a global, shallow, multi-layer perceptron (MLP)\r\nFΘDto decode the feature code and obtain the color of dy-\r\nnamic objects ˆCdy(r)with the alpha value of the motion\r\nmask ˆM(r)at the ray r.\r\nMotion mask optimization. We compute the final color\r\nˆCm(r)by compositing dynamic results ˆCdy(r)with static\r\nresults ˆCst\r\nm(r), rendered by RFΘm:\r\nˆCm(r) =ˆM(r)ˆCdy(r) + (1 −ˆM(r))ˆCst\r\nm(r). (10)\r\nWe update the mask module, consisting of the feature plane\r\nset and MLP, with radiance fields by propagating the L1\r\nphotometric loss of rendered colors (Section 3.2).\r\nThe mask blended dynamic color in Eq. (10) has a vari-\r\nety of combinations of a mask and a dynamic color. When\r\ntransient components have an intermediate alpha value due\r\nto the ambiguity in factorization, radiance fields try to com-\r\npensate residuals and leave floating artifacts. Therefore, we\r\nsupervise a dynamic color by Gaussian noise added input\r\ncolor ˜C(r):\r\nLmask\r\nrgb=X\r\nr∈R\r\r\rˆCdy(r)−˜C(r)\r\r\r\r\n1, (11)\r\nwhich ensures a unique factorization while preventing our\r\nmodel from relying on the mask module to express fine de-\r\ntails. We regularize the motion mask by the total variation\r\n(TV) loss and the binary loss for forcing the mask converged\r\ninto a binary value while smoothing it as:\r\nLmask\r\nreg=Lmask\r\nTV+Lmask\r\nbin. (12)\r\nRefer to the supplementary material for details on the mask\r\nregularizers.\r\n3.4. Progressive Optimization\r\nWe optimize RFΘblocks, camera poses, feature plane sets,\r\nand mask MLP by sliding a window over the input video.\r\n6875\r\nWe insert frames into the window and optimize poses us-\r\ningRFΘc(Eq. (3)). If the poses fall outside the contrac-\r\ntion range of RFΘc, currently targeted radiance field, we\r\nmove to the refining step. In the refining step, we perform\r\nbidirectional optimization by simultaneously updating the\r\nRFΘcand the randomly selected RFΘpthat has been pre-\r\nviously optimized. We use LocalRF’s optical flow loss and\r\nnormalized monocular depth supervision for robust pose es-\r\ntimation. To render novel views, we search for the nearest\r\nframe from the given viewpoints. If a frame is used to train\r\ntwo adjacent radiance blocks, we blend the results based on\r\ntheir position among overlapped frames.\r\n4. Experimental Results\r\nOur method takes 12 hours for 125 frames on a machine\r\nequipped with a single NVIDIA A6000 GPU and an Intel\r\nXeon Silver 4214R 2.40 GHz CPU with 256 GB RAM. Re-\r\nfer to the supplemental material for further implementation\r\ndetails. To evaluate OmniLocalRF in large-scale view syn-\r\nthesis from 360◦videos, we compare our approach against\r\nMip-NeRF360 [2] and EgoNeRF [11], known for strong\r\nperformance in omnidirectional view synthesis. Addition-\r\nally, we also compare our approach with D2NeRF [54] and\r\nRobustNeRF [37], both of which aim to reconstruct static\r\nstructures from dynamic videos. Since these methods re-\r\nquire precomputed camera pose, we utilize the pose esti-\r\nmated by OpenVSLAM [40] as camera priors. We also\r\ncompare ours with LocalRF [26] that self-calibrate poses\r\nduring view synthesis. For a comprehensive comparison,\r\nwe evaluate both LocalRF and our method under two condi-\r\ntions: one with camera priors provided for pose refinement\r\nand the other without camera priors, starting from scratch\r\nand estimating the poses during view synthesis.\r\n4.1. Dataset and Metrics\r\nWe capture and provide a new dataset from six outdoor\r\nscenes captured with an Insta360 camera, consisting of\r\n5760×2880 resolution 30 fps 360◦videos each. These\r\nare casual videos designed to capture backgrounds that in-\r\nclude a photographer and dynamic objects like pedestrians.\r\nConsidering reasonable training time and memory size, we\r\nuse half-resolution input images. However, in the case of\r\nD2NeRF, we have to use a quarter of the spatial resolution\r\nfor rendering due to GPU memory constraints of 48 GB. We\r\nuse a total of 125 images, taking every fourth frame from\r\nthe first 500 frames. Test frames are selected for every tenth\r\nframe among 125 images, including the first image, result-\r\ning in 112 training images with 13 test images. For all test\r\nimages in every scene, we manually create motion masks\r\nfor validation. We then compare the rendered results with\r\nthe dynamic objects masked ground truth and report PSNR,\r\nSSIM [50], and LPIPS [55]. We also measure weighted-\r\nto-spherically uniform PSNR and SSIM [41] (PSNRWSand\r\nGround truth\r\n Mip- NeRF360\r\n OursFigure 6. Masked results for evaluation in the real dataset. We\r\nmanually create motion masks for the test views and compute\r\nthe metrics after inpainting masked regions with gray. While our\r\nmethod masks the dynamic areas robustly, conventional neural\r\nrendering methods still exhibit residual artifacts due to temporal\r\nand spatial inconsistency.\r\nSSIMWS), taking into account the distortion near the pole in\r\nequirectangular images.\r\nWe generate three synthetic 360◦videos, each contain-\r\ning dynamic objects floating within the scene for evaluating\r\npose estimation accuracy by absolute trajectory error (ATE)\r\nand relative pose error (RPE) between ground truth poses\r\nas standard visual odometry metrics [20, 39, 56]. The syn-\r\nthetic videos have a resolution of 2880 ×1440, consisting of\r\n125 images each. Out of these, 13 images are allocated for\r\ntesting, following the procedure used with the real dataset.\r\nAdditionally, we provide view synthesis results of existing\r\nmethods and our method for comprehensive comparisons.\r\n4.2. Quantitative Evaluation\r\nDynamic objects in test images are excluded using manu-\r\nally created masks when computing metrics. As both Lo-\r\ncalRF and our method can estimate camera poses during\r\nview synthesis, we present the metrics for models trained\r\nwith and without preprocessed camera poses by OpenVS-\r\nLAM [40]. According to Tables 1 and 2, scalable models\r\nlike Mip-NeRF360, EgoNeRF, and LocalRF showed rela-\r\ntively high performance. D2NeRF and RobustNeRF, which\r\ntarget reconstructing static structures from dynamic videos,\r\nachieve lower scores due to a limited model capacity. To en-\r\nsure fair comparisons, we exclude the transient data of test\r\nviews. However, dynamic objects frequently appear outside\r\nthe masks as artifacts, leading to a decrease in the metrics,\r\nas illustrated in Figure 6. As a result, our method outper-\r\nforms existing methods thanks to its effective dynamic ob-\r\nject removal and locality even without given camera poses.\r\n4.3. Qualitative Evaluation\r\nIn Figures 7 and 8, we conduct a qualitative comparison of\r\nour OmniLocalRF with the baseline methods. Our method\r\neffectively mitigates ghosting artifacts while preserving lo-\r\ncality, allowing us to represent details in large-scale real-\r\n6876\r\nGT\r\n Test view\r\nMSE Rendered\r\n0 0.05\r\nOurs\r\n Mip- NeRF360\r\n EgoNeRF\r\n RobustNeRF\r\n Mip- NeRF360,\r\nstatic only\r\nFigure 7. An example of qualitative comparisons on the Lone Monk scene within the synthetic dataset. We additionally address results\r\nof Mip-NeRF360 from static-only videos ( static only ) that do not include dynamic objects at the first column. Our results demonstrate the\r\neffective removal of dynamic objects while preserving details in the expansive scene.\r\nOur rendered views Ours Mip-NeRF360 EgoNeRF LocalRF RobustNeRF D2NeRF Test view\r\nRocket\r\n Yongsil\r\n Library\r\n Red building\r\nFigure 8. Qualitative comparisons on the real dataset. Our method can render locally unobserved areas with fine details throughout the\r\nbidirectional optimization with a self-supervised mask module. D2NeRF sometimes encounters challenges in distinguishing between static\r\nbackgrounds and dynamic objects, which results in the omission of certain regions in the Library scene. Refer to the supplemental video.\r\nTable 1. Quantitative comparisons of the synthetic dataset. The\r\naverages of the metrics are measured across three synthetic scenes.\r\nRefer to Figure 7 for qualitative comparisons.\r\nPSNR↑ PSNRWS↑ SSIM↑ SSIMWS↑ LPIPS↓\r\nMip-NeRF360 [2], static only 29.48 29.85 0.8625 0.8628 0.2699\r\nOurs wo/ pose, static only 30.29 30.24 0.8679 0.8681 0.2561\r\nMip-NeRF360 [2] 25.59 25.32 0.8455 0.8464 0.2973\r\nEgoNeRF [11] 23.99 23.67 0.8044 0.7951 0.3949\r\nLocalRF [26] w/ pose 25.50 25.31 0.8454 0.8427 0.2897\r\nD2NeRF [54] 19.91 19.43 0.6212 0.5929 0.6298\r\nRobustNeRF [37] 20.59 19.79 0.7326 0.7096 0.4734\r\nOurs w/ pose 29.76 29.68 0.8633 0.8628 0.2624\r\nLocalRF [26] wo/ pose 25.22 25.02 0.8389 0.8354 0.2949\r\nOurs wo/ pose 29.93 29.85 0.8648 0.8648 0.2610\r\nTable 2. Quantitative comparisons of the real dataset. We report\r\nthe averages of the metrics measured across six real scenes. Refer\r\nto Figure 8 for qualitative comparisons.\r\nPSNR↑ PSNRWS↑ SSIM↑ SSIMWS↑ LPIPS↓\r\nMip-NeRF360 [2] 26.88 26.44 0.8094 0.7977 0.3585\r\nEgoNeRF [11] 25.95 25.38 0.7609 0.7424 0.4383\r\nLocalRF [26] w/ pose 26.56 26.22 0.8041 0.7966 0.3471\r\nD2NeRF [54] 20.95 20.34 0.6105 0.5829 0.5100\r\nRobustNeRF [37] 20.78 19.56 0.7093 0.6679 0.4864\r\nOurs w/ pose 27.72 27.09 0.8171 0.8085 0.3299\r\nLocalRF [26] wo/ pose 26.56 26.23 0.8034 0.7984 0.3410\r\nOurs wo/ pose 27.73 27.13 0.8165 0.8088 0.3297\r\nworld data, even in locally occluded regions. Methods like\r\nEgoNeRF, Mip-NeRF360, and LocalRF, which lack dy-namic handling, face challenges when reconstructing static\r\nscenes, relying solely on geometric consistency across in-\r\nput data. Even though their masked metrics indicate high\r\nperformance, these methods suffer from ghosting artifacts,\r\nmaking them less suitable for practical applications that de-\r\nmand high-quality static view synthesis.\r\nSince D2NeRF parameterizes radiance along spatial and\r\ntemporal domains with a single NeRF module, it often\r\nfails to separate dynamic components. It renders blurry\r\nresults because of the large spatial and time complex-\r\nity of input videos. RobustNeRF employs the iteratively\r\nreweighted least squares (IRLS) approach for patch-wise\r\noutlier down-weighting, which decouples transient artifacts\r\nfrom geometrically consistent structures through adaptive\r\nloss reweighting. However, in the omnidirectional videos,\r\nthe overall geometry reconstruction quality is compromised\r\ndue to the large scene scale, making it challenging to distin-\r\nguish static from dynamic objects based on the photometric\r\nerror over patches. For a fair comparison, we additionally\r\ntrain RobustNeRF for twice the number of iterations com-\r\n6877\r\nInput video\r\nFigure 9. An example of pose comparisons on the Pavillion scene\r\nincluding dynamic objects. Our method shows robustness in esti-\r\nmating camera pose in the presence of dynamic objects.\r\nTable 3. Comparisons of pose accuracy on the synthetic dataset.\r\nWe average the results from three scenes.\r\nRPEr↓ RPEt↓ ATE↓\r\nOpenMVG [29] 0.10761 0.01799 0.00218\r\nLocalRF [26] 0.10404 0.00096 0.00376\r\nOurs 0.10398 0.00081 0.00165\r\nOpenMVG [29], static only 0.10706 0.01796 0.00187\r\nLocalRF [26], static only 0.10398 0.00071 0.00208\r\nOurs, static only 0.10399 0.00074 0.00165\r\npared to Mip-NeRF360, taking into account the slowdown\r\ncaused by the IRLS approach, but still fail to represent fine\r\ndetails effectively.\r\n4.4. Pose Estimation Comparison\r\nWe compare our method with OpenMVG [29], an SfM-\r\nbased pose estimator, and LocalRF in Figure 9. We re-\r\nport the relative rotation error (RPE r), relative translation\r\nerror (RPE t), and ATE. We also estimate the camera trajec-\r\ntories from the ground truth videos to eliminate the influ-\r\nence of dynamic objects. We align the estimated pose with\r\nthe ground truth, addressing scale factor and rotation dis-\r\ncrepancies, before computing ATE. As shown in Table 3,\r\nthe pose optimized through our approach is robust in the\r\npresence of dynamic objects, showing similar results in the\r\nstatic-only videos.\r\n4.5. Ablation Study\r\nWe conduct an ablation study for our proposed mask mod-\r\nule and bidirectional optimization: (a) Removing Eq. (6)\r\ndegrades overall quality as the local blocks use fewer in-\r\nput images for training. (b) The omission of source rays’\r\nphotometric supervision during the backward step (Eq. (7))\r\nsignificantly degrades the model as it tends to converge on\r\nreconstructing distant regions as described in Figure 4(c).\r\n(c) We observe that incorporating distant frames through the\r\nbackward step enables the model to capture and refine de-\r\ntailed information. (d) Geometrical inconsistencies in the\r\ntemporal domain of dynamic objects often lead to the intro-\r\nduction of larger artifacts compared to the regions masked\r\nout during test views, as shown in Figure 6. Consequently,\r\nthe absence of the mask module adversely affects both the\r\nqualitative and quantitative aspects of the results. (e) The\r\nexclusion of Eq. (3) during mask optimization results in\r\nintermediate alpha values within the motion mask due to\r\nthe ambiguity in factorization and leaves floating artifacts,Table 4. Ablation study results of our model. Reported metrics\r\nare averaged over the six scenes on our real dataset (Section 4.5).\r\nPSNR↑SSIM↑LPIPS ↓\r\n(a) No forward step 27.64 0.8127 0.3356\r\n(b) Backward step w/o Lback\r\nrgb, s 26.23 0.7800 0.3866\r\n(c) No backward step 27.63 0.8086 0.3447\r\n(d) No mask module 26.08 0.7876 0.3630\r\n(e) No mask photometric supervision 25.27 0.7894 0.3593\r\nComplete model 27.73 0.8165 0.3297\r\nwhich yields lower metrics than the entire model. Table 4\r\nquantitatively compares results.\r\n5. Discussion and Conclusions\r\nWe have presented OmniLocalRF, a novel method for om-\r\nnidirectional view synthesis in dynamic 360◦videos. Our\r\napproach integrates LocalRF with a mask module and bidi-\r\nrectionally refines distant NeRF blocks to remove dynamic\r\nartifacts and fill in occluded regions, resulting in accurate\r\nstatic structure reconstruction while preserving fine details\r\nwithin large scenes. Our method also accurately estimates\r\ncamera trajectories during view synthesis, making it suit-\r\nable for various applications such as street viewers and aug-\r\nmented reality environments.\r\nOur model can synthesize static structures from 360◦\r\nvideos without motion masks and camera priors. How-\r\never, it faces the usual challenges associated with neural\r\nrendering-based view synthesis. For instance, it is unable to\r\ninpaint regions that are completely occluded in the videos\r\nbecause NeRF-based models are trained using photomet-\r\nric loss between input images. To overcome this limita-\r\ntion, incorporating perceptual loss [28] or generative mod-\r\nels [34, 52], such as stable diffusion, can be helpful.\r\nWe use linear interpolation in equirectangular space,\r\nwhich has grids with the same size of zenith and azimuth an-\r\ngles, to predict motion masks. Operating in this space can\r\nmitigate data redundancy compared to utilizing an undis-\r\ntorted cube map. However, this space can lead to inefficient\r\noversampling near polar regions in mask predictions. To ad-\r\ndress this issue, we could use uniformly sampled spherical\r\ngrids in future works.\r\nEven though our model globally refines the local blocks\r\nbased on photometric error, we do not deal with the global\r\nbundle adjustment and loop closure for pose estimation,\r\nwhich are used in completed SLAM systems. Adding these\r\ncomponents to our approach would enable more robust and\r\naccurate pose estimation. While our model is capable of\r\ngenerating static structures from 360◦videos, it faces sev-\r\neral challenges that require further refinement.\r\nAcknowledgements\r\nMin H. Kim acknowledges the MSIT/IITP of Korea (RS-\r\n2022-00155620, 2022-0-00058, and 2017-0-00072), LIG,\r\nand Samsung Electronics.\r\n6878\r\nReferences\r\n[1] Benjamin Attal, Selena Ling, Aaron Gokaslan, Christian\r\nRichardt, and James Tompkin. MatryODShka: Real-Time\r\n6DoF Video View Synthesis using Multi-Sphere Images. In\r\nECCV , 2020. 2\r\n[2] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P\r\nSrinivasan, and Peter Hedman. Mip-NeRF 360: Unbounded\r\nAnti-Aliased Neural Radiance Fields. In CVPR , pages 5470–\r\n5479, 2022. 1, 2, 3, 6, 7\r\n[3] Tobias Bertel, Mingze Yuan, Reuben Lindroos, and Christian\r\nRichardt. OmniPhotos: Casual 360° VR Photography. TOG ,\r\n39(6):266:1–12, 2020. 1, 2\r\n[4] Wenjing Bian, Zirui Wang, Kejie Li, Jia-Wang Bian, and\r\nVictor Adrian Prisacariu. NoPe-NeRF: Optimising Neural\r\nRadiance Field with No Pose Prior. In CVPR , pages 4160–\r\n4169, 2023. 2\r\n[5] Michael Broxton, Jay Busch, Jason Dourgarian, Matthew\r\nDuVall, Daniel Erickson, Dan Evangelakos, John Flynn,\r\nRyan Overbeck, Matt Whalen, and Paul Debevec. A Low\r\nCost Multi-Camera Array for Panoramic Light Field Video\r\nCapture. In SIGGRAPH Asia Posters , New York, NY , USA,\r\n2019. Association for Computing Machinery. 1, 2\r\n[6] Michael Broxton, Jay Busch, Jason Dourgarian, Matthew\r\nDuVall, Daniel Erickson, Dan Evangelakos, John Flynn, Pe-\r\nter Hedman, Ryan Overbeck, Matt Whalen, and Paul De-\r\nbevec. DeepView Immersive Light Field Video. In ACM\r\nSIGGRAPH Immersive Pavilion . Association for Computing\r\nMachinery, 2020.\r\n[7] Michael Broxton, John Flynn, Ryan Overbeck, Daniel Erick-\r\nson, Peter Hedman, Matthew Duvall, Jason Dourgarian, Jay\r\nBusch, Matt Whalen, and Paul Debevec. Immersive Light\r\nField Video with a Layered Mesh Representation. TOG , 39\r\n(4), 2020. 1, 2\r\n[8] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and\r\nHao Su. TensoRF: Tensorial Radiance Fields. In ECCV ,\r\npages 333–350. Springer, 2022. 3\r\n[9] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian\r\nSchroff, and Hartwig Adam. Encoder-Decoder with Atrous\r\nSeparable Convolution for Semantic Image Segmentation. In\r\nECCV , pages 801–818, 2018. 2\r\n[10] Bowen Cheng, Maxwell D Collins, Yukun Zhu, Ting Liu,\r\nThomas S Huang, Hartwig Adam, and Liang-Chieh Chen.\r\nPanoptic-Deeplab: A Simple, Strong, and Fast Baseline for\r\nBottom-Up Panoptic Segmentation. In CVPR , pages 12475–\r\n12485, 2020. 2\r\n[11] Changwoon Choi, Sang Min Kim, and Young Min Kim.\r\nBalanced Spherical Grid for Egocentric View Synthesis. In\r\nCVPR , pages 16590–16599, 2023. 1, 2, 3, 6, 7\r\n[12] Sara Fridovich-Keil, Giacomo Meanti, Frederik Rahbæk\r\nWarburg, Benjamin Recht, and Angjoo Kanazawa. K-planes:\r\nExplicit Radiance Fields in Space, Time, and Appearance. In\r\nCVPR , pages 12479–12488, 2023. 5\r\n[13] Miguel Granados, Hans-Peter Seidel, and Hendrik PA\r\nLensch. Background Estimation from Non-Time Sequence\r\nImages. In Proceedings of Graphics Interface 2008 , pages\r\n33–40, 2008. 1[14] Miguel Granados, James Tompkin, Kwang In Kim, Oliver\r\nGrau, Jan Kautz, and Christian Theobalt. How Not to Be\r\nSeen—Object Removal from Videos of Crowded Scenes. In\r\nCGF , pages 219–228. Wiley Online Library, 2012. 1\r\n[15] Charles Guyon, Thierry Bouwmans, and El-Hadi Zahzah.\r\nForeground Detection via Robust Low Rank Matrix Decom-\r\nposition Including Spatio-Temporal Constraint. In ACCV ,\r\npages 315–320. Springer, 2012. 1\r\n[16] Charles Guyon, Thierry Bouwmans, and El-Hadi Zahzah.\r\nMoving Object Detection via Robust Low Rank Matrix De-\r\ncomposition with IRLS Scheme. In Int. Symp. Visual Com-\r\nputing , pages 665–674. Springer, 2012. 1\r\n[17] Kaiming He, Georgia Gkioxari, Piotr Doll ´ar, and Ross Gir-\r\nshick. Mask R-CNN. In ICCV , pages 2961–2969, 2017. 2\r\n[18] Hyeonjoong Jang, Andr ´eas Meuleman, Dahyun Kang,\r\nDonggun Kim, Christian Richardt, and Min H. Kim.\r\nEgocentric Scene Reconstruction from an Omnidirectional\r\nVideo. TOG , 41(4), 2022. 1, 2\r\n[19] Yoonwoo Jeong, Seokjun Ahn, Christopher Choy, Anima\r\nAnandkumar, Minsu Cho, and Jaesik Park. Self-Calibrating\r\nNeural Radiance Fields. In ICCV , pages 5846–5854, 2021.\r\n2\r\n[20] Johannes Kopf, Xuejian Rong, and Jia-Bin Huang. Robust\r\nConsistent Video Depth Estimation. In CVPR , pages 1611–\r\n1621, 2021. 6\r\n[21] Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang.\r\nNeural Scene Flow Fields for Space-Time View Synthesis of\r\nDynamic Scenes. In CVPR , 2021. 2\r\n[22] Zhengqi Li, Qianqian Wang, Forrester Cole, Richard Tucker,\r\nand Noah Snavely. DynIBaR: Neural Dynamic Image-Based\r\nRendering. In CVPR , 2023. 5\r\n[23] Chen-Hsuan Lin, Wei-Chiu Ma, Antonio Torralba, and Si-\r\nmon Lucey. BARF: Bundle-Adjusting Neural Radiance\r\nFields. In CVPR , pages 5741–5751, 2021. 2\r\n[24] Geng Lin, Chen Gao, Jia-Bin Huang, Changil Kim, Yipeng\r\nWang, Matthias Zwicker, and Ayush Saraf. OmnimatteRF:\r\nRobust Omnimatte with 3D Background Modeling. In ICCV ,\r\n2023. 2\r\n[25] Yu-Lun Liu, Chen Gao, Andreas Meuleman, Hung-Yu\r\nTseng, Ayush Saraf, Changil Kim, Yung-Yu Chuang, Jo-\r\nhannes Kopf, and Jia-Bin Huang. Robust Dynamic Radiance\r\nFields. In CVPR , 2023. 2\r\n[26] Andreas Meuleman, Yu-Lun Liu, Chen Gao, Jia-Bin Huang,\r\nChangil Kim, Min H Kim, and Johannes Kopf. Progressively\r\nOptimized Local Radiance Fields for Robust View Synthe-\r\nsis. In CVPR , pages 16539–16548, 2023. 2, 3, 6, 7, 8\r\n[27] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\r\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. NeRF:\r\nRepresenting Scenes as Neural Radiance Fields for View\r\nSynthesis. In ECCV , pages 405–421. Springer, 2020. 2\r\n[28] Ashkan Mirzaei, Tristan Aumentado-Armstrong, Konstanti-\r\nnos G Derpanis, Jonathan Kelly, Marcus A Brubaker, Igor\r\nGilitschenski, and Alex Levinshtein. SPIn-NeRF: Multiview\r\nSegmentation and Perceptual Inpainting with Neural Radi-\r\nance Fields. In CVPR , pages 20669–20679, 2023. 8\r\n[29] Pierre Moulon, Pascal Monasse, Romuald Perrot, and Re-\r\nnaud Marlet. OpenMVG: Open Multiple View Geometry. In\r\n6879\r\nInt. Workshop on Reproducible Research in Pattern Recog-\r\nnition , pages 60–74. Springer, 2017. 2, 8\r\n[30] Thomas M ¨uller, Alex Evans, Christoph Schied, and Alexan-\r\nder Keller. Instant Neural Graphics Primitives with a Mul-\r\ntiresolution Hash Encoding. TOG , 41(4):1–15, 2022. 5\r\n[31] Ryan S. Overbeck, Daniel Erickson, Daniel Evangelakos,\r\nMatt Pharr, and Paul Debevec. A System for Acquiring,\r\nProcessing, and Rendering Panoramic Light Field Stills for\r\nVirtual Reality. TOG , 37(6), 2018. 1, 2\r\n[32] Keunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien\r\nBouaziz, Dan B Goldman, Steven M Seitz, and Ricardo\r\nMartin-Brualla. Nerfies: Deformable Neural Radiance\r\nFields. In ICCV , pages 5865–5874, 2021. 2\r\n[33] Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T\r\nBarron, Sofien Bouaziz, Dan B Goldman, Ricardo Martin-\r\nBrualla, and Steven M Seitz. HyperNeRF: a Higher-\r\nDimensional Representation for Topologically Varying Neu-\r\nral Radiance Fields. TOG , 40(6):1–12, 2021. 2\r\n[34] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-\r\nhall. DreamFusion: Text-to-3D using 2D Diffusion. arXiv\r\npreprint arXiv:2209.14988 , 2022. 8\r\n[35] Albert Parra Pozo, Michael Toksvig, Terry Filiba Schrager,\r\nJoyce Hsu, Uday Mathur, Alexander Sorkine-Hornung, Rick\r\nSzeliski, and Brian Cabral. An Integrated 6DoF Video Cam-\r\nera and System Design. TOG , 38(6), 2019. 1, 2\r\n[36] Konstantinos Rematas, Andrew Liu, Pratul P Srini-\r\nvasan, Jonathan T Barron, Andrea Tagliasacchi, Thomas\r\nFunkhouser, and Vittorio Ferrari. Urban Radiance Fields.\r\nInCVPR , pages 12932–12942, 2022. 2\r\n[37] Sara Sabour, Suhani V ora, Daniel Duckworth, Ivan Krasin,\r\nDavid J Fleet, and Andrea Tagliasacchi. RobustNeRF: Ignor-\r\ning Distractors with Robust Losses. In CVPR , pages 20626–\r\n20636, 2023. 1, 2, 3, 6, 7\r\n[38] Johannes Lutz Sch ¨onberger and Jan-Michael Frahm.\r\nStructure-from-Motion Revisited. In CVPR , 2016. 2\r\n[39] J ¨urgen Sturm, Nikolas Engelhard, Felix Endres, Wolfram\r\nBurgard, and Daniel Cremers. A Benchmark for the Eval-\r\nuation of RGB-D SLAM Systems. In IROS , pages 573–580.\r\nIEEE, 2012. 6\r\n[40] Shinya Sumikura, Mikiya Shibuya, and Ken Sakurada.\r\nOpenVSLAM: A Versatile Visual SLAM Framework. In\r\nACMMM , pages 2292–2295, 2019. 6\r\n[41] Yule Sun, Ang Lu, and Lu Yu. Weighted-to-Spherically-\r\nUniform Quality Evaluation for Omnidirectional Video.\r\nSPL, 24(9):1408–1412, 2017. 6\r\n[42] Towaki Takikawa, Joey Litalien, Kangxue Yin, Karsten\r\nKreis, Charles Loop, Derek Nowrouzezahrai, Alec Jacobson,\r\nMorgan McGuire, and Sanja Fidler. Neural Geometric Level\r\nof Detail: Real-Time Rendering with Implicit 3D Shapes. In\r\nCVPR , pages 11358–11367, 2021. 5\r\n[43] Matthew Tancik, Vincent Casser, Xinchen Yan, Sabeek Prad-\r\nhan, Ben Mildenhall, Pratul P Srinivasan, Jonathan T Bar-\r\nron, and Henrik Kretzschmar. Block-NeRF: Scalable Large\r\nScene Neural View Synthesis. In CVPR , pages 8248–8258,\r\n2022. 2\r\n[44] Matthew Tancik, Ethan Weber, Evonne Ng, Ruilong Li,\r\nBrent Yi, Terrance Wang, Alexander Kristoffersen, JakeAustin, Kamyar Salahi, Abhik Ahuja, et al. Nerfstudio:\r\nA Modular Framework for Neural Radiance Field Develop-\r\nment. In ACM SIGGRAPH Conference Proceedings , pages\r\n1–12, 2023. 3\r\n[45] Zachary Teed and Jia Deng. RAFT: Recurrent All-Pairs Field\r\nTransforms for Optical Flow. In ECCV , 2020. 2\r\n[46] Vadim Tschernezki, Diane Larlus, and Andrea Vedaldi. Neu-\r\nralDiff: Segmenting 3D Objects that Move in Egocentric\r\nVideos. In 3DV, 2021. 2\r\n[47] Haithem Turki, Deva Ramanan, and Mahadev Satya-\r\nnarayanan. Mega-NeRF: Scalable Construction of Large-\r\nScale NeRFs for Virtual Fly-Throughs. In CVPR , pages\r\n12922–12931, 2022. 2, 5\r\n[48] Peng Wang, Yuan Liu, Zhaoxi Chen, Lingjie Liu, Ziwei Liu,\r\nTaku Komura, Christian Theobalt, and Wenping Wang. F2-\r\nNeRF: Fast Neural Radiance Field Training with Free Cam-\r\nera Trajectories. In CVPR , pages 4150–4159, 2023. 3\r\n[49] Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul P\r\nSrinivasan, Howard Zhou, Jonathan T Barron, Ricardo\r\nMartin-Brualla, Noah Snavely, and Thomas Funkhouser.\r\nIBRNet: Learning Multi-View Image-Based Rendering. In\r\nCVPR , pages 4690–4699, 2021. 5\r\n[50] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Si-\r\nmoncelli. Image Quality Assessment: From Error Visibility\r\nto Structural Similarity. TIP, 13(4):600–612, 2004. 6\r\n[51] Zirui Wang, Shangzhe Wu, Weidi Xie, Min Chen, and\r\nVictor Adrian Prisacariu. NeRF–: Neural Radiance\r\nFields without Known Camera Parameters. arXiv preprint\r\narXiv:2102.07064 , 2021. 2\r\n[52] Silvan Weder, Guillermo Garcia-Hernando, Aron Monsz-\r\npart, Marc Pollefeys, Gabriel J Brostow, Michael Firman,\r\nand Sara Vicente. Removing Objects from Neural Radiance\r\nFields. In CVPR , pages 16528–16538, 2023. 8\r\n[53] Changhee Won, Jongbin Ryu, and Jongwoo Lim. SweepNet:\r\nWide-baseline Omnidirectional Depth Estimation. In ICRA ,\r\npages 6073–6079. IEEE, 2019. 1\r\n[54] Tianhao Wu, Fangcheng Zhong, Andrea Tagliasacchi, For-\r\nrester Cole, and Cengiz Oztireli. D2NeRF: Self-Supervised\r\nDecoupling of Dynamic and Static Objects from a Monocu-\r\nlar Video. NIPS , 35:32653–32666, 2022. 2, 6, 7\r\n[55] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,\r\nand Oliver Wang. The Unreasonable Effectiveness of Deep\r\nFeatures as a Perceptual Metric. In CVPR , pages 586–595,\r\n2018. 6\r\n[56] Zichao Zhang and Davide Scaramuzza. A Tutorial on Quan-\r\ntitative Trajectory Evaluation for Visual (-Inertial) Odome-\r\ntry. In IROS , pages 7244–725. IEEE, 2018. 6\r\n[57] Xiaowei Zhou, Can Yang, and Weichuan Yu. Moving Ob-\r\nject Detection by Detecting Contiguous Outliers in the Low-\r\nRank Representation. TPAMI , 35(3):597–610, 2012. 1\r\n[58] Nikolaos Zioulis, Antonis Karakottas, Dimitrios Zarpalas,\r\nand Petros Daras. Omnidepth: Dense Depth Estimation for\r\nIndoors Spherical Panoramas. In ECCV , pages 448–465,\r\n2018. 1\r\n[59] Nikolaos Zioulis, Antonis Karakottas, Dimitrios Zarpalas,\r\nFederico Alvarez, and Petros Daras. Spherical View Syn-\r\nthesis for Self-Supervised 360° Depth Estimation. In 3DV,\r\npages 690–699, 2019. 1\r\n6880"}, 'dist': 0.9286905527114868}
Result 24: {'text': 'data', 'metadata': {'created_at': 1736932570, 'modified_at': 1736932570, 'owner': 'sayandeep', 'path': 'RAG_CONTENT/conferences/cvpr_ocr/Chen_PeLK_Parameter-efficient_Large_Kernel_ConvNets_with_Peripheral_Convolution_CVPR_2024_paper.txt', 'size': 53430, 'seen_at': 1737191136, 'data': 'PeLK: Parameter-efﬁcient Large Kernel ConvNets with Peripheral Convolution\nHonghao Chen1,2*Xiangxiang Chu3Yongjian Ren1,2Xin Zhao1,2Kaiqi Huang1,2,4†\n1Institute of Automation, Chinese Academy of Sciences\n2School of Artiﬁcial Intelligence, University of Chinese Academy of Sciences\n3Meituan4CAS Center for Excellence in Brain Science and Intelligence Technology\nAbstract\nRecently, some large kernel convnets strike back with\nappealing performance and efﬁciency. However, given\nthe square complexity of convolution, scaling up kernels\ncan bring about an enormous amount of parameters and\nthe proliferated parameters can induce severe optimization\nproblem. Due to these issues, current CNNs compromise\nto scale up to 51×51in the form of stripe convolution\n(i.e.,51×5 + 5×51) and start to saturate as the ker-\nnel size continues growing. In this paper, we delve into\naddressing these vital issues and explore whether we can\ncontinue scaling up kernels for more performance gains.\nInspired by human vision, we propose a human-like periph-\neral convolution that efﬁciently reduces over 90% parame-\nter count of dense grid convolution through parameter shar-\ning, and manage to scale up kernel size to extremely large.\nOur peripheral convolution behaves highly similar to hu-\nman, reducing the complexity of convolution from O(K2)to\nO(logK)without backﬁring performance. Built on this, we\npropose Parameter-efﬁcient Large Kernel Network ( PeLK ).\nOur PeLK outperforms modern vision Transformers and\nConvNet architectures like Swin, ConvNeXt, RepLKNet and\nSLaK on various vision tasks including ImageNet classiﬁca-\ntion, semantic segmentation on ADE20K and object detec-\ntion on MS COCO. For the ﬁrst time, we successfully scale\nup the kernel size of CNNs to an unprecedented 101×101\nand demonstrate consistent improvements.\n1. Introduction\nConvolutional Neural Networks (CNNs) have played a piv-\notal role in machine learning for decades [ 16,19,20,35].\nHowever, their dominance has been greatly challenged by\nVision Transformers (ViTs) [ 6,12,24,42,47] over re-\ncent years. Some works [ 32,44] attribute the powerful\nperformance of ViTs to their large receptive ﬁelds: Facil-\n*Work done during internship at Meituan Inc.\n†Corresponding author.itated by self-attention mechanism, ViTs can capture con-\ntext information from a large spatial scope and model long-\nrange dependencies. Inspired by this, recent advances in\nCNNs [ 11,23,25] have revealed that when equipped with\nlarge kernel size (e.g., 31×31), pure CNN architecture can\nperform on par with or even better than state-of-the-art ViTs\non various vision tasks.\nAlthough large kernel convnets exhibit strong perfor-\nmance and appealing efﬁciency, a fatal problem exists: the\nsquare complexity O(K2)with respect to kernel size K.\nDue to this problem, directly scaling up kernels will bring\nabout a huge amount of parameters. For instance, the pa-\nrameter of a 31×31kernel is more than 100 ×larger than\nthat of a typical 3×3counterpart in ResNet [ 16] and about\n20×as many as that of the 7×7kernel used in Con-\nvNeXt [ 25]. The proliferated parameters subsequently in-\nduce severe optimization problem, making it useless or even\nharmful to directly scale up kernel size [ 11,23,25]. To\nsolve, RepLKNet [ 11] re-parameterize a 5 ×5 kernel par-\nallel to the large one to make up the optimization issue,\nSLaK [ 23] compromise to use stripe convolution to reduce\nthe complexity to linear and scales up to 51×51(i.e.,\n51×5 + 5×51). However, this is still a limited inter-\naction range for the resolution of downstream tasks (e.g.,\n2048×512on ADE20K) and more importantly, stripe con-\nvolution lacks the range perception of dense convolution,\nthus we conjecture it may undermine the model’s spatial\nperception capacity.\nIn this paper, we ﬁrst conduct a comprehensive dissec-\ntion of convolution forms under a uniﬁed modern frame-\nwork (i.e., SLaK [ 23]). We empirically verify our conjec-\nture that dense grid convolution outperforms stripe convo-\nlution with consistent improvements across multiple kernel\nsizes. This phenomenon holds not only for classiﬁcation\ntask, but even more pronounced for downstream tasks, in-\ndicating the essential advantage of dense convolution over\nstripe form. Nevertheless, as mentioned above, the square\ncomplexity of large dense convolution leads to the prolif-\nerated parameters , causing rapidly increasing model size,\ngreater optimization difﬁculty and thus preventing it from\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n5557\nfurther scaling. This non-trivial problem naturally leads to\na question: Is there a way to preserve the form of dense grid\nconvolution while reducing the parameters required? And if\nso, can we further scale up dense grid convolution for more\nperformance gains?\nUnlike the dense computation of convolution or self-\nattention, human vision possesses a more efﬁcient vi-\nsual processing mechanism termed peripheral vision [ 21].\nSpeciﬁcally, human vision partitions the entire visual ﬁeld\ninto central region and peripheral region conditioned on the\ndistance to the center of the gaze, and the number of pho-\ntoreceptor cells (cones and rods) in the central region is\nmore than 100 times that in the peripheral region [ 36]. Such\na physiological structure gives human vision the character-\nistic of blur perception: we have strong perception and see\nclearly in the central region, recognizing shapes and colors;\nwhereas in the peripheral region, the visual ﬁeld is blurred\nand the resolution decreases so we can only recognize ab-\nstract visual features such as motion and high-level con-\ntexts. This mechanism enables us to perceive important de-\ntails within a small portion of the visual ﬁeld ( <5%) while\nminimizing unnecessary information in the remaining por-\ntion (>95%), thereby facilitating efﬁcient visual process-\ning in the human brain [ 2,9,10,26,33,34,48,50].\nInspired by human vision and to answer the question\nabove, we propose a novel peripheral convolution to reduce\nthe parameter complexity of convolutions from O(K2)to\nO(logK)while maintaining the dense computational form.\nOur peripheral convolution consists of three designs: i)Fo-\ncus and blur mechanism. We keep ﬁne-grained parame-\nters in the central region of the convolution kernel and use\nwide-range parameter sharing in the peripheral regions; ii)\nExponentially-increasing sharing granularity. Our sharing\ngrid grows in an exponentially-increasing way, which is\nmore effective than ﬁxed granularity; iii)Kernel-wise posi-\ntional embedding. We introduce kernel-wise positional em-\nbedding to solve the problem of detail blurring caused by\nwide-range peripheral sharing in an elegant and cheap way.\nSince our peripheral convolution dramatically reduces the\nparameters for large kernels (over 90%), we are able to de-\nsign large dense kernel convnets with strong performance.\nBuilt upon the peripheral convolution above, we pro-\npose Parameter-efﬁcient Large Kernel Network ( PeLK ), a\nnew pure CNN architecture with Effective Receptive Field\n(ERF) growing exponentially with parameters. Facilitated\nby the elaborately designed parameter sharing mechanism,\nPeLK scales up kernel size at a remarkably minor pa-\nrameter cost, realizing extremely large dense kernel (e.g.,\n51×51,101×101) with consistent improvements. Our\nPeLK achieves state-of-the-art performance across a variety\nof vision tasks, exhibiting the potential of pure CNN archi-\ntecture when equipped with extremely large kernel size.\nPeLK is shown to be able to cover a much larger ERFregion than prior large kernel paradigms, which we be-\nlieve leads to its strong performance. More interestingly,\nour analysis and ablations demonstrate that the optimal de-\nsign principles of peripheral convolution share striking sim-\nilarities with human vision, suggesting that biologically in-\nspired mechanisms can be promising candidates for design-\ning strong modern networks.\n2. Related Work\n2.1. Large Kernel Convolutional Networks\nLarge kernel convolutional networks can date back to a\nfew old fashion models from the early days of deep learn-\ning [19,38,39]. After VGG-Net [ 35], it becomes a common\npractice to use a stack of small kernels (e.g., 1×1or3×3)\nto obtain a large receptive ﬁeld over the past decade. Global\nConvolutional Network (GCNs) [ 30] enlarges the kernel\nsize to 15 by employing a combination of stripe convolu-\ntions (1×M+M×1) to improve the semantic segmentation\ntask. However, the proposed method is reported to harm the\nperformance on ImageNet. Recently, large kernel convnets\nstrike back with appealing performance [ 11,23,25,43].\nConvMixer [ 43] use9×9depthwise convolution to replace\nthe spatial mixer of ViT [ 12] and MLP-Mixer [ 40] (i.e., self-\nattention block and fully-connection block respectively).\nConvNeXt [ 25] aligns with Swin’s [ 24] design philosophy\nto explore a strong modern CNN architecture equipped with\n7×7depthwise convolution. RepLKNet [ 11] impressively\nscales up the kernel size to 31×31by re-parameterizing\na small kernel (e.g., 5×5) parallel to it and performs on\npar with Swin Transformer [ 24]. Our work is also inspired\nby LargeKernel3D [ 5], which introduces large kernel de-\nsign into 3D networks and scales up to 17×17×17. In\ncontrast, we explore the extremety of 2D universal convolu-\ntion, scaling up to a much larger 101×101in a human-like\npattern. SLaK [ 23] combines decomposed convolution with\ndynamic sparsity to scale up kernels to 51×51in the form of\nstripe convolution (e.g., 51×5+5×51). However, it starts\nto saturate as the kernel size continuous growing. Different\nfrom those prior arts, we investigate which kind of convo-\nlution form is more effective in large kernel designs. More\nimportantly, we explore the design of extremely large dense\nkernel and test whether it can bring further gains.\n2.2. Peripheral Vision for Machine Learning\nHuman vision has a special visual processing system termed\nperipheral vision [ 21]. It partitions the entire visual ﬁeld\ninto multiple contour regions depending on the distances\nto the fovea, each characterized by a distinct resolution\ngranularity for recognition. The work of Rosenholtz [ 33]\ndiscusses in depth important ﬁndings and existing myths\nabout peripheral vision, suggesting that peripheral vision\nis more crucial to human perception on a range of differ-\n5558\n(a) Parameter Sharing.\nperipheral vision\n (b) Peripheral Convolution.\nFigure 1. (a) Illustration of parameter sharing. Using a 3 ×3 convolution to parameterize a 5 ×5 convolution, the positions with the\nsame color share the same parameter. The corresponding sharing grid is [2,1,2].(b) Illustration of peripheral convolution. Our sharing\ngrid contains two designs: i) focus and blur mechanism; ii) exponentially-increasing sharing grid.\nent tasks than previously thought. Following this, many\nstudies [ 2,9,10,26,34,50] have been devoted to uncov-\nering the underlying principles and deep implications of pe-\nripheral vision mechanisms. Since peripheral vision plays\nsuch a vital role in human vision, a number of pioneering\nworks [ 10,13–15,27,46] dig into the linkage between pe-\nripheral vision and machine vision (e.g., CNNs). [ 45] in-\ntroduces a biologically-inspired mechanism to improve the\nrobustness of neural networks to small adversarial pertur-\nbations. FoveaTer [ 18] uses radial-polar pooling regions to\ndynamically allocate more ﬁxation/computational resources\nto more challenging images. PerViT [ 29] proposes to incor-\nporate peripheral position encoding to the multi-head self-\nattention layers to partition the visual ﬁeld into diverse pe-\nripheral regions, showing that the network learns to perceive\nvisual data similarly to the way that human vision does.\nContinuing previous study, this paper explores to blending\nhuman peripheral vision with large kernel convnets, and in-\ntroduces a novel peripheral convolution to efﬁciently reduce\ndense convolution’s parameters.\n3. Dense Outperforms Stripe Consistently\nWe ﬁrst investigate whether dense grid convolutions are\nbetter than stripe convolutions. We take a uniﬁed modern\nframework SLaK [ 23] to conduct this study. According\nto RepLKNet [ 11], large kernel convolution boosts down-\nstream tasks much more than ImageNet classiﬁcation. So\nwe not only evaluate on ImageNet-1K but also on ADE20K\nas our benchmark. We adopt the efﬁcient large-kernel im-\nplementation developed by MegEngine [ 1] in this paper.\nFollowing SLaK [ 23], we train all models for a 120-\nepoch schedule on ImageNet. The data augmentations,\nregularization and hyper-parameters are all set the same.\nWe then use the pretrained models as the backbones on\nADE20K. Speciﬁcally, we use the UperNet [ 52] imple-\nmented by MMSegmentation [ 7] with the 80K-iteration\ntraining schedule. We do not use any advanced techniques\nnor custom algorithms since we seek to evaluate the back-\nbone only.\nSLaK introduce a two-step recipe for scaling up kernel to51×51: 1) Decomposing a large kernel into two rectangu-\nlar, parallel kernels; 2) Using dynamic sparsity and expand-\ning more width. In order to thoroughly analyze the effect of\nconvolution form, we conduct experiments both w/ and w/o\nsparsity. By default, we re-parameterize a 5×5convolution\nto ease the optimization problem as taken by SLaK and Re-\npLKNet. The results of Table 1show that dense grid con-\nvolution exceeds stripe convolution regardless of dynamic\nsparsity.\nWe further explore convolution forms (i.e., K ×K v.s.\nK×N) under different kernel sizes. Speciﬁcally, we ﬁx the\nshorter edge of SLaK’s stripe conv to be 5 as the default\nsetting (N=5), and then gradually decrease K from 51 to\n7. We do not use dynamic sparsity to give a sheer ablation\non convolutional forms. As shown in Fig. 2, dense grid con-\nvolution outperforms stripe convolution consistently among\nmultiple kernel sizes and the gains increase with the kernel\nsize, demonstrating the essential advantage of dense grid\nlarge kernel convolution.\nNevertheless, as discussed in Section 1, the square com-\nplexity of dense grid convolution can bring about prolifer-\nated parameters. For instance, as shown in Fig. 2, scaling\nup kernel from 7 to 51 only bring about 7.3 ×params for\nstripe conv while that for dense conv is 53.1 ×. Given that\nthe human’s peripheral vision has only a minimal number\nof photoreceptor cells in the peripheral regions, we argue\nthat dense parameters are not necessary for peripheral in-\nteractions. Motivated by this, we seek to reduce parameter\ncomplexity by introducing the peripheral vision mechanism\nwhile preserving the dense computation to keep dense con-\nTable 1. Comparison w/ and w/o dynamic sparsity. Dense con-\nvolution outperforms stripe convolution both on ImageNet and\nADE20K.\nMethod Kernel Spasity Acc mIoU\nSLaK-51 51 ×5 + 5×51 w/ 81.6 46.5\nRepLK-51 51 ×51 w/ 81.7 46.9 (+0.4)\nSLaK-51 51 ×5 + 5×51 w/o 81.3 46.1\nRepLK-51 51 ×51 w/o 81.6 46.6 (+0.5)\n5559\n/uni0000001a /uni00000014/uni00000016 /uni00000016/uni00000014 /uni00000018/uni00000014\n/uni0000002e/uni00000048/uni00000055/uni00000051/uni00000048/uni0000004f/uni00000003/uni00000036/uni0000004c/uni0000005d/uni00000048/uni00000017/uni00000018/uni00000011/uni00000019/uni00000017/uni00000018/uni00000011/uni0000001c/uni00000017/uni00000019/uni00000011/uni00000015/uni00000017/uni00000019/uni00000011/uni00000018/uni00000017/uni00000019/uni00000011/uni0000001b/uni00000024/uni00000027/uni00000028/uni00000015/uni00000013/uni0000002e/uni00000003/uni00000050/uni0000002c/uni00000052/uni00000038/uni00000003/uni0000000b/uni00000008/uni0000000c\n0.7 ×2.4 ×13.7 ×37.2 ×\n1.0 ×1.9 ×4.4 ×7.3 ×\n+0.2%+0.3%+0.3%+0.5%\n/uni00000027/uni00000048/uni00000051/uni00000056/uni00000048/uni00000003/uni00000026/uni00000052/uni00000051/uni00000059\n/uni00000036/uni00000057/uni00000055/uni0000004c/uni00000053/uni00000048/uni00000003/uni00000026/uni00000052/uni00000051/uni00000059Figure 2. Comparison under different kernel sizes. We depict\nthe mIoU gains on ADE20K and the multiple of convolutional pa-\nrameters. Dense grid convolution exceeds stripe convolution con-\nsistently but brings rapidly-increasing parameters.\nvolution’s strong performance.\n4. Parameter-efﬁcient Large Kernel Network\n4.1. Peripheral Convolution\nFormally, a standard 2D convolution kernel consists of a 4-\nD vector: w ∈Rcin×cout×k×k, wherecinstands for input\nchannels, coutis output channels, and kmeans the spatial\nkernel dimension. We seek to parameterize w by a smaller\nkernel w θ∈Rcin×cout×k′×k′through spatial-wise parame-\nter sharing, where 0<k′≤k.\nFirstly, we deﬁne the sharing grid S= [s0,s1,...,sk′−1],\nwhere/summationtextk′−1\ni=0si= k. According to S, we partition the k×k\npositions into k′×k′regions:\nfora,b= 0,1,...,k′−1,\nZa,b=\uf8f1\n\uf8f2\n\uf8f3(x,y)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglea−1/summationdisplay\ni=0si≤x <a/summationdisplay\ni=0si,b−1/summationdisplay\nj=0sj≤y <b/summationdisplay\nj=0sj\uf8fc\n\uf8fd\n\uf8fe\n(1)\nFor brevity, we stipulate that/summationtext−1\ni=0si= 0in Eq. 1. Then\nfor any position (x,y)∈Za,b, we set w (x,y) = wθ(a,b).\nIn this way, we can utilize a small kernel to parameterize a\nmuch larger kernel, achieving spatial-wise parameter shar-\ning. Fig. 1adepicts the illustration of this design.\nNext, we elaborate on the key designs of our pe-\nripheral convolution. We denote the kernel radius of\nwθasr. For easier comprehension, here we reformu-\nlate the sharing grid into an axisymmetric form: S=\n[¯s−r,¯s−r+1,...,¯s−1,¯s0,¯s1,...,¯sr−1,¯sr], wherer=k′−1\n2.\nAkin to human’s peripheral vision, the sharing grid of\nour peripheral convolution mainly consists of two core de-\nsigns: i) Focus and blur mechanism. As shown in Fig. 1b,Kernel Weight Positional Embedding Actual Kernel Weight\nFigure 3. Illustration of kernel-wise positional embedding. The\nposition embedding enables the kernel to distinguish speciﬁc posi-\ntions in the sharing region, making up the detail-capturing ability\nof large kernels.\nWe keep ﬁne-grained parameters in the central region of\nthe convolution kernel, where the sharing grid is set to 1\n(i.e., not sharing). For the peripheral region, we utilize\nlarge-range parameter sharing to exploit the spatial redun-\ndancy of peripheral vision. We demonstrate in Section 5.4\nthat the ﬁne granularity in the central region is of vital im-\nportance, while the peripheral region can withstand a wide\nrange of parameter sharing without backﬁring performance;\nii) Exponentially-increasing sharing granularity. Hu-\nman vision declines in a quasi-exponential mode [ 31]. In-\nspired by this, we design our sharing grid to grow in an\nexponentially-increasing way. This design can elegantly re-\nduce the parameter complexity of convolution from O(K2)\ntoO(logK), making it possible to further enlarge dense\nconvolution’s kernel size. Speciﬁcally, the sharing grid Sis\nconstructed by:\n¯si=/braceleftbigg1, if|i| ≤rc\nm(|i|−rc),ifrc<|i| ≤r(2)\nwherercis the radius of the central ﬁne-grained region, m\nis the base of the exponential growth and mis set to 2 by\ndefault.\n4.2. Kernel\xadwise Positional Embedding\nDespite that the proposed peripheral convolution effectively\nreduces the parameters for dense convolution, the large\nrange of parameter sharing may bring another issue: local\ndetail blurring in peripheral regions. Especially when the\nkernel size is scaled up to more than 50 or even 100 in the\nform of peripheral convolution, this phenomenon will be\nfurther ampliﬁed when a single parameter needs to process\n8×8or even16×16peripheral regions.\nTo solve, we propose the kernel-wise positional embed-\nding. Formally, given a set of input features X, We pro-\ncess these features by a convolution with kernel weights\nw∈Rcin×cout×k×k. We initialize the position embedding\nh∈Rcin×k×kwith trunc normal [49] initialization. The\nconvolution process at the output position (x,y)can be rep-\nresented as:\n5560\nY(x,y) =rw/summationdisplay\ni=−rwrw/summationdisplay\nj=−rww(i,j)·/parenleftbig\nX(x+i,y+j)+h(i,j)/parenrightbig\n(3)\nwhereYis the output. rwis the radius of the kernel w\nand we have rw=k−1\n2.\nAs illustrated in Fig. 3, by introducing kernel-wise po-\nsitional embedding for kernel, we can distinguish speciﬁc\nlocations in shared areas, so as to make up for the problem\nof vague local details caused by sharing. Actually, this can\nbe viewed as adding bias with relative position information\nto the input features. It is worth noting that all the kernels in\na stage share the same positional embedding h, thus the ad-\nditional parameters brought by hare negligible. This design\nsolves the position insensitivity problem caused by sharing\nweights in a cheap and elegant way, especially for extremely\nlarge kernels, e.g., 51×51and101×101.\n4.3. Partial Peripheral Convolution\nLarge kernel convnets have been shown to have high chan-\nnel redundancy [ 53] and suit well with sparsity [ 23]. Since\nour peripheral convolution enables us to design larger dense\nconvolution with stronger spatial perception ability, we\nhope to further exploit the channel redundancy of large con-\nvolution. We introduce an Inception-style design where\nonly partial channels of the feature map will be processed\nby convolution. We follow a simple philosophy: more iden-\ntity mapping to exploit the channel redundancy. Specif-\nically, for input X, we split it into two groups along the\nchannel dimension,\nXconv,Xid=Split(X)\n=X:,:,:g,X:,:,g:(4)\nwhere g is the channel numbers of convolution branches and\nset to3\n8Cinby default. Then the split inputs are fed into\nperipheral convolution and identity mapping respectively,\nX′\nconv=Peripheral Conv (Xconv)\nX′\nid=Xid(5)\nFinally, the outputs from two branches are concatenated to\nrestore the original shape,\nX′=Concat(X′\nconv,X′\nid). (6)\nThis design can be seen as a special case of Inception-\nstyle structure, such as Inception [ 37], Shufﬂenet [ 28,55]\nand InceptionNeXt [ 53]. They utilize different operators in\nparallel branches while we take a much simpler philosophy:\nonly peripheral convolution and identity mapping. We em-\npirically ﬁnd that this design suits well for peripheral convo-\nlutions with extremely large kernels, signiﬁcantly reducing\nFLOPs without backﬁring performance.4.4. Architecture Speciﬁcation\nBuilt on the above designs and observations, we now elabo-\nrate the architectures of our Parameter-efﬁcient Large Ker-\nnel Network (PeLK). We mainly follow ConvNeXt and\nSLaK to construct models with several sizes. Speciﬁcally,\nPeLK also adopts a 4-stage framework. We build the stem\nwith a convolution layer with 4×4kernels and 4 stride.\nThe block numbers of stages are [3,3,9,3]for tiny size and\n[3,3,27,3]for small/base size. The kernel sizes for PeLK’s\ndifferent stages are [51,49,47,13]by default. For PeLK-\n101, the kernel sizes are scaled up to [101,69,67,13].\nBy default, we keep the central 5×5region to be ﬁne-\ngrained. For PeLK-101, we enlarge the central region to\n7×7to adjust the increased kernel. Following SLaK, we\nalso use dynamic sparsity to enhance model capacity. All\nthe hyperparameters are set the same ( 1.3×width, 40%\nsparsity). We give thorough ablations for kernel conﬁgu-\nrations in section 5.4.\n5. Experiments\nIn this section, we ﬁrst conduct experiments on various es-\nsential vision tasks to evaluate PeLK with state-of-the-art\nbaselines. Then in section 5.4we comprehensively ablate\non the design principles of our peripheral convolution.\n5.1. Semantic Segmentation\nFor semantic segmentation, we evaluate PeLK backbones\non the ADE20K benchmark [ 56], which consists of 25K\nimages and 150 semantic categories. We use the Uper-\nNet [ 51] task layer for semantic segmentation. Following\nSwin and ConvNeXt, We train Upernet for 160K iterations\nwith single-scale inference. The results are reported in Ta-\nble2with mean Intersection of Union (mIoU) as the eval-\nuation metric. Our proposed PeLK exceeds previous state-\nof-the-art models with remarkable improvements, demon-\nstrating the effectiveness of our framework.\n5.2. Object Detection\nFor object detection/segmentation, we conduct experiments\nwith Cascade Mask R-CNN [ 3,17] on MS-COCO [ 22].\nFollowing ConvNeXt, we use the multi-scale setting and\ndefault conﬁgurations in MMDetection [ 4]. The Cascade\nMask R-CNN model is trained with the 3x (36-epoch) train-\ning schedule. As shown in Table 3, PeLK achieves higher\nmAP than state-of-the-art methods, samely validating our\nsuperiority.\n5.3. ImageNet Classiﬁcation\nThe ImageNet-1K [ 8] dataset consists of 1000 object\nclasses with 1.28M training images and 50,000 validation\nimages. We extend the aforementioned training schedule\nin Section 3to 300 epochs for a fair comparison. we\n5561\nTable 2. Semantic segmentation comparison on ADE20K of dif-\nferent methods. We report the single-scale mIoU following Con-\nvNeXt and SLaK. FLOPs are based on input sizes of (2048, 512).\nMethodKernel Params FLOPs mIoU\nsize (M) (G) (%)\nSwin-T [ 24] N/A 60 945 44.5\nConvNeXt-T [ 25] 7-7-7-7 60 939 46.0\nSLaK-T [ 23] 51-49-47-13 64 957 47.6\nPeLK-T 51-49-47-13 62 970 48.1\nSwin-S [ 24] N/A 81 1038 47.6\nConvNeXt-S [ 25] 7-7-7-7 82 1027 48.7\nSLaK-S [ 23] 51-49-47-13 89 1057 49.4\nPeLK-S 51-49-47-13 84 1077 49.7\nSwin-B [ 24] N/A 121 1188 48.1\nConvNeXt-B [ 25] 7-7-7-7 122 1170 49.1\nRepLKNet-B [ 11] 31-29-27-13 112 1170 49.9\nSLaK-B [ 23] 51-49-47-13 131 1210 50.2\nPeLK-B 51-49-47-13 126 1237 50.4\nPeLK-B-101 101-69-67-13 126 1339 50.6\nTable 3. Object detection comparison on COCO of different meth-\nods. FLOPs are based on input sizes of (1280, 800).\nMethodParams FLOPsAPboxAPmask\n(M) (G)\nSwin-T [ 24] 86 745 50.5 43.7\nConvNeXt-T [ 25] 86 741 50.4 43.7\nPeLK-T 86 770 51.4 44.6\nSwin-S [ 24] 107 838 51.8 44.7\nConvNeXt-S [ 25] 108 827 51.9 45.0\nPeLK-S 108 874 52.2 45.3\nSwin-B [ 24] 145 982 51.9 45.0\nRepLKNet-B [ 11] 137 965 52.2 45.2\nSLaK-B [ 23] 152 1001 52.5 45.5\nConvNeXt-B [ 25] 146 964 52.7 45.6\nPeLK-B 147 1028 52.9 45.9\nPeLK-B-101 147 1127 53.1 46.1\nconduct experiments for PeLK-T/S/B with input resolution\n224×224. For PeLK-B and PeLK-B-101, we further ex-\nperiment with input resolution of 384×384. More details\nof the training conﬁgurations can be found in Appendix A.\nWe compare PeLK with other state-of-the-art architec-\ntures under similar model size and FLOPs. As shown in Ta-\nble4, our model outperforms powerful modern CNNs and\ntransformers like ConvNeXt [ 25] and Swin [ 24] by large\nmargins. Notably, further scaling up the kernel size to ex-\ntremely large (e.g., PeLK-101) can achieve consistent im-\nprovements. It is important to note that very large dense\nkernels are not intended for ImageNet classiﬁcation, but our\nPeLK still exhibits a promising performance.Table 4. Image classiﬁcation accuracy (%) comparison on\nImageNet-1K. We report the top-1 accuracy. Although very large\ndense kernels are not intended for ImageNet classiﬁcation, our\nPeLK still exhibits a promising performance.\nMethodInput Params FLOPs Top-1\nsize (M) (G) acc\nSwin-T [ 24] 224228 4.5 81.3\nT2T-ViT t-14 [ 54]224222 6.1 81.7\nPerViT-S [ 29] 224221 4.4 82.1\nConvNeXt-T [ 25]224229 4.5 82.1\nPeLK-T 224229 5.6 82.6\nPVT-Large [ 47]224261 9.8 81.7\nT2T-ViT t-19 [ 54]224239 9.8 82.4\nPerViT-M [ 29] 224244 9.0 82.9\nSwin-S [ 24] 224250 8.7 83.0\nConvNeXt-S [ 25]224250 8.7 83.1\nPeLK-S 224250 10.7 83.9\nDeiT-B/16 [ 41] 224287 17.6 81.8\nRepLKNet-31B [ 11]224279 15.3 83.5\nSwin-B [ 24] 224288 15.4 83.5\nConvNeXt-B [ 25]224289 15.4 83.8\nSLaK-B [ 23] 224295 17.1 84.0\nPeLK-B 224289 18.3 84.2\nViT-B/16 [ 12] 384287 55.5 77.9\nDeiT-B/16 [ 41] 384287 55.4 83.1\nSwin-B [ 24] 384288 47.1 84.5\nRepLKNet-31B [ 11]384279 45.1 84.8\nConvNeXt-B [ 25]384289 45.0 85.1\nSLaK-B [ 23] 384295 50.3 85.5\nPeLK-B 384289 54.0 85.6\nPeLK-B-101 384290 68.3 85.8\n5.4. Ablation Studies\nAblation on the sharing grid. We dive into what kind\nof sharing and granularity beneﬁts most. For ease of un-\nderstanding, we ﬁrstly give two instances to clearly indi-\ncate the sharing grid. For example, in Fig. 1a, we pa-\nrameterize a 5×5convolution using a 3×3convolution,\nwhere the corresponding sharing grid is [2,1,2]. Each num-\nber represents the grid size parameterized by a single pa-\nrameter. For Fig. 1b, we parameterize 31×31convolu-\ntion with a 11×11convolution, the corresponding gird is\n[7,4,2,1,1,1,1,1,2,4,7]. Since the grid is symmetric at\nthe center 1 (which is the central point in the kernel), we\ndenote only half grid in Table 5for simplicity.\nWe conduct experiments with the same 120-epoch\nschedule on ImageNet as in Section 3. We use PeLK-T\nwithout dynamic sparsity to give a sheer ablation on the\nsharing grid. For the baseline, we make the sharing grid\nto be all one (i.e., [1, 1, ..., 1]), in this way, it is equal to a\n33×33dense convolution as taken in RepLKNet. Results in\nTable 5demonstrate that: 1)the central ﬁne granularity is of\nvital importance, while the peripheral regions can withstand\n5562\nTable 5. Ablation study on sharing grid. No kernel-wise positional\nembedding is used.\n# Sharing Grid Param Top-1 Acc\n1 [1,1,...,1,1] 1.00× 81.4\n2[2,2,2,2,2,2,2,2,1] 0.27× 81.0\n3[2,2,2,2,2,2,2,1,1,1] 0.33× 81.4\n4[4,4,4,2,1,1,1] 0.16× 81.3\n5 [8,4,2,1,1,1] 0.11× 81.4\n6 [1,1,2,4,8,1] 0.11× 80.5\nTable 6. Ablation on the central ﬁne-grained kernel size. Kernel-\nwise positional embedding is used.\nSharing Grid Central Kernel Ratio Top-1 Acc\n[11,8,4,2,1] 1×1 0.04% 80.8\n[10,8,4,2,1,1] 3×3 0.35% 81.1\n[9,8,4,2,1,1,1] 5×5 0.96% 81.6\n[8,8,4,2,1,1,1,1] 7×7 1.88% 81.6\nwide range of sharing. # 2, 3 show that keeping the central\n5×5region unshared is the key to keep performance; # 3,\n4, 5 exhibit that sharing in peripheral regions will not back-\nﬁre performance evidently. We term this characteristic as\nfocus-and-blur mechanism; 2)an exponentially-increasing\ngrid works best. Comparing # 4 with # 5, exponential gird\nnot only reduces the parameters needed but also boosts the\naccuracy. From the above analysis, it can be seen that our\ndesign enjoys both the least amount of parameters and the\nhighest performance.\nAblation on the central ﬁne-grained area ratio. Ta-\nble6ablates the effect of varying central ﬁne-grained kernel\nsize (i.e., the focus region). We also report the proportion of\nthe central region to the total kernel size. The results show\nthat the central region only takes about 1% proportion to\nmaintain the model’s high performance. However, the cen-\ntral region can not be too small, which will lead to severe\nperformance degradation. Further increasing the central re-\ngion does not bring additional beneﬁts, but it brings addi-\ntional parameters. In our main experiments, we keep the\ncentral5×5region of PeLK as ﬁne-grained, and for PeLK-\n101, we enlarge the central region to 7×7to maintain a\nsimilar central ratio.\nAblation on the kernel conﬁguration. Table 7ablates\nthe conﬁguration of kernel size in a 120 epoch schedule as\nin Section 3. For the input resolution of 2242, enlarging\nkernel size to 101×101will not bring additional beneﬁts;\nwhile for input resolution of 3842, PeLK-101 obtains a clear\nadvantage over PeLK. Increasing kernel size to 152×152\nleads to performance degradation, especially for input res-\nolution of 2242. These phenomena are reasonable consid-\nering the input resolution. For a typical convnet like Con-\nvNeXt or our PeLK, the stem layer will result in a 4×down-\n/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013\n/uni00000013\n/uni00000015/uni00000013/uni00000013\n/uni00000017/uni00000013/uni00000013\n/uni00000019/uni00000013/uni00000013\n/uni0000001b/uni00000013/uni00000013\n/uni00000014/uni00000013/uni00000013/uni00000013/uni00000035/uni00000048/uni00000053/uni0000002f/uni0000002e/uni00000003/uni0000003e/uni00000016/uni00000014/uni0000000f/uni00000003/uni00000015/uni0000001c/uni0000000f/uni00000003/uni00000015/uni0000001a/uni0000000f/uni00000003/uni00000014/uni00000016/uni00000040\n/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013\n/uni00000013\n/uni00000015/uni00000013/uni00000013\n/uni00000017/uni00000013/uni00000013\n/uni00000019/uni00000013/uni00000013\n/uni0000001b/uni00000013/uni00000013\n/uni00000014/uni00000013/uni00000013/uni00000013/uni00000036/uni0000002f/uni00000044/uni0000002e/uni00000003/uni0000003e/uni00000018/uni00000014/uni0000000f/uni00000003/uni00000017/uni0000001c/uni0000000f/uni00000003/uni00000017/uni0000001a/uni0000000f/uni00000003/uni00000014/uni00000016/uni00000040\n/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013\n/uni00000013\n/uni00000015/uni00000013/uni00000013\n/uni00000017/uni00000013/uni00000013\n/uni00000019/uni00000013/uni00000013\n/uni0000001b/uni00000013/uni00000013\n/uni00000014/uni00000013/uni00000013/uni00000013/uni00000033/uni00000048/uni0000002f/uni0000002e/uni00000003/uni0000003e/uni00000018/uni00000014/uni0000000f/uni00000003/uni00000017/uni0000001c/uni0000000f/uni00000003/uni00000017/uni0000001a/uni0000000f/uni00000003/uni00000014/uni00000016/uni00000040\n/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013Figure 4. Effective receptive ﬁeld (ERF) comparison. Our PeLK\nhas larger ERFs than SLaK and RepLK, spreading a wider area.\nsampling of the input images. So for input 2242, a51×51\nkernel is roughly able to cover the global feature map af-\nter stem. And for input 3842, a101×101kernel is equal\nto a global convolution, thus continuing scaling up kernel\ncan not bring more global perception but only wasted pa-\nrameters. This essentially suggests that kernel conﬁguration\nshould be tightly related to the input size. Currently, for the\nmost commonly used 2242and3842training, PeLK and\nPeLK-101 are the suitable options respectively. Moreover,\nwith the development of hardware devices and computing\npower in the future, our approach will hopefully shine fur-\nther when it is affordable to pretrain at higher resolutions.\n6. Analysis\n6.1. Visualization of ERFs.\nPrevious large kernel convnets like RepLKNet and SLaK\nattribute their performance gains to their large Effective Re-\nceptive Fields (ERFs). Facilitated by peripheral convolu-\ntion, PeLK has a much larger perception range. There-\nfore, we argue that PeLK’s strong performance comes from\nlarger ERFs. To verify, we depict the ERFs following Re-\npLKNet and SLaK, we sample and resize 50 images from\nthe validation set to 1024×1024 , and measure the con-\ntribution of the pixel on input images to the central point\nof the feature map generated in the last layer. The con-\ntribution scores are further accumulated and projected to\na1024×1024 matrix, as visualized in Fig 4. Our PeLK\nspreads high-contribution pixels in a much larger ERF, vali-\ndating our hypothesis and further exhibiting our effectivess.\nTable 7. Ablation on the kernel size conﬁguration. Kernel-wise\npositional embedding is used.\nModel Input Size Kernel Size Top-1 Acc\nPeLK 224×224 51-49-47-13 81.6\nPeLK-101 224×224 101-69-67-13 81.6\nPeLK-151 224×224 151-89-87-13 81.2\nPeLK 384×384 51-49-47-13 82.7\nPeLK-101 384×384 101-69-67-13 83.0\nPeLK-151 384×384 151-89-87-13 82.8\n5563\n0.05 %\n3.20 %\n24.80 %\n71.95 %87.4 %\n12.6 %\nFFN Conv Down-Sample Pos-Embed Head Backbone\n(a) FLOPs proportion of head & backbone (b) FLOPs proportion of backbone’s componentsFigure 5. Analysis of FLOPs. (a) FLOPs proportion of head &\nbackbone. (b) FLOPs proportion of backbone’s components. The\nhead is UperNet and the backbone is PeLK-T respectively. FLOPs\nare based on input sizes of (2048, 512).\n6.2. Analysis of FLOPs\nWe provide a detailed breakdown of the FLOPs for the\nPeLK-T architecture utilized in semantic segmentation in\nFig.5. As shown in Fig. 5(a), we depict the FLOPs distri-\nbution between the head (i.e., UperNet [ 51]) and backbone\n(i.e., PeLK-T) of the model. In Fig. 5(b), we give a com-\nprehensive analysis of the FLOPs contributions from dif-\nferent components of the backbone (i.e., PeLK-T), includ-\ning FFNs, large-kernel convolutions, down-sampling lay-\ners, and kernel-wise positional embedding. There are two\nnoteworthy points. Firstly, large kernel convolutions ac-\ncount for approximately 25% of the overall FLOPs of the\nbackbone, thus further scaling up the kernel size does not\nsigniﬁcantly increase the overall FLOPs. Secondly, the ex-\ntra FLOPs introduced by positional embedding are minimal,\naccounting for only 0.05% of the backbone’s FLOPs. So,\nkernel-wise positional embed is both cheap and elegant.\n6.3. Inference Throughput Measurement\nWe compare inference throughput measurement in Table 8.\nThe results are obtained on an A100 GPU with input res-\nolution of 224×224. We use PyTorch 1.10.0 + cuDNN\n8.2.0 and FP32 precision. Although SLaK uses stripe con-\nvolution to speed up the computation of very large kernel,\nwe still hold a clear speed advantage (i.e., 1.5×speedup).\nThis advantage is particularly remarkable considering that\nPeLK outperforms SLaK on ADE20K, COCO and Ima-\ngeNet. More importantly, scaling up kernel to 101 only\nbrings minor speed overhead, further exhibiting our de-\nsign’s merits in scaling properties.\nTable 8. Inference throughput comparison on ImageNet-1K. The\nresults are in FP32 precision. We use an A100 GPU with PyTorch\n1.10.0 + cuDNN 8.2.0 to conduct this experiment.\nModels Input Kernel Size Throughput\nSLaK-T [ 23]224251-49-47-13 754\nPeLK-T 224251-49-47-13 1138\nPeLK-101-T 2242101-69-67-13 1077\n/uni0000001a /uni00000014/uni00000016 /uni00000015/uni00000014 /uni00000016/uni00000014 /uni00000017/uni00000014 /uni00000018/uni00000014 /uni00000019/uni00000014 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000018/uni00000014\n/uni0000002e/uni00000048/uni00000055/uni00000051/uni00000048/uni0000004f/uni00000003/uni00000036/uni0000004c/uni0000005d/uni00000048/uni00000015/uni00000018/uni00000018/uni00000013/uni0000001a/uni00000018/uni00000014/uni00000013/uni00000013/uni00000014/uni00000015/uni00000018/uni00000014/uni00000018/uni00000013/uni00000014/uni0000001a/uni00000018/uni00000006/uni00000033/uni00000044/uni00000055/uni00000044/uni00000050/uni00000003/uni0000000b/uni00000030/uni0000000c/uni00000027/uni00000048/uni00000051/uni00000056/uni00000048/uni00000003/uni00000026/uni00000052/uni00000051/uni00000059\n/uni00000033/uni00000048/uni00000055/uni0000004c/uni00000053/uni0000004b/uni00000048/uni00000055/uni00000044/uni0000004f/uni00000003/uni00000026/uni00000052/uni00000051/uni00000059Figure 6. Scaling efﬁciency comparison. We compare the model\nsize with a set of kernel sizes from 7 to 151. Our peripheral convo-\nlution has a clear advantage, bringing minor parameter overhead.\n6.4. Kernel Scaling Efﬁciency.\nOur peripheral convolution reduces the parameter complex-\nity of dense convolutions from O(K2)toO(logK), which\nenables us to scale up kernel size with a remarkably minor\nmodel size overhead. To demonstrate this, we simply re-\nplace all the kernels in stages of ConvNeXt-T with a set of\nkernel sizes from 7 to 151 and report the required number\nof parameters. As shown in Fig 6, our approach exhibits a\nremarkable scaling advantage, and we can see a clear gap\nwhen the kernel size is larger than 50. Using dense con-\nvolution results in a rapidly growing model size, which is\nunacceptable in practice. In contrast, our peripheral convo-\nlution incurs only a minor model size overhead, making it\npossible to design extremely large kernel convnets.\n7. Conclusion\nThis paper explores the design of extremely large kernel\nconvolutional neural networks. We propose a new form of\nconvolution termed peripheral convolution, which can re-\nduce the parameter complexity of dense convolution from\nO(K2)toO(logK)while keeping dense convolution’s\nmerits. Built upon the proposed peripheral convolution, we\ndesign extremely large dense kernel CNNs and achieve no-\ntable improvements across a variety of vision tasks. Our\nstrong results suggest biologically inspired mechanisms can\nmake a promising tool to boost modern network design.\nAcknowledgments\nThis work is supported in part by the National Key R&D\nProgram of China (Grant No.2022ZD0116403), the Na-\ntional Natural Science Foundation of China (Grant No.\n61721004), and the Strategic Priority Research Program of\nChinese Academy of Sciences (Grant No. XDA27000000).\nWe thank Yurong Zhang for the help in the depiction of\nFig.1(b) and Bo Zhang for technical support.\n5564\nReferences\n[1] Megengine:a fast, scalable and easy-to-use deep learning\nframework. https://github.com/MegEngine/\nMegEngine , 2020. 3\n[2] Benjamin Balas, Lisa Nakano, and Ruth Rosenholtz. A\nsummary-statistic representation in peripheral vision ex-\nplains visual crowding. Journal of vision , 9(12):13–13, 2009.\n2,3\n[3] Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: Delv-\ning into high quality object detection. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 6154–6162, 2018. 5\n[4] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu\nXiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu,\nJiarui Xu, et al. Mmdetection: Open mmlab detection tool-\nbox and benchmark. arXiv preprint arXiv:1906.07155 , 2019.\n5\n[5] Yukang Chen, Jianhui Liu, Xiangyu Zhang, Xiaojuan Qi, and\nJiaya Jia. Largekernel3d: Scaling up kernels in 3d sparse\ncnns. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition , pages 13488–13498,\n2023. 2\n[6] Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haib-\ning Ren, Xiaolin Wei, Huaxia Xia, and Chunhua Shen.\nTwins: Revisiting the design of spatial attention in vision\ntransformers. Advances in Neural Information Processing\nSystems , 34:9355–9366, 2021. 1\n[7] MMSegmentation Contributors. MMSegmentation:\nOpenmmlab semantic segmentation toolbox and\nbenchmark. https://github.com/open-\nmmlab/mmsegmentation , 2020. 3\n[8] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE conference on computer vision and\npattern recognition , pages 248–255. Ieee, 2009. 5\n[9] Arturo Deza and Miguel Eckstein. Can peripheral repre-\nsentations improve clutter metrics on complex scenes? Ad-\nvances in neural information processing systems , 29, 2016.\n2,3\n[10] Arturo Deza and Talia Konkle. Emergent proper-\nties of foveated perceptual systems. arXiv preprint\narXiv:2006.07991 , 2020. 2,3\n[11] Xiaohan Ding, Xiangyu Zhang, Jungong Han, and Guiguang\nDing. Scaling up your kernels to 31x31: Revisiting large\nkernel design in cnns. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition , pages\n11963–11975, 2022. 1,2,3,6\n[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. arXiv preprint\narXiv:2010.11929 , 2020. 1,2,6\n[13] Lex Fridman, Benedikt Jenik, Shaiyan Keshvari, Bryan\nReimer, Christoph Zetzsche, and Ruth Rosenholtz. Sideeye:\nA generative neural network based simulator of human pe-\nripheral vision. arXiv preprint arXiv:1706.04568 , 2017. 3[14] Stephen Gould, Joakin Arfvidsson, Adrian Kaehler, Ben-\njamin Sapp, Marius Messner, Gary Bradski, Paul Baum-\nstarck, Sukwon Chung, Andrew Y Ng, et al. Peripheral-\nfoveal vision for real-time object recognition and tracking\nin video. 2007.\n[15] Anne Harrington and Arturo Deza. Finding biological plau-\nsibility for adversarially robust features via metameric tasks.\nInSVRHM 2021 Workshop@ NeurIPS , 2021. 3\n[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition , pages 770–778, 2016. 1\n[17] Kaiming He, Georgia Gkioxari, Piotr Doll ´ar, and Ross Gir-\nshick. Mask r-cnn. In Proceedings of the IEEE international\nconference on computer vision , pages 2961–2969, 2017. 5\n[18] Aditya Jonnalagadda, William Yang Wang, BS Manjunath,\nand Miguel P Eckstein. Foveater: Foveated transformer\nfor image classiﬁcation. arXiv preprint arXiv:2105.14173 ,\n2021. 3\n[19] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.\nImagenet classiﬁcation with deep convolutional neural net-\nworks. Advances in neural information processing systems ,\n25, 2012. 1,2\n[20] Yann LeCun, L ´eon Bottou, Yoshua Bengio, and Patrick\nHaffner. Gradient-based learning applied to document recog-\nnition. Proceedings of the IEEE , 86(11):2278–2324, 1998.\n1\n[21] Jerome Y Lettvin et al. On seeing sidelong. The Sciences ,\n16(4):10–20, 1976. 2\n[22] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll ´ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nComputer Vision–ECCV 2014: 13th European Conference,\nZurich, Switzerland, September 6-12, 2014, Proceedings,\nPart V 13 , pages 740–755. Springer, 2014. 5\n[23] Shiwei Liu, Tianlong Chen, Xiaohan Chen, Xuxi Chen, Qiao\nXiao, Boqian Wu, Mykola Pechenizkiy, Decebal Mocanu,\nand Zhangyang Wang. More convnets in the 2020s: Scal-\ning up kernels beyond 51x51 using sparsity. arXiv preprint\narXiv:2207.03620 , 2022. 1,2,3,5,6,8\n[24] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\nHierarchical vision transformer using shifted windows. In\nProceedings of the IEEE/CVF international conference on\ncomputer vision , pages 10012–10022, 2021. 1,2,6\n[25] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feicht-\nenhofer, Trevor Darrell, and Saining Xie. A convnet for the\n2020s. In Proceedings of the IEEE/CVF conference on com-\nputer vision and pattern recognition , pages 11976–11986,\n2022. 1,2,6\n[26] Chin Ian Lou, Daria Migotina, Joao P Rodrigues, Joao\nSemedo, Feng Wan, Peng Un Mak, Pui In Mak, Mang I Vai,\nFernando Melicio, J Gomes Pereira, et al. Object recognition\ntest in peripheral vision: a study on the inﬂuence of object\ncolor, pattern and shape. In Brain Informatics: International\nConference, BI 2012, Macau, China, December 4-7, 2012.\nProceedings , pages 18–26. Springer, 2012. 2,3\n5565\n[27] Hristofor Lukanov, Peter K ¨onig, and Gordon Pipa. Bio-\nlogically inspired deep learning model for efﬁcient foveal-\nperipheral vision. Frontiers in Computational Neuroscience ,\n15:746204, 2021. 3\n[28] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun.\nShufﬂenet v2: Practical guidelines for efﬁcient cnn architec-\nture design. In Proceedings of the European conference on\ncomputer vision (ECCV) , pages 116–131, 2018. 5\n[29] Juhong Min, Yucheng Zhao, Chong Luo, and Minsu Cho.\nPeripheral vision transformer. Advances in Neural Informa-\ntion Processing Systems , 35:32097–32111, 2022. 3,6\n[30] Chao Peng, Xiangyu Zhang, Gang Yu, Guiming Luo, and\nJian Sun. Large kernel matters–improve semantic segmen-\ntation by global convolutional network. In Proceedings of\nthe IEEE conference on computer vision and pattern recog-\nnition , pages 4353–4361, 2017. 2\n[31] RT Pramod, Harish Katti, and SP Arun. Human peripheral\nblur is optimal for object recognition. Vision research , 200:\n108083, 2022. 4\n[32] Maithra Raghu, Thomas Unterthiner, Simon Kornblith,\nChiyuan Zhang, and Alexey Dosovitskiy. Do vision trans-\nformers see like convolutional neural networks? Advances\nin Neural Information Processing Systems , 34:12116–12128,\n2021. 1\n[33] Ruth Rosenholtz. Capabilities and limitations of peripheral\nvision. Annual review of vision science , 2:437–457, 2016. 2\n[34] Ruth Rosenholtz. Demystifying visual awareness: Periph-\neral encoding plus limited decision complexity resolve the\nparadox of rich visual experience and curious perceptual fail-\nures. Attention, Perception, & Psychophysics , 82(3):901–\n925, 2020. 2,3\n[35] Karen Simonyan and Andrew Zisserman. Very deep convo-\nlutional networks for large-scale image recognition. arXiv\npreprint arXiv:1409.1556 , 2014. 1,2\n[36] Hans Strasburger, Ingo Rentschler, and Martin J ¨uttner. Pe-\nripheral vision and pattern recognition: A review. Journal of\nvision , 11(5):13–13, 2011. 2\n[37] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,\nScott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent\nVanhoucke, and Andrew Rabinovich. Going deeper with\nconvolutions. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition , pages 1–9, 2015.\n5\n[38] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,\nScott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent\nVanhoucke, and Andrew Rabinovich. Going deeper with\nconvolutions. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition , pages 1–9, 2015.\n2\n[39] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon\nShlens, and Zbigniew Wojna. Rethinking the inception archi-\ntecture for computer vision. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition , pages\n2818–2826, 2016. 2\n[40] Ilya O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lu-\ncas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung,\nAndreas Steiner, Daniel Keysers, Jakob Uszkoreit, et al.Mlp-mixer: An all-mlp architecture for vision. Advances\nin neural information processing systems , 34:24261–24272,\n2021. 2\n[41] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herv ´e J´egou. Training\ndata-efﬁcient image transformers & distillation through at-\ntention. In International conference on machine learning ,\npages 10347–10357. PMLR, 2021. 6\n[42] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herv ´e J´egou. Training\ndata-efﬁcient image transformers & distillation through at-\ntention. In International conference on machine learning ,\npages 10347–10357. PMLR, 2021. 1\n[43] Asher Trockman and J Zico Kolter. Patches are all you need?\narXiv preprint arXiv:2201.09792 , 2022. 2\n[44] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. Advances in neural\ninformation processing systems , 30, 2017. 1\n[45] Manish Reddy Vuyyuru, Andrzej Banburski, Nishka Pant,\nand Tomaso Poggio. Biologically inspired mechanisms for\nadversarial robustness. Advances in Neural Information Pro-\ncessing Systems , 33:2135–2146, 2020. 3\n[46] Panqu Wang and Garrison W Cottrell. Central and peripheral\nvision for scene recognition: A neurocomputational model-\ning exploration. Journal of vision , 17(4):9–9, 2017. 3\n[47] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao\nSong, Ding Liang, Tong Lu, Ping Luo, and Ling Shao.\nPyramid vision transformer: A versatile backbone for dense\nprediction without convolutions. In Proceedings of the\nIEEE/CVF international conference on computer vision ,\npages 568–578, 2021. 1,6\n[48] William H Warren and Kenneth J Kurtz. The role of cen-\ntral and peripheral vision in perceiving the direction of self-\nmotion. Perception & psychophysics , 51(5):443–454, 1992.\n2\n[49] Ross Wightman. Pytorch image models. https:\n//github.com/rwightman/pytorch-image-\nmodels , 2019. 4\n[50] Maarten WA Wijntjes and Ruth Rosenholtz. Context miti-\ngates crowding: Peripheral object recognition in real-world\nimages. Cognition , 180:158–164, 2018. 2,3\n[51] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and\nJian Sun. Uniﬁed perceptual parsing for scene understand-\ning. In Proceedings of the European conference on computer\nvision (ECCV) , pages 418–434, 2018. 5,8\n[52] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and\nJian Sun. Uniﬁed perceptual parsing for scene understand-\ning. In Proceedings of the European conference on computer\nvision (ECCV) , pages 418–434, 2018. 3\n[53] Weihao Yu, Pan Zhou, Shuicheng Yan, and Xinchao Wang.\nInceptionnext: When inception meets convnext. arXiv\npreprint arXiv:2303.16900 , 2023. 5\n[54] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi,\nZi-Hang Jiang, Francis EH Tay, Jiashi Feng, and Shuicheng\nYan. Tokens-to-token vit: Training vision transformers from\n5566\nscratch on imagenet. In Proceedings of the IEEE/CVF in-\nternational conference on computer vision , pages 558–567,\n2021. 6\n[55] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun.\nShufﬂenet: An extremely efﬁcient convolutional neural net-\nwork for mobile devices. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition , pages\n6848–6856, 2018. 5\n[56] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fi-\ndler, Adela Barriuso, and Antonio Torralba. Semantic under-\nstanding of scenes through the ade20k dataset. International\nJournal of Computer Vision , 127:302–321, 2019. 5\n5567'}, 'dist': 0.9286905527114868}
Result 25: {'text': 'data', 'metadata': {'created_at': 1736932570, 'modified_at': 1736932570, 'owner': 'sayandeep', 'path': 'RAG_CONTENT/conferences/cvpr_ocr/Chen_ViTamin_Designing_Scalable_Vision_Models_in_the_Vision-Language_Era_CVPR_2024_paper.txt', 'size': 68481, 'seen_at': 1737191136, 'data': 'ViTamin: Designing Scalable Vision Models in the Vision-Language Era\nJieneng Chen1*Qihang Yu2*Xiaohui Shen2Alan Yuille1Liang-Chieh Chen2\n1Johns Hopkins University2ByteDance *equal contribution\nhttps://beckschen.github.io/vitamin.html\nAbstract\nRecent breakthroughs in vision-language models (VLMs)\nstart a new page in the vision community. The VLMs\nprovide stronger and more generalizable feature embed-\ndings compared to those from ImageNet-pretrained models,\nthanks to the training on the large-scale Internet image-text\npairs. However, despite the amazing achievement from the\nVLMs, vanilla Vision Transformers (ViTs) remain the de-\nfault choice for the image encoder. Although pure trans-\nformer proves its effectiveness in the text encoding area,\nit remains questionable whether it is also the case for im-\nage encoding, especially considering that various types of\nnetworks are proposed on the ImageNet benchmark, which,\nunfortunately, are rarely studied in VLMs. Due to small\ndata/model scale, the original conclusions of model design\non ImageNet can be limited and biased. In this paper, we\naim at building an evaluation protocol of vision models\nin the vision-language era under the contrastive language-\nimage pretraining (CLIP) framework. We provide a com-\nprehensive way to benchmark different vision models, cov-\nering their zero-shot performance and scalability in both\nmodel and training data sizes. To this end, we introduce\nViTamin, a new vision models tailored for VLMs. ViTamin-\nL significantly outperforms ViT-L by 2.0% ImageNet zero-\nshot accuracy, when using the same publicly available\nDataComp-1B dataset and the same OpenCLIP training\nscheme. ViTamin-L presents promising results on 60 di-\nverse benchmarks, including classification, retrieval, open-\nvocabulary detection and segmentation, and large multi-\nmodal models. When further scaling up the model size, our\nViTamin-XL with only 436M parameters attains 82.9% Im-\nageNet zero-shot accuracy, surpassing 82.0% achieved by\nEVA-E that has ten times more parameters (4.4B).\n1. Introduction\nThe past decades have witnessed significant progress in\ncomputer vision, like visual recognition tasks. The advent\nof AlexNet [53] marked a significant milestone, catalyz-\ning the extensive evolution and dominance of Convolutional\ntext model“a photo of cat”contrastive loss\nvision model\nData ScalabilityModel ScalabilityCNN+TransformerFeature Resolutiondesigningbenchmark modern backbones in CLIPImageNetavg. 38 datasetretrieval81.880.367.264.765.7VTABdist. shift62.572.470.261.861.1LMM(VQAv2)LMM(LLAVA-Bench)78.975.966.160.6OV pan. seg (ADE)OV sem. seg (A-150)OV detection(OV-LVIS)27.324.631.835.632.535.6ViTamin-LViT-L/14ViTamin38 zero-shot classification/retrieval tasks10 open-vocab dense tasks12 LMM tasks (e.g., VQA)Figure 1. Practices of designing scalable vision models in\nthe vision-language era. We benchmark modern vision mod-\nels with various model and data scales under CLIP setting using\nDataComp-1B [30], leading to findings about data and model scal-\nability, feature resolution, and hybrid architecture, which motivate\nus to develop ViTamin for VLM. ViTamin-L achieves superior\nzero-shot performance over ViT-L/14 [60] on ImageNet [86] and\naverage 38 datasets [30], and advances a suite of 22 downstream\ntasks for Open-V ocabulary (OV) detection [111] and segmenta-\ntion [124], and Large Multi-modal Model (LMM) tasks [67].\nNeural Networks (ConvNets) [8, 32, 37, 38, 46, 55, 72, 73]\nin computer vision. More recently, with the debut of Vision\nTransformer [23, 104], a growing number of transformer-\nbased architectures [18, 71, 103, 108, 116, 121] have shown\ngreat potential to surpass the prior ConvNet counterparts.\nThe rapid advancement of neural network design in com-\nputer vision can be attributed to a combination of factors.\nAmong them, an important factor is the well-established\nbenchmarks, allowing the community to examine the devel-\nopments in a standardized way. Particularly, ImageNet [86]\nhas become the de facto testing ground for new vision\nmodels. It not only sets a standard benchmark for vi-\nsual recognition, but also serves as a mature pre-training\ndataset for transferring the network backbone to a vari-\nety of downstream tasks ( e.g., detection and segmenta-\ntion) [9, 10, 15, 38, 51, 66, 73, 95, 107, 117, 122, 123].\nRecently, the emergence of vision-language models\n(VLMs) [50, 82] has changed the paradigm by leveraging\nthe pre-training schedule on the extremely large scale noisy\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n12954\nInternet data up to billions of image-text pairs [89], much\nlarger than the ImageNet scale. VLMs not only produce\nstrong and generalizable features [50, 82], but also excel in\nzero-shot downstream tasks [31, 54, 68, 77, 85, 124, 137].\nHowever, unlike the ImageNet benchmark, where many\ntypes of neural networks are designed and blossomed [37,\n44, 46, 53, 91, 97], the existing VLMs mostly employ the\nvanilla Vision Transformer (ViT) architecture [23] *, and\nthe recent benchmark DataComp [30] focuses on the data\ncuration under the common (yet unverified) belief that ViTs\nscale much better than any other architectures in this vision-\nlanguage era [20, 61] and thus ViT is all we need.\nThe current trend can be characterized by several key ob-\nservations: (1) The high computational demand, requiring\nextensive resources [48] for months, is a significant barrier\nfor advancing VLMs [82], limiting exploring diverse vision\nmodels. (2) Traditional vision models are mainly optimized\nfor the ImageNet benchmark, which may not scale well\nfor larger datasets [30, 89], unlike purely transformer-based\narchitectures [104] that have proven scalable in language\ntasks [81, 102] and are now being adopted for VLMs as\nimage encoders [23]. (3) Current VLM benchmarks focus\non zero-shot classification/retrieval tasks [30], with a no-\ntable lack of downstream tasks involving open-vocabulary\ndense prediction [21, 22, 31, 54, 114, 115, 124, 129, 135],\nas well as a gap in assessing Large Multi-modal Models\n(LMMs) [57, 67, 68, 137].\nIn this paper, we aim to address the aforementioned is-\nsues with practices as shown in Fig. 1. To begin with,\nwe establish a new test bed for designing vision models\nunder the CLIP framework [50, 82] using the DataComp-\n1B dataset [30], which is one of the largest publicly avail-\nable datasets with high quality. Specifically, we employ\ntwo training protocols: short schedule for fast benchmark-\ning vision models across model and data scales, and long\nschedule for training best performing vision models. With\ntheshort schedule , we re-benchmark state-of-the-art vision\nmodels found on ImageNet settings for VLMs. Particularly,\nwe select ViT [23], ConvNeXt [72], CoAtNet [18], as rep-\nresentatives for pure Transformer, pure ConvNet, and hy-\nbrid architecture, respectively. We combine various model\nscales and data scales to provide a comprehensive evalua-\ntion towards different architectures, revealing several criti-\ncal findings. First, increasing data scales improves all vision\nmodels across all model sizes, while ViT scale slightly bet-\nter than others in terms of model parameters. Second, the\nfinal resolution of the extracted features affects prediction\nperformance. Third, CoAtNet performs better than ViT and\nConvNeXt in general, though it is hard to scale up CoAtNet-\n4 to billions of data due to computational constraints.\nThose findings motivate us to develop a new vision\nmodel, named ViTamin tailored for VLM. ViTamin is a 3-\n*with only a few exceptions, e.g., ConvNeXt [72] by OpenCLIP [48].stage hybrid architectures, combining two stages of MB-\nConv blocks with a final stage of Transformer blocks. This\nhybrid design leverages its Transformer stage to enhance\ndata and model scalability, along with output stride of 16\nto enjoy high feature resolution. As a result, ViTamin-L\noutshines its ViT-L/14 counterpart [30] by +2.0% zero-shot\nimageNet accuracy in identical OpenCLIP training scheme\nand identical 256 token length. When increasing feature\nresolution to 576 patches, ViTamin-L further attains 81.8%\nzero-shot imageNet accuracy, surpassing the prior art ViT-\nL/14 CLIPA-v2 [60] by +1.5%. In average performance\nacross 38 datasets, it not only exceeds ViT-L/14 counter-\npart [60] by +1.5%, but also outperforms the larger ViT-\nH/14 model [60] by +0.4% while having only half parame-\nters. When further scaling up the model size, our ViTamin-\nXL with only 436M parameters attains 82.9% ImageNet\nzero-shot accuracy, surpassing 82.0% achieved by EV A-\nE (i.e., EV A-02-CLIP-E/14 [94]) that has ten times more\nparameters (4.4B). Furthermore, we introduce an effective\ntraining scheme Locked-Text Tuning (LTT), which guides\nthe training of vision backbone with a frozen pretrained text\nencoder. It enhances the small variant by +4.0% and the\nbase variant by +4.9% without any extra cost.\nOur another intriguing observation is the prevailing em-\nphasis on data filtering over vision architecture design in\nVLM. For instance, while the best DataComp challenge\nsolution [119] achieved only a +2.3% gain, our ViTamin\nwith LTT largely improves performance by +23.3% on the\nsame dataset size, without intensive data filtering. Finally,\nwe introduce a suite of downstream tasks, including open-\nvocabulary detection and segmentation, and LMMs, for\nevaluating VLM-specific vision models. ViTamin outper-\nforms the ViT-L model, enhancing detector by +3.1% on\nOV-LVIS and segmentor by +2.6% on average 8 datasets,\nand excelling across 12 LMM benchmarks. Notably, ViTa-\nmin sets a new state-of-the-art on 7 benchmarks for open-\nvocabulary segmentation.\nWe aim for our findings to encourage a reevaluation of\nthe current limitations in VLM designs and hope that our\nextensive benchmarking and evaluations will drive the de-\nvelopment of more advanced vision models for VLMs.\n2. Related Work\nVision Backbone: On the ImageNet benchmark [86], Con-\nvNets [37, 46, 53, 72, 88, 91, 96–98, 113, 128] have\nbeen the dominant networks choice since the advent of\nAlexNet [53]. Recently, the vision community has wit-\nnessed impressive emergence of the Transformer architec-\nture [104], a trend that began with the widespread adop-\ntion of the ViT [23] and its subsequent developments [26,\n58, 62, 71, 83, 99, 106, 108, 126, 136]. Among these, hy-\nbrid architectures [14, 18, 24, 34, 41, 63, 76, 93, 103, 109,\n112, 116] combine Transformer self-attention with convo-\n12955\nlution, where CoAtNet [18] particularly obtains impres-\nsive results on ImageNet. Notably, MaX-DeepLab [107],\nemerged as early as 2020, successfully developed a hy-\nbrid network backbone for dense pixel predictions, where\nthe first two stages utilize residual bottleneck blocks [37],\nfollowed by two subsequent stages employing axial atten-\ntion [106]. More recently, by leveraging the design prac-\ntices of a Vision Transformer, a ResNet [37] can be mod-\nernized to ConvNeXt [72], competing favorably with ViT.\nAlong the same direction, but not limited to the ImageNet\nscale, our work aims to develop a novel vision model for\ntraining with billions of data [30] in the vision-language era.\nLanguage-Image Pre-training: Language-image pre-\ntraining has seen significant advancements [1, 3, 50, 57, 68,\n82, 120] with the emergence of LLMs [7, 81, 101]. The\nhuge progress can be attributed to the pre-training on an\nimmense scale of noisy web-collected image-text data [30,\n89], much larger than the ImageNet. Notably, CLIP [50, 82]\ngenerates strong image features and excels in zero-shot\ntransfer learning [31, 54, 68, 77, 85, 124, 137], which make\nit an essential role in large multi-modal model [13, 57, 67,\n68]. CLIP has been improved by advanced training strate-\ngies including self-supervised learning [65, 79], efficient\ntuning [69, 132] and training [59, 61, 94, 110, 133]. These\nstudies predominantly employ ViT [23] as the only vision\nmodel. As a result, the architectural design for the CLIP\nvision model has not been thoroughly investigated. Thus,\nwe attempt to bridge the gap by developing a novel vision\nmodel for VLMs.\n3. Method\nIn the section, we revisit the problem definition of CLIP and\npropose two training protocols ( short andlong schedules)\non DataComp-1B (Sec. 3.1). With short schedule , we re-\nbenchmark modern vision models found on ImageNet un-\nder the CLIP setting (Sec. 3.2). We then introduce the pro-\nposed ViTamin architecture design, motivated by the dis-\ncoveries in the re-benchmarking results (Sec. 3.3).\n3.1. CLIP and Training Protocols\nCLIP Framework: Given a batch of Nimage-text pairs\n{(I1, T1), ...,(IN, TN)}(where IiandTidenote image and\ntext for ithpair), the objective of CLIP [82] learns to align\nthe image embeddings xiand text embeddings yifor each\npair. Formally, the loss function is defined as follows:\n−1\n2NNX\ni=1(logexT\niyi/τ\nPN\nj=1exT\niyj/τ\n| {z }\nimage to text+ logeyT\nixi/τ\nPN\nj=1eyT\nixj/τ\n| {z }\ntext to image),(1)\nwhere xi=f(Ii)\n∥f(Ii)∥2,yi=g(Ti)\n∥g(Ti)∥2, andτis a temperature\nvariable. A vision model f(.)and a text model g(.)aretrained to minimize the loss function. We focus on vision\nmodel design and use the text models from OpenCLIP [48].\nTraining Protocols: We employ two training protocols:\nshort schedule andlong schedule . The short schedule is\ndesigned for efficiently benchmarking vision models up to\n1 training epoch on DataComp-1B [30] ( i.e., 1.28B seen\nsamples). As detailed in Tab. 2, given a descent amount\nof resources ( e.g., 32 A100 GPUs), it takes less than two\ndays to train a small ( ∼25M parameters) model variant. The\nlong schedule is designed for training the best performing\nmodels with up to 40B seen samples.\n3.2. Benchmarking Vision Models in CLIP Setting\nThe short schedule allows us to efficiently re-benchmark\nstate-of-the-art vision models found on ImageNet under the\nCLIP setting using DataComp-1B. The experimented mod-\nels are ViT [23] (a pure Transformer), ConvNeXt [72] (a\npure ConvNet), and CoAtNet [18] (a hybrid model). We\nexamine their scalability in terms of both model scales and\ndata sizes. Each vision model has sizes varying from small\n(∼25M parameters), base ( ∼85M) to large ( ∼300M), while\nthe data sizes range from 128M, 512M to 1.28B training\nseen samples (1 epoch is equal to 1.28B seen samples). The\nmetric is zero-shot accuracy on ImageNet, supplemented by\nthe results on the 38 tasks following DataComp [30]. As\nshown in Fig. 2, we analyze the benchmarked results from\nfour aspects, including data scalability, model scalability,\nfeature resolution, and hybrid architecture. For simplicity,\nwe use “X@Y” to denote the vision model X trained with\nY seen samples. See appendix for numerical results.\nData Scalability: When training seen samples increase\nfrom 128M to 1.28B, we observe a consistent trend of\nimprovements across all model sizes and all vision mod-\nels (a1-a5). Interestingly, ViT-S/16@512M (22M param-\neter) attains the zero-shot performance of 53.8% on Im-\nageNet, which is better than 45.8% by ViT-B/16@128M\n(86M parameter). It shows the effectiveness of training\nlarge scale data that quadrupling training seen samples can\nbe more impactful than quadrupling the number of model\nparameters. Additionally, ViT-B/16@512M & @1.28B sig-\nnificantly boost ViT-B/16@128M from 45.8% to 60.0%\n(+14.2%) and 65.6% (+19.8).\n→As the training seen samples increase, the perfor-\nmances consistently improves in all cases.\nModel Scalability: When the model sizes increase, the\nperformances of all vision models are also boosted (b1-b3).\nHowever, we observe a different gain among them (b4).\nFor example, ConvNeXt-XL@128M brings only +1.4%\ngain over ConvNeXt-B@128M, while ViT-L/16@128M\nenhances ViT-B/16@128M by +3.6%. Given plenty of data,\nViT still shows a better model scalability, especially scaling\nfrom base to large ( e.g., +6.4% for ViT vs. +3.6% for both\nCoAtNet and ConvNeXt at 512M samples; +6.3% for ViT\n12956\n_\nData Scalability : As the training seen samples increase, the performances consistently improves in all cases.\nModel Scalability : ViT demonstrates the most effective scaling in terms of model parameters.\nFeature Resolution : The final resolution of extracted features affects prediction accuracy. Models with smaller patch sizes performs better.\nHybrid Architecture : CoAtNet performs better than ViT and ConvNeXt in general, but hard to scale up CoAtNet -4 to billions of data.VIT/32\nseen 512M seen 1.28B\nVIT-L/16 > ConvNeXt ConvNeXt > ViT/32 VIT/16 boosts ViT/32 VIT-L/14  boosts ViT -L/16 ViTamin > CoAtNet\nCoAtNet -4 > ViT/16 except 1.28B CoAtNet -2 > others CoAtNet -0 > othersVIT/16 ViTamin\n128M seen samples\n(a2) (a3) (a1) ConvNeXt CoAtNet (a4) (a5)\n(b1) (b2) (b3)\n(c1) (c2) (c3) (c4) (c5)\n(d1) (d2) (d3) (d4) (d5) ViTamin as a good hybrid model relative gain: CoAtNet to ViT\n-0.4small base large small base large small base largeConvNeXtViT/16CoAtNetViTamin\nS@128M L@512M L@1.28B B@128Msmallbaselarge\nB@512Msamples0+4\n-2+4.0\n+2.7+3.3\n+0.6512M seen samples 1.28B seen samples\n_\ngain by scaling model @ different seen samples40506070\nsmallbaselarge\n128M seen samples 1.28B seen samples 512M seen samples+1.4+3.6+3.6+6.4(b4) gain of scaling models\n+4.4+7.4+4.0\n+6.1+7.5+1.9+6.1 +6.2+6.9+3.6\n+6.7+4.7\n+5.4+4.9\n+5.8+6.3\n+6.8+2.8\n+6.5+5.0relative accuracy gain\nFigure 2. Benchmarking vision models under CLIP setting on DataComp-1B , including ViT (a pure Transformer), ConvNeXt (a pure\nConvNet), and CoAtNet (a hybrid model). We examine their scalability in terms of both data sizes (1st row) and model scales (2nd row),\nand further analyze the results from the aspects of feature resolution (3rd row) and hybrid architecture (4th row).\nvs. +2.8% for CoAtNet and + 4.9% for ConvNeXt at 1.28B\nsamples). As a result, ViT shows the best scalability.\n→ViT demonstrates the most effective scaling in terms\nof model parameters.\nFeature Resolution: Across all model scales and data\nsizes, ConvNeXt performs better than ViT/32 but loses its\nadvantage to ViT/16 (c1 & c2). This trend deviates sig-\nnificantly from what is observed in ImageNet era, where\nConvNeXt consistently outperforms ViT/16 (also see our\nImageNet-scale VLM experiments in appendix). We hy-\npothesize that, comparing to ImageNet’s object class label,\nthe text in CLIP captures broader area of information, and\nthus is beneficial from higher feature resolution. Besides,\nViT also benefits from using smaller patch sizes (thus high\nfeature resolution) over larger path sizes (c3 & c4).\n→The final resolution of extracted features affects the\nprediction performance. ViT with smaller patch sizes out-performs ViT with larger patch sizes and ConvNeXt.\nHybrid Architecture: We observe that ConvNeXt con-\nsistently lags behind ViT- {S,B}/16 and particularly ViT-\nL/14, suggesting that a pure ConvNet has limited capacity\nunder the CLIP setting when presented with abundant of\ndata (d1-d3). By contrast, CoAtNet significantly surpasses\nboth ViT and ConvNeXt ( e.g., CoAtNet-2@1.28B has a\nremarkable +2.9% and +3.2% gain over ViT-B/16@1.28B\nand ConvNeXt-B@1.28B, respectively), indicating the ef-\nfectiveness of hybrid models. However, CoAtNet requires\nthe most GPU memory; we can only train CoAtNet-4 with\nbatch size 8k on 64 A100 GPUs, while all the other large\nmodels are trained with batch size 16k on 32 A100 GPUs.\nThis affects CoAtNet’s scalability in large variant.\n→CoAtNet surpasses ViT and ConvNeXt in general, yet\nit is hard to scale up CoAtNet-4 to billions of data.\n12957\n3.3. Novel Vision Transformer for Vision-Language\nHerein, we distill from the aforementioned observations,\nculminating in the proposed vision model, ViTamin ( Vision\nTrAnsfor Mer for v Ision-la Nguage), which notably takes\nthe lead in the benchmarking results across all settings in\nFig. 2. To introduce ViTamin, we commence by its macro-\nlevel network design (Sec. 3.3.1), followed by the micro-\nlevel block design (Sec. 3.3.2). Finally, we develop a vision\nmodel family with a simple scaling rule (Sec. 3.3.3).\n3.3.1 Macro-level Network Design\nOverview: The macro-level network design of ViTamin\nis inspired by the ViT and CoAtNet. Specifically, on top\nof a simple convolutional stem ( i.e., two 3×3convolu-\ntions) [18], we adopt a 3-stage network architecture, where\nthe first two stages employ the Mobile Convolution Blocks\n(MBConv) [43, 88] and the third stage uses the Transformer\nBlocks (TFB) [23, 104]. Fig. 3 shows the overview of Vi-\nTamin. We detail the design principles below, based on the\ndiscoveries from the re-benchmarking results\nData and Model Scalability: ViT demonstrates the best\nscalability in terms of both model scales and data sizes. We\nthus opt for using Transformer Blocks in our last stage, and\nwe stack most blocks here across different model sizes.\nFeature Resolution: We tailor the network to generate\nhigh resolution feature maps in the end. Our 3-stage net-\nwork design thus yields a feature map with output stride 16\n(i.e., a downsampling factor of 16).\nHybrid Architecture: Similar to CoAtNet, we employ\nMBConv in the first two stages, resulting in a hybrid model.\nHowever, unlike CoAtNet that is constrained by its large\nmemory usage, we propose a light-weight design of stage 1\nand 2, which contain only two and four MBConv blocks.\nGiven the macro-level network design, we then move on\nto further improve the micro-level block design below.\n3.3.2 Micro-level Block Design\nOverview: The proposed ViTamin depends on two types of\nblocks: Mobile Convolution Blocks (MBConv) and Trans-\nformer Blocks (TFB). We refine each block in our model.\nMBConv-LN: The Mobile Convolution Block (MB-\nConv) [88] employs the “inverted bottleneck” design [37],\nstarting with a first 1×1convolution to expand the channel\nsize, followed by a 3×3depthwise convolution [44] for spa-\ntial interaction, and ending with another 1×1convolution\nto revert to the original channel size. Modern MBConv, as\nin MobileNetv3 [43], adds numerous batch normalization\n(BN) [49] layers and squeeze-and-excitation (SE) [45]. We\nadopt a simple modification by removing all BN layers and\nSE, and just using a single layer normalization (LN) [4] as\nthe first layer in our block, akin to the pre-norm layer in\nthe Transformer block, resulting in the proposed MBConv-\nLN. Ablation (in appendix) shows that MBConv-LN enjoysViTamin-S ViTamin-B ViTamin-L ViTamin-XL\nblock stride B C B C B C B C\nconv-stem 2 2 64 2 128 2 160 2 192\nMBConv-LN 4 2 64 2 128 2 160 2 192\nMBConv-LN 8 4 128 4 256 4 320 4 384\nTFB-GeGLU 16 14 384 14 768 31 1024 32 1152\nTable 1. ViTamin model variants. ViTamin variants differ in the\nnumber of blocks Band number of channels Cin each stage.\na simple design while attaining a similar performance to the\noriginal MBConv-BN-SE in MobileNetv3.\nTFB-GeGLU: The Transformer Block (TFB) [104] con-\ntains two residual blocks: one with self-attention and\nthe other with feed-forward network (FFN). We empiri-\ncally discover that substituting the first linear layer with\nGeGLU [90], an enhanced version of the Gated Linear\nUnit [19] that has a 2×expansion rate, can enhance ac-\ncuracy in the FFN. We denote the Transformer Block\nwith the updated FFN as TFB-GeGLU. Ablation (in ap-\npendix) shows that TFB-GeGLU requires 12% fewer pa-\nrameters than TFB due to half expansion ratio, allowing us\nto stack additional Transformer blocks towards deeper ar-\nchitectures [96, 100, 136].\n3.3.3 Meta Architecture and Scaling Rule\nMeta Architecture: After introducing our macro-level net-\nwork and micro-level block designs, we now put everything\ntogether to form the meta architecture of ViTamin. Specif-\nically, ViTamin is a hybrid architecture that contains only\nthree stages, built on top of a simple convolutional stem\n(i.e., two 3×3convolutions). The first two stages are\ncomposed of MBConv-LN, where we stack two and four of\nthem for stage 1 and 2, respectively. The third stage are ob-\ntained by stacking NBTFB-GeGLU blocks. With the meta\narchitecture in mind, we are ready to discuss the scaling rule\nto generate a family of ViTamin with different model sizes.\nScaling Rule: Our scaling rule is extremely simple and\nstraightforward, controlled by two hyper-parameters: width\n(i.e., the channel sizes of those three stages) and depth ( i.e.,\nNB, the number of TFB-GeGLU blocks in stage 3). Note\nthat our convolutional stem has the same channel size as\nthe first stage. We define four model sizes: Small, Base,\nLarge, and X-Large (S, B, and L variants have a similar\namount of model parameters to ViT [23, 131]). We use the\nsame channel size as ViT in our 3rd stage for each model\nvariant. Specifically, we set the channel sizes of our three\nstages as (C,2C,6C), where 6C={384,768,1024,1152}\nfor Small, Base, Large and X-Large model variant, respec-\ntively†. Subsequently, given the target model parameter,\nthe value of NB(i.e., the number of TFB-GeGLU blocks\nin stage 3) can be easily found. We show the family of\nViTamin- {S, B, L, XL }in Tab. 1.\n†We calculate the channel size for stage 1 as 1/6C, rounding to the\nnearest value that is divisible by 32.\n12958\n…MBConvBlockMBConvBlockTransformerBlockstemstage 1: 2xstage 2: 4xstage 3: NBxLNSALNGELULinearLNLinear*\nLNGELUConv 1x1 Conv 1x1\nText Transformer…(a) Proposed Vision Model ViTamin(b)  Contrastive Language-Image PretrainingImageGELUDWConv 3x3MBConv-LNTFB-GeGLU+++\nConv3x3Conv3x3Linear𝐶×𝐻2×𝑊2𝐶×𝐻4×𝑊42𝐶×𝐻8×𝑊86𝐶×𝐻𝑊16!Figure 3. Overview of ViTamin architecture . (a) ViTamin begins with a convolutional stem, followed by Mobile Convolution Blocks\n(MBConv) in stage 1 and 2, and Transformer Blocks (TFB) in stage 3. The 2D input to the stage 3 is flatten to 1D. For the macro-level\ndesigns, the three-stage layout generates the final feature map with output stride 16, similar to ViT/16 [23]. We set channels sizes for\nthe three stages to be ( C,2C,6C). For the micro-level designs, the employed MBConv-LN modifies MBConv [88] by using a single\nLayerNorm [4]. TFB-GeGLU upgrades TFB’s FFNs [104] (Feed-Forward Networks) with GELU Gated Linear Units [90]. (b) In the CLIP\nframework, given Nimage-text pairs, the vision model’s output Iiis learned to align with its corresponding text Transformer’s output Ti.\nOur text Transformers are the same as OpenCLIP [48]. +: Addition. *: Multiplication.\nshort schedule for benchmarking long schedule\nViTamin-S ViTamin-B ViTamin-L ViTamin-L ViTamin-XL\nbatch size 8k 8k 16k 90k 90k 90k\nimage size 224 224 224 224 256 256\n# A100 GPUs 32 32 32 184 312 312\n# epochs 1 1 1 10 10 30\nseen samples (B) 1.28 1.28 1.28 12.8 12.8 40.0\ntraining days 1.8 3.3 5.6 11 15 46\nTable 2. Short and long training schedules on DataComp-1B.\nLocked-Text Tuning for CLIP: Besides model design,\nwe propose Locked-Text Tuning (LTT) to exploits a pre-\ntrained frozen text encoder. In light of the aligned image\nand text embeddings in CLIP, we leverage the pretrained\ntext encoder from a large VLM to guide the training of im-\nage encoders of smaller VLMs. Specifically, when training\nother ViTamin variants ( e.g., ViTamin-S and ViTamin-B),\nwe initialize their text encoder with the one from a pre-\ntrained ViTamin-L. The text encoder is then frozen, used\nas a teacher to guide the training of the randomly initialized\nimage encoder. This scheme can be considered as a way\nto distill the knowledge [42] from a pretrained frozen text\nencoder to a randomly initialized image encoder.\n4. Experimental Results\nIn this section, we detail the implementation in Sec. 4.1,\ncompare with the state-of-the-arts in Sec. 4.2, and deploy\nViTamin to downstream tasks, including open-vocabulary\ndetection/segmentation, and large multi-modal models in\nSec. 4.3. See appendix for ablation studies.\n4.1. Implementation Details\nTraining Strategy: We train the VLMs using Open-\nCLIP [48] on the public dataset DataComp-1B [30]. Tab. 2\nsummarizes the settings for our training schedules andmodel variants. We use the short schedule to benchmark\nvision models and conduct our ablation studies, and long\nschedule to train our best ViTamin-L. We closely follow\nthe training hyper-parameter settings in OpenCLIP [30, 48].\nThe training and fine-tuning details are in the appendix.\nEvaluation Strategy: We follow DataComp [30] to\nzero-shot evaluate VLMs with a testbed of 38 tasks, includ-\ning ImageNet [86], 6 distribution shift tasks [5, 39, 40, 84,\n105], VTAB tasks [130], WILDS tasks [52, 87], and 3 re-\ntrieval tasks [6, 11, 118].\nOther Downstream Tasks: We evaluate the trained\nVLM in downstream tasks. For open-vocabulary detec-\ntion, we exploit the F-ViT framework [111], while for open-\nvocabulary segmentation, we adopt the FC-CLIP frame-\nwork [124] and zero-shot evaluate on multiple segmentation\ndatasets. Finally, we evaluate VLMs in LLaV A-1.5 [67] for\nLMMs across multiple benchmarks. In all the cases, F-ViT,\nFC-CLIP, and LLaV A employ the frozen VLM backbone to\neffectively ablate different pretrained VLMs.\n4.2. Main Results\nComparison with other State-of-the-arts: Tab. 3 summa-\nrizes the comparison between ViTamin-L and other state-\nof-the-art models, which exclusively employ the ViT back-\nbone [23] but use different training schemes and datasets.\nFor a fair comparison, we focus on the methods that use the\nsame training data DataComp-1B [30], but still list other\nmethods in the table for reference. For simplicity, we use\n“X@Z” to denote the vision model X trained with input\nsize Z‡. ImageNet zero-shot accuracy is our main met-\nric; other results are still reported in the table. As shown\n‡Notation @ here is slightly abused to denote the training seen samples.\n12959\nimage num text encoder seen training training trainable params MACs ImageNet avg. 38 ImageNet VTAB retrieval\nimage encoder size patches depth/width samples (B) scheme dataset Image+Text (M) Image+Text (G) Acc. datasets dist. shift.\nViT-L/14 [30] 224 256 12 / 768 12.8 OpenCLIP DataComp-1B 304.0 + 123.7 77.8 + 6.6 79.2 66.3 67.9 65.2 60.8\nViT-L/14 [60] 224 256 12 / 768 12.8 + 0.5 CLIPA-v2 DataComp-1B 304.0 + 110.3 77.8 + 2.7 79.7 65.4 68.6 62.9 60.6\nViT-L/14 [60] 336 576 12 / 768 12.8 + 0.5 + 0.1 CLIPA-v2 DataComp-1B 304.3 + 110.3 174.7 + 2.7 80.3 65.7 70.2 62.5 61.1\nViTamin-L 224 196 12 / 768 12.8 OpenCLIP DataComp-1B 333.3 + 123.7 72.6 + 6.6 80.8 66.7 69.8 65.3 60.3\nViTamin-L 256†256 12 / 768 12.8 + 0.2 OpenCLIP DataComp-1B 333.4 + 123.7 94.8 + 6.6 81.2 67.0 71.1 65.3 61.2\nViTamin-L 336 441 12 / 768 12.8 + 0.2 OpenCLIP DataComp-1B 333.6 + 123.7 163.4 + 6.6 81.6 67.0 72.1 64.4 61.6\nViTamin-L 384†576 12 / 768 12.8 + 0.2 OpenCLIP DataComp-1B 333.7 + 123.7 213.4 + 6.6 81.8 67.2 72.4 64.7 61.8\nViTamin-L2 224 196 24 / 1024 12.8 OpenCLIP DataComp-1B 333.6 + 354.0 72.6 + 23.3 80.9 66.4 70.6 63.4 61.5\nViTamin-L2 256†256 24 / 1024 12.8 + 0.5 OpenCLIP DataComp-1B 333.6 + 354.0 94.8 + 23.3 81.5 67.4 71.9 64.1 63.1\nViTamin-L2 336 441 24 / 1024 12.8 + 0.5 OpenCLIP DataComp-1B 333.8 + 354.0 163.4 + 23.3 81.8 67.8 73.0 64.5 63.6\nViTamin-L2 384†576 24 / 1024 12.8 + 0.5 OpenCLIP DataComp-1B 334.0 + 354.0 213.4 + 23.3 82.1 68.1 73.4 64.8 63.7\nViTamin-XL 256†256 27 / 1152 12.8 + 0.5 OpenCLIP DataComp-1B 436.1 + 488.7 125.3 + 33.1 82.1 67.6 72.3 65.4 62.7\nViTamin-XL 384†576 27 / 1152 12.8 + 0.5 OpenCLIP DataComp-1B 436.1 + 488.7 281.9 + 33.1 82.6 68.1 73.6 65.6 63.8\nViTamin-XL 256†256 27 / 1152 40.0 OpenCLIP DataComp-1B 436.1 + 488.7 125.3 + 33.1 82.3 67.5 72.8 64.0 62.1\nViTamin-XL 336†441 27 / 1152 40.0 + 1.0 OpenCLIP DataComp-1B 436.1 + 488.7 215.9 + 33.1 82.7 68.0 73.9 64.1 62.6\nViTamin-XL 384†576 27 / 1152 40.0 + 1.0 OpenCLIP DataComp-1B 436.1 + 488.7 281.9 + 33.1 82.9 68.1 74.1 64.0 62.5\nViT-L/14 [94] 224 256 12 / 768 4.0 EV A-CLIP Merged-2B 333.3 + 123.7 72.6 + 6.6 79.8 64.9 68.9 62.8 63.3\nViT-L/14 [94] 336 576 12 / 768 6.0 EV A-CLIP Merged-2B 333.3 + 123.7 72.6 + 6.6 80.4 65.8 70.9 63.2 63.5\nViT-L/16 [133] 256 256 24 / 1024 40.0 SigLIP WebLI 316.0 + 336.2 78.1 + 19.3 80.5 65.6 70.2 62.5 61.1\nViT-L/16 [133] 384 576 24 / 1024 40.0 + 5.0 SigLIP WebLI 316.3 + 336.2 175.8 + 19.3 82.1 66.8 70.9 63.1 68.7\nViT-G/14 [48] 224 256 32 / 1280 39.0 OpenCLIP LAION-2B 1844.9 + 694.7 473.4 + 48.5 80.1 66.7 69.1 64.6 63.5\nViT-H/14 [60] 336 576 24 / 1024 12.8 + 0.5 + 0.1 CLIPA-v2 DataComp-1B 632.5 + 354.0 363.7 + 9.7 81.8 66.8 72.4 63.7 62.6\nViT-E/14 [94] 224 256 24 / 1024 4.0 EV A-CLIP LAION-2B 4350.6 + 354.0 1117.3 + 23.3 82.0 66.9 72.0 63.6 62.8\nViT-G/14 [60] 336 576 32 / 1280 12.8 + 0.5 + 0.1 CLIPA-v2 DataComp-1B 1845.4 + 672.3 1062.9 + 20.2 83.1 68.4 74.0 64.5 63.1\nSoViT-400M/14 [2] 224 256 27 / 1152 40.0 SigLIP WebLI 428.2 + 449.7 106.2 + 6.6 82.0 68.1 69.5 64.8 66.8\nSoViT-400M/14 [2] 384 729 27 / 1152 40.0 + 5.0 SigLIP WebLI 428.2 + 449.7 302.3 + 26.3 83.1 69.2 72.4 64.6 69.8\nViT-H/14 [27] 224 256 24 / 1024 39.0 OpenCLIP DFN-5B 632.1 + 354.0 162.0 + 23.3 83.4 69.6 69.9 67.5 68.3\nViT-H/14 [27] 378 729 24 / 1024 39.0 + 5.0 OpenCLIP DFN-5B 632.7 + 354.0 460.1 + 23.3 84.4 70.8 72.8 68.5 69.5\nTable 3. Comparison with state-of-the-art models. Our models are only trained on the publicly available DataComp-1B [30]. CLIPA-\nv2 [60] uses an advanced progressive training scheme (from smaller images to larger ones) than the original OpenCLIP [30, 48] scheme that\nwe follow. Other methods that use different settings are marked in gray for reference. Specifically, EV A-CLIP [94] uses EV A weights [28],\nbetter training scheme FLIP [65], and different training datasets [28, 89]. SigLIP [133] employs better sigmoid loss, stronger text encoders,\nand an extremely long schedule on the proprietary WebLI dataset [12] (40B for training and another 5B seen samples for fine-tuning).\n†: ViT-L/14 benefits from more image tokens by using a smaller output stride 14 than 16 that we use. To have the same image tokens,\nwe slightly enlarge the image size ( e.g.,224/14 = 256 /16and336/14 = 384 /16). We note that all compared results are from the\nOpenCLIP-results that are evaluated under the same setting to ensure a fair comparison.\nin the table, ViTamin-L@224 outperforms ViT-L/14@224\nOpenCLIP [48] by +1.6%. However, ViT-L/14 benefits\nfrom more image tokens by using a smaller output stride\n14 than 16 that we use (as benchmarked in the appendix).\nTo have the same image tokens, we slightly enlarge the im-\nage size. As a result, our ViTamin-L@256 surpasses ViT-\nL/14@224 OpenCLIP [48] and CLIPA-v2 [60] by 2.0% and\n1.5%, respectively. After fine-tuning on larger input sizes,\nViTamin-L@384 and ViTamin-L@336 still performs better\nthan ViT-L/14@336 CLIPA-v2 [60] by +1.5% and +1.3%,\nrespectively. Impressively, with only half the parameters,\nour ViTamin-L attains an average of 67.2% performance\nacross 38 datasets, exceeding the larger ViT-H/14 CLIPA-\nv2 model’s performance by +0.4%. Scaling up the text en-\ncoder to match the model size of the image encoder (specif-\nically, ViTamin-L2) notably increases zero-shot ImageNet\naccuracy to 82.1% and average 38 datasets performance\nto 68.1%. Further scaling up the model parameters ( i.e.,\nViTamin-XL) and 40 billion seen samples reaches 82.9%\nzero-shot ImageNet accuracy.\nLocked-Text Tuning: Fig. 4 shows that our LTT im-\nproves our ViTamin-S/-B by a large margin, especially\nwhen data sizes are small. Notably, LTT lifts ViTamin-B to\nthe next scale of model performance, surpassing ViT-L/16\nby +14% in 128M samples and +1.1% in 512M seen sam-\nples. Interestingly, LTT can save 10% training budget for\nViTamin-B as the text tower is completely frozen.\nFigure 4. Locked-text tuning (LTT) . LTT exploits a pretrained\nfrozen text encoder, and effectively boosts the model performance.\nData Quality vs. Model Capacity: The DataComp\nchallenge [30] underscores the role of data filtering for\nVLM, however, using a fixed ViT model. As shown\nin Tab. 4, the leading solution [119] of DataComp challenge\nin ICCV 2023 employed a complicated 24 filtering rules to\nimprove the dataset quality, resulting in +2.3% gain. Sur-\nprisingly, our ViTamin-B improves the performance by a\nhealthy margin of +12.8% accuracy, and locked-text tuning\ncan lift the gain to +23.3%. The result highlights the impor-\ntance to co-design the vision-language dataset and model.\n4.3. New Suite of Downstream Tasks\nThe evaluations so far are mostly on classification/retrieval-\nbased task, highlighting a lack of downstream tasks simi-\nlar to those employed in the ImageNet era. Yet, in con-\ntrast to ImageNet-based vision models where downstream\ntasks mainly involve transfer learning for conventional de-\n12960\ndataset seen IN acc. avg. 38\nimage encoder data filtering size samp. (%) datasets (%)\nleaderboard\nViT-B/32 DataComp [30] 14M 128M 29.7 32.8\nViT-B/32 SIEVE [75] 24M 128M 30.3 (+0.6) 35.3 (+2.5)\nViT-B/32 Top-1 Solution [119] 23M 128M 32.0 (+2.3) 37.1 (+4.3)\nour experiments\nViT-B/32 DataComp [30] 14M 128M 29.4 31.5\nViT-B/16 DataComp [30] 14M 128M 35.8 (+6.4) 34.6 (+3.1)\nViTamin-B DataComp [30] 14M 128M 42.2 (+12.8) 38.3 (+6.8)\nViT-B/16-LTT DataComp [30] 14M 128M 43.6 (+14.2) 41.1 (+9.6)\nViTamin-B-LTT DataComp [30] 14M 128M 52.7 (+23.3) 47.2 (+15.7)\nTable 4. Data quality vs. model capacity. The leaderboard results\nare from ICCV 2023 DataComp challenge medium filtering track.\npretraining OV-COCO [129] OV-LVIS [35]\nimage encoder dataset scheme (APnovel\n50) (AP r)\nViT-L/14 DataComp-1B CLIPA-v2 36.1 32.5\nConvNeXt-L LAION-2B OpenCLIP 36.4 29.1\nViTamin-L DataComp-1B OpenCLIP 37.5 35.6\nTable 5. Open-vocabulary detection. Different image encoders\n(ViT-L/14 by [60], ConvNeXt-L by [48]) are using the F-ViT\nframework [111] in a sliding window manner [125], trained on\nOV-COCO [129] and OV-LVIS [35]. ConvNeXt-L is marked in\ngray due to different pretrained dataset.\npanoptic dataset (PQ) semantic dataset (mIoU)\nimage pretraining ADE Cityscapes MV A-150 A-847 PC-459 PC-59 PAS-21\nencoder dataset scheme [134] [17] [80] [134] [134] [78] [78] [25]\nViT-L/14 DataComp-1B CLIPA-v2 24.6 40.7 16.5 31.8 14.3 18.3 55.1 81.5\nConvNeXt-L LAION-2B OpenCLIP 26.8 44.0 18.3 34.1 14.8 18.2 58.4 81.8\nViTamin-L DataComp-1B OpenCLIP 27.3 44.0 18.2 35.6 16.1 20.4 58.4 83.4\nTable 6. Open-vocabulary segmentation. Different image en-\ncoders (ViT-L/14 by [60], ConvNeXt-L by [48]) are using the FC-\nCLIP framework [124] in a sliding window manner [125], trained\non COCO [66] and zero-shot evaluated on the other datasets.\nConvNeXt-L is marked in gray due to different pretrained dataset.\ntection and segmentation, VLMs excel with zero-shot capa-\nbility and provides feature embeddings that are well-aligned\nacross the vision-language domain. In light of this, we\nintroduce a novel suite of downstream tasks aimed at the\nholistic evaluation of VLMs, including open-vocabulary de-\ntection and segmentation and multi-modal LLM.\nOpen-Vocabulary Detection and Segmentation: To\nexamine how well the trained VLMs can adapt to down-\nstream tasks, we consider two simple yet effective frame-\nworks F-ViT [111] and FC-CLIP [124] which utilize a\nfrozen CLIP backbone for open-vocabulary detection and\nsegmentation, respectively. Specifically, we consider differ-\nent VLMs as plug-in frozen backbones to these frameworks,\nwhile for ViT and ViTamin that may not easily generalize\nto high resolution input, we extract the feature in a sliding\nwindow manner [125], with window size equal to the pre-\ntrain image size, resulting in Sliding F-ViT and Sliding FC-\nCLIP, respectively. Tab. 5 illustrates that ViTamin-L serves\nas a stronger image encoder for open-vocabulary detection,\nsurpassing its ViT-L/14 counterpart by 1.4% and 3.1% on\nOV-COCO and OV-LVIS. Tab. 6 shows that ViTamin-L out-\nperforms ViT-L/14 by 2.6% on average 3 panoptic datasets\nand by 2.6% on average 5 semantic datasets. Notably,\nsurpassing prior art, ViTamin-L sets a new state-of-the-artimage training\nVQAv2\nGQA\nVizWiz\nSQA\nT-VQA\nPOPE\nMME\nMMBench\nMMBCN\nSEED\nLLaV AW\nMM-Vet\nencoder scheme [33] [47] [36] [74] [92] [29] [64] [70] [70] [56] [68] [127]\nViT-L/14 OpenAI 78.5 62.0 50.0 66.8 58.2 85.9 1511 64.3 58.3 58.6 65.4 31.1\nViT-L/14 CLIPA-v2 75.9 60.3 48.8 65.6 55.0 84.9 1396 60.8 54.6 54.6 60.6 28.6\nViTamin-L OpenCLIP 78.4 61.6 51.1 66.9 58.7 84.6 1421 65.4 58.4 57.7 64.5 33.6\nViTamin-L†OpenCLIP 78.9 61.6 55.4 67.6 59.8 85.5 1447 64.5 58.3 57.9 66.1 33.6\nTable 7. Large Multi-modal Model (LMM) performance with\ndifferent VLMs. The results in 1st row originate from LLaVa-\n1.5 paper [67] and are marked in gray due to pretraining on Ope-\nnAI WIT dataset [82] unlike DataComp-1B [30] used by other\nrows. All listed models are trsained following the same settings in\nLLaV A-1.5 [67] with Vicuna-V1.5-7B [16], for a fair comparison.\n†: image size of 384 rather than the default 336.\nperformance across seven benchmarks for open-vocabulary\npanoptic segmentation and semantic segmentation.\nLarge Multi-modal Models: Another key application\nof VLMs lies in their role as vision encoders within LMMs\n[57, 68, 137], as image features in VLMs that is well-\naligned with text, thereby bridging the visual comprehen-\nsion gap for LLMs. Specifically, we consider LLaV A-\n1.5 [67] as the evaluated framework. We follow [67] for all\nexperimental settings, where the image is processed through\na frozen CLIP model and a MLP projector, retaining the im-\nage as visual tokens, which are prepended to a text sequence\nand fed into a frozen Vicuna-v1.5-7B [16]. We run evalua-\ntion on 12 LMM benchmarks following [67], with results in\nTab. 7. It should be noted that while OpenAI-trained ViT-\nL/14 underperforms CLIPAv2-trained counterpart by -3.7%\nImageNet accuracy, it excels remarkably in LLaV A (+4.4%\non VQAv2 and +4.3% on VizWiz). This highlights the need\nfor incorporating a variety of downstream tasks to ensure\na comprehensive evaluation. Surprisingly, simply replac-\ning LLaV A’s image encoder to ViTamin-L can achieve new\nstate-of-the-art across various benchmarks.\n5. Conclusion\nIn this work, we build an evaluation protocols of modern\nvision models in VLM and re-benchmark them under CLIP\nsetting. We examine vision models from four aspects of\ndata scalability, model scalability, feature resolution and hy-\nbrid architecture. The four pillars motivate us to propose\nViTamin, which not only competes favorably with ViT in\nzero-shot ImageNet accuracy and average 38 dataset ac-\ncuracy, but also achieves the state-of-the-art on 22 down-\nstream tasks covering open-vocaburary detection and seg-\nmentation and large multi-modal models. We hope that\nour design practices will drive the development of more ad-\nvanced vision models for VLMs.\nAcknowledgement : We thank Haichao Yu and Zhan-\npeng Zeng for the discussions about DataComp challenge\nand micro-level block design, respectively. This work was\nsupported in part by ONR N00014-23-1-2641.\n12961\nReferences\n[1] Gpt-4v(ision) system card. 2023. 3\n[2] Ibrahim Alabdulmohsin, Xiaohua Zhai, Alexander\nKolesnikov, and Lucas Beyer. Getting vit in shape: Scaling\nlaws for compute-optimal model design. NeurIPS , 2023. 7\n[3] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An-\ntoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur\nMensch, Katherine Millican, Malcolm Reynolds, et al.\nFlamingo: a visual language model for few-shot learning.\nNeurIPS , 2022. 3\n[4] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.\nLayer normalization. arXiv preprint arXiv:1607.06450 ,\n2016. 5, 6\n[5] Andrei Barbu, David Mayo, Julian Alverio, William Luo,\nChristopher Wang, Dan Gutfreund, Josh Tenenbaum, and\nBoris Katz. Objectnet: A large-scale bias-controlled\ndataset for pushing the limits of object recognition models.\nNeurIPS , 2019. 6\n[6] Yonatan Bitton, Nitzan Bitton Guetta, Ron Yosef, Yu-\nval Elovici, Mohit Bansal, Gabriel Stanovsky, and Roy\nSchwartz. Winogavil: Gamified association benchmark to\nchallenge vision-and-language models. NeurIPS , 2022. 6\n[7] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.\nLanguage models are few-shot learners. NeurIPS , 2020. 3\n[8] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,\nKevin Murphy, and Alan Yuille. Semantic image segmenta-\ntion with deep convolutional nets and fully connected crfs.\nInICLR , 2015. 1\n[9] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,\nKevin Murphy, and Alan L Yuille. Deeplab: Semantic im-\nage segmentation with deep convolutional nets, atrous con-\nvolution, and fully connected crfs. TPAMI , 40(4):834–848,\n2017. 1\n[10] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Flo-\nrian Schroff, and Hartwig Adam. Encoder-decoder with\natrous separable convolution for semantic image segmenta-\ntion. In ECCV , 2018. 1\n[11] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna\nVedantam, Saurabh Gupta, Piotr Doll ´ar, and C Lawrence\nZitnick. Microsoft coco captions: Data collection and eval-\nuation server. arXiv preprint arXiv:1504.00325 , 2015. 6\n[12] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergio-\nvanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman,\nAdam Grycner, Basil Mustafa, Lucas Beyer, et al. Pali: A\njointly-scaled multilingual language-image model. arXiv\npreprint arXiv:2209.06794 , 2022. 7\n[13] Xi Chen, Xiao Wang, Lucas Beyer, Alexander Kolesnikov,\nJialin Wu, Paul V oigtlaender, Basil Mustafa, Sebastian\nGoodman, Ibrahim Alabdulmohsin, Piotr Padlewski, et al.\nPali-3 vision language models: Smaller, faster, stronger.\narXiv preprint arXiv:2310.09199 , 2023. 3\n[14] Yinpeng Chen, Xiyang Dai, Dongdong Chen, Mengchen\nLiu, Xiaoyi Dong, Lu Yuan, and Zicheng Liu. Mobile-\nformer: Bridging mobilenet and transformer. In CVPR ,\n2022. 2[15] Bowen Cheng, Maxwell D Collins, Yukun Zhu, Ting Liu,\nThomas S Huang, Hartwig Adam, and Liang-Chieh Chen.\nPanoptic-DeepLab: A simple, strong, and fast baseline for\nbottom-up panoptic segmentation. In CVPR , 2020. 1\n[16] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhang-\nhao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang,\nYonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and\nEric P. Xing. Vicuna: An open-source chatbot impressing\ngpt-4 with 90%* chatgpt quality, 2023. 8\n[17] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo\nRehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe\nFranke, Stefan Roth, and Bernt Schiele. The cityscapes\ndataset for semantic urban scene understanding. In CVPR ,\n2016. 8\n[18] Zihang Dai, Hanxiao Liu, Quoc V Le, and Mingxing Tan.\nCoatnet: Marrying convolution and attention for all data\nsizes. NeurIPS , 2021. 1, 2, 3, 5\n[19] Yann N Dauphin, Angela Fan, Michael Auli, and David\nGrangier. Language modeling with gated convolutional net-\nworks. In ICML , 2017. 5\n[20] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr\nPadlewski, Jonathan Heek, Justin Gilmer, Andreas Peter\nSteiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdul-\nmohsin, et al. Scaling vision transformers to 22 billion pa-\nrameters. In ICML , 2023. 2\n[21] Xueqing Deng, Qihang Yu, Peng Wang, Xiaohui Shen, and\nLiang-Chieh Chen. Coconut: Modernizing coco segmenta-\ntion. In CVPR , 2024. 2\n[22] Zheng Ding, Jieke Wang, and Zhuowen Tu. Open-\nvocabulary universal image segmentation with maskclip. In\nICML , 2023. 2\n[23] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold,\nSylvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. In ICLR , 2021. 1,\n2, 3, 5, 6\n[24] St ´ephane d’Ascoli, Hugo Touvron, Matthew L Leavitt,\nAri S Morcos, Giulio Biroli, and Levent Sagun. Convit:\nImproving vision transformers with soft convolutional in-\nductive biases. In ICML , 2021. 2\n[25] Mark Everingham, Luc Van Gool, Christopher KI\nWilliams, John Winn, and Andrew Zisserman. The pascal\nvisual object classes (voc) challenge. IJCV , 88:303–338,\n2010. 8\n[26] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li,\nZhicheng Yan, Jitendra Malik, and Christoph Feichten-\nhofer. Multiscale vision transformers. In ICCV , 2021. 2\n[27] Alex Fang, Albin Madappally Jose, Amit Jain, Ludwig\nSchmidt, Alexander Toshev, and Vaishaal Shankar. Data\nfiltering networks. arXiv preprint arXiv:2309.17425 , 2023.\n7\n[28] Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu,\nXinggang Wang, Tiejun Huang, Xinlong Wang, and Yue\nCao. Eva: Exploring the limits of masked visual represen-\ntation learning at scale. In CVPR , 2023. 7\n[29] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin,\nMengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui\n12962\nYang, Xiawu Zheng, et al. Mme: A comprehensive eval-\nuation benchmark for multimodal large language models.\narXiv preprint arXiv:2306.13394 , 2023. 8\n[30] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan\nHayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten,\nMitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Dat-\nacomp: In search of the next generation of multimodal\ndatasets. arXiv preprint arXiv:2304.14108 , 2023. 1, 2, 3,\n6, 7, 8\n[31] Golnaz Ghiasi, Xiuye Gu, Yin Cui, and Tsung-Yi Lin. Scal-\ning open-vocabulary image segmentation with image-level\nlabels. In ECCV , 2022. 2, 3\n[32] Ross Girshick. Fast r-cnn. In CVPR , 2015. 1\n[33] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv\nBatra, and Devi Parikh. Making the v in vqa matter: El-\nevating the role of image understanding in visual question\nanswering. In CVPR , 2017. 8\n[34] Benjamin Graham, Alaaeldin El-Nouby, Hugo Touvron,\nPierre Stock, Armand Joulin, Herv ´e J´egou, and Matthijs\nDouze. Levit: a vision transformer in convnet’s clothing\nfor faster inference. In CVPR , 2021. 2\n[35] Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui.\nOpen-vocabulary object detection via vision and language\nknowledge distillation. In ICLR , 2022. 8\n[36] Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi\nLin, Kristen Grauman, Jiebo Luo, and Jeffrey P Bigham.\nVizwiz grand challenge: Answering visual questions from\nblind people. In CVPR , 2018. 8\n[37] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In CVPR ,\n2016. 1, 2, 3, 5\n[38] Kaiming He, Georgia Gkioxari, Piotr Doll ´ar, and Ross Gir-\nshick. Mask r-cnn. In CVPR , 2017. 1\n[39] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kada-\nvath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu,\nSamyak Parajuli, Mike Guo, et al. The many faces of ro-\nbustness: A critical analysis of out-of-distribution general-\nization. In ICCV , 2021. 6\n[40] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Stein-\nhardt, and Dawn Song. Natural adversarial examples. In\nCVPR , 2021. 6\n[41] Byeongho Heo, Sangdoo Yun, Dongyoon Han, Sanghyuk\nChun, Junsuk Choe, and Seong Joon Oh. Rethinking spatial\ndimensions of vision transformers. In ICCV , 2021. 2\n[42] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distill-\ning the knowledge in a neural network. arXiv preprint\narXiv:1503.02531 , 2015. 6\n[43] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh\nChen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu,\nRuoming Pang, Vijay Vasudevan, et al. Searching for mo-\nbilenetv3. In ICCV , 2019. 5\n[44] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry\nKalenichenko, Weijun Wang, Tobias Weyand, Marco An-\ndreetto, and Hartwig Adam. Mobilenets: Efficient convolu-\ntional neural networks for mobile vision applications. arXiv\npreprint arXiv:1704.04861 , 2017. 2, 5\n[45] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation\nnetworks. In CVPR , 2018. 5[46] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil-\nian Q Weinberger. Densely connected convolutional net-\nworks. In CVPR , 2017. 1, 2\n[47] Drew A Hudson and Christopher D Manning. Gqa: A new\ndataset for real-world visual reasoning and compositional\nquestion answering. In CVPR , 2019. 8\n[48] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade\nGordon, Nicholas Carlini, Rohan Taori, Achal Dave,\nVaishaal Shankar, Hongseok Namkoong, John Miller, Han-\nnaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Open-\nclip, 2021. 2, 3, 6, 7, 8\n[49] Sergey Ioffe and Christian Szegedy. Batch normalization:\nAccelerating deep network training by reducing internal co-\nvariate shift. In ICML , 2015. 5\n[50] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana\nParekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li,\nand Tom Duerig. Scaling up visual and vision-language\nrepresentation learning with noisy text supervision. In\nICML , 2021. 1, 2, 3\n[51] Alexander Kirillov, Kaiming He, Ross Girshick, Carsten\nRother, and Piotr Doll ´ar. Panoptic segmentation. In CVPR ,\n2019. 1\n[52] Pang Wei Koh, Shiori Sagawa, Henrik Marklund,\nSang Michael Xie, Marvin Zhang, Akshay Balsubramani,\nWeihua Hu, Michihiro Yasunaga, Richard Lanas Phillips,\nIrena Gao, et al. Wilds: A benchmark of in-the-wild distri-\nbution shifts. In ICML , 2021. 6\n[53] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.\nImagenet classification with deep convolutional neural net-\nworks. NeurIPS , 2012. 1, 2\n[54] Weicheng Kuo, Yin Cui, Xiuye Gu, AJ Piergiovanni, and\nAnelia Angelova. F-vlm: Open-vocabulary object detection\nupon frozen vision and language models. In ICLR , 2023. 2,\n3\n[55] Yann LeCun, L ´eon Bottou, Yoshua Bengio, and Patrick\nHaffner. Gradient-based learning applied to document\nrecognition. Proceedings of the IEEE , 86(11):2278–2324,\n1998. 1\n[56] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yix-\niao Ge, and Ying Shan. Seed-bench: Benchmarking multi-\nmodal llms with generative comprehension. arXiv preprint\narXiv:2307.16125 , 2023. 8\n[57] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.\nBlip: Bootstrapping language-image pre-training for uni-\nfied vision-language understanding and generation. In\nICML , 2022. 2, 3, 8\n[58] Kunchang Li, Yali Wang, Junhao Zhang, Peng Gao, Guan-\nglu Song, Yu Liu, Hongsheng Li, and Yu Qiao. Uniformer:\nUnifying convolution and self-attention for visual recogni-\ntion. TPAMI , 2023. 2\n[59] Runze Li, Dahun Kim, Bir Bhanu, and Weicheng Kuo. Re-\nclip: Resource-efficient clip by training with small images.\narXiv preprint arXiv:2304.06028 , 2023. 3\n[60] Xianhang Li, Zeyu Wang, and Cihang Xie. Scaling clip\ntraining with 81.1% zero-shot imagenet accuracy within a\n$10,000 budget; an extra $4,000 unlocks 81.8% accuracy.\narXiv preprint arXiv:2306.15658 , 2023. 1, 2, 7, 8\n12963\n[61] Xianhang Li, Zeyu Wang, and Cihang Xie. An inverse scal-\ning law for clip training. NeurIPS , 2023. 2, 3\n[62] Yanghao Li, Chao-Yuan Wu, Haoqi Fan, Karttikeya Man-\ngalam, Bo Xiong, Jitendra Malik, and Christoph Feichten-\nhofer. Mvitv2: Improved multiscale vision transformers for\nclassification and detection. In CVPR , 2022. 2\n[63] Yanyu Li, Geng Yuan, Yang Wen, Ju Hu, Georgios Evan-\ngelidis, Sergey Tulyakov, Yanzhi Wang, and Jian Ren.\nEfficientformer: Vision transformers at mobilenet speed.\nNeurIPS , 2022. 2\n[64] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin\nZhao, and Ji-Rong Wen. Evaluating object hallucina-\ntion in large vision-language models. arXiv preprint\narXiv:2305.10355 , 2023. 8\n[65] Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feicht-\nenhofer, and Kaiming He. Scaling language-image pre-\ntraining via masking. In CVPR , 2023. 3, 7\n[66] Tsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and\nC Lawrence Zitnick. Microsoft coco: Common objects in\ncontext. In ECCV , 2014. 1, 8\n[67] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\nImproved baselines with visual instruction tuning. arXiv\npreprint arXiv:2310.03744 , 2023. 1, 2, 3, 6, 8\n[68] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee. Visual instruction tuning. NeurIPS , 2023. 2, 3, 8\n[69] Haotian Liu, Kilho Son, Jianwei Yang, Ce Liu, Jianfeng\nGao, Yong Jae Lee, and Chunyuan Li. Learning customized\nvisual models with retrieval-augmented knowledge. In\nCVPR , 2023. 3\n[70] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li,\nSongyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang,\nConghui He, Ziwei Liu, et al. Mmbench: Is your multi-\nmodal model an all-around player? arXiv preprint\narXiv:2307.06281 , 2023. 8\n[71] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\nHierarchical vision transformer using shifted windows. In\nICCV , 2021. 1, 2\n[72] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feicht-\nenhofer, Trevor Darrell, and Saining Xie. A convnet for the\n2020s. In CVPR , 2022. 1, 2, 3\n[73] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully\nconvolutional networks for semantic segmentation. In\nCVPR , 2015. 1\n[74] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-\nWei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark,\nand Ashwin Kalyan. Learn to explain: Multimodal rea-\nsoning via thought chains for science question answering.\nNeurIPS , 2022. 8\n[75] Anas Mahmoud, Mostafa Elhoushi, Amro Abbas, Yu Yang,\nNewsha Ardalani, Hugh Leather, and Ari S Morcos. Sieve:\nMultimodal dataset pruning using image captioning mod-\nels.arXiv preprint arXiv:2310.02110 , 2023. 8\n[76] Sachin Mehta and Mohammad Rastegari. Mobilevit: light-\nweight, general-purpose, and mobile-friendly vision trans-\nformer. arXiv preprint arXiv:2110.02178 , 2021. 2[77] Matthias Minderer, Alexey Gritsenko, Austin Stone,\nMaxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy,\nAravindh Mahendran, Anurag Arnab, Mostafa Dehghani,\nZhuoran Shen, et al. Simple open-vocabulary object detec-\ntion. In ECCV , 2022. 2, 3\n[78] Roozbeh Mottaghi, Xianjie Chen, Xiaobai Liu, Nam-Gyu\nCho, Seong-Whan Lee, Sanja Fidler, Raquel Urtasun, and\nAlan Yuille. The role of context for object detection and\nsemantic segmentation in the wild. In CVPR , 2014. 8\n[79] Norman Mu, Alexander Kirillov, David Wagner, and Sain-\ning Xie. Slip: Self-supervision meets language-image pre-\ntraining. In ECCV , 2022. 3\n[80] Gerhard Neuhold, Tobias Ollmann, Samuel Rota Bulo, and\nPeter Kontschieder. The mapillary vistas dataset for seman-\ntic understanding of street scenes. In ICCV , 2017. 8\n[81] OpenAI. Gpt-4 technical report. arXiv preprint\narXiv:2303.08774 , 2023. 2, 3\n[82] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In ICML , 2021. 1, 2, 3, 8\n[83] Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan\nBello, Anselm Levskaya, and Jon Shlens. Stand-alone self-\nattention in vision models. NeurIPS , 2019. 2\n[84] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and\nVaishaal Shankar. Do imagenet classifiers generalize to im-\nagenet? In ICML , 2019. 6\n[85] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj ¨orn Ommer. High-resolution image\nsynthesis with latent diffusion models. In CVPR , 2022. 2,\n3\n[86] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause,\nSanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej\nKarpathy, Aditya Khosla, Michael Bernstein, et al. Ima-\ngenet large scale visual recognition challenge. IJCV , 115\n(3):211–252, 2015. 1, 2, 6\n[87] Shiori Sagawa, Pang Wei Koh, Tony Lee, Irena Gao,\nSang Michael Xie, Kendrick Shen, Ananya Kumar, Weihua\nHu, Michihiro Yasunaga, Henrik Marklund, et al. Extend-\ning the wilds benchmark for unsupervised adaptation. In\nICLR , 2022. 6\n[88] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey\nZhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted\nresiduals and linear bottlenecks. In CVPR , 2018. 2, 5, 6\n[89] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon, Ross Wightman, Mehdi Cherti, Theo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, et al. Laion-5b: An open large-scale dataset for train-\ning next generation image-text models. NeurIPS , 2022. 2,\n3, 7\n[90] Noam Shazeer. Glu variants improve transformer. arXiv\npreprint arXiv:2002.05202 , 2020. 5, 6\n[91] Karen Simonyan and Andrew Zisserman. Very deep convo-\nlutional networks for large-scale image recognition. arXiv\npreprint arXiv:1409.1556 , 2014. 2\n12964\n[92] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang,\nXinlei Chen, Dhruv Batra, Devi Parikh, and Marcus\nRohrbach. Towards vqa models that can read. In CVPR ,\n2019. 8\n[93] Aravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon\nShlens, Pieter Abbeel, and Ashish Vaswani. Bottleneck\ntransformers for visual recognition. In CVPR , 2021. 2\n[94] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue\nCao. Eva-clip: Improved training techniques for clip at\nscale. arXiv preprint arXiv:2303.15389 , 2023. 2, 3, 7\n[95] Shuyang Sun, Weijun Wang, Andrew Howard, Qihang Yu,\nPhilip Torr, and Liang-Chieh Chen. Remax: Relaxing for\nbetter training on efficient panoptic segmentation. NeurIPS ,\n2024. 1\n[96] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,\nScott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent\nVanhoucke, and Andrew Rabinovich. Going deeper with\nconvolutions. In CVPR , 2015. 2, 5\n[97] Mingxing Tan and Quoc Le. Efficientnet: Rethinking\nmodel scaling for convolutional neural networks. In ICML ,\n2019. 2\n[98] Mingxing Tan and Quoc Le. Efficientnetv2: Smaller mod-\nels and faster training. In ICML , 2021. 2\n[99] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herv ´e J´egou. Train-\ning data-efficient image transformers & distillation through\nattention. In ICML , 2021. 2\n[100] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles,\nGabriel Synnaeve, and Herv ´e J´egou. Going deeper with\nimage transformers. In ICCV , 2021. 5\n[101] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth ´ee Lacroix, Bap-\ntiste Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar,\net al. Llama: Open and efficient foundation language mod-\nels.arXiv preprint arXiv:2302.13971 , 2023. 3\n[102] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,\nAmjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.\nLlama 2: Open foundation and fine-tuned chat models.\narXiv preprint arXiv:2307.09288 , 2023. 2\n[103] Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang,\nPeyman Milanfar, Alan Bovik, and Yinxiao Li. Maxvit:\nMulti-axis vision transformer. In ECCV , 2022. 1, 2\n[104] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. NeurIPS ,\n2017. 1, 2, 5, 6\n[105] Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P\nXing. Learning robust global representations by penalizing\nlocal predictive power. NeurIPS , 2019. 6\n[106] Huiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam,\nAlan Yuille, and Liang-Chieh Chen. Axial-DeepLab:\nStand-Alone Axial-Attention for Panoptic Segmentation. In\nECCV , 2020. 2, 3\n[107] Huiyu Wang, Yukun Zhu, Hartwig Adam, Alan Yuille, and\nLiang-Chieh Chen. Max-deeplab: End-to-end panoptic\nsegmentation with mask transformers. In CVPR , 2021. 1, 3[108] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao\nSong, Ding Liang, Tong Lu, Ping Luo, and Ling Shao.\nPyramid vision transformer: A versatile backbone for dense\nprediction without convolutions. In ICCV , 2021. 1, 2\n[109] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu,\nXiyang Dai, Lu Yuan, and Lei Zhang. Cvt: Introducing\nconvolutions to vision transformers. In ICCV , 2021. 2\n[110] Kan Wu, Houwen Peng, Zhenghong Zhou, Bin Xiao,\nMengchen Liu, Lu Yuan, Hong Xuan, Michael Valenzuela,\nXi Stephen Chen, Xinggang Wang, et al. Tinyclip: Clip\ndistillation via affinity mimicking and weight inheritance.\nInICCV , 2023. 3\n[111] Size Wu, Wenwei Zhang, Lumin Xu, Sheng Jin, Xiangtai\nLi, Wentao Liu, and Chen Change Loy. Clipself: Vision\ntransformer distills itself for open-vocabulary dense predic-\ntion. arXiv preprint arXiv:2310.01403 , 2023. 1, 6, 8\n[112] Tete Xiao, Mannat Singh, Eric Mintun, Trevor Darrell, Pi-\notr Doll ´ar, and Ross Girshick. Early convolutions help\ntransformers see better. NeurIPS , 2021. 2\n[113] Saining Xie, Ross Girshick, Piotr Doll ´ar, Zhuowen Tu, and\nKaiming He. Aggregated residual transformations for deep\nneural networks. In ICCV , 2017. 2\n[114] Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiao-\nlong Wang, and Shalini De Mello. Open-vocabulary panop-\ntic segmentation with text-to-image diffusion models. In\nCVPR , 2023. 2\n[115] Mengde Xu, Zheng Zhang, Fangyun Wei, Yutong Lin,\nYue Cao, Han Hu, and Xiang Bai. A simple baseline for\nzero-shot semantic segmentation with pre-trained vision-\nlanguage model. In ECCV , 2022. 2\n[116] Chenglin Yang, Siyuan Qiao, Qihang Yu, Xiaoding Yuan,\nYukun Zhu, Alan Yuille, Hartwig Adam, and Liang-Chieh\nChen. Moat: Alternating mobile convolution and attention\nbrings strong vision models. In ICLR , 2023. 1, 2\n[117] Xuan Yang, Liangzhe Yuan, Kimberly Wilber, Astuti\nSharma, Xiuye Gu, Siyuan Qiao, Stephanie Debats,\nHuisheng Wang, Hartwig Adam, Mikhail Sirotenko, and\nLiang-Chieh Chen. Polymax: General dense prediction\nwith mask transformer. arXiv preprint arXiv:2311.05770 ,\n2024. 1\n[118] Peter Young, Alice Lai, Micah Hodosh, and Julia Hock-\nenmaier. From image descriptions to visual denotations:\nNew similarity metrics for semantic inference over event\ndescriptions. Transactions of the Association for Computa-\ntional Linguistics , 2:67–78, 2014. 6\n[119] Haichao Yu, Yu Tian, Sateesh Kumar, Linjie Yang, and\nHeng Wang. The devil is in the details: A deep dive\ninto the rabbit hole of data filtering. arXiv preprint\narXiv:2309.15954 , 2023. 2, 7, 8\n[120] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung,\nMojtaba Seyedhosseini, and Yonghui Wu. Coca: Con-\ntrastive captioners are image-text foundation models. arXiv\npreprint arXiv:2205.01917 , 2022. 3\n[121] Qihang Yu, Yingda Xia, Yutong Bai, Yongyi Lu, Alan L\nYuille, and Wei Shen. Glance-and-gaze vision transformer.\nNeurIPS , 2021. 1\n[122] Qihang Yu, Huiyu Wang, Dahun Kim, Siyuan Qiao,\nMaxwell Collins, Yukun Zhu, Hartwig Adam, Alan Yuille,\n12965\nand Liang-Chieh Chen. Cmt-deeplab: Clustering mask\ntransformers for panoptic segmentation. In CVPR , 2022.\n1\n[123] Qihang Yu, Huiyu Wang, Siyuan Qiao, Maxwell Collins,\nYukun Zhu, Hartwig Adam, Alan Yuille, and Liang-Chieh\nChen. k-means Mask Transformer. In ECCV , 2022. 1\n[124] Qihang Yu, Ju He, Xueqing Deng, Xiaohui Shen, and\nLiang-Chieh Chen. Convolutions die hard: Open-\nvocabulary segmentation with single frozen convolutional\nclip. NeurIPS , 2023. 1, 2, 3, 6, 8\n[125] Qihang Yu, Xiaohui Shen, and Liang-Chieh Chen. Towards\nopen-ended visual recognition with large language model.\narXiv preprint arXiv:2311.08400 , 2023. 8\n[126] Weihao Yu, Mi Luo, Pan Zhou, Chenyang Si, Yichen\nZhou, Xinchao Wang, Jiashi Feng, and Shuicheng Yan.\nMetaformer is actually what you need for vision. In CVPR ,\n2022. 2\n[127] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang,\nKevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang.\nMm-vet: Evaluating large multimodal models for inte-\ngrated capabilities. arXiv preprint arXiv:2308.02490 , 2023.\n8\n[128] Xiaowei Yu, Yao Xue, Lu Zhang, Li Wang, Tianming Liu,\nand Dajiang Zhu. Exploring the influence of informa-\ntion entropy change in learning systems. arXiv preprint\narXiv:2309.10625 , 2023. 2\n[129] Alireza Zareian, Kevin Dela Rosa, Derek Hao Hu, and\nShih-Fu Chang. Open-vocabulary object detection using\ncaptions. In CVPR , 2021. 2, 8\n[130] Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov,\nPierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djo-\nlonga, Andre Susano Pinto, Maxim Neumann, Alexey\nDosovitskiy, et al. The visual task adaptation benchmark.\n2019. 6\n[131] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and\nLucas Beyer. Scaling vision transformers. In CVPR , 2022.\n5\n[132] Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner,\nDaniel Keysers, Alexander Kolesnikov, and Lucas Beyer.\nLit: Zero-shot transfer with locked-image text tuning. In\nCVPR , 2022. 3\n[133] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and\nLucas Beyer. Sigmoid loss for language image pre-training.\nInICCV , 2023. 3, 7\n[134] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela\nBarriuso, and Antonio Torralba. Scene parsing through\nade20k dataset. In CVPR , 2017. 8\n[135] Chong Zhou, Chen Change Loy, and Bo Dai. Extract free\ndense labels from clip. In ECCV , 2022. 2\n[136] Daquan Zhou, Bingyi Kang, Xiaojie Jin, Linjie Yang, Xi-\naochen Lian, Zihang Jiang, Qibin Hou, and Jiashi Feng.\nDeepvit: Towards deeper vision transformer. arXiv preprint\narXiv:2103.11886 , 2021. 2, 5\n[137] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-\nhamed Elhoseiny. Minigpt-4: Enhancing vision-language\nunderstanding with advanced large language models. arXiv\npreprint arXiv:2304.10592 , 2023. 2, 3, 8\n12966'}, 'dist': 0.9286905527114868}
Result 26: {'text': 'data', 'metadata': {'created_at': 1736932570, 'modified_at': 1736932570, 'owner': 'sayandeep', 'path': 'RAG_CONTENT/conferences/cvpr_ocr/Cazenavette_FakeInversion_Learning_to_Detect_Images_from_Unseen_Text-to-Image_Models_by_CVPR_2024_paper.txt', 'size': 50055, 'seen_at': 1737191136, 'data': 'FakeInversion: Learning to Detect Images from Unseen\nText-to-Image Models by Inverting Stable Diffusion\nGeorge Cazenavette1* Avneesh Sud2Thomas Leung2Ben Usman2\n1Massachusetts Institute of Technology2Google Research\nfake-inversion.github.io\nSynRIS – New Evaluation Protocol \nFake Images from DALL·E 3, Imagen, etc. \nReal Images with matching themes and styles  from Internet SD v1.5 \n Inversion \nReal vs Fake \nDetector Input Image FakeInversion – New GenAI Detector Training Set \nFake Images: Stable Diffusion \nReal Images: LAION \nReconstruction \nSD v1.5 \nDenoising Inverted Noise Map \nFigure 1. (left) We propose a new synthetic image detectornew synthetic image detectornew synthetic image detectornew synthetic image detectornew synthetic image detectornew synthetic image detectornew synthetic image detectornew synthetic image detectornew synthetic image detectornew synthetic image detectornew synthetic image detectornew synthetic image detectornew synthetic image detectornew synthetic image detectornew synthetic image detectornew synthetic image detectornew synthetic image detector that uses two additional input signals derived from a fixed pre-trained Stable\nDiffusion [ 45]: an inverted latent noise map and the reconstructed input image. (middle) Our detector is trained using fake images generated\nusing Stable Diffusion and real LAION images. It achieves state-of-the-art generalization performancestate-of-the-art generalization performancestate-of-the-art generalization performancestate-of-the-art generalization performancestate-of-the-art generalization performancestate-of-the-art generalization performancestate-of-the-art generalization performancestate-of-the-art generalization performancestate-of-the-art generalization performancestate-of-the-art generalization performancestate-of-the-art generalization performancestate-of-the-art generalization performancestate-of-the-art generalization performancestate-of-the-art generalization performancestate-of-the-art generalization performancestate-of-the-art generalization performancestate-of-the-art generalization performance in detecting unseen text-to-image\ngenerators. (right) To ensure that the performance evaluation does not favor detectors that are biased towards particular themes or styles,\nwe introduce a new thematically and stylistically aligned evaluation benchmarknew thematically and stylistically aligned evaluation benchmarknew thematically and stylistically aligned evaluation benchmarknew thematically and stylistically aligned evaluation benchmarknew thematically and stylistically aligned evaluation benchmarknew thematically and stylistically aligned evaluation benchmarknew thematically and stylistically aligned evaluation benchmarknew thematically and stylistically aligned evaluation benchmarknew thematically and stylistically aligned evaluation benchmarknew thematically and stylistically aligned evaluation benchmarknew thematically and stylistically aligned evaluation benchmarknew thematically and stylistically aligned evaluation benchmarknew thematically and stylistically aligned evaluation benchmarknew thematically and stylistically aligned evaluation benchmarknew thematically and stylistically aligned evaluation benchmarknew thematically and stylistically aligned evaluation benchmarknew thematically and stylistically aligned evaluation benchmark – we measure detector’s ability to discriminate fake images\n(e.g. DALL·E 3, Imagen) from real images with matching content and style found on the Internet using reverse image search (RIS).\nAbstract\nDue to the high potential for abuse of GenAI systems, the\ntask of detecting synthetic images has recently become of\ngreat interest to the research community. Unfortunately, ex-\nisting image-space detectors quickly become obsolete as new\nhigh-fidelity text-to-image models are developed at blinding\nspeed. In this work, we propose a new synthetic image detec-\ntor that uses features obtained by inverting an open-source\npre-trained Stable Diffusion model. We show that these inver-\nsion features enable our detector to generalize well to unseen\ngenerators of high visual fidelity (e.g., DALL ·E 3) even when\nthe detector is trained only on lower fidelity fake images\ngenerated via Stable Diffusion. This detector achieves new\nstate-of-the-art across multiple training and evaluation se-\ntups. Moreover, we introduce a new challenging evaluation\nprotocol that uses reverse image search to mitigate stylistic\nand thematic biases in the detector evaluation. We show that\nthe resulting evaluation scores align well with detectors’ in-\nthe-wild performance, and release these datasets as public\nbenchmarks for future research.\n*Work done during an internship at Google Research.1. Introduction\nRecent advances in text-to-image modeling have made it\neasier than ever to generate harmful or misrepresentative\ncontent at scale. Moreover, new versions of most photoreal-\nistic commercial models are being continuously updated and\nreleased behind closed APIs, making it harder to keep fake\nimage detectors up to date. In this work, we make significant\nstrides towards building a GenAI detector that can reliably\nidentify images from unseen photorealistic text-to-image\nmodels. Specifically, we propose a model that can be trained\nusing fake images only from Stable Diffusion (SD) [ 45] and\nreliably detect images generated by recent open (Kandin-\nsky [ 51], W ¨uerstchen [ 39], PixArt- α[16], etc.) and closed-\nsource text-to-image models (Imagen [ 46], Midjourney [ 2],\nDALL·E 3 [12], etc.) of significantly higher visual fidelity.\nExisting methods [ 17,37,54] focus primarily on detect-\ning traces left by convolutional generators in a way that is ro-\nbust to re-compression, resizing and other in-the-wild trans-\nformations. While these methods worked well for GANs and\nearly diffusion models, we show that they, unfortunately, fail\nto generalize well to current photorealistic generative models,\neven when re-trained using better data. Recent diffusion de-\ntectors that rely on CLIP embeddings [ 37] or inversions [ 55]\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n10759\nfail to generalize to challenging benchmarks. Drawing in-\nspiration from recent works that showed that GANs tend to\n“omit hard objects” [ 11] and that text-to-image models lean\ntowards “easily captionable” images [ 50], in this paper, we\nfocus on detecting fake images by analyzing internal repre-\nsentations of an existing off-the-shelf text-to-image model.\nIn this paper, we introduce a new synthetic imagesynthetic imagesynthetic imagesynthetic imagesynthetic imagesynthetic imagesynthetic imagesynthetic imagesynthetic imagesynthetic imagesynthetic imagesynthetic imagesynthetic imagesynthetic imagesynthetic imagesynthetic imagesynthetic image\ndetection methoddetection methoddetection methoddetection methoddetection methoddetection methoddetection methoddetection methoddetection methoddetection methoddetection methoddetection methoddetection methoddetection methoddetection methoddetection methoddetection method :FakeInversion . Our method uses features\nextracted from a lower-fidelity open-source text-to-image\nmodel (Stable Diffusion [ 45]) to detect images generated\nby unseen text-to-image generators. Specifically, our model\ntakes as input 1) the original image, 2) the approximate noise\nmap recovered via text-conditioned DDIM [ 52] inversion\nwith Stable Diffusion (SD), and 3) the reconstruction ob-\ntained by “denoising” the approximate noise map (Figure 1).\nWe show that these additional signals significantly improve\nthe performance of the resulting detector on unseen new pro-\nprietary and open-source photorealistic text-to-image mod-\nels, attaining a new state-of-the-art. We also provide an in-\ntuitive justification for why a diffusion detector needs such\nfeatures to generalize well to unseen diffusion generators.\nTo deploy a synthetic image detector at scale, we need\nto make sure that it is not relying on content signals such as\nthe presence of specific objects or styles in the image. If left\nunmitigated, such bias towards particular themes or styles\nwould disproportionately marginalize particular groups when\napplied to detecting healthcare misinfo [ 1] or forged art [ 3] at\nscale. Unfortunately, existing evaluation protocols that mea-\nsure a detector’s ability to differentiate between real and fake\nimages drawn from very visually and thematically different\ndistributions can not be used to test for the presence of such\nbias in the detector. For example, evaluating a fake detector\nusing real COCO [ 33] images and fake images generated by\nDALL ·E 3 [ 12] could favour a detector that assigns higher\nfakeness score to digital art, since COCO contains mostly\nnatural images. To circumvent these issues, we propose a\nnew evaluation protocolnew evaluation protocolnew evaluation protocolnew evaluation protocolnew evaluation protocolnew evaluation protocolnew evaluation protocolnew evaluation protocolnew evaluation protocolnew evaluation protocolnew evaluation protocolnew evaluation protocolnew evaluation protocolnew evaluation protocolnew evaluation protocolnew evaluation protocolnew evaluation protocol :SynRIS . For each set of synthetic\nimages, we evaluate a given detector against a set of real\nimages obtained by applying reverse image search (Figure 1)\nto given fake images – the resulting evaluation does not favor\nmodels biased towards any particular topic, theme, or style.\nWe show that the proposed evaluation protocol is more reli-\nable at evaluating the quality of the synthetic image detector,\nespecially when applied to closed-source text-to-image mod-\nels. We will release our evaluation benchmark (including\ndatasets) for future research.\nTo summarize our contributions: 1) we introduce a new\nsynthetic image detector that uses text-conditioned inversion\nmaps extracted from Stable Diffusion; 2) we show that this\nadditional feature improves the detector’s ability to detect\nimages generated by unseen text-to-image models, achieving\nnew state-of-the-art; 3) we propose a new challenging evalua-\ntion protocol that uses reverse image search to ensure that theclassifier is not biased towards any particular theme or style;\n4) we verify that this evaluation protocol reliably measures\ndetector generalization to closed text-to-image models; and\n5) we release our challenging benchmark for future research.\n2. Related Work\nIn this section we first give a brief overview of the state-of-\nart in image-space detectors, then we discuss how recent\nworks attempt to detect semantic inconsistencies in gener-\nated images, and finally discuss how our evaluation protocol\ncompares to evaluation protocols used in prior work.\nArtifact Detectors. Wang et al. [54] were among the first\nto show that a CNN detector (CNNDet) generalized well\nfrom more powerful GANs ( e.g. ProGAN) to less powerful\nones. Soon after, Chai et al. [14] extended this idea with a\nfully-convolutional network that classified individual image\npatches, Ju et al. [27] explored fusing global and local image\nfeatures, Corvi et al. [17] explored better augmentation and\ndownsampling strategies, Zhang et al. [61] and Frank et al.\n[20] explored artifacts in the spectrum of GAN-generated\nimages, and Marra et al. [35] explored GAN fingerprinting.\nGeneration Inconsistencies. Several works have focused on\nunderstanding the semantic properties of generated images.\nFor example, Bau et al. [11] showed that GANs avoid gen-\nerating “hard objects” such as mirrors and TVs – that both\nhumans and discriminators fail to notice missing. Recently,\nOjha et al. [37] showed that image CLIP [ 42] embeddings\nare highly predictive of whether an image is fake, and Sha\net al. [50] showed that images generated using text-to-image\nmodels tend to have higher CLIP similarity to their automat-\nically inferred captions, suggesting that images generated by\ntext-to-image models can often be described more fully by\nshort text captions compared to images naturally occurring\non the web. Inspired by these works, we also focus on proper-\nties of images beyond their low-level convolutional traces by\nexamining internal representations of diffusion models ob-\ntained using DDIM inversion [ 52]. A concurrent work [ 55]\nfound that DDIM image reconstruction residuals are predic-\ntive of whether an image is fake. In this paper we justify\nwhy image-space residuals are insufficient, and empirically\nverify that a detector that uses internal representations of a\ndiffusion model generalizes better. We evaluate our model\nagainst the official DIRE checkpoint and perform an ablation\nusing only reconstruction residuals.\nEvaluation Protocols. Given that internal representations\nof diffusion models lack the low-level features necessary to\nperform generator trace detection, we need a way to ensure\nthat the learned classifier does not overfit to particular\nobjects or styles. Unfortunately, prior works focus either\non open-source models with known training sets but lower\nvisual fidelity or use dataset pairs of real and in-the-wild\nfake images that are too different both in style and content\n10760\nFake (Stable Diffusion) \n Real (LAION) Fake (ProGAN) \n Real (LSUN) Training Data \nProGAN+LSUN Stable Diffusion+LAION \nDALL ·E 3\nFake \n Real (RIS) \nPixArt-α \nFake \n Real (RIS) \n Fake \n Real (RIS) \n Fake \n Real (RIS) Midjourney Playground 2.5 Evaluation Data (SynRIS) \n⇒ ⇒ ⇒ ⇒\nFigure 2. Training and evaluation datasets. We train all methods on two training sets (top) : ProGAN+LSUN from [ 54] and Stable\nDiffusion+LAION with fake images taken from DiffusionDB [ 56]. We construct a new evaluation benchmark SynRIS (bottom) using\nreverse image search (RIS) on fake images generated by both proprietary ( e.g., DALL ·E 3 [ 12], Midjourney [ 2]) and open-source ( e.g., Play-\nground [ 31], PixArt- α) models. Note that generators used for evaluation are of significantly higher visual fidelity than those used for training.\n10761\nto ensure the lack of such bias. For example, recent works\nof Corvi et al. [17] and Ojha et al. [37] measure detectors’\nability to discriminate between DALL ·E 2 images and a mix\nof Imagenet, COCO and UCID [ 48] or LAION respectively,\nand DIRE [ 55] focuses only on open-source lower fidelity\ngenerators such as SD and older generators trained on\nImageNet and LSUN-Bedrooms [ 59]. In a concurrent\nwork, Epstein et al. [19] evaluate how adding training data\nfrom older models affects the performance of the classifier\non newer fakes, which is an important problem but different\nfrom the one we address in this paper.\nTo summarize, we are the first to show that text-\nconditioned DDIM inversion feature maps extracted from\none diffusion model improve the ability of a detector to\nidentify images generated by other higher-fidelity diffusion\nmodels. Moreover, we are the first to propose an evaluation\nprocedure for GenAI detectors that ensures that the learned\ndetector is not biased towards any style or theme, and to quan-\ntitatively verify that the resulting evaluation is more reliable.\n3. Method\nIn this section, we first provide a background on diffusion\nmodels and DDIM inversion. Then, we introduce our de-\ntection method that makes use of text-conditioned DDIM\ninversion and give an intuitive justification for why having\nthis signal is helpful for generalization.\nLatent Diffusion Models . LDMs [ 45] first map high-\nresolution (in our case, 512 ×512×3) RGB images xinto\nlow-resolution (64 ×64×4) latent images zusing a pre-\ntrained encoder E:X → Z . The original image can\nbe recovered almost perfectly via a pre-trained decoder\nD:Z → X . In the derivation below z∗correspond to\nsuch latent images , rather than RGB images.\nConditional Diffusion Models and DDIM Inversion. To\ngenerate a new latent image zconditioned on some vector\nc, a conditional denoising diffusion model [ 25] starts from\na random noise map zTof the same shape and iteratively\nstochastically denoises it using a learned denoising network\nϵθfor a fixed number of steps, until a clean latent image z0is\nobtained. The process of sampling from a pre-trained diffu-\nsion model can be discretized into fewer steps and made de-\nterministic through the use of DDIM sampling [ 52]. Notably,\nthis sampling procedure enables “inverting” a clean image z0\ninto a corresponding noise map zT, such that when zTis de-\nnoised via DDIM sampling, we obtain a new latent ˆz0that is\nvery close to the original z0. Formally, to invert an image z0,\ni.e., to obtain a corresponding noise map zT, we iteratively\nadd noise to its current estimate ztvia the following condi-\ntional forward process starting from a clean latent image z0:\nzt+1=√¯αt+1fθ(zt, t, c) +p\n1−¯αt+1ϵθ(zt, t, c)(1)\nwhere ztis the noisy latent at time t, vector cis the\nconditioner, value ¯αis the DDIM noise scaling factor [ 52],noise ϵθ(zt, t, c)is the prediction of the learned denoising\nfunction ϵθat time t, and fθ(zt, t, c)is the best current\nestimate of the clean latent z0:\nfθ(zt, t, c) =zt−√1−¯αtϵθ(zt, t, c)√¯αt(2)\nA imperfect reconstruction ˆz0can then be obtained via the\ndeterministic conditional reverse process starting from zT:\nˆzt−1=√¯αt−1fθ(ˆzt, t, c) +p\n1−¯αt−1ϵθ(ˆzt, t, c).(3)\nWe will refer to such full forward and reverse mapping as:\nˆzT=Fθ(z0, c),ˆz0=Rθ(ˆzT, c). (4)\nText Conditioning. In our case, the conditioning vector c\nused to modulate the forward and reverse sampling pro-\ncesses is the embedding of the text prompt describing an\nimage. In this work, we use an off-the-shelf captioner (BLIP\n2 [32]) to obtain a text prompt describing an input image,\nand CLIP [ 42] to embed this text. Prior work showed that the\nrealism of generated images can be improved through the\nuse of classifier-free guidance [ 24]. Later, Mokady et al. [36]\nshowed that classifier-free guidance leads to instability in\nDDIM inversion and proposed a mitigation strategy through\nfine-tuning parts of the model. Since we cannot afford fine-\ntuning on each incoming image, in this paper we do not use\nclassifier-free guidance during inversion and sampling and\nuse the original conditional update rules described above.\nGenAI Detector. As shown in Figure 3, given an input\nimage x, we first caption that image using BLIP [ 32] and\nembed that caption into a vector cusing CLIP [ 42]. Then\nwe compute the corresponding latent image z0=E(x)\nusing a pre-trained encoder and obtain a latent DDIM noise\nmapˆzTusing text-conditioned DDIM inversion with a pre-\ntrained diffusion model. Then, we obtain a reconstructed\nlatent image ˆz0using text-conditioned DDIM sampling and\ndecode both the latent noise map ˆzTand the reconstruction\nˆz0to image space using the decoder D. Finally, we apply a\nlearned mapping hϕ:X3→Rto these “images” to get a\nprediction logit. We learn the parameters of this function ϕ\nvia backpropagation of the binary cross-entropy loss ℓ(ˆy, y)\non the training set of known fake and real images {(x, y)}:\nc= CLIP(BLIP( x)) (5)\nˆzT=Fθ(E(x), c),ˆz0=Rθ(ˆzT, c) (6)\nϕ∗= argmin\nϕEx,y[ℓ(hϕ(x, D(ˆzT), D(ˆz0)), y)] (7)\nIntuition. But why would a diffusion detector benefit from\nhaving access to DDIM inversion of an image if it already\nhas access to the image itself? Recent works [ 34,47] showed\nthat DDIM can be viewed as a first-order discretization of\na neural probability-flow ODE. The bijection between ob-\nservations z0and noise maps ztinduced by this ODE can\nbe used to evaluate the likelihood of the data via the change\nof variable. If we view the forward DDIM mapping Fθas\nan approximation of that true bijective mapping between\n10762\ndecoded latent \nnoise map \nResNet50 \nReal \nor Fake input image \nBLIP \n“a picture \nof a fairy …” \nCLIP \ncaption \nembedding latent noise map latent image \nreconstruction decoded image \nreconstruction \nE D D \nlatent image Fθ Rθ Figure 3. Proposed method. In addition to the original image itself ( x), we also train our detector using the (decoded) noise map D(ˆzT)and\nreconstruction D(ˆz0)obtained by inverting the image through Stable Diffusion using DDIM. The original image is first mapped to the latent\nspace with Stable Diffusion’s V AE Encoder . The latent image is then inverted and reconstructed through Stable Diffusion’s U-Net using\nDDIM while conditioned on the CLIP embedding of the image’s predicted BLIP caption. The latent noise map and reconstruction are\nmapped back to image space using Stable Diffusion’s V AE Decoder . The original image, decoded noise map, and decoded reconstruction\nare then concatenated and used as input for our ResNet Classifier .\nthez0andzTthat introduces a discretization error δinto\nthe inverted noise maps ˆzT, causing the resampled image\nˆz0to deviate from the original image z0, it can be shown\n(see App. B.1) that, in the first-order approximation, the log-\nlikelihood of the data given that underlying model can be\nestimated from the input image z0, its imperfect reconstruc-\ntionˆz0, and the noise map zTalone:\nlogp(z0)∝logpz(zT)− ⟨δ,ˆz0−z0⟩/∥δ∥2(8)\nNotably, the expression above does not explicitly depend\non the parameters θother than through these three signals.\nGiven that, under model misspecification, likelihood-based\nmodels tend to overgeneralize [ 26], producing samples that\nare unlikely under the true data distributions, and assuming\nthat the class of diffusion-based models is similar enough to\novergeneralize in similar ways, we propose that a model that\nhas access to all signals required to internally perform some\nform of likelihood testing on input data against a particular\ntext-to-image model (the image, its imperfect reconstruction,\nand the intermediate noise map) would generalize better to\ndetect images generated via other diffusion models. Text\nconditioning cfurther amplifies differences between log-\nlikelihoods of distributions of fake and real images, making\nthe corresponding test more powerful and consequently mak-\ning inversions even more useful for detecting fake images. To\nsum up, GenAI detectors find discrepancies between the real\ndata distribution and the approximation learned by the GenAI\nmodel. The equation above shows that proposed features en-\nable a detector to get a rough estimate of whether a given im-\nage is of high probability under the approximate distribution\nlearned by Stable Diffusion. We empirically verify that this\n“SD-likelihood” signal helps in detecting other generators.4. Experiments\nIn this section, we discuss our training and evaluation sets,\nwhich baseline methods we use to compare, and the details\nof how we train our classifier.\nBaselines. We compare our model to a representative set\nof the most recent state-of-art baselines (all published in\n20232023202320232023202320232023202320232023202320232023202320232023 ) that open-sourced their training or inference code.\nDMDetDMDetDMDetDMDetDMDetDMDetDMDetDMDetDMDetDMDetDMDetDMDetDMDetDMDetDMDetDMDetDMDet [17] is a state-of-art RGB-only method that achieved\nsignificant generalization performance through the use of\naugmentations and a modified down-sampling strategy; au-\nthors released only the inference checkpoint. UFDUFDUFDUFDUFDUFDUFDUFDUFDUFDUFDUFDUFDUFDUFDUFDUFD [37] is\nanother recent state-of-the-art method that trains a linear\nclassification head on top of the CLIP [ 42] embeddings of\nreal and fake images. We use the official checkpoint and\nalso retrain it from scratch on each of our training sets us-\ning the official code. DIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIRE [55] is a concurrent work that\nshowed that using image-space DDIM reconstruction resid-\nuals helps detection. The official checkpoint open-sourced\nby the authors has an issue causing its performance to be\nmuch lower than the performance reported in the paper; we\ndiscuss this in more detail in App. E.1. We also provide an\nablation of our method that uses only DDIM residuals. This\nserves as a close approximation of what a DIRE-like method\ncould achieve. We also include an older convolutional base-\nlineCNNDetCNNDetCNNDetCNNDetCNNDetCNNDetCNNDetCNNDetCNNDetCNNDetCNNDetCNNDetCNNDetCNNDetCNNDetCNNDetCNNDet [54] using its official checkpoint and code to\nretrain on our data.\nTraining data – ProGAN+LSUN. Most prior works use\nthe ProGAN training set introduced in CNNDet [ 54]. This\ntraining set consists of 350k images from class-conditioned\npre-trained ProGAN [ 30] combined with a set of real images\n10763\nDataset Data Size Real Data Real/Fake Source\nDALL·E 2 [44] 700/700 RIS (ours) fakes from [17]\nDALL·E 3 [12] 3.3k/3.3k RIS (ours) fakes from [8]\nMidjourney [2] 4.4k/4.4k RIS (ours) fakes from [9]\nImagen [46] 700/700 RIS (ours)our fakes\n(see App. A)\nOpen-Source13.5k/3.5k\n(x11)RIS (ours)our fakes\n(see App. A)\nDALL·E 2 (A) 1k/5kImagenet,\nCOCO, UCID [48]both reals/fakes\nfrom [17]\nCraiyon (A) 1k/1k LAION both from [37]\nLDM (A) 1k/1k LAION both from [37]\nTable 1. Evaluation Datasets. We evaluate using 15 new RIS-based\nevaluation benchmarks ( SynRIS (ours) – top) as well as existing\nacademic (A) text-to-image evaluation dataset (bottom).\nEval Data Training DataFIDKID\n×10−2Fake Real ProGAN+LSUN SD+LAION\nDALL·E 2LAION 0.233 0.043 163.5 2.7\nRIS (ours) 0.457 0.406 88.5 0.3\nDALL·E 3LAION 0.794 0.360 126.1 2.6\nRIS (ours) 0.920 0.795 93.6 0.4\nImagenLAION 0.406 0.360 127.1 2.1\nWebLI 0.559 0.664 101.9 1.2\nRIS (ours) 0.620 0.720 83.6 0.2\nKandinsky 2LAION 0.655 0.189 118.0 2.1\nRIS (ours) 0.857 0.686 88.9 0.4\nSDXLLAION 0.689 0.106 108.7 1.6\nRIS (ours) 0.874 0.551 93.0 0.4\nTable 2. Difficulty of RIS vs LAION eval. FPR@0.8 recall for\nthe state-of-the-art detector (UFD [ 37]) evaluated using fakes from\nrespective generators and real images from LAION, reverse image\nsearch (RIS), and WebLI (Imagen’s training set [ 60]) - higher FPR\nis harder; FID and KID between reals and fakes - lower is closer.\nfrom LSUN [ 59]. Training on these images has yielded good\nresults in the detection of GAN-generated images [ 17,37,\n54], and we find that this set continues to show promise\nwhen applied to images from newer diffusion models.\nTraining data – Stable Diffusion+LAION. Similar to the\nconcurrent work of Epstein et al. [19], we first train de-\ntectors using 300k fake Stable Diffusion v1 images from\nDiffusionDB [ 56] and 300k random real LAION [ 49] im-\nages. While state-of-the-art at the time of its release, Stable\nDiffusion (v1) has since been eclipsed in quality by many\nnew text-to-image models. We find that training on fake\nimages from this relatively old diffusion model still yieldsmodels capable of identifying fakes from much newer and\nmore powerful generators.\nEvaluation data (fakes). We obtain several thousand im-\nages generated by closed-source photorealistic text-to-image\nmodels using APIs (Imagen [ 46]), using existing databases\nof fakes on HuggingFace (Midjourney [ 9], DALL ·E 3 [ 8])\nand by taking fake images from prior academic benchmarks\n(DALL ·E 2 from [ 17]). We also generate several thousand im-\nages using high-fidelity open-source text-to-image models1\nconditioned on Midjourney prompts [57].\nEvaluation data (reals) – Reverse Image Search (RIS).\nTo ensure that detectors are not biased toward any particular\ntheme or style, we need sets of real and fake images that are\nthemselves stylistically and thematically aligned. We address\nthis issue using a reverse image search API to find a visually\nand thematically similar real image for each fake image from\nthe eval fake set defined above. Examples of images found\nusing this procedure can be found in Figure 2. We define\nreal images as images not generated using a text-to-image\nmodel, even if other tools (such as Photoshop) were used. To\nensure that our real images are not contaminated with similar\nimages generated by text-to-image models, we include only\nmatches found on pages created before January 1, 2021. As\na result, our real sets include only images published before\nthe original DALL ·E [43] was announced. The exact sizes\nof all evaluation and training sets can be found in Table 1.\nEvaluation data – prior academic benchmarks. We eval-\nuate competing methods on academic text-to-image bench-\nmarks from published prior work [ 17,37] that evaluate meth-\nods’ abilities to differentiate between a set of fakes from\na text-to-image model ( e.g., DALL ·E 2) and an unrelated\nset of real images ( e.g., Imagenet, COCO). Consequently,\nthese benchmarks can not be used to test whether a detector\nfocuses on the styles and themes of a particular generator.\nDetector architecture. We use ResNet50 trained from\nscratch as a detector backbone. In each experiment, we select\nthe best checkpoint via validation on the held-out set sam-\npled from the same source as the training set. We augment\neach image via a suite of random transforms before perform-\ning DDIM inversion: flip, crop, color jitter, grayscale, cutout,\nnoise, blur, jpeg, and rotate. We use BLIP [ 32] to compute\nimage captions. See App. C for more details.\nMetrics. We report detection AUCROC as the main metric.\nWe also provide tables with average precision and accuracy,\nalong with PR, ROC and DET curves in the appendix. To\nensure that trained and evaluated detectors can not exploit dif-\nferences in image resolutions and aspect ratios, each image is\nresized to 256px along the shortest side and saved losslessly.\n1Open-Source dataset includes fake images from Kandinsky 2 [ 51],\nKandinsky 3 [ 10], PixArt- α[16], SDXL-DPO [ 53], SDXL [ 41],\nSegMoE [58], SSD-1B [ 21], Stable-Cascade [ 39], Segmind-Vega [ 21],\nand W ¨urstchen 2 [39].\n10764\nTrain Data ProGAN + LSUN Stable Diffusion + LAION\nEval Set |Model DIRE CNNDet DMDet UFD Ours CNNDet†DMDet∗UFD†Ours\nDALL·E 2 [44] 0.561 0.455 0.656 0.728 0.854 0.680 0.672 0.776 0.747\nDALL·E 3 [12] 0.524 0.378 0.409 0.323 0.642 0.716 0.415 0.480 0.759\nMidjourney v5/6 [2] 0.538 0.473 0.544 0.397 0.750 0.630 0.484 0.592 0.664\nImagen [46] 0.562 0.452 0.502 0.637 0.776 0.714 0.573 0.575 0.807\nKandinsky 2 [51] 0.463 0.492 0.468 0.474 0.758 0.600 0.478 0.562 0.699\nKandinsky 3 [10] 0.491 0.480 0.593 0.469 0.845 0.659 0.614 0.637 0.743\nPixArt- α[16] 0.478 0.487 0.599 0.506 0.854 0.627 0.580 0.647 0.730\nPlayground 2.5 [31] 0.453 0.528 0.661 0.466 0.778 0.582 0.517 0.587 0.625\nSDXL-DPO [53] 0.458 0.486 0.603 0.464 0.841 0.843 0.563 0.702 0.881\nSDXL [41] 0.459 0.525 0.667 0.464 0.764 0.814 0.568 0.663 0.807\nSeg-MOE [58] 0.459 0.429 0.467 0.401 0.796 0.663 0.476 0.620 0.713\nSSD-1B [21] 0.449 0.589 0.689 0.515 0.827 0.726 0.556 0.628 0.794\nStable-Cascade [39] 0.465 0.447 0.603 0.341 0.882 0.705 0.565 0.682 0.749\nSegmind Vega [21] 0.471 0.556 0.645 0.468 0.823 0.742 0.540 0.623 0.811\nW¨urstchen 2 [39] 0.456 0.510 0.671 0.616 0.792 0.610 0.675 0.697 0.705\nDALL·E 2 [44] (A) 0.554 0.466 0.646 0.662 0.623 0.566 0.727 0.590 0.571\nCraiyon [18] (A) 0.523 0.660 0.941 0.974 0.874 0.763 0.988 0.918 0.886\nLDM [45] (A) 0.512 0.653 0.854 0.924 0.878 0.913 1.000 0.919 0.979\nAverage 0.493 0.504 0.623 0.546 0.798 0.697 0.611 0.661 0.759\nTable 3. Main Results – Detector AUCROC. Detectors trained on ProGAN+LSUN and SD+LAION are evaluated using proprietary (first\npanel) and open-source (second panel) generators, and academic (A) benchmarks from prior work (last panel).∗Note: This DMDet classifier\nwas trained with fakes from an LDM checkpoint rather than Stable Diffusion.†These models were re-trained by us.\n5. Results\nIn this section we discuss following key findings: 1) the\nproposed thematically and stylistically aligned RIS-based\nevaluation protocol is harder and is more reliable then proto-\ncols used in prior work; 2) the proposed detector outperforms\nprior work on both prior academic and this new RIS-based\nevaluation; 3) the DDIM inversion features were crucial in\nachieving high generalization in all cases.\nRIS-based evaluation is harder and more reliable. Table 2\ncompares False Positive Rate (FPR) of the state-of-the-art\ndetector [ 37] on different evaluation sets at the threshold that\nattains 80% recall (fake images are the same, so the thresh-\nold at given recall is the same as well). Results show that\nLAION-based evaluation significantly underestimates the\nfalse positive rate of the detector when evaluating its ability\nto discriminate fakes from closed-source text-to-image mod-\nels (Imagen, DALL ·E 2/3) across both training sets. We also\nobtained real examples from the multimodal dataset used\nto train Imagen (WebLI [ 60]), and evaluated the detector\nagainst these real examples and these results closely align\nwith our RIS-based eval (see Fig. A.1 for PR curves). The\nFID [ 23] and KID [ 13] between real and fake images is also\nlower for RIS eval, and matches FID/KID between WebLIand Imagen fakes, suggesting better stylistic and thematic\nalignment. Similar trends can be seen on open-source mod-\nels (Kadnisky, SDXL) and across both training sets. These\nresults suggest that our RIS-based eval is a more reliable\nway to estimate a model’s ability to detect images from\nclosed-sourced text-to-image models trained on unknown\ndata.\nFakeInversion achieves state-of-the-art performance.\nTable 3 shows that our method consistently scores best at de-\ntecting both closed and open-source methods across various\ntraining sets. It also matches the performance of prior work\non academic benchmarks. On average, our method outper-\nforms prior work by at least 4pp onboth training setsboth training setsboth training setsboth training setsboth training setsboth training setsboth training setsboth training setsboth training setsboth training setsboth training setsboth training setsboth training setsboth training setsboth training setsboth training setsboth training sets.\nInversions are crucial for generalization. To ensure that\nthe observed gains are not coming from a particular choice\nof hyperparameters in our detector, we performed an abla-\ntion training the exact same network using only RGB images\nand only absolute DDIM image reconstruction residuals\nRes = |x−D(ˆz0)|(similar to DIRE [ 55]). Table 4 shows\nthat both RGB and reconstruction residual-based models per-\nform significantly worse than the proposed method that uses\nboth the input image, its reconstruction, and the inversion\nmap, confirming all three are essential to achieve state-of-art\ngeneralization to unseen detectors. In the appendix we show\n10765\nTrain Data ProGAN + LSUN SD + LAION\nEval Set |Model RGB Res Ours RGB Res Ours\nDALL·E 2 [43] 0.410 0.650 0.854 0.592 0.650 0.747\nDALL·E 3 [12] 0.399 0.672 0.642 0.676 0.672 0.759\nMidjourney v5 [2] 0.434 0.590 0.750 0.530 0.590 0.664\nImagen [46] 0.530 0.670 0.776 0.729 0.670 0.807\nKandinsky 2 [51] 0.462 0.600 0.716 0.614 0.607 0.714\nKandinsky 3 [10] 0.434 0.617 0.824 0.606 0.679 0.774\nPixArt- α[16] 0.470 0.604 0.647 0.594 0.570 0.707\nPlayground 2.5 [31] 0.439 0.604 0.726 0.510 0.533 0.660\nSDXL-DPO [53] 0.338 0.643 0.704 0.738 0.711 0.837\nSDXL [41] 0.410 0.612 0.691 0.784 0.709 0.884\nSeg-MOE [58] 0.416 0.585 0.799 0.607 0.611 0.781\nSSD-1B [21] 0.494 0.672 0.775 0.690 0.648 0.813\nStable-Cascade [39] 0.448 0.674 0.743 0.557 0.686 0.766\nSegmind Vega [21] 0.465 0.677 0.781 0.683 0.631 0.829\nW¨urstchen 2 [39] 0.563 0.624 0.664 0.588 0.605 0.702\nTable 4. Input Signal Ablation – AUCROC . Detectors trained on\nProGAN+LSUN and Stable Diffusion+LAION and evaluated on\nproprietary and open generators. Using the original images, inver-\nsion maps, and reconstructions ( Ours ) yields better performance\nthan RGB or DDIM Residuals (as in DIRE [55]) alone.\nProGAN + LSUN SD + LAION\nCNNDet UFD Ours CNNDet UFD OursnoiseImagen 0.477 0.579 0.758 0.730 0.529 0.822\nMJ 0.481 0.383 0.665 0.600 0.533 0.624\nDALL·E 3 0.390 0.315 0.598 0.700 0.449 0.750blurImagen 0.447 0.595 0.793 0.730 0.570 0.812\nMJ 0.463 0.379 0.747 0.639 0.583 0.658\nDALL·E 3 0.375 0.315 0.639 0.738 0.501 0.756JPEGImagen 0.463 0.651 0.769 0.715 0.555 0.804\nMJ 0.466 0.383 0.743 0.624 0.610 0.654\nDALL·E 3 0.372 0.327 0.632 0.713 0.477 0.754cropImagen 0.436 0.561 0.781 0.704 0.546 0.797\nMJ 0.471 0.383 0.742 0.623 0.597 0.680\nDALL·E 3 0.375 0.298 0.642 0.702 0.457 0.769\nTable 5. Performance on Corrupted Images – AUCROC . Our\nmethod remains robust to common image degredations [20].\nthat text conditioning also helps generalization.\nRobustness and Interpretability. Table 5 shows that our\nmethod is sufficiently robust to in-the-wild transformations\n[20] such as JPEG re-compression and blur. Figure 4 shows\nthat a model that uses inversion maps not only generalizes\nbetter but also focuses more on features that humans recog-\nnize as GenAI artifacts ( e.g. malformed hands).\nDiscussion. Our results suggest that, while both our method\nand recent methods (UFD [ 37], DMDet [ 17]) consistently\noutperform older prior methods (CNNDet [ 54]) on prior\nacademic benchmarks, recent methods struggle to maintain\ninput image \nzoomed-in \nOurs XRAI RGB XRAI \nfakefake\nreal\nFigure 4. Saliency Analysis . Green boxes highlight the most salient\nregions according to our model and purple boxes for an equiva-\nlent RGB-only model. We use a post-hoc explainability technique,\nXRAI [ 29]. The regions of anatomical inconsistencies in fakes are\nmost salient in our model.\nthe same level of exceptional performance when evaluated\nagainst our new RIS-based evaluation benchmark, even when\nretrained on better data. Our method and some of the older\nbaselines, on the other hand, perform well on both. We at-\ntribute this discrepancy to the drastic shift between real and\nfake images used in prior evaluation – suggesting that some\nof the recent methods were in part overfitting to the distribu-\ntions of styles and content of natural images, which appears\nto be less of an issue for our method.\n6. Conclusion\nIn this paper, we introduce FakeInversion : a GenAI de-\ntection method that uses text-conditioned inversion maps\nextracted from a pre-trained Stable Diffusion to achieve a\nnew state-of-the-art at detecting images generated via unseen\ntext-to-image diffusion models. We also propose SynRIS : a\nnew challenging evaluation protocol that uses reverse image\nsearch to ensure that the evaluation is not biased towards\nany styles and themes. We show that the new protocol is\nalso more reliable at evaluating detectors on images gen-\nerated using proprietary models trained on unknown data.\nWhile FakeInversion improves upon the state-of-the-art on\nthis challenging benchmark, there clearly remains much\nwork to be done ; the detection performance on the new\nevaluation benchmark is far from saturated. We invite future\nresearchers to use these new datasets to explore, build, and\ndeploy better GenAI detectors at scale, with confidence that\ntheir solutions will not favor any content and style.\n10766\nReferences\n[1]Deepfakes could supercharge health care’s misin-\nformation problem. https : / / www . axios .\ncom/2023/11/14/ai- deepfake- health-\nmisinformation-fake-pictures-videos .\n2\n[2]Midjourney. https://www.midjourney.com/ .\n1, 3, 6, 7, 8, 15, 16, 25\n[3]An a.i.-generated picture won an art prize. artists\naren’t happy. https://www.nytimes.com/\n2022/09/02/technology/ai-artificial-\nintelligence-artists.html . 2\n[4]Sd pok ´emon diffusers. https://huggingface.\nco/lambdalabs/sd- pokemon- diffusers .\n17, 39\n[5] Pok ´emon. pokemon.com . 39\n[6]Danbooru 2022 dataset. https://huggingface.\nco/datasets/animelover/danbooru2022 ,\n2022. 17, 40\n[7]Animagine XL 2.0. https://huggingface.co/\nLinaqruf/animagine-xl-2.0 , 2023. 17, 40\n[8]laion/dalle-3-dataset ·datasets at hugging face, 2023. 6\n[9]wanng/midjourney-v5-202304-clean ·datasets at hug-\nging face, 2023. 6\n[10] Vladimir Arkhipkin, Andrei Filatov, Viacheslav\nVasilev, Anastasia Maltseva, Said Azizov, Igor Pavlov,\nJulia Agafonova, Andrey Kuznetsov, and Denis Dim-\nitrov. Kandinsky 3.0 technical report, 2023. 6, 7, 8, 13,\n15, 16, 29\n[11] David Bau, Jun-Yan Zhu, Jonas Wulff, William Pee-\nbles, Hendrik Strobelt, Bolei Zhou, and Antonio Tor-\nralba. Seeing what a gan cannot generate. In ICCV ,\n2019. 2\n[12] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jian-\nfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang,\nJoyce Lee, Yufei Guo, Wesam Manassra, Prafulla\nDhariwal, Casey Chu, and Yunxin Jiao. Improving im-\nage generation with better captions. https://cdn.\nopenai.com/papers/dall-e-3.pdf . 1, 2, 3,\n6, 7, 8, 15, 16, 25, 27\n[13] Mikołaj Bi ´nkowski, Danica J Sutherland, Michael Ar-\nbel, and Arthur Gretton. Demystifying mmd gans.\narXiv preprint arXiv:1801.01401 , 2018. 7\n[14] Lucy Chai, David Bau, Ser-Nam Lim, and Phillip Isola.\nWhat makes fake images detectable? understanding\nproperties that generalize. In ECCV , 2020. 2\n[15] Huiwen Chang, Han Zhang, Jarred Barber, AJ\nMaschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang,\nKevin Murphy, William T Freeman, Michael Ru-\nbinstein, et al. Muse: Text-to-image generation\nvia masked generative transformers. arXiv preprint\narXiv:2301.00704 , 2023. 12\n[16] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao,Enze Xie, Yue Wu, Zhongdao Wang, James Kwok,\nPing Luo, Huchuan Lu, et al. Pixart-alpha: Fast train-\ning of diffusion transformer for photorealistic text-to-\nimage synthesis. arXiv preprint arXiv:2310.00426 ,\n2023. 1, 6, 7, 8, 13, 15, 16, 30\n[17] Riccardo Corvi, Davide Cozzolino, Giada Zingarini,\nGiovanni Poggi, Koki Nagano, and Luisa Verdoliva.\nOn the detection of synthetic images generated by dif-\nfusion models. In ICASSP , 2023. 1, 2, 4, 5, 6, 8, 12,\n26\n[18] Boris Dayma, Suraj Patil, Pedro Cuenca, Khalid Sai-\nfullah, Tanishq Abraham, Ph ´uc Lˆe Khac, Luke Melas,\nand Ritobrata Ghosh. Dall-e mini, 2021. 7, 15, 16\n[19] David C. Epstein, Ishan Jain, Oliver Wang, and Richard\nZhang. Online detection of ai-generated images. In\nICCV Workshop , 2023. 4, 6\n[20] Joel Frank, Thorsten Eisenhofer, Lea Sch ¨onherr, Asja\nFischer, Dorothea Kolossa, and Thorsten Holz. Lever-\naging frequency analysis for deep fake image recogni-\ntion. In ICML , 2020. 2, 8\n[21] Yatharth Gupta, Vishnu V . Jaddipal, Harish Prabhala,\nSayak Paul, and Patrick V on Platen. Progressive knowl-\nedge distillation of stable diffusion xl using layer level\nloss, 2024. 6, 7, 8, 13, 15, 16, 35, 37\n[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. Deep residual learning for image recognition.\nInProceedings of the IEEE conference on computer\nvision and pattern recognition , pages 770–778, 2016.\n14\n[23] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. Gans trained\nby a two time-scale update rule converge to a local\nnash equilibrium. NeurIPS , 2017. 7\n[24] Jonathan Ho and Tim Salimans. Classifier-free diffu-\nsion guidance. arXiv preprint arXiv:2207.12598 , 2022.\n4\n[25] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising\ndiffusion probabilistic models. NeurIPS , 2020. 4\n[26] Ferenc Husz ´ar. How (not) to train your generative\nmodel: Scheduled sampling, likelihood, adversary?\narXiv preprint arXiv:1511.05101 , 2015. 5\n[27] Yan Ju, Shan Jia, Lipeng Ke, Hongfei Xue, Koki\nNagano, and Siwei Lyu. Fusing global and local fea-\ntures for generalized ai-synthesized image detection.\nInICIP , 2022. 2\n[28] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik\nPark, Eli Shechtman, Sylvain Paris, and Taesung Park.\nScaling up gans for text-to-image synthesis. In CVPR ,\n2023. 12\n[29] Andrei Kapishnikov, Tolga Bolukbasi, Fernanda\nVi´egas, and Michael Terry. Xrai: Better attributions\nthrough regions. In ICCV , 2019. 8\n[30] Tero Karras, Timo Aila, Samuli Laine, and Jaakko\n10767\nLehtinen. Progressive growing of gans for improved\nquality, stability, and variation. ICML , abs/1710.10196,\n2017. 5, 12\n[31] Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet,\nLinmiao Xu, and Suhail Doshi. Playground v2.5: Three\ninsights towards enhancing aesthetic quality in text-to-\nimage generation, 2024. 3, 7, 8, 13, 15, 16, 31\n[32] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2: Bootstrapping language-image pre-training\nwith frozen image encoders and large language models.\narXiv preprint arXiv:2301.12597 , 2023. 4, 6, 14, 17,\n39\n[33] Tsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and\nC Lawrence Zitnick. Microsoft coco: Common objects\nin context. In ECCV , 2014. 2\n[34] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen,\nChongxuan Li, and Jun Zhu. Dpm-solver: A fast ode\nsolver for diffusion probabilistic model sampling in\naround 10 steps. NeurIPS , 2022. 4\n[35] Francesco Marra, Diego Gragnaniello, Luisa Verdoliva,\nand Giovanni Poggi. Do gans leave artificial finger-\nprints? In IEEE conference on multimedia information\nprocessing and retrieval (MIPR) , 2019. 2\n[36] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch,\nand Daniel Cohen-Or. Null-text inversion for editing\nreal images using guided diffusion models. In CVPR ,\n2023. 4\n[37] Utkarsh Ojha, Yuheng Li, and Yong Jae Lee. Towards\nuniversal fake image detectors that generalize across\ngenerative models. In CVPR , 2023. 1, 2, 4, 5, 6, 7, 8,\n15, 16\n[38] Gaurav Parmar, Krishna Kumar Singh, Richard Zhang,\nYijun Li, Jingwan Lu, and Jun-Yan Zhu. Zero-shot\nimage-to-image translation. In ACM TOG , 2023. 14\n[39] Pablo Pernias, Dominic Rampas, Mats L. Richter,\nChristopher Pal, and Marc Aubreville. Wuerstchen:\nEfficient pretraining of text-to-image models, 2023. 1,\n6, 7, 8, 13, 15, 16, 36, 38\n[40] Justin N. M. Pinkney. Pokemon blip captions.\nhttps : / / huggingface . co / datasets /\nlambdalabs / pokemon - blip - captions/ ,\n2022. 17, 39\n[41] Dustin Podell, Zion English, Kyle Lacey, Andreas\nBlattmann, Tim Dockhorn, Jonas M ¨uller, Joe Penna,\nand Robin Rombach. Sdxl: Improving latent diffu-\nsion models for high-resolution image synthesis. arXiv\npreprint arXiv:2307.01952 , 2023. 6, 7, 8, 13, 15, 16,\n33\n[42] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.\nLearning transferable visual models from natural lan-guage supervision. In ICML , 2021. 2, 4, 5\n[43] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott\nGray, Chelsea V oss, Alec Radford, Mark Chen, and\nIlya Sutskever. Zero-shot text-to-image generation. In\nICML , 2021. 6, 8\n[44] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey\nChu, and Mark Chen. Hierarchical text-conditional\nimage generation with clip latents. arXiv preprint\narXiv:2204.06125 , 2022. 6, 7, 15, 16, 26\n[45] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj ¨orn Ommer. High-resolution im-\nage synthesis with latent diffusion models. In CVPR ,\n2022. 1, 2, 4, 7, 14, 15, 16\n[46] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Sal-\nimans, et al. Photorealistic text-to-image diffusion\nmodels with deep language understanding. NeurIPS ,\n2022. 1, 6, 7, 8, 15, 16, 24\n[47] Tim Salimans and Jonathan Ho. Progressive distillation\nfor fast sampling of diffusion models. In ICLR , 2022.\n4\n[48] Gerald Schaefer and Michal Stich. Ucid: An uncom-\npressed color image database. In Storage and retrieval\nmethods and applications for multimedia 2004 . SPIE,\n2003. 4, 6\n[49] Christoph Schuhmann, Romain Beaumont, Richard\nVencu, Cade Gordon, Ross Wightman, Mehdi Cherti,\nTheo Coombes, Aarush Katta, Clayton Mullis, Mitchell\nWortsman, et al. Laion-5b: An open large-scale\ndataset for training next generation image-text mod-\nels.NeurIPS , 2022. 6, 12, 14\n[50] Zeyang Sha, Zheng Li, Ning Yu, and Yang Zhang. De-\nfake: Detection and attribution of fake images gener-\nated by text-to-image generation models, 2023. 2\n[51] Arseniy Shakhmatov, Anton Razzhigaev, Aleksandr\nNikolich, Vladimir Arkhipkin, Igor Pavlov, Andrey\nKuznetsov, and Denis Dimitrov. kandinsky 2.2, 2023.\n1, 6, 7, 8, 12, 15, 16, 28\n[52] Jiaming Song, Chenlin Meng, and Stefano Ermon. De-\nnoising diffusion implicit models. In ICLR , 2020. 2,\n4\n[53] Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi\nZhou, Aaron Lou, Senthil Purushwalkam, Stefano Er-\nmon, Caiming Xiong, Shafiq Joty, and Nikhil Naik.\nDiffusion model alignment using direct preference op-\ntimization. arXiv preprint arXiv:2311.12908 , 2023. 6,\n7, 8, 13, 15, 16, 32\n[54] Sheng-Yu Wang, Oliver Wang, Richard Zhang, Andrew\nOwens, and Alexei A Efros. Cnn-generated images are\nsurprisingly easy to spot...for now. In CVPR , 2020. 1,\n2, 3, 5, 6, 8, 12, 14, 15, 16\n[55] Zhendong Wang, Jianmin Bao, Wengang Zhou, Weilun\n10768\nWang, Hezhen Hu, Hong Chen, and Houqiang Li. Dire\nfor diffusion-generated image detection. ICCV , 2023.\n1, 2, 4, 5, 7, 8, 15\n[56] Zijie J Wang, Evan Montoya, David Munechika,\nHaoyang Yang, Benjamin Hoover, and Duen Horng\nChau. Diffusiondb: A large-scale prompt gallery\ndataset for text-to-image generative models. arXiv\npreprint arXiv:2210.14896 , 2022. 3, 6, 12\n[57] wangjunjie. midjourney-v5-202304-clean.\nhttps : / / huggingface . co / datasets /\nwanng / midjourney - v5 - 202304 - clean ,\n2023. 6\n[58] Harish Prabhala Yatharth Gupta, Vishnu V Jaddi-\npal. Segmoe: Segmind mixture of diffusion experts.\nhttps : / / github . com / segmind / segmoe ,\n2024. 6, 7, 8, 13, 15, 16, 34\n[59] Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song,\nThomas Funkhouser, and Jianxiong Xiao. Lsun: Con-\nstruction of a large-scale image dataset using deep\nlearning with humans in the loop. arXiv preprint\narXiv:1506.03365 , 2015. 4, 6, 12\n[60] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong,\nGunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander\nKu, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchin-\nson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Ja-\nson Baldridge, and Yonghui Wu. Scaling autoregres-\nsive models for content-rich text-to-image generation.\nTMLR , 2022. 6, 7, 14\n[61] Xu Zhang, Svebor Karaman, and Shih-Fu Chang. De-\ntecting and simulating artifacts in gan fake images. In\nIEEE international workshop on information forensics\nand security (WIFS) , 2019. 2\n10769'}, 'dist': 0.9286905527114868}
Result 27: {'text': 'data', 'metadata': {'created_at': 1736932570, 'modified_at': 1736932570, 'owner': 'sayandeep', 'path': 'RAG_CONTENT/conferences/cvpr_ocr/Jiang_Efficient_Hyperparameter_Optimization_with_Adaptive_Fidelity_Identification_CVPR_2024_paper.txt', 'size': 53230, 'seen_at': 1737191136, 'data': 'Efﬁcient Hyperparameter Optimization with Adaptive Fidelity Identiﬁcation\r\nJiantong Jiang1, Zeyi Wen2,3*, Atif Mansoor1, Ajmal Mian1\r\n1The University of Western Australia,2HKUST (Guangzhou),3HKUST\r\njiantong.jiang@research.uwa.edu.au, wenzeyi@ust.hk, {atif.mansoor, ajmal.mian }@uwa.edu.au\r\nAbstract\r\nHyperparameter Optimization and Neural Architecture\r\nSearch are powerful in attaining state-of-the-art machine\r\nlearning models, with Bayesian Optimization (BO) stand-\r\ning out as a mainstream method. Extending BO into the\r\nmulti-ﬁdelity setting has been an emerging research topic\r\nin this ﬁeld, but faces the challenge of determining an ap-\r\npropriate ﬁdelity for each hyperparameter conﬁguration to\r\nﬁt the surrogate model. To tackle the challenge, we propose\r\na multi-ﬁdelity BO method named FastBO, which excels in\r\nadaptively deciding the ﬁdelity for each conﬁguration and\r\nproviding strong performance while ensuring efﬁcient re-\r\nsource usage. These advantages are achieved through our\r\nproposed techniques based on the concepts of efﬁcient point\r\nand saturation point for each conﬁguration, which can be\r\nobtained from the empirical learning curve of the conﬁgu-\r\nration, estimated from early observations. Extensive experi-\r\nments demonstrate FastBO’s superior anytime performance\r\nand efﬁciency in identifying high-quality conﬁgurations and\r\narchitectures. We also show that our method provides a way\r\nto extend any single-ﬁdelity method to the multi-ﬁdelity set-\r\nting, highlighting the wide applicability of our approach.\r\n1. Introduction\r\nHyperparameters are crucial in machine learning pipelines.\r\nHyperparameter optimization (HPO) [ 11] and Neural Ar-\r\nchitecture Search (NAS) [ 9] aims to ﬁnd the hyperparame-\r\nters or architectures that can yield good performance with-\r\nout human experts. Among different HPO and NAS meth-\r\nods, Bayesian Optimization (BO) [ 2,14,40] is an effec-\r\ntive model-based method that has shown remarkable suc-\r\ncess [ 8,39]. BO maintains a surrogate model of the tar-\r\nget performance metric based on past evaluations of hyper-\r\nparameter conﬁgurations, which guides the choice of more\r\npromising conﬁgurations to evaluate.\r\nDespite its sample efﬁciency, standard BO requires a\r\nfull evaluation of each conﬁguration, involving full-scale\r\n*Zeyi Wen is the corresponding author.training and testing of models, which can be highly time-\r\nconsuming, particularly with the recent trend to larger mod-\r\nels. To avoid expensive full evaluations, multi-ﬁdelity\r\nmethods [ 4,16,25,26] have been proposed, where the ﬁ-\r\ndelities refer to the levels of performance metrics obtained\r\nunder different resource levels. These methods follow the\r\nprinciple of successive halving (SHA) [ 16]: initially, they\r\nevaluate a set of randomly selected conﬁgurations using a\r\nsmall number of resources; then, based on the low-ﬁdelity\r\nperformances, the poorly-performing ones are successively\r\neliminated, while the well-performing ones continue to\r\nbe evaluated with increasing resources. Follow-up stud-\r\nies [10,22,27,37,47] propose model-based multi-ﬁdelity\r\nmethods, replacing the random conﬁguration selection with\r\na more informed model to improve sample efﬁciency.\r\nNevertheless, current model-based multi-ﬁdelity meth-\r\nods face a major limitation: they are built upon the SHA\r\nframework, which operates under the assumption that learn-\r\ning curves of different conﬁgurations rarely intersect. This\r\nassumption does not hold in practice [ 46], i.e., early perfor-\r\nmance observations cannot always indicate the ﬁnal ﬁdelity\r\nperformance at the full resource level. This leads to a fun-\r\ndamental challenge when extending model-based methods\r\nto the multi-ﬁdelity setting: What is the appropriate ﬁdelity\r\nfor each conﬁguration to ﬁt the surrogate model? In other\r\nwords, which ﬁdelity can provide performance observations\r\nthat reliably indicate the ﬁnal ﬁdelity performance? Exist-\r\ning methods struggle to address this fundamental challenge.\r\nIn particular, BOHB [ 10] and Hyper-Tune [ 27] ﬁt separate\r\nsurrogate models for different ﬁdelities, failing to capture\r\ninter-ﬁdelity correlations. FTBO [ 44] and A-BOHB [ 22]\r\nﬁt a joint model but require strong assumptions to remain\r\ntractable. Another work by Salinas et al. [ 37] suggests us-\r\ning the last observed ﬁdelity performance to ﬁt the surro-\r\ngate model. However, it widens the gap between poorly-\r\nand well-performing conﬁgurations at the early stage, po-\r\ntentially leading to an inaccurate surrogate model.\r\nTo this end, we propose a multi-ﬁdelity extension of BO,\r\nnamely FastBO, which tackles the challenge of deciding the\r\nappropriate ﬁdelity for each conﬁguration to ﬁt the surro-\r\ngate model. FastBO identiﬁes a so-called efﬁcient point for\r\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\r\nExcept for this watermark, it is identical to the accepted version;\r\nthe final published version of the proceedings is available on IEEE Xplore.\r\n26181\r\neach conﬁguration to be the ﬁdelity. The point balances\r\ncomputational cost and performance quality while captur-\r\ning valuable learning curve trends. In essence, FastBO se-\r\nlects the ﬁdelity for each conﬁguration instead of evaluating\r\nall the conﬁgurations at the same ﬁdelity. Additionally, a\r\nsaturation point for each conﬁguration is identiﬁed to be an\r\napproximation of the ﬁnal ﬁdelity, leading to high-quality\r\nperformance while reducing resource wastage. The two cru-\r\ncial points are adaptively derived from the estimated learn-\r\ning curve of each conﬁguration. Furthermore, the warm-up\r\nand post-processing stages are carefully designed to enable\r\njudicious early-termination detection and efﬁcient satura-\r\ntion level evaluation. Empirical evaluation against the state-\r\nof-the-art methods shows that FastBO has strong anytime\r\nperformance and can considerably save up to 87% of the\r\ntime required to identify a good conﬁguration or architec-\r\nture, lowering the barriers for engaging in HPO and NAS.\r\nIn summary, we make the following major contributions.\r\n1.We propose a multi-ﬁdelity model-based HPO method\r\nthat adaptively decides the ﬁdelities for conﬁgurations\r\nand efﬁciently offers strong performance, thanks to the\r\nintroduced concepts of efﬁcient and saturation points.\r\n2.We develop a learning curve modeling module to enable\r\nadaptive derivation of the key points, a warm-up stage to\r\nallow early-termination detection, and a post-processing\r\nstage to ensure efﬁcient saturation-level evaluation.\r\n3.We show that our strategy can be used to extend exist-\r\ning single-ﬁdelity methods to the multi-ﬁdelity setting,\r\ndemonstrating the effectiveness and generality of our\r\nmethod and highlighting promising future opportunities.\r\n2. Related Work\r\nHPO and NAS incur signiﬁcant costs, especially consider-\r\ning the escalating model evaluation overhead. Despite ef-\r\nforts to directly accelerate computation [ 17–19,52], smarter\r\nHPO strategies are still required for the widespread adop-\r\ntion of automation. Two crucial directions include model-\r\nbased and multi-ﬁdelity methods, which can also be com-\r\nbined. Here, we review the methods in these categories.\r\nModel-based methods. BO is the representative of model-\r\nbased methods. Based on the surrogate model constructed\r\nby historical evaluation results, BO selects the conﬁgura-\r\ntions to evaluate via an acquisition function that balances\r\nexploration and exploitation. Commonly used surrogate\r\nmodels are Gaussian processes [ 40], random forests [ 14],\r\ntree-structured Parzen estimator [ 2], and deep networks [ 41,\r\n42]. Popular acquisition functions include Expected Im-\r\nprovement [ 28], Knowledge Gradient [ 12], Upper Conﬁ-\r\ndence Bound [ 43], and Predictive Entropy Search [ 13]. Re-\r\ncent studies on BO have explored the utilization of expert\r\npriors [ 15,24,33,38] and derivative information [ 1,34,49].\r\nThere also has been a recent focus on enhancing the inter-\r\npretability [ 5,50,51] of the HPO process [ 3,31,32].Multi-ﬁdelity methods. Multi-ﬁdelity methods exploit low\r\nand high ﬁdelities for conﬁgurations to save the evaluation\r\ntime. Successive halving (SHA) [ 16] runs a set of conﬁg-\r\nurations using a small number of resources and promotes\r\nonly the best-performing half of conﬁgurations to continue\r\nfor twice as many resources. Hyperband [ 25] calls SHA as\r\na sub-routine with varying maximum resources and intro-\r\nduces a reduction factor to control the fraction of promo-\r\ntion. ASHA [ 26] extends SHA to the asynchronous setting\r\nby aggressive early-stopping. Later, PASHA [ 4] further ex-\r\ntends ASHA through more aggressive early-stopping based\r\non the ranking of conﬁgurations during the tuning process.\r\nCombination of model-based and multi-ﬁdelity meth-\r\nods. BOHB [ 10] and a parallel work [ 47] ﬁrst propose\r\nto combine model-based and multi-ﬁdelity methods by re-\r\nplacing the random sampling in Hyperband with BO. A-\r\nBOHB [ 22] employs a joint GP surrogate over ﬁdelities and\r\nsupports asynchronous scheduling. Hyper-Tune [ 27] im-\r\nproves its Hyperband by a delayed strategy to decrease in-\r\naccurate promotions. Salinas et al. [ 37] proposed to extend\r\nmethods to multi-ﬁdelity settings by using the performance\r\nof the last ﬁdelity in an ASHA running. DyHPO [ 48] and\r\nDPL [ 20] introduce new surrogates for multi-ﬁdelity BO\r\nconsidering the learning curves; the former uses deep GP\r\nkernels while the latter integrates deep power law functions.\r\n3. Problem Formulation\r\nGiven an algorithm having hyperparameters \x001,. . . ,\x00 mwith\r\ndomains ⇤1,. . . ,⇤m, we deﬁne its hyperparameter space as\r\n⇤=⇤1⇥...⇥⇤m. Here, we deﬁne the problem and out-\r\nline the key challenge related to hyperparameter optimiza-\r\ntion (HPO). Notations are in Supp. 7for reference.\r\nSingle-ﬁdelity setting. For each hyperparameter conﬁg-\r\nuration \x00, we denote f(\x00)as the performance achieved\r\nusing \x00. For consistency, the metric in this paper refers\r\nto descending metrics like validation loss, with ascending\r\nmetrics being treated similarly. In the single-ﬁdelity HPO\r\nsetting, we aim to ﬁnd \x00⇤minimizing function f(\x00), i.e.,\r\n\x00⇤= arg min\x002⇤f(\x00). BO is one of the most popu-\r\nlar single-ﬁdelity HPO methods. The vanilla BO has two\r\nkey components: a surrogate model Mto approximate the\r\nobjective function f(\x00), and an acquisition function ato\r\nidentify a promising conﬁguration from search space. With\r\nthese ingredients, BO iterates three steps: (i) select a con-\r\nﬁguration \x00iby maximizing the acquisition function; (ii)\r\nevaluate \x00ito get yiand add the data (\x00i,yi)into the cur-\r\nrent observation set Di\x001={(\x001,y1),. . . ,(\x00i\x001,yi\x001)};\r\n(iii) update the surrogate model and the acquisition function\r\nbased on the augmented Di. In this work, Mis a Gaussian\r\nProcess and ais Expected Improvement.\r\nMulti-ﬁdelity setting. Multi-ﬁdelity methods consider re-\r\nsource information, such as training epochs or training sub-\r\nset ratios. Evaluations at various resource levels results in\r\n26182\r\ndifferent performance levels, known as the ﬁdelities . Differ-\r\nent ﬁdelities provide a way to balance computational cost\r\nand performance quality. In multi-ﬁdelity HPO problems,\r\nthe target is extended to \x00⇤= arg min\x002⇤f(\x00,r), where\r\nf(\x00,r)is the objective function obtained for \x00atr. We use\r\nrto denote the resource level, which can also be interpreted\r\nas the ﬁdelity, and r2{rmin,. . . ,r max}.\r\nExtending single-ﬁdelity methods to the multi-ﬁdelity\r\nsetting. The inefﬁciency of single-ﬁdelity methods stems\r\nfrom their reliance on the ﬁnal ﬁdelity evaluation of\r\nf(\x00,rmax)to be the evaluation of its objective f(\x00,r).\r\nFitting surrogate models by such ﬁnal ﬁdelity evaluations\r\nincurs high cost due to the full evaluation of the conﬁgu-\r\nrations. Notably, low-ﬁdelity evaluations at r<r maxpro-\r\nvide informative insights into the objective but are computa-\r\ntionally cheaper, which is valuable to the optimization pro-\r\ncess. Therefore, we seek an effective way to extend single-\r\nﬁdelity methods like BO to the multi-ﬁdelity setting. More\r\nspeciﬁcally, recalling the earlier steps of BO, when eval-\r\nuating the conﬁguration \x00iin the second step, we instead\r\nacquire its low-ﬁdelity performance yri\r\niatri, where ride-\r\nnotes the ﬁdelity used for \x00ito ﬁt the surrogate model. The\r\nobservations Dithen becomes {(\x001,yr1\r\n1),. . . ,(\x00i,yri\r\ni)}.T o\r\nconclude, in order to extend single-ﬁdelity methods to the\r\nmulti-ﬁdelity setting, the key challenge to be addressed is\r\nto determine rifor each \x00i.\r\n4. Methodology\r\nIn this section, we propose a novel multi-ﬁdelity model-\r\nbased algorithm FastBO. We ﬁrst propose the key concepts\r\nof efﬁcient point and saturation point, which are crucial in\r\ndeciding the ﬁdelity level to ﬁt the surrogate model and to\r\napproximate the ﬁnal ﬁdelity respectively. Secondly, we\r\nelaborate on the details of learning curve modeling, where\r\nthe two crucial points can be extracted. Then, we present\r\nthe techniques associated with the auxiliary warm-up and\r\npost-processing stages. Finally, we summarize FastBO and\r\ndiscuss its wide applicability to any single-ﬁdelity methods.\r\n4.1. Estimation of Efﬁcient and Saturation Points\r\nIn our method, we adaptively identify efﬁcient and satura-\r\ntion points for each conﬁguration. The two points are cru-\r\ncial in the optimization process. We ﬁrst formally deﬁne the\r\nefﬁcient point as follows.\r\nDeﬁnition 1 (Efﬁcient point) .For a given learning curve\r\nCi(r)of hyperparameter conﬁguration \x00i, where rrepre-\r\nsents the resource level (also referred to as ﬁdelity), the ef-\r\nﬁcient point eiof\x00iis deﬁned as: ei=m i n {r|Ci(r)\x00\r\nCi(2r)<\x00 1}, where \x001is a predeﬁned small threshold.\r\nThe semantic of Deﬁnition 1is that starting from the ef-\r\nﬁcient point onwards, when the resources are doubled (i.e.,from rto2r), the performance improvement falls below a\r\nsmall threshold \x001. Consequently, this point characterizes\r\nthe ﬁdelity at which a conﬁguration demonstrates strong\r\nperformance while still efﬁciently utilizing resources. In\r\nsimpler terms, it signiﬁes an appropriate ﬁdelity of perfor-\r\nmance that can be achieved with comparably efﬁcient re-\r\nsource usage. Therefore, we make the following remark.\r\nRemark 1. The efﬁcient points of the hyperparameter con-\r\nﬁgurations can serve as their appropriate ﬁdelities used for\r\nﬁtting the surrogate model. This is due to their (i) opti-\r\nmal resource-to-performance balance, (ii) ability to capture\r\nvaluable learning curve trends, and (iii) customization for\r\ndifferent hyperparameter conﬁgurations.\r\nWe elaborate on the reasons in Remark 1as follows.\r\nFirstly, efﬁcient points balance the trade-off between com-\r\nputational cost and result quality. Beyond the efﬁcient point\r\nof a given conﬁguration, allocating additional resources to\r\nthat conﬁguration becomes less efﬁcient. Secondly, ef-\r\nﬁcient points capture valuable trends within the learning\r\ncurves. For example, the learning rate inﬂuences the shape\r\nof learning curves; the identiﬁcation of efﬁcient points for\r\nconﬁgurations with smaller learning rates often occurs at\r\nlater stages. The insights into learning curve behaviors en-\r\nable more informed decision-making. Thirdly, the ability\r\nto customize the ﬁdelity for each speciﬁc conﬁguration is a\r\nsigniﬁcant advantage. This adaptive approach is more rea-\r\nsonable than previous studies that use a ﬁxed ﬁdelity for all\r\nconﬁgurations, as it better accounts for the unique charac-\r\nteristics of individual learning curves.\r\nThis insight leads us to use the efﬁcient point eiiden-\r\ntiﬁed for each conﬁguration \x00ias its ﬁdelity used to ﬁt the\r\nsurrogate model. Speciﬁcally, we evaluate \x00iuntil reaching\r\neiand obtain the observed performance yei\r\ni. The resulting\r\ndata point (\x00i,yei\r\ni)is then added into the current observa-\r\ntion set Di\x001to reﬁt the surrogate model. We proof the\r\nsuperiority of FastBO over SHA-based methods in Supp. 8.\r\nBesides efﬁcient points, we identify saturation points for\r\nall conﬁgurations from their learning curves as well. We\r\nprovide the deﬁnition of the saturation point as follows.\r\nDeﬁnition 2 (Saturation point) .For a given learning curve\r\nCi(r)of conﬁguration \x00i, where rrepresents the resource\r\nlevel (also referred to as ﬁdelity), the saturation point siof\r\n\x00iis deﬁned as: si=m i n {r|8r0>r , |Ci(r0)\x00Ci(r)|<\r\n\x002}, where \x002is a predeﬁned small threshold.\r\nThe semantic of Deﬁnition 2is that beyond the satura-\r\ntion point, the observed performance no longer exhibits no-\r\ntable variations with more resources. Thus, this point char-\r\nacterizes the ﬁdelity at which the performance of a conﬁg-\r\nuration stabilizes. The concept of saturation point is well-\r\nrecognized within the machine learning community. Build-\r\ning on the above deﬁnition, we make the following remark.\r\n26183\r\nRemark 2. The saturation points of the hyperparameter\r\nconﬁgurations can serve as their approximate ﬁnal ﬁdeli-\r\nties, as they provide performance results that meet prede-\r\nﬁned quality thresholds while reducing resource wastage.\r\nThis insight leads us to use the saturation point siidenti-\r\nﬁed for each conﬁguration \x00ias its ﬁnal ﬁdelity approxima-\r\ntion. The point is used in the post-processing stage for pro-\r\nmoting some well-performing conﬁgurations to get higher-\r\nﬁdelity performances. In essence, when aiming for a full\r\nevaluation of the conﬁgurations, we suggest that terminat-\r\ning the evaluation at the saturation point is sufﬁcient. A\r\nmore intuitive illustration of the concepts of efﬁcient and\r\nsaturation points is provided in Supp. 9.\r\n4.2. Learning Curve Modeling\r\nFrom Deﬁnitions 1and 2, we can extract the efﬁcient\r\nand saturation points of conﬁgurations from their learn-\r\ning curves. The curve Ci(r)corresponds to conﬁguration\r\n\x00iand describes the predictive performance with \x00ias a\r\nfunction of the ﬁdelity r. Here, rcan be either the num-\r\nber of training instances or the number of training epochs.\r\nIn the context of learning curves, the former is referred\r\nto as observation learning curves, while the latter is iter-\r\nation learning curves [ 29]. Both types are applicable to\r\nFastBO, so we use the term learning curve to encompass\r\nboth. Given the observation set Ow\r\ni={(r, yr\r\ni)}r=rmin,...,w\r\nfor\x00i, which comprises pairs of data points representing\r\nﬁdelities r2{rmin,. . . ,w }and the corresponding evalua-\r\ntions yr\r\ni, where wis a pre-deﬁned warm-up point to stop\r\ncollecting data, FastBO can estimate a learning curve for\r\n\x00ibased on Ow\r\niby ﬁrst constructing a parametric learning\r\ncurve model, then estimating the parameters.\r\nConstructing a parametric learning curve model. Empir-\r\nical learning curves can be modeled with function classes\r\nrelying on some parameters. Viering and Loog [ 46] com-\r\nprehensively summarized the parametric models studied in\r\nmachine learning. In practice, different problems have dif-\r\nferent learning curves; even under the same problem, differ-\r\nent hyperparameter conﬁgurations ( e.g., learning rate, regu-\r\nlarization, etc.) may lead to signiﬁcantly different learning\r\ncurves. Since one single parametric model is not enough\r\nto characterize all the learning curves by itself, we consider\r\ncombining different parametric models into a single model.\r\nSpeciﬁcally, we consider three parametric models POW3,\r\nEXP3 and LOG2, as listed in Tab. 1, which have shown\r\nTable 1. Parametric learning curve models used.\r\nModel Formula Family\r\nPOW3 y=d+ax\x00↵Power law\r\nEXP3 y=d+e\x00ax+bExponential\r\nLOG2 y=d+alog(x)Logarithmicgood ﬁtting and predicting performance in previous empiri-\r\ncal studies [ 29,46]. We provide detailed discussions on the\r\nchoice of parametric models in Supp. 10.\r\nHere, we denote each parametric model as cj(r|✓j)with\r\nparameters ✓j, where the independent variable rrepresents\r\nthe ﬁdelity. We combine three models into one model via a\r\nweighted linear combination:\r\nC(r|\x00)=X\r\nj2{1,2,3}!jcj(r|✓j), (1)\r\nwhere \x00={!1,!2,!3,✓1,✓2,✓3}is the parameter of the\r\ncombined model, which consists of parameters {✓1,✓2,✓3}\r\nand weight {!1,!2,!3}of every single model. Therefore,\r\neach pair of observations (r, yr\r\ni)inOw\r\nican be modeled by\r\nthe combined model as yr\r\ni=C(r|\x00)+✏, where yr\r\niis the\r\nobserved dependent variable and ✏represents the error term.\r\nEstimating parameters in the parametric learning curve\r\nmodel. We employ maximum likelihood estimation to esti-\r\nmate the parameters \x00in the parametric model C(r|\x00). As-\r\nsuming that ✏⇠N(0,\x002), the probability of an observed\r\nperformance yr\r\niunder parameters is given by p(yr\r\ni|\x00,\x002)=\r\nN(yr\r\ni;C(r|\x00),\x002). Given the observations Ow\r\niof\x00ithat\r\ncontains a set of observed data points (r, yr\r\ni), the likelihood\r\nfunction can be expressed as:\r\nL(\x00,\x002;r,yr\r\ni)=Y\r\np(yk\r\ni|\x00,\x002)\r\n=wY\r\nk=rmin1\r\n\x00p\r\n2⇡exp✓\r\n\x00(yr\r\ni\x00C(r=k|\x00))2\r\n2\x002◆\r\n.(2)\r\nWe estimate \x00by maximizing log-likelihood function,\r\nwhich is easily calculated given Eq. 2.\r\nAn existing model-free method [ 7] also considers us-\r\ning learning curves for the HPO problem. However, it tar-\r\ngets predicting the high-ﬁdelity performance from the low-\r\nﬁdelity observations and thus stopping conﬁgurations that\r\nare unlikely to beat the current best values, which is differ-\r\nent from our main target of identifying appropriate ﬁdelity\r\nlevels for the conﬁgurations to ﬁt the surrogate model from\r\ntheir estimated learning curves.\r\n4.3. Warm-up And Post-processing Stages\r\nIn addition to its core components, FastBO incorporates\r\ntwo auxiliary stages: the warm-up and post-processing\r\nstages. For the completeness of our method, we provide an\r\noverview of these stages, outlining their targets and present-\r\ning the key techniques of early-termination detection and\r\nsaturation-level evaluation that are applied within.\r\nWarm-up stage. The warm-up stage prepares the early\r\nobservation set Ow\r\nifor each conﬁguration \x00ithat is used\r\nto estimate its learning curve, as discussed in § 4.2. Here\r\nw2(rmin,rmax)is a pre-determined ﬁdelity, denoted as\r\nwarm-up point. Speciﬁcally, we initiate the evaluation of\r\n26184\r\neach newly selected \x00i, proceeding until reaching w. Dur-\r\ning this process, we record each ﬁdelity rand its evaluation\r\nresult yr\r\ni, forming pairs (r, yr\r\ni). Upon reaching w, we pause\r\nthe evaluation for \x00iand obtain its early observation set\r\nOw\r\ni={(r, yr\r\ni)}r=rmin,...,w, and start modeling the learning\r\ncurve. During the warm-up stage, we monitor the perfor-\r\nmance changes across every two continuous ﬁdelities. If we\r\ndetect that the performance of \x00ihas consecutively dropped\r\ntwice by more than a ratio ↵, i.e., ( yr\x001\r\ni\x00yr\x002\r\ni)>↵ yr\x002\r\ni\r\nand(yr\r\ni\x00yr\x001\r\ni)>↵ yr\x001\r\ni, we promptly terminate the eval-\r\nuation for \x00iat its current ﬁdelity r, because such con-\r\nsecutive performance deterioration indicates \x00iis unlikely\r\nto achieve satisfactory performance. Once terminated, we\r\ndirectly incorporate the current performance yr\r\niof\x00iinto\r\nDi\x001that is used for updating the surrogate model. Thus,\r\nfurther operations like learning curve modeling are dis-\r\ncontinued for \x00i. Moreover, if we observe a single case\r\nof performance drop without subsequent occurrences, i.e,\r\nyr\x001\r\ni\x00yr\x002\r\ni>↵ yr\x002\r\niandyr\r\ni\x00yr\x001\r\ni\uf8ff↵yr\x001\r\ni, we opt not\r\nto include data from ﬁdelity r\x001inOw\r\ni. This is to manu-\r\nally ﬁlter out potential noise in the data that may adversely\r\naffect the ﬁtting of the learning curve.\r\nPost-processing stage. The post-processing stage aims\r\nat two tasks: promoting the well-performing conﬁgura-\r\ntions for saturation-level evaluations and identifying the\r\nbest conﬁguration and its performance. Firstly, FastBO pro-\r\nmotes the top- kwell-performing conﬁgurations and evalu-\r\nates them to their saturation points to ensure high-quality\r\nperformance while maintaining efﬁcient resource utiliza-\r\ntion. We set kto be always less than or equal to the number\r\nof parallel workers available, ensuring a manageable over-\r\nhead of saturation-level evaluations. It is worth noting that\r\nthe additional time required is factored into the overall time.\r\nSecondly, FastBO ﬁnds the best conﬁguration along with its\r\nperformance achieved so far, which is a standard ﬁnal step\r\nin most HPO methods. However, an increase in ﬁdelities\r\ndoes not always result in performance improvement, possi-\r\nbly due to overﬁtting, resource saturation, or problem com-\r\nplexity. Therefore, we treat the evaluation at each ﬁdelity\r\nas an individual task and record all these intermediate eval-\r\nuation results, which is also a common practice in recent\r\nimplementations. In this way, FastBO ﬁnds the best per-\r\nformance by considering all the results, rather than relying\r\nsolely on the highest-ﬁdelity performances of the conﬁgu-\r\nrations. In the parallel setting, treating each ﬁdelity evalua-\r\ntion as an individual task offers an added beneﬁt due to its\r\nﬁner granularity. More speciﬁcally, when a worker is idle,\r\nit takes on a new task of evaluating a conﬁguration at a spe-\r\nciﬁc ﬁdelity, rather than evaluating an entire conﬁguration.\r\n4.4. FastBO and Generalization\r\nAlgorithm 1summarizes our proposed FastBO. It takes sur-\r\nrogate model M, acquisition function a, warm-up point w,Algorithm 1: FastBO algorithm\r\ninput : M,a,w,↵,k,\x001,\x002.\r\noutput: \x00⇤,y⇤\r\n1i 0,D ;\r\n2while not meet the stop criterion do\r\n3 ﬁnd\x00i arg max\x002⇤a(\x00,Mi\x001)\r\n4 Ow\r\ni,t warm-up given w,↵//cf.§4.3\r\n5 ifOw\r\niis not empty then\r\n6 ﬁtCi(r)toOw\r\ni//cf.§4.2\r\n7 ﬁndei,sigiven Ci(r),\x001,\x002//cf.§4.1\r\n8 yei\r\ni continue evaluating \x00itoei\r\n9 else\r\n10 ei t,si rmax\r\n11 Di D i\x001[(\x00i,yei\r\ni)\r\n12 reﬁtMitoDi\r\n13 i i+1\r\n14\x00⇤,y⇤ post-process given s={si},k//cf.§4.3\r\nperformance decrease ratio ↵, promotion number k, and\r\nthresholds \x001,\x002as inputs, and output the best-founded\r\nconﬁguration \x00⇤and its performance y⇤. FastBO follows\r\na similar iterative process of model-based methods but re-\r\nplaces the expensive full evaluations with a more intelligent\r\nalternative ( cf. Lines 4-10). Speciﬁcally, each conﬁguration\r\n\x00iﬁrst enters a warm-up stage to collect its early observa-\r\ntion set Ow\r\niand to be detected and terminated if it exhibits\r\nconsecutive performance deterioration ( cf. Line 4). If \x00i\r\nis not terminated, FastBO then estimates a learning curve\r\nCi(r)for\x00ibased on Ow\r\ni(cf. Line 6), and thus the efﬁcient\r\npoint and saturation point of \x00ican be obtained ( cf. Line\r\n7). After that, \x00icontinues to be evaluated until reaching\r\nei(cf. Line 8); the result is added to the observation set D\r\n(cf. Line 11) that is used for updating M(cf. Line 12). On\r\nthe other hand, the poorly-performing conﬁguration will be\r\nterminated early at ﬁdelity twith its result being added di-\r\nrectly to D(cf. Lines 10, 11). Finally, the post-processing\r\nstage promotes the most promising conﬁgurations to their\r\nsaturation points and ﬁnds the best-founded conﬁguration\r\n\x00⇤and its performance y⇤(cf. Line 14).\r\nGeneralizing FastBO to single-ﬁdelity methods. The core\r\nof FastBO is to tackle the key challenge of deciding an ap-\r\npropriate ﬁdelity for each conﬁguration to ﬁt the surrogate\r\nmodel by adaptively identifying its efﬁcient point. This\r\nstrategy of using the efﬁcient point performances for sur-\r\nrogate model ﬁtting also provides a simple but effective\r\nway to bridge the gap between single- and multi-ﬁdelity\r\nmethods. While it is primarily described in the context of\r\nmodel-based methods, the strategy can be generalized to\r\nvarious single-ﬁdelity methods. For example, when evalu-\r\nating conﬁgurations within the population for an evolution-\r\n26185\r\nFashion-MNIST\r\nAirlinesAlbertCovertypeChristineFigure 1. Performance of average validation accuracy on the LCBench benchmark.\r\nSlice\r\nCIFAR-10 CIFAR-100ProteinImageNet16-120\r\n(a) NAS-Bench-201 benchmark(b) FCNet benchmark\r\nFigure 2. Performance of (a)average validation error on NAS-Bench-201 and (b)average validation loss on FCNet.\r\nary algorithm-based HPO method, we can similarly evalu-\r\nate the efﬁcient point performances instead of the ﬁnal per-\r\nformances of these conﬁgurations and integrate the perfor-\r\nmances in the subsequent processes, such as selection and\r\nvariation. Relying on the efﬁcient point rather than the ﬁnal\r\nﬁdelity or all ﬁdelities available simpliﬁes the extension of\r\nthe single-ﬁdelity methods to the multi-ﬁdelity setting. The\r\nrationale behind this adaptive ﬁdelity identiﬁcation strategy\r\nis discussed in Remark 1. We also demonstrate in our ex-\r\nperiments the efﬁcacy of this strategy in extending a range\r\nof single-ﬁdelity methods to the multi-ﬁdelity setting.\r\n5. Experiments\r\nWe empirically evaluate the performance of FastBO and\r\ncompare it with the random search baseline (RS) and 9\r\ncompetitive baselines from 3 related categories, includ-\r\ning (i) model-based methods: standard Gaussian Process-\r\nbased BO [ 40]; (ii) multi-ﬁdelity methods: ASHA [ 26],\r\nHyperband [ 25], PASHA [ 4]; and (iii) model-based multi-\r\nﬁdelity methods: A-BOHB [ 22], A-CQR [ 37], BOHB [ 10],\r\nDyHPO [ 48], Hyper-Tune [ 27]. RS and BO are single-\r\nﬁdelity baselines, while the others are multi-ﬁdelity ones.\r\nOur experiments are conducted on 10 datasets from 3\r\npopular benchmarks LCBench [ 53], NAS-Bench-201 [ 8]\r\nand FCNet [ 21]. Detailed information on the benchmarks\r\nis provided in Supp. 13.1. All the experiments are evalu-\r\nated with four parallel workers and 10 random seeds. Weallocate 20% total budget for warm-up, i.e., w=rmin+\r\n0.2·(rmax\x00rmin). Ratio ↵is set to 0.1; thresholds \x001and\r\n\x002are set to 0.001 and 0.00051. We set kbased on the num-\r\nber of workers #wand the number of started conﬁgurations\r\n#c:k= max {d#c/10e,#w}. We provide more experi-\r\nments and discussions on the hyperparameters in Supp. 12.\r\nWe use implementations of the baselines in Syne Tune [ 36].\r\nDetails of the baseline settings are in Supp. 13.2.\r\n5.1. Anytime Performance\r\nTo evaluate the anytime performance, we compare FastBO\r\nagainst the baselines on wall-clock time. For fair compar-\r\nisons, all the baselines, even single-ﬁdelity BO and RS, are\r\nextended to consider intermediate results at all the ﬁdelities\r\nwhen identifying the conﬁguration, akin to FastBO as dis-\r\ncussed in § 4.3. Consequently, all the baselines are able to\r\nachieve their best possible anytime performance.\r\nThe results on LCBench, NAS-Bench-201, and FCNet\r\nare shown in Figs. 1and2. We report the validation ac-\r\ncuracy, error, and loss over wall-clock time for the three\r\nbenchmarks, as provided by the benchmarks. We provide\r\nthe results on NAS-Bench-301 [ 39] in Supp. 11.1. Overall,\r\nFastBO can handle various performance metrics and shows\r\nstrong anytime performance. We can observe that FastBO\r\ngains an advantage earlier than other methods, rapidly con-\r\n1Parameters \x001and\x002given here are derived after standardizing met-\r\nrics to a uniform scale from 0 to 1.\r\n26186\r\nTable 2. Comparison of relative efﬁciency on conﬁguration identiﬁcation. FastBO is set as the baseline with a relative efﬁciency of 1.00.\r\nWall-clock time (abbr. WC time) reports the elapsed time spent for each method on ﬁnding conﬁgurations with similar performance\r\nmetrics, i.e., validation error ( ⇥10\x002) for Covertype and ImageNet16-120 and validation loss ( ⇥10\x005) for Slice.\r\nDatasetMetric Method\r\nFastBO BO PASHA A-BOHB A-CQR BOHB DyHPO Hyper-Tune\r\nCovertypeVal. error 22.9 ±0.223.0 ±0.3 25.1 ±2.5 23.5 ±1.1 31.6 ±1.9 32.5 ±0.8 23.0 ±0.3 23.0 ±0.2\r\nWC time (h) 0.7±0.3 2.9±0.7 3.9±1.0 2.0±1.0 3.9±0.2 2.5±1.0 1.7±0.6 1.8±0.7\r\nRel. efﬁciency 1.00 0.25 0.18 0.37 0.19 0.29 0.41 0.40\r\nImageNet\r\n16-120Val. error 55.3 ±0.257.4 ±1.2 55.7 ±0.3 55.8 ±1.6 55.5 ±0.9 55.5 ±1.1 55.5 ±1.0 55.3 ±2.0\r\nWC time (h) 2.2±0.7 6.6±0.9 2.5±1.2 5.9±1.1 6.0±1.3 3.2±0.7 4.3±1.0 3.4±1.1\r\nRel. efﬁciency 1.00 0.34 0.90 0.38 0.37 0.68 0.51 0.67\r\nSliceVal. loss 26.3 ±2.626.4 ±4.4 26.8 ±9.5 26.3 ±6.3 27.1 ±4.2 26.8 ±5.6 27.4 ±2.3 28.7 ±1.3\r\nWC time (h) 0.4±0.1 3.1±0.7 1.2±0.9 2.1±0.7 2.5±0.7 2.2±0.9 2.5±0.5 1.8±0.6\r\nRel. efﬁciency 1.00 0.13 0.35 0.20 0.17 0.19 0.17 0.24\r\nverging to the global optimum after the initial phase.\r\nThe superiority can be attributed to two main factors.\r\nFirstly, FastBO maintains, and in some cases even sur-\r\npasses, the sample efﬁciency of vanilla BO, thanks to\r\nour techniques that enable quick and precise identiﬁcation\r\nof the ﬁdelities for conﬁgurations to update the surrogate\r\nmodel. We provide more explanations and experiments on\r\nsample efﬁciency in Supp. 11.2. Secondly, the multi-ﬁdelity\r\nextension speeds up the evaluations, contributing to its over-\r\nall efﬁciency. In contrast, the single-ﬁdelity baselines tend\r\nto waste more time on the full evaluations. While the multi-\r\nﬁdelity baselines efﬁciently explore numerous conﬁgura-\r\ntions, they limit their evaluations to only constrained ﬁdeli-\r\nties for some time, thus struggling to provide relatively high\r\nperformance in a short time. This issue in multi-ﬁdelity\r\nmethods is particularly pronounced in PASHA when ap-\r\nplied to NAS-Bench-201 and FCNet, as shown in Fig. 2.\r\nIn Supp. 11.3, we further provide the ranks of all methods\r\nand statistically show FastBO’s superiorty on an early stage.\r\nIt is worth noting that all the additional overhead introduced\r\nby FastBO is taken into account in the wall-clock time.\r\nRegarding the ﬁnal performance, most methods are able\r\nto converge to satisfactory solutions, with negligible differ-\r\nences among them in most cases. Although our goal is not\r\nto offer the best ﬁnal performance as we limit the evalua-\r\ntions to at most the saturation point even for those we con-\r\nsider most promising, FastBO still achieves top-2 ﬁnal per-\r\nformance on 8 out of 10 datasets. In contrast, model-free\r\nmethods sometimes cannot obtain a satisfactory ﬁnal per-\r\nformance because they randomly select the conﬁgurations.\r\nFor example, on the “Covertype” dataset, only 3 out of 2000\r\nconﬁgurations yield a validation accuracy exceeding 75%.\r\nAs a result, all the model-free methods face challenges in\r\nconverging to a satisfactory ﬁnal performance.5.2. Efﬁciency on Conﬁguration Identiﬁcation\r\nOne explanation for PASHA’s suboptimal anytime perfor-\r\nmance ( cf. Fig. 2) lies in its primary goal [ 4]: the goal of\r\nPASHA is not high accuracy but to identify the best conﬁg-\r\nuration more quickly. To ensure equitable comparisons, we\r\nreport the time spent for each method on identifying a sat-\r\nisfactory conﬁguration, consistent with the experiments de-\r\nscribed in PASHA [ 4]. Results on three expensive datasets\r\n“Covertype”2, “ImageNet16-120”, and “Slice” of the three\r\nbenchmarks are shown in Tab. 2. Similar results on addi-\r\ntional datasets can be found in Supp. 11.4. Besides PASHA,\r\nresults of other model-free multi-ﬁdelity methods are not in-\r\ncluded, as PASHA demonstrates its superiority over them.\r\nTab. 2shows that FastBO saves 10% to 87% wall-clock\r\ntime over other methods when achieving up to 9.6% better\r\nperformance values. It can be observed from the “rel. efﬁ-\r\nciency” rows, where we set FastBO as the baseline with a\r\nrelative efﬁciency of 1.00 and report the efﬁciency of other\r\nmethods relative to ours. When compared with vanilla BO,\r\nFastBO signiﬁcantly shortens the time in identifying a good\r\nconﬁguration by a factor of 3 to 8, because FastBO pauses\r\na conﬁguration earlier at an appropriate ﬁdelity and ﬁts the\r\nsurrogate model to guide the next conﬁguration search. This\r\nadvantage creates opportunities to efﬁciently explore more\r\nconﬁgurations. Another observation is that PASHA always\r\ngets a relatively high variance in wall-clock time. This is\r\ndue to the fact that different random seeds can have a larger\r\nimpact on such model-free methods.\r\n5.3. Effectiveness of Adaptive Fidelity Identiﬁcation\r\nAs discussed in § 4.1, FastBO is able to adaptively identify\r\nthe efﬁcient point eifor each conﬁguration \x00iand serves\r\n2We convert the accuracy of “Covertype” into error for readability.\r\n26187\r\nSliceImageNet16-120Covertype\r\nSliceImageNet16-120Covertype(a) Impact of adaptive fidelity identification(b) Generality of FastBOFigure 3. Performance comparison: (a)Performance of FastBO that adaptively sets ri=eiwith the schemes that use ﬁxed rifor all\r\nconﬁgurations. (b)Performance of single-ﬁdelity methods CQR, BORE, REA and their multi-ﬁdelity variants using our extension method.\r\neias its ﬁdelity rifor surrogate model ﬁtting. To investi-\r\ngate the effectiveness of the adaptive ﬁdelity identiﬁcation\r\nstrategy, we conduct an ablation study to compare the per-\r\nformance achieved with and without applying this strategy.\r\nSpeciﬁcally, we compare FastBO, where riis adaptively set\r\ntoei, with the partial evaluation schemes that employ ﬁxed\r\npredeﬁned values as the ﬁdelity for all the conﬁgurations\r\nto ﬁt the surrogate model. We consider three representative\r\nﬁxed ﬁdelities, including 25%, 50%, and 75% of the total\r\nresource budget. In addition, we include a comparison with\r\nvanilla BO that can be viewed as using 100% resource bud-\r\nget as the ﬁxed ﬁdelity for all conﬁgurations.\r\nWe provide the results on three representative datasets\r\nin Fig. 3(a), with more results available in Supp. 11.5.W e\r\nhave three main observations. Firstly, FastBO always out-\r\nperforms the partial evaluation baselines that use a ﬁxed ﬁ-\r\ndelity, indicating the effectiveness of the adaptive strategy.\r\nSecondly, FastBO shows stronger performance than vanilla\r\nBO. The limitation of vanilla BO lies in the additional time\r\nrequired for full evaluations. Secondly, compared to the\r\nvanilla BO, partial evaluation schemes with ﬁxed ricon-\r\nverge faster in the initial stage due to their ability to evaluate\r\nmore conﬁgurations promptly, but this advantage is gradu-\r\nally offset over time because they fail to ﬁnd appropriate\r\nﬁdelities to create an accurate surrogate model. This causes\r\na suboptimal ﬁnal performance compared to vanilla BO, as\r\nshown in the ﬁrst two ﬁgures in Figs. 3(a). In the case of the\r\nlast one, we can observe a noticeable upward trend exhib-\r\nited by the vanilla BO towards the end of the evaluation, in-\r\ndicating its potential to improve the ﬁnal performance given\r\nabundant time. The comparison between the partial evalua-\r\ntion baselines and vanilla BO also demonstrates the impor-\r\ntance of our adaptive strategy, which ensures that the ﬁdeli-\r\nties align optimally with each conﬁguration.\r\n5.4. Generality of The Proposed Extension Method\r\nThe adaptive ﬁdelity identiﬁcation strategy provides a sim-\r\nple way to extend single-ﬁdelity methods to the multi-\r\nﬁdelity setting, as discussed in § 4.4. To examine the abil-\r\nity of our extension method, we conduct experiments using\r\nthree popular single-ﬁdelity methods CQR [ 37], BORE [ 45]and REA [ 35], extending them to the multi-ﬁdelity variants\r\nwith our extension method, referred to as FastCQR, Fast-\r\nBORE, and FastREA respectively. Similar to FastBO, all\r\nthe multi-ﬁdelity extensions evaluate the conﬁgurations to\r\nthe adaptively identiﬁed efﬁcient point and use the corre-\r\nsponding performances for the subsequent operations. The\r\nresults on three datasets are illustrated in Fig. 3(b) and\r\nsimilar results on other datasets are in Supp. 11.6.W e\r\ncan clearly observe that the multi-ﬁdelity variants with our\r\nextension method always outperform their single-ﬁdelity\r\ncounterparts. It is worth noting that REA is an evolu-\r\ntionary algorithm-based HPO method and is also signiﬁ-\r\ncantly improved by our extension. The observation high-\r\nlights the ability of the proposed adaptive strategy to extend\r\nany single-ﬁdelity method to the multi-ﬁdelity setting. It\r\nalso suggests future opportunities to extend other advanced\r\nsingle-ﬁdelity techniques into the multi-ﬁdelity setting.\r\n6. Conclusion\r\nIn this paper, we propose a model-based multi-ﬁdelity HPO\r\nmethod FastBO, which adaptively identiﬁes the appropriate\r\nﬁdelity for each conﬁguration to ﬁt the surrogate model and\r\noffers high-quality performance while ensuring efﬁcient re-\r\nsource utilization. The advantages are achieved through\r\nour concepts of efﬁcient and saturation point, the proposed\r\ntechniques of learning curve modeling, and well-designed\r\nwarm-up and post-processing stages with judicious early-\r\ntermination detection and efﬁcient saturation-level evalua-\r\ntion. Moreover, the proposed adaptive ﬁdelity identiﬁcation\r\nstrategy provides a simple way to extend any single-ﬁdelity\r\nmethod to the multi-ﬁdelity setting. Experiments demon-\r\nstrate the effectiveness and wide generality of our pro-\r\nposed techniques. FastBO source code is freely available\r\nathttps://github.com/jjiantong/FastBO .\r\nAcknowledgment\r\nThis research was funded by ARC Grant number\r\nDP190102443. Ajmal Mian is the recipient of an Australian\r\nResearch Council Future Fellowship Award (project num-\r\nber FT210100268) funded by the Australian Government.\r\n26188\r\nReferences\r\n[1]Sebastian E Ament and Carla P Gomes. Scalable ﬁrst-order\r\nBayesian Optimization via structured automatic differenti-\r\nation. In International Conference on Machine Learning ,\r\npages 500–516. PMLR, 2022. 2\r\n[2]James Bergstra, R ´emi Bardenet, Yoshua Bengio, and Bal ´azs\r\nK´egl. Algorithms for hyper-parameter optimization. Ad-\r\nvances in Neural Information Processing Systems , 24, 2011.\r\n1,2\r\n[3]Bernd Bischl, Martin Binder, Michel Lang, Tobias Pielok,\r\nJakob Richter, Stefan Coors, Janek Thomas, Theresa Ull-\r\nmann, Marc Becker, Anne-Laure Boulesteix, et al. Hyper-\r\nparameter optimization: Foundations, algorithms, best prac-\r\ntices, and open challenges. Wiley Interdisciplinary Reviews:\r\nData Mining and Knowledge Discovery , 13(2):e1484, 2023.\r\n2\r\n[4]Ondrej Bohdal, Lukas Balles, Martin Wistuba, Beyza Ermis,\r\nC´edric Archambeau, and Giovanni Zappella. PASHA: ef-\r\nﬁcient HPO and NAS with progressive resource allocation.\r\nInInternational Conference on Learning Representations .\r\nOpenReview.net, 2023. 1,2,6,7,4,9\r\n[5]Chaofan Chen, Oscar Li, Daniel Tao, Alina Barnett, Cynthia\r\nRudin, and Jonathan K Su. This looks like that: deep learn-\r\ning for interpretable image recognition. Advances in Neural\r\nInformation Processing Systems , 32, 2019. 2\r\n[6]Tianqi Chen and Carlos Guestrin. Xgboost: A scalable tree\r\nboosting system. In Proceedings of the 22nd acm sigkdd\r\ninternational conference on knowledge discovery and data\r\nmining , pages 785–794, 2016. 9\r\n[7]Tobias Domhan, Jost Tobias Springenberg, and Frank Hutter.\r\nSpeeding up automatic hyperparameter optimization of deep\r\nneural networks by extrapolation of learning curves. In In-\r\nternational Joint Conference on Artiﬁcial Intelligence , 2015.\r\n4\r\n[8]Xuanyi Dong and Yi Yang. NAS-Bench-201: Extending the\r\nscope of reproducible neural architecture search. In Interna-\r\ntional Conference on Learning Representations , 2020. 1,6,\r\n8\r\n[9]Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter.\r\nNeural architecture search: A survey. The Journal of Ma-\r\nchine Learning Research , 20(1):1997–2017, 2019. 1\r\n[10] Stefan Falkner, Aaron Klein, and Frank Hutter. BOHB:\r\nRobust and efﬁcient hyperparameter optimization at scale.\r\nInInternational Conference on Machine Learning , pages\r\n1437–1446. PMLR, 2018. 1,2,6,9\r\n[11] Matthias Feurer and Frank Hutter. Hyperparameter opti-\r\nmization. Automated Machine Learning: Methods, Systems,\r\nChallenges , pages 3–33, 2019. 1\r\n[12] Peter I Frazier, Warren B Powell, and Savas Dayanik. A\r\nknowledge-gradient policy for sequential information col-\r\nlection. SIAM Journal on Control and Optimization , 47(5):\r\n2410–2439, 2008. 2\r\n[13] Jos´e Miguel Hern ´andez-Lobato, Matthew W Hoffman, and\r\nZoubin Ghahramani. Predictive entropy search for efﬁcient\r\nglobal optimization of black-box functions. Advances in\r\nNeural Information Processing systems , 27, 2014. 2[14] Frank Hutter, Holger H Hoos, and Kevin Leyton-Brown.\r\nSequential model-based optimization for general algorithm\r\nconﬁguration. In Learning and Intelligent Optimization ,\r\npages 507–523. Springer, 2011. 1,2\r\n[15] Carl Hvarfner, Danny Stoll, Artur L. F. Souza, Marius Lin-\r\ndauer, Frank Hutter, and Luigi Nardi. $ \\pi$BO: Augmenting\r\nacquisition functions with user beliefs for bayesian optimiza-\r\ntion. In International Conference on Learning Representa-\r\ntions . OpenReview.net, 2022. 2\r\n[16] Kevin Jamieson and Ameet Talwalkar. Non-stochastic best\r\narm identiﬁcation and hyperparameter optimization. In Ar-\r\ntiﬁcial Intelligence and Statistics , pages 240–248. PMLR,\r\n2016. 1,2,9\r\n[17] Kasra Jamshidi, Harry Xu, and Keval V ora. Accelerating\r\ngraph mining systems with subgraph morphing. In European\r\nConference on Computer Systems , pages 162–181, 2023. 2\r\n[18] Jiantong Jiang, Zeyi Wen, Zeke Wang, Bingsheng He, and\r\nJian Chen. Parallel and distributed structured svm training.\r\nIEEE Transactions on Parallel and Distributed Systems , 33\r\n(5):1084–1096, 2021.\r\n[19] Jiantong Jiang, Zeyi Wen, and Ajmal Mian. Fast parallel\r\nbayesian network structure learning. In IEEE International\r\nParallel and Distributed Processing Symposium , pages 617–\r\n627. IEEE, 2022. 2\r\n[20] Arlind Kadra, Maciej Janowski, Martin Wistuba, and Josif\r\nGrabocka. Scaling laws for hyperparameter optimization. In\r\nAdvances in Neural Information Processing Systems , 2023.\r\n2\r\n[21] Aaron Klein and Frank Hutter. Tabular benchmarks for\r\njoint architecture and hyperparameter optimization. arXiv\r\npreprint arXiv:1905.04970 , 2019. 6,8\r\n[22] Aaron Klein, Louis C Tiao, Thibaut Lienart, Cedric Archam-\r\nbeau, and Matthias Seeger. Model-based asynchronous hy-\r\nperparameter and neural architecture search. arXiv preprint\r\narXiv:2003.10865 , 2020. 1,2,6,9\r\n[23] Prasanth Kolachina, Nicola Cancedda, Marc Dymetman, and\r\nSriram Venkatapathy. Prediction of learning curves in ma-\r\nchine translation. In Annual Meeting of the Association for\r\nComputational Linguistics (Volume 1: Long Papers) , pages\r\n22–30, 2012. 2\r\n[24] Cheng Li, Santu Rana, Sunil Gupta, Vu Nguyen, Svetha\r\nVenkatesh, Alessandra Sutti, David Rubin de Celis Leal, Teo\r\nSlezak, Murray Height, Mazher Mohammed, and Ian Gib-\r\nson. Accelerating experimental design by incorporating ex-\r\nperimenter hunches. In International Conference on Data\r\nMining , pages 257–266. IEEE Computer Society, 2018. 2\r\n[25] Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Ros-\r\ntamizadeh, and Ameet Talwalkar. Hyperband: A novel\r\nbandit-based approach to hyperparameter optimization.\r\nJournal of Machine Learning Research , 18(1):6765–6816,\r\n2017. 1,2,6,9\r\n[26] Liam Li, Kevin Jamieson, Afshin Rostamizadeh, Ekaterina\r\nGonina, Jonathan Ben-Tzur, Moritz Hardt, Benjamin Recht,\r\nand Ameet Talwalkar. A system for massively parallel hy-\r\nperparameter tuning. Proceedings of Machine Learning and\r\nSystems , 2:230–246, 2020. 1,2,6,9\r\n[27] Yang Li, Yu Shen, Huaijun Jiang, Wentao Zhang, Jixiang\r\nLi, Ji Liu, Ce Zhang, and Bin Cui. Hyper-tune: Towards\r\n26189\r\nefﬁcient hyper-parameter tuning at scale. Proceedings of the\r\nVLDB Endowment , 15(6):1256–1265, 2022. 1,2,6,9\r\n[28] Jonas Mockus. The application of Bayesian methods for\r\nseeking the extremum. Towards global optimization , 2:117,\r\n1998. 2\r\n[29] Felix Mohr and Jan N van Rijn. Learning curves for deci-\r\nsion making in supervised machine learning–a survey. arXiv\r\npreprint arXiv:2201.12150 , 2022. 4,2\r\n[30] Felix Mohr, Tom J Viering, Marco Loog, and Jan N van Rijn.\r\nLcdb 1.0: An extensive learning curves database for classi-\r\nﬁcation tasks. In Joint European Conference on Machine\r\nLearning and Knowledge Discovery in Databases , pages 3–\r\n19. Springer, 2022. 2\r\n[31] Julia Moosbauer, Julia Herbinger, Giuseppe Casalicchio,\r\nMarius Lindauer, and Bernd Bischl. Explaining hyperpa-\r\nrameter optimization via partial dependence plots. Advances\r\nin Neural Information Processing Systems , 34:2280–2291,\r\n2021. 2\r\n[32] Julia Moosbauer, Giuseppe Casalicchio, Marius Lindauer,\r\nand Bernd Bischl. Improving accuracy of interpretability\r\nmeasures in hyperparameter optimization via Bayesian al-\r\ngorithm execution. arXiv preprint arXiv:2206.05447 , 2022.\r\n2\r\n[33] ChangYong Oh, Efstratios Gavves, and Max Welling.\r\nBOCK: Bayesian optimization with cylindrical kernels.\r\nInInternational Conference on Machine Learning , pages\r\n3868–3877. PMLR, 2018. 2\r\n[34] Misha Padidar, Xinran Zhu, Leo Huang, Jacob Gardner, and\r\nDavid Bindel. Scaling gaussian processes with derivative\r\ninformation using variational inference. Advances in Neural\r\nInformation Processing Systems , 34:6442–6453, 2021. 2\r\n[35] Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V\r\nLe. Regularized evolution for image classiﬁer architecture\r\nsearch. In Proceedings of the AAAI Conference on Artiﬁcial\r\nIntelligence , pages 4780–4789, 2019. 8,5,9\r\n[36] David Salinas, Matthias Seeger, Aaron Klein, Valerio Per-\r\nrone, Martin Wistuba, and Cedric Archambeau. Syne tune:\r\nA library for large scale hyperparameter tuning and repro-\r\nducible research. In International Conference on Automated\r\nMachine Learning , pages 16–1. PMLR, 2022. 6,9\r\n[37] David Salinas, Jacek Golebiowski, Aaron Klein, Matthias W.\r\nSeeger, and C ´edric Archambeau. Optimizing hyperpa-\r\nrameters with conformal quantile regression. In Inter-\r\nnational Conference on Machine Learning , pages 29876–\r\n29893. PMLR, 2023. 1,2,6,8,5,9\r\n[38] Bobak Shahriari, Alexandre Bouchard-C ˆot´e, and Nando Fre-\r\nitas. Unbounded Bayesian Optimization via regularization.\r\nInArtiﬁcial intelligence and statistics , pages 1168–1176.\r\nPMLR, 2016. 2\r\n[39] Julien Siems, Lucas Zimmer, Arber Zela, Jovita Lukasik,\r\nMargret Keuper, and Frank Hutter. Nas-bench-301 and the\r\ncase for surrogate benchmarks for neural architecture search.\r\nCoRR , abs/2008.09777, 2020. 1,6,3\r\n[40] Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practi-\r\ncal Bayesian optimization of machine learning algorithms.\r\nAdvances in Neural Information Processing Systems , 25,\r\n2012. 1,2,6,9[41] Jasper Snoek, Oren Rippel, Kevin Swersky, Ryan Kiros, Na-\r\ndathur Satish, Narayanan Sundaram, Mostofa Patwary, Mr\r\nPrabhat, and Ryan Adams. Scalable Bayesian optimization\r\nusing deep neural networks. In International Conference on\r\nMachine Learning , pages 2171–2180. PMLR, 2015. 2\r\n[42] Jost Tobias Springenberg, Aaron Klein, Stefan Falkner, and\r\nFrank Hutter. Bayesian optimization with robust bayesian\r\nneural networks. Advances in Neural Information Process-\r\ning Systems , 29, 2016. 2\r\n[43] Niranjan Srinivas, Andreas Krause, Sham M Kakade, and\r\nMatthias Seeger. Gaussian process optimization in the bandit\r\nsetting: No regret and experimental design. arXiv preprint\r\narXiv:0912.3995 , 2009. 2\r\n[44] Kevin Swersky, Jasper Snoek, and Ryan Prescott Adams.\r\nFreeze-thaw Bayesian optimization. arXiv preprint\r\narXiv:1406.3896 , 2014. 1\r\n[45] Louis C Tiao, Aaron Klein, Matthias W Seeger, Edwin V\r\nBonilla, Cedric Archambeau, and Fabio Ramos. BORE:\r\nBayesian optimization by density-ratio estimation. In In-\r\nternational Conference on Machine Learning , pages 10289–\r\n10300. PMLR, 2021. 8,5,9\r\n[46] Tom Viering and Marco Loog. The shape of learning curves:\r\na review. IEEE Transactions on Pattern Analysis and Ma-\r\nchine Intelligence , 2022. 1,4,2\r\n[47] Jiazhuo Wang, Jason Xu, and Xuejun Wang. Combina-\r\ntion of hyperband and bayesian optimization for hyper-\r\nparameter optimization in deep learning. arXiv preprint\r\narXiv:1801.01596 , 2018. 1,2\r\n[48] Martin Wistuba, Arlind Kadra, and Josif Grabocka. Super-\r\nvising the multi-ﬁdelity race of hyperparameter conﬁgura-\r\ntions. Advances in Neural Information Processing Systems ,\r\n35:13470–13484, 2022. 2,6,9\r\n[49] Jian Wu, Matthias Poloczek, Andrew G Wilson, and Peter\r\nFrazier. Bayesian optimization with gradients. Advances in\r\nneural information processing systems , 30, 2017. 2\r\n[50] Peiyu Yang, Naveed Akhtar, Zeyi Wen, Mubarak Shah, and\r\nAjmal Mian. Re-calibrating feature attributions for model in-\r\nterpretation. In International Conference on Learning Rep-\r\nresentations , 2022. 2\r\n[51] Peiyu Yang, Naveed Akhtar, Zeyi Wen, and Ajmal Mian.\r\nLocal path integration for attribution. In Proceedings of\r\nthe AAAI Conference on Artiﬁcial Intelligence , pages 3173–\r\n3180, 2023. 2\r\n[52] Xueying Zhu, Jie Sun, Zhenhao He, Jiantong Jiang, and\r\nZeke Wang. Staleness-reduction mini-batch k-means. IEEE\r\nTransactions on Neural Networks and Learning Systems ,\r\n2023. 2\r\n[53] Lucas Zimmer, Marius Thomas Lindauer, and Frank Hutter.\r\nAuto-Pytorch: Multi-ﬁdelity metalearning for efﬁcient and\r\nrobust AutoDL. IEEE Transactions on Pattern Analysis and\r\nMachine Intelligence , 43:3079–3090, 2021. 6,8\r\n26190'}, 'dist': 0.9286905527114868}
Result 28: {'text': 'data', 'metadata': {'created_at': 1736932570, 'modified_at': 1736932570, 'owner': 'sayandeep', 'path': 'RAG_CONTENT/conferences/cvpr_ocr/Chen_TiNO-Edit_Timestep_and_Noise_Optimization_for_Robust_Diffusion-Based_Image_Editing_CVPR_2024_paper.txt', 'size': 40818, 'seen_at': 1737191136, 'data': 'TiNO-Edit: Ti mestep and N oise O ptimization\nfor Robust Diffusion-Based Image Edit ing\nSherry X Chen*1, Yaron Vaxman2, Elad Ben Baruch2, David Asulin2, Aviad Moreshet2,\nKuo-Chin Lien3, Misha Sra1, and Pradeep Sen1\n1.University of California, Santa Barbara2.Cloudinary3.Layer AI\nAbstract\nDespite many attempts to leverage pre-trained text-to-\nimage models (T2I) like Stable Diffusion (SD) [25] for con-\ntrollable image editing, producing good predictable results\nremains a challenge. Previous approaches have focused\non either fine-tuning pre-trained T2I models on specific\ndatasets to generate certain kinds of images (e.g., with a\nspecific object or person), or on optimizing the weights,\ntext prompts, and/or learning features for each input im-\nage in an attempt to coax the image generator to produce\nthe desired result. However, these approaches all have\nshortcomings and fail to produce good results in a pre-\ndictable and controllable manner. To address this problem,\nwe present TiNO-Edit, an SD-based method that focuses on\noptimizing the noise patterns and diffusion timesteps dur-\ning editing, something previously unexplored in the liter-\nature. With this simple change, we are able to generate\nresults that both better align with the original images and\nreflect the desired result. Furthermore, we propose a set\nof new loss functions that operate in the latent domain of\nSD, greatly speeding up the optimization when compared\nto prior losses, which operate in the pixel domain. Our\nmethod can be easily applied to variations of SD includ-\ning Textual Inversion [13] and DreamBooth [27] that en-\ncode new concepts and incorporate them into the edited re-\nsults. We present a host of image-editing capabilities en-\nabled by our approach. Our code is publicly available at\nhttps://github.com/SherryXTChen/TiNO-Edit.\n1. Introduction\nComputer-generated image synthesis has been studied\nfor decades for its wide range of applications includ-\ning content creation, marketing and advertising, visual-\nization/simulation, entertainment, and storytelling. Re-\n*Corresponding author email: xchen774@ucsb.eduPure text-guided & Reference-guided image editing\nOriginal Ref w/o ref w/ref\n“sandwich” →“cak\ne”\n+“a\nmagenta hat”\nStr\noke-guided image editing\nOriginal User input Result Original User input Result\n+“a\nsunflower” +“curtains”\nImage\ncomposition\nOriginal User input Result Original User input Result\n+“a scarecrow” “white” →“yellow plaid”\nImage editing with DreamBooth (DB) or Textual Inversion (TI)\nOriginal DB Result Original TI Result\n“photo” →\n⟨concept⟩ “sky”→ ⟨concept⟩\nFigure\n1.Overview of capabilities enabled by TiNO-Edit.\nTiNO-Edit offers various image-editing capabilities and can be run\nwith DreamBooth (DB) [27] or Textual Inversion (TI) [13]. By\nleveraging diffusion timestep and noise optimization techniques,\nit can generate realistic and high quality outputs.\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n6337\ncent advances in diffusion-based text-to-image (T2I) gen-\nerations [10, 19, 23,25,30] models have allowed users to\nspecify, modify, and enhance images in novel ways using\ntext-based prompts or other inputs [35]. Still, these often\ngive undesired results, so the main question is how to use\nthese models in an artistic workflow to enable controllable\nimage editing that can produce the desired results.\nTo this end, researchers have fine-tuned existing model\nbackbones on new datasets to accommodate specific use\ncases, including conditioning image editing on instruc-\ntions [3], using images with inpainting masks [34] or\nbounding boxes [17], and including other visual infor-\nmation such as edges, segmentation maps, and depth\nmaps [35]. Another line of work has focused on opti-\nmizing weights [6, 36], intermediate attention/feature lay-\ners [22, 31], or inputs [21, 33] of existing T2I backbones\nwith respect to each image the user wants to edit to try and\nproduce the desired result.\nIn this last thrust, optimizing text-prompt inputs has\ngained the most attention. We have seen considerable work\non prompt engineering [25] which studies the effect of dif-\nferent (positive or negative) text prompts, including opti-\nmizing the text prompt encoding in the CLIP domain [24]\nas well as adjusting the weights of text prompts [33]. How-\never, as shown in the examples in this paper, these previous\napproaches still leave a lot to be desired when it comes to\nhigh-quality image editing.\nDespite previous work that attempts to optimize input\nparameters to the diffusion model, we observe that there\nare two other kinds of inputs whose optimization has not\nbeen thoroughly explored yet: the noise used to corrupt the\ninput images, and the diffusion timesteps. For noisy im-\nages, most work either inverts/reconstructs them with re-\nspect to the noise scheduler they use with the diffusion\nbackbones [21, 22,31,32] or simply distorts the original\nimage based on a pre-chosen distortion level [1, 8,20],\nwhich is determined by the starting diffusion timestep. Al-\nthough preliminary research has been conducted to explore\nhow this affects the similarity between the output and the\noriginal image [1, 8,20], these observations have yet to be\nincorporated into the methods in an automated manner, so\nit still requires extensive trial-and-error for users to find the\noptimal settings. Furthermore, the remaining timestep val-\nues are fixed based on method hyper-parameters.\nGiven the important role noisy input images and\ntimesteps play in the diffusion-based image synthesis pro-\ncess, we argue that they should also be optimized based on\nthe image editing objective. To this end, we present TiNO-\nEdit, an optimization-based method that is built up of one\nT2I backbone, namely Stable Diffusion (SD) [25], to auto-\nmatically find the best level of noise that should be added\nand removed from the original image to achieve the best\nresults. We design a set of loss functions that operate inthe latent domain of SD to save computational time and\nresources. More importantly, TiNO-Edit supports a line\nof image editing capabilities (Fig. 1), including pure text-\nguided image editing, image editing guided by reference\nimages, stroke-based image editing, and image composi-\ntion. It can also be applied with variations of SD including\nTextual Inversion [13] and DreamBooth [27] that encode\nnew concepts in order to generate outputs with concepts that\nmay otherwise be hard to describe, which is a capability that\nhas been largely overseen.\nIn summary, our contributions include:\n• A novel SD algorithm for image editing that supports var-\nious image-editing capabilities.\n• A new set of loss functions that save computational time\nand resources.\n• A novel image editing capability of incorporating new\nconcepts from variations of SD\n2. Related Work\n2.1. Text-to-image diffusion-based models\nDiffusion models have shown unprecedented quality in im-\nage generation tasks and become the backbone of many\ntext-to-image (T2I) applications [3, 21,22,31–36]. These\nmodels use text prompts as inputs to the diffusion model\nto condition the reverse denoising process. Like the gen-\neral diffusion models, T2I diffusion-based models are able\nto take Gaussian noise and denoise it using text prompts to\ngenerate images that align with those text prompts.\n2.2. Text-guided diffusion-based image editing with\npre-trained models\nT2I diffusion-based models have drawn a lot of attention\nwith their high-quality synthesis results. One line of work\ntakes these pre-trained models as backbones and fine-tunes\nthem to perform specific image editing tasks. For exam-\nple, ControlNet [35] takes a copy of the U-Net [26] encoder\ncomponent of SD [25] and attaches it back to the SD back-\nbone to accept visual information including image edge\nmaps, depth maps, segmentation maps, etc. This attached\nencoder is then fine-tuned to encode that information and\nguide the generated image output accordingly. Although\nControlNet primarily focuses on T2I applications, other\nwork built on it supports text-guided image editing [14].\nOn the other hand, InstructPix2Pix [3] fine-tunes SD to\naccept instructions via text prompts to perform image edit-\ning. They tailor a dataset of image pairs and corresponding\ninstructions that reflect changes in the image pairs, using the\nT2I backbone and a large language model GPT-3 [4]. Sim-\nilarly, Paint by Example [34] is an exemplar-based image\nediting method via inpainting, where they attach a CLIP-\nbased classification model on SD to accept image examples\nthat will be used to fill the masked region of an input im-\n6338\nage. The pre-trained SD backbone is then fine-tuned to be\nconditioned on these examples.\nAnother approach centers on optimizing weights [6, 36],\nintermediate attention/feature layers [22, 31], or inputs [21,\n33] for each specific image. Similar to the previously\nmentioned methods, these approaches also need a specific\ndataset. The dataset may be formed from a single image,\nsuch as its patches [36]. The model takes in the noisified\npatches as well as their positions with respect to the origi-\nnal image and learns to denoise those patches. During in-\nference, both models (before and after fine-tuning) are used\nto denoise the input. Subsequently, the outputs from each\nstage are combined through diffusion (reverse denoising)\nsteps to generate the final image.\nIntermediate layers of pre-trained models can be used to\nguide image editing processes. Existing work [11, 22, 31]\nhas looked into various aspects including cross and self-\nattention maps, as well as activation functions and their ef-\nfects on object location, shape, size, and appearance in the\ngenerated results. Again, image inversion may be applied\nwith respect to the original image [31] to learn relevant in-\nformation of intermediate layers, which can then be injected\nin the denoising process when editing this image [31] or be\ncompared with the ones with respect to the edited results\nand optimize the latter to be aligned with the former [22].\nFinally, inputs of T2I models include text prompts, in-\nput noise, and timesteps, all of which may be optimized for\nmore desirable editing outcomes. Image editing work that\nutilizes input optimization usually starts from image inver-\nsion (reconstructing images by representing them in the do-\nmain of the generative model), which involves finding the\nreverse denoising path that will lead to the input image [21].\nAmong all inputs of T2I models, prompts have gained a\nfair amount of attention as they have a lot of power over\nthe semantics of generative images and, thus are very rele-\nvant to image editing results. In the context of SD, prompts\ncan either be positive to describe what the generated images\nshould contain or negative, to describe undesirable visuals\nthat should be excluded from the outputs (e.g. “low quality,\njpeg artifacts, ugly...”). Optimization can be performed on\nboth positive [33] or negative prompts [21].\nOur optimization-based method falls into the above cat-\negory. In contrast to prior work, our method maintains both\npositive and negative prompts while focusing on optimizing\ninput noise and timesteps instead to get the corresponding\nedits entailed by the prompts.\n3. Preliminaries\nIn this section, we first provide an overview of Diffusion\nDenoising Implicit Model (DDIM) [30] and Stable Diffu-\nsion (SD) [25] algorithms, which our method is based on,\nfollowed by an experiment that gives us insight into the ef-\nfect of various components in them on edited results.\nFigure 2. Effect of starting timestep and noise on image editing.\nSuppose we want to change the cat in the left image to a dog,\nwe can input this image and the target prompt “a photo of a dog”\nto Stable Diffusion (SD) Img2Img [15, 25], along with random\nGaussian noise Nand a starting time T∈[0,1]to produce results\nsuch as those shown in the grid on the right. Here, we vary T(fixed\nper column) and N(fixed per row). As Tincreases, the output\nmatches the target prompt better, but it also diverges more from\nthe original image in terms of composition and pose. Furthermore,\ndifferent random noise inputs can lead to different visual features.\nDDIMs are generative models that produce natural im-\nages from noise iteratively. During training, DDIMs model\na forward diffusion (FD) process transforming images to\nnoise in Stimesteps, denoted as {tk=k·1\nS, k∈[0, S]},\nwhere the corresponding noisified image Ikper timestep\nfrom a pristine image Iis\nIk=FDk(I, N) =p\nα(tk)·I+p\n1−α(tk)·N, (1)\nwhere noise N∼ N (0,I)andα(·)is a pre-defined diffu-\nsion function. Starting from ˜Ik=Ik, DDIMs then model\na reverse diffusion (RD) that remove noise iteratively and\nrecover intermediate images as\n˜Ik−1=RDk,k−1(˜Ik,˜Nk)\n=p\nα(tk−1) ˜Ik−p\n1−α(tk)·˜Nkp\nα(tk)!\n+p\n1−α(tk−1)·˜Nk,(2)\n, where ˜Nk=UNet DDIM(˜Ik, tk)is the predicted noise from\na denoising UNet UNet DDIM [26].\nThe same can be applied to Stable Diffusion (SD) [25]\nwith two key differences. First, SD performs diffusion steps\nin a latent space of a Variational Auto-Encoder (V AE) [12]\nwith an encoder V AE encand a decoder V AE dec. The latent\nrepresentation of IisL=V AE enc(I). Secondly, the denois-\ning UNet in SD is conditioned on an additional text prompt\np, where ˜Nk=UNet SD(˜Lk, tk, p), which enables its text-\nto-image synthesis functionality.\nSD Img2Img variant [15] can edit images with text.\nGiven an image Iand time T∈[0,1],Iis first noisi-\nfied with respect to timestep max tk≤Ttkusing Eq. 1 and\nthen denoised. The idea behind this, first introduced in\n6339\nFigure 3. Optimization parameters. We find optimization pa-\nrameters by studying the SD denoising process. The output ˜L0is\nonly affected by the timesteps tk(k∈[1, K]) and the noisy la-\ntent image input ˜Lkfor each of the Kdenoising steps. Note we\nare assuming that the learning models are all fixed (denoted by the\nsnowflake symbol) and that the number of timesteps Kis a con-\nstant. ˜Lkcan then be traced back through Kiterations to the ini-\ntial latent image input ˜LKthat is computed from starting timestep\ntK=Tand the Gaussian noise N. Hence, we can achieve our\ngoal by simply optimizing Nand time steps tkfor all k∈[1, K].\nSDEdit [20], is that the less we distort the input, the more\nlikely we are to generate a result similar to it.\nHowever, this raises the question as to what the value\nofTshould be. To give us insight into the problem, we\ncan run a simple experiment, shown in Fig. 2. As we can\nsee, changes in TandNcan lead to drastically different re-\nsults, which motivates our timestep and noise optimization\napproach explained in the next section.\n4. TiNO-Edit\nRather than finding the optimal values manually as in pre-\nvious approaches, TiNO-Edit aims to automate the process\nby optimizing the variable parameters related to NandT\nso that they produce desired results. From the SD denoising\nprocess (Fig. 3), we see that the output result ˜L0is only af-\nfected by the timesteps tk(k∈[1, K]) and the input Gaus-\nsian noise N, assume all models are fixed and the number\nof steps Kis a constant. We optimize them, starting from\ntk=k·T\nKandN(0,I), with respect to our loss functions\ndefined in Sec. 4.2 via gradient descent with the Jacobian\nfrom UNet SD. We optimizes all tk’s so they can be non-\nuniformly spaced and is more flexible than only optimizing\nTand/or k. The updated tk’s are used in the next optimiza-\ntion step as the timestep inputs to the UNet for the reverse\ndiffusion process. Note that we don’t optimize α(tk)be-\ncause doing so along with changing tkwill break the de-\npendency between the two learnt by the models and may\ncause generation quality degrade.\nThe inputs to our method include the input image I,\nthe target prompt p, the input image description/prompt\npOand additional inputs A={I∗, Ma,∗ ∈ { r, s, c}}\nwhere a, r, s, c denote the additional image and mask in-\nput to the editing capabilities of adding objects, reference-\nguided image editing, stroked-guided image editing, and\nimage composition if applicable.\nFigure 4. Training LatentCLIP. Our LatentCLIP visual encoder\n(LatentCLIP vis) is a copy of a pre-trained CLIP image encoder\n(CLIP vis) [24], except the first convolution layer is replaced to ac-\ncommodate for taking the latent vector V AE enc(I)[12] as input and\noutput the image feature fL. The entire LatentCLIP visual en-\ncoder is unfrozen (indicated by the fire symbol, as opposed to the\nsnowflake symbol which means the model is frozen) and is trained\nto minimize the cosine difference between fLandf, which is the\nimage feature of Ifrom the CLIP image encoder.\n4.1. Masking mechanism\nDuring the editing process, it does not make sense to add\nnoise or denoise regions that we want to maintain. So\nwe design a masking mechanism MSK (I, pO, p,A)to lo-\ncate the editing region Mfrom aforementioned inputs. To\nreplace objects in pure text-guided image editing, M=\nCLIPSeg (I, o), where CLIPSeg (·,·)[18] computes regions\ninIthat correspond to objects {o:o∈pO, o /∈p}. To per-\nform style transfer, Mij= 1for all pixel locations (i, j). To\nadd objects for pure text-guided or reference-guided image\nediting, M=Ma∈ A. For stroke-guided image editing or\nimage composition, Mij= 1−δ(Iij,(I∗)ij)using the Kro-\nnecker delta function ( δ(x, y) = 1 ifx=y, otherwise 0).\n4.2. Optimization loss functions\nTo achieve the desired output ˜I0or its latent representation\n˜L0, we need to design a set of optimization functions that\ncaptures key characteristics the output should have with re-\nspect to the original image Ior its latent representation L.\nFirst, we want to enforce semantic similarity between the\ntwo with a CLIP-based[24] semantic loss function. But in-\nstead of operating it in the pixel domain like previous meth-\nods [9, 33], we have designed our own CLIP visual encoder\nthat operates in the latent domain called LatentCLIP. This\ngreatly speed up the optimization as the latent domain is\nmuch more compacted than the pixel domain.\nLatentCLIP visual encoder LatentCLIP vistakes a latent\nimage and is trained to output the same feature as its pixel\ndomain counterpart from the original CLIP visual encoder\nCLIP vis(Fig. 4), where the model training objective is\n1−cos(LatentCLIP vis(L),CLIP vis(I)). (3)\nLatentCLIP visis initialized from a pretrained CLIP vis,\nwhich the first convolution layer replaced to accommodate\nfor the dimension of latent images.\nTo incorporate LatentCLIP visand the CLIP text\nencoder CLIP text(·)[7, 24] to the semantic loss\n6340\nAlgorithm 1 TiNO-Edit\n1:function TINO-E DIT(I, pO, p,A)\n2: K∈Z+,T∈[0,1]\n3: tk←k·T\nK, k∈[0, K]\n4: N∼ N(0,I)\n5: L←V AE enc(I)\n6: M=MSK (I, pO, p,A)\n7: forw←1toWdo\n8: ˜LK←FDK(L, N) ▷Eq. 1\n9: ˜LK←˜LKJM+LJ(1−M)\n10: fork←Kto1do\n11: ˜Nk←UNet SD(˜Lk, tk, p)\n12: ˜Lk−1←RDk,k−1(˜Lk,˜Nk) ▷Eq. 2\n13: ˜Lk−1←˜Lk−1JM+LJ(1−M)\n14: end for\n15: argmin\nN,tkLtotal(L,˜L0, pO, p,A) ▷Eq. 8\n16: end for\n17: return V AE dec(˜L0)\n18:end function\nfunction, we may calculate the cosine distance be-\ntween the output feature and the prompt feature\ncos(LatentCLIP vis(˜L0),CLIP text(p))[24], the distance\nbetween the original prompt and the target prompt in CLIP\ntext domain CLIP text(pO)−CLIP text(p), as well as the\ndistance between the original image and the output in CLIP\nimage domain (LatentCLIP vis(L)−LatentCLIP vis(˜L0)) [33]\nif we adopt directly from these prior methods.\nHowever, all of them rely on the assumption that the im-\nage encoder and the text encoder are from the same CLIP\npre-trained model without any modifications or fine-tuning\nwith the same feature dimensions. This prevent different\nvariations of SD and new concepts learned through Dream-\nBooth [27] or Textual Inversion [13] outside what the origi-\nnal CLIP text encoder to be incorporated in the loss. To this\nend, we design a new semantic loss function that minimizes\nthe difference between the cosine distance between origi-\nnal/target images in the CLIP image domain and the one\nbetween original/target prompt in the CLIP text domain as\nLsem(L,˜L0, pO, p)\n= cos( LatentCLIP vis(L),LatentCLIP vis(˜L0))\n−cos(CLIP text(pO),CLIP text(p)),(4)\nwhere CLIP textmay include new concepts learned through\nDreamBooth [27] or Textual Inversion [13].\nSimilarly, to ensure semantic similarity to the latent rep-\nresentation of the reference image Lr=V AE enc(Ir)for\nreference-guided image editing, we define\nLref(L, Lr) = cos( LatentCLIP vis(L),LatentCLIP vis(Lr).\n(5)\nFigure 5. Compounded image editing. We present a com-\npounded image-editing workflow by applying our method repeat-\nedly on a single image. For each step, the user can perform any\nof the supported editing operations. Any additional information\nsuch as inputs including masks, reference images, user strokes,\nuser-composed images, and concept images used to train custom\nconcepts are shown next to the corresponding arrows.\nNext, we want to enforce visual similarity between Land\n˜L0. Following the same idea in LatentCLIP, we designed\nLatentVGG, a VGG encoder [29] that operates in the latent\ndomain, which is initialized and trained along with a pre-\ntrained VGG encoder. The training objective is\n∥LatentVGG (L),VGG (I)∥1. (6)\nAfter training, our perceptual loss is\nLperc(L,˜L0) =∥LatentVGG (L)−LatentVGG (˜L0)∥1.\n(7)\nTo put everything together, our final loss function is\nLtotal(L,˜L0, pO, p,A)\n=λsem· Lsem(L,˜L0, pO, p)\n+λref· Lref(L, Lr)\n+λperc· Lperc(L,˜L0),(8)\nwhere λsem= 1,λperc= 0.5, and λref= 1if applicable.\nIn the end, TiNO-Edit (Alg. 1) optimize Nand all tk’s\nfor a pre-defined Wnumber of optimization steps with re-\nspect to Ltotal, each of which consists of Ktimesteps as\none complete denoising process.\n5. Experiments\nTiNO-Edit uses SD v2.1 [25] with K= 10 denoising steps,\nstarting timestep value T= 0.75, and the number of op-\n6341\nOriginal T2L DD P2P0 SINE EDICT IP2P PnP NTI Ours\n“a cat” →“a dog”\n“roses” →“sunflowers”\n+“a bracelet”\n+“a magenta hat”\n“oil painting ”→ “watercolor”\n“3d\nrendering” →“crochet”\nFigure 6. Pure text-guided image editing comparison. We compare with the following baselines: Text2LIVE (T2L) [2], Disentangle-\nmentDiffusion (DD) [33], Pix2Pix-Zero (P2P0) [22], SINE [36], EDICT [32], InstructPix2Pix (IP2P) [3], Plug-and-Play (PnP) [31], and\nNull-text Inversion (NTI) [21]. The user indicates the desired editing via text, where the part that reflects the edit is shown below each row.\ntimization steps W= 50. To use LatentCLIP and La-\ntentVGG for our loss function, each model has been trained\non a subset of LAION-5B [28] datasets for 105iterations\nwith a batch size of 16. We use the AdamW [16] optimizer\nwith a learning rate of lr=1e-5.\nWe use two AdamW [16] optimizers, one for tk’s with\nlr= 1 and the other for Nwithlr= 0.005. Our method\ncan also run on DreamBooth (DB) [27] or Textual Inversion\n(TI) [13], where we use the same SD and noise scheduler\nDB/TI is trained with to avoid performance degradation.\n5.1. Compounded Image Editing\nWe present a compounded image editing workflow by ap-\nplying our method repeatedly on a single image with dif-\nferent operations(Fig. 5). For each step, the user can per-\nform any of the supported editing operations, which gives\nthe user creative control over how they want to edit images.5.2. Qualitative comparisons\nPure text-guided image editing. We compare TiNO-\nEdit with other text-guided image editing baselines, in-\ncluding Text2LIVE (T2L) [2], DisentanglementDiffu-\nsion (DD) [33], Pix2Pix-Zero (P2P0) [22], SINE [36],\nEDICT [32], InstructPix2Pix (IP2P) [3], Plug-and-Play\n(PnP) [31], and Null-text Inversion (NTI) [21] and test vari-\nous use cases (Fig. 6). While some baselines generate good\nresults in sevaeral scenarios (e.g., EDICT rows 2, 4; NTI\nrows 2, 3, 5), many still pose various issues, including in-\ncohesive edit (T2L rows 3, 4, 5; DD row 4), failing to add\nnew objects (P2P0, EDICT, PnP for row 3), concept leak-\ning (IP2P row 2: image becomes yellow; row 3: shirt is\nmagenta-colored; NTI row 4: man gets a hat texture), and\naltering other attributes (P2P0, SINE, PnP row 4; NTI row\n6). On the other hand, our method is robust across use cases.\n6342\nOriginal Mask Ref VCT GLIGEN PbE Ours\nFigure\n7.Comparison with reference-guided image editing\nbaselines. We compare our method with Visual Concept Trans-\nlator (VCT) [6], GLIGEN [17], and Paint-by-Example (PbE) [34],\nwhere the masks (Mask) indicate edit regions where the object in\nthe reference image (Ref) should be included.\nOriginal Strokes BLD 0.5 BLD 0.75 SDE 0.5 SDE 0.75 Ours\n+“boots”\n“chandelier” →“stained\nglass chandelier”\n+“an\napple tree”\n“hair”→“blue\ncurly hair”\n+“a\nteddy bear”\n+“a\ngift box”\nFigure 8. Stroke-based image editing and image composition\nbaseline comparison. We compare to Blended Latent Diffusion\n(BLD) [1] and SDEdit (SDE) [20] for stroke-guided image editing\n(row 1-3) and image composition (row 4-6). BLD and SDEdit\nboth accept a starting timestep Tas input, and we test two values\n0.5 and 0.75 (shown next to the method names).\nReference-guided image editing. For reference-guided\nimage editing, we compare our method with Visual Concept\nTranslator (VCT) [6], GLIGEN [17], and Paint-by-Example\n(PbE) [34] (Fig. 7). our method is better at preserving de-\ntails (a dog with its tongue sticking out, white stripes on\nthe hat) when maintaining other image regions. In con-\ntrast, VCT failed to add objects, GLIGEN does not com-T2L\nDD P2P0 SINE EDICT IP2P PnP NTI Ours\nCLIP-T 0.299\n0.316 0.271 0.314 0.327 0.315 0.321 0.316 0.328\nCLIP-I 0.879\n0.853 0.793 0.912 0.898 0.845 0.835 0.843 0.924\nDINO-I 0.864\n0.862 0.776 0.805 0.838 0.828 0.756 0.799 0.874\nT\nable 1. Pure text-guided image editing qualitative evaluation.\nVCT\nGLIGEN PbE Ours\nCLIP-T 0.267\n0.295 0.300 0.311\nCLIP-I 0.899\n0.814 0.841 0.928\nCLIP-I r 0.537\n0.616 0.570 0.674\nDINO-I 0.919 0.770\n0.813 0.869\nTable 2. Reference-guided image editing qualitative evaluation\nBLD\n0.5 BLD 0.75 SDEdit 0.5 SDEdit 0.75 Ours\nCLIP-T 0.298 0.287\n0.295 0.295 0.298\nCLIP-I 0.930\n0.924 0.897 0.953 0.959\nCLIP-I s 0.956\n0.927 0.986 0.956 0.987\nDINO-I 0.953\n0.959 0.961 0.973 0.978\nTable 3. Stroke-guided image editing qualitative evaluation\nBLD\n0.5 BLD 0.75 SDEdit 0.5 SDEdit 0.75 Ours\nCLIP-T 0.273\n0.273 0.294 0.267 0.298\nCLIP-I 0.955\n0.950 0.903 0.937 0.958\nCLIP-I c 0.943\n0.945 0.965 0.964 0.978\nDINO-I 0.977\n0.975 0.972 0.973 0.983\nTable 4. Image composition qualitative evaluation\npose edited objects to the original images (dog standing on\nthe sofa) and PbE generates objects that are similar to the\nreferences at a high level without keeping their details.\nStroke-guided image editing. We compare to Blended La-\ntent Diffusion (BLD) [1] and SDEdit (SDE) [20] for stroke-\nguided image editing (row 1-3 of Fig. 8), both of which take\na starting timestep as input. BLD largely generates results\nthat do not align with the strokes and the same problem ap-\npears in SDEdit. On the other hand, TiNO-Edit can generate\nobjects from strokes that blend well with the original.\nImage composition. We also compare with BLD and\nSDEdit for image composition use cases (Fig. 8row 4-6),\nwhere TiNO-Edit refine user-composed images and gener-\nate cohesive and nature-looking outputs (Fig. 8) such as\nmaking the teddy bear sit on the sofa rather than floating\nin front of it. Again, BLD changes the user input dras-\ntically. SDEdit with the starting timestep of 0.75 largely\nomits the new object that should be added to the results.\nWhile SDEdit with the starting timestep of 0.5 generates\nbetter results, the output may not look cohesive.\n5.3. Quantitative Comparisons\nWe created a test set of 200 samples in the form of\n(I, p O, p, Ir, Is, Ic), where we ran each method for each\nsample and report the following popular metrics averaged\nacross the test set ( Tabs. 1to4):\n• CLIP-T: cos(CLIPvis( ˜I),CLIPtext(p))\n• CLIP-I: cos(CLIP vis(˜I),CLIP vis(I))\n• CLIP-I ∗:cos(CLIP vis(˜I),CLIP vis(I∗)),I∗∈ {Ir, Is, Ic}\n• DINO-I: cos(DINO vis(˜I),DINO vis(I))\n6343\nwhere ˜Iis the output, DINO visrefers to DINO ViT [5]\nwhich can be used to evaluate image perceptual alignment.\nOur method outperforms other methods in most metrics, in-\ncluding DINO-I, a non-CLIP-based metric not used during\ntraining. In Tab. 2, VCT scores the highest in DINO-I as\nits outputs are very similar to the original images without\nrespecting the target prompts/images (Fig. 7), which is also\nsupported by the fact that it gets the worst CLIP-T score.\n5.4. Ablation studies\nChange in timesteps and noise throughout optimization.\nWe look at how tk’s and N change across W optimization\nsteps and average the values across our test set in Fig. 9.\nTimesteps earlier in the diffusion process are changed more\ncompared to timesteps later in the diffusion process because\nthe former has a larger effect on the editing result. The\nrange and the mean of Nis quite stable over time because\nthey are optimized with respect to a lower learning rate and\nFigure\n9.Changes in timesteps and noise through optimization.\nWe plot the values of tk’s alongside the minimum, maximum, and\nmean of Nacross Woptimization steps. This indicates that N\nremains within the noise distribution that SD has been learned,\npreventing potential degradation of the output image.\nconst tk\nconst N\nM=1\nW=0\nW=10 W=20 W=30 W=40 W=50\nFigure 10. Effect of optimizing timesteps, noise, and using the\nmasking mechanism. Optimizing both tk’s and Nleads to opti-\nmize results across different number of optimization steps.Original w/ latent w/ pixel Concept\nw/ latent w/ pixel\n“\nAudrey Hepburn” →“Marilyn Monroe” “Audrey Hepburn” → ⟨concept⟩\nFigure 11. Effect of LatentCLIP and LatentVGG on image\nediting quality and custom concepts. Compared with the orig-\ninal CLIP [24] and VGG [29] that operate in pixel spaces (“w/\npixel”), using our LatentCLIP and LatentVGG (“w/ latent”) is not\nonly more efficient but leads to better quality outputs.\nhandle more local changes with respect to the changing tk’s.\nStability in Nalso means that we don’t need to worry about\nNgoing outside the noise distribution SD has learnt and\ncause performance degrade.\nEffect of optimizing timesteps, noise, and using the\nmasking mechanism. We first study the effect of optimiz-\ning timesteps, noise, and using the masking mechanism by\ngenerating results with different optimization configuration\nwith a fixed seed (Fig. 10). As we can see, TiNO-Edit is the\nmost robust across various W. Keeping tk’s constant causes\nthe outputs to degrade over time because when the timestep\nvalues are sub-optimal, Nhas to change more drastically\nto over-compensate for our loss function, which pushes it\naway from the Gaussian noise distribution that SD learns.\nOmitting optimizing Nhas a smaller effect on the results\nwith larger Wbut is more prone to artifacts (the shadow\naround the coat collar). Moreover, when Wis small, we\nmay get drastically different images compared to the origi-\nnal. Lastly, when the editing region is not constrained by the\nmask M, the global structure of the image may get changed\nentirely. This suggest that optimizing both tk’s and Nis\nnecessary for optimal results.\nEffect of LatentCLIP and LatentVGG. We study the ef-\nfect of using LatentCLIP and LatentVGG compared to us-\ning the original CLIP [24] and VGG [29] on a NVIDIA\nA6000. In terms of image editing quality, using our Latent-\nCLIP and LatentVGG leads to high-quality image editing\nresults with better alignment with the original image and\nless artifacts (Fig. 11row 1) while performing better with\ncustom concepts in Stable Diffusion variants such as Tex-\ntual Inversion [13] (Fig. 11row 2).\n6. Conclusion\nIn this paper, we have presented TiNO-Edit, an image\nediting method with pre-trained Stable Diffusion models\nby optimizing diffusion timesteps and input noise. We\nhave designed a set of loss functions that greatly reduce\nthe computation resources required for optimization.\nComparing TiNO-Edit with various task-specifically\nbaselines, our method is able to generalize across\nspecific image editing tasks and produce superior results.\n6344\nReferences\n[1] Omri Avrahami, Ohad Fried, and Dani Lischinski. Blended\nlatent diffusion. ACM Transactions on Graphics (TOG), 42\n(4):1–11, 2023. 2,7\n[2] Omer Bar-Tal, Dolev Ofri-Amar, Rafail Fridman, Yoni Kas-\nten, and Tali Dekel. Text2LIVE: Text-driven layered image\nand video editing. In European Conference on Computer\nVision (ECCV), pages 707–723. Springer, 2022. 6\n[3] Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-\nstructPix2Pix: Learning to follow image editing instructions.\nInConference on Computer Vision and Pattern Recognition\n(CVPR), pages 18392–18402, 2023. 2,6\n[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-\nguage models are few-shot learners. Advances in Neural\nInformation Processing Systems (NeurIPS), 33:1877–1901,\n2020. 2\n[5] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv ´e J´egou,\nJulien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-\ning properties in self-supervised vision transformers. In In-\nternational Conference on Computer Vision (ICCV), 2021.\n8\n[6] Bin Cheng, Zuhao Liu, Yunbo Peng, and Yue Lin. General\nimage-to-image translation with one-shot image guidance. In\nInternational Conference on Computer Vision (ICCV), pages\n22736–22746, 2023. 2,3,7\n[7] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell\nWortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuh-\nmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scal-\ning laws for contrastive language-image learning. In Confer-\nence on Computer Vision and Pattern Recognition (CVPR),\npages 2818–2829, 2023. 4\n[8] Guillaume Couairon, Jakob Verbeek, Holger Schwenk,\nand Matthieu Cord. DiffEdit: Diffusion-based seman-\ntic image editing with mask guidance. arXiv preprint\narXiv:2210.11427, 2022. 2\n[9] Katherine Crowson, Stella Biderman, Daniel Kornis,\nDashiell Stander, Eric Hallahan, Louis Castricato, and Ed-\nward Raff. VQGAN-CLIP: Open domain image genera-\ntion and editing with natural language guidance. In Euro-\npean Conference on Computer Vision (ECCV), pages 88–\n105. Springer, 2022. 4\n[10] Prafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat GANs on image synthesis. Advances in Neural Infor-\nmation Processing Systems (NeurIPS), 34:8780–8794, 2021.\n2\n[11] Dave Epstein, Allan Jabri, Ben Poole, Alexei A. Efros, and\nAleksander Holynski. Diffusion self-guidance for control-\nlable image generation. arXiv preprint arXiv:2306.00986,\n2023. 3\n[12] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming\ntransformers for high-resolution image synthesis. In Confer-\nence on Computer Vision and Pattern Recognition (CVPR),\npages 12873–12883, 2021. 3,4\n[13] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik,\nAmit H Bermano, Gal Chechik, and Daniel Cohen-Or.An Image is Worth One Word: Personalizing text-to-\nimage generation using textual inversion. arXiv preprint\narXiv:2208.01618, 2022. 1,2,5,6,8\n[14] Shanghua Gao, Zhijie Lin, Xingyu Xie, Pan Zhou, Ming-\nMing Cheng, and Shuicheng Yan. EditAnything: Empow-\nering unparalleled flexibility in image editing and gener-\nation. In ACM International Conference on Multimedia\n(ACMMM), 2023. 2\n[15] HuggingFace. HuggingFace Stable diffusion image-\nto-image, 2023. https : / / huggingface . co /\ndocs / diffusers / api / pipelines / stable _\ndiffusion/img2img. 3\n[16] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. arXiv preprint arXiv:1412.6980,\n2014. 6\n[17] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jian-\nwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee.\nGLIGEN: Open-set grounded text-to-image generation. In\nConference on Computer Vision and Pattern Recognition\n(CVPR), pages 22511–22521, 2023. 2,7\n[18] Timo L ¨uddecke and Alexander Ecker. Image segmentation\nusing text and image prompts. In Conference on Computer\nVision and Pattern Recognition (CVPR), pages 7086–7096,\n2022. 4\n[19] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang\nZhao. Latent consistency models: Synthesizing high-\nresolution images with few-step inference. arXiv preprint\narXiv:2310.04378, 2023. 2\n[20] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia-\njun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided\nimage synthesis and editing with stochastic differential equa-\ntions. In International Conference on Learning Representa-\ntions (ICLR), 2022. 2,4,7\n[21] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and\nDaniel Cohen-Or. Null-text inversion for editing real images\nusing guided diffusion models. In Conference on Computer\nVision and Pattern Recognition (CVPR), pages 6038–6047,\n2023. 2,3,6\n[22] Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun\nLi, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-to-image\ntranslation. In SIGGRAPH, pages 1–11, 2023. 2,3,6\n[23] Dustin Podell, Zion English, Kyle Lacey, Andreas\nBlattmann, Tim Dockhorn, Jonas M ¨uller, Joe Penna, and\nRobin Rombach. SDXL: Improving latent diffusion mod-\nels for high-resolution image synthesis. arXiv preprint\narXiv:2307.01952, 2023. 2\n[24] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In International Conference on Machine Learning\n(ICML), pages 8748–8763. PMLR, 2021. 2,4,5,8\n[25] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj ¨orn Ommer. High-resolution image syn-\nthesis with latent diffusion models. In Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pages 10684–\n10695, 2022. 1,2,3,5\n6345\n[26] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-\nNet: Convolutional networks for biomedical image segmen-\ntation. In Medical Image Computing and Computer-Assisted\nIntervention–MICCAI 2015: 18th International Conference,\nMunich, Germany, October 5-9, 2015, Proceedings, Part III\n18, pages 234–241. Springer, 2015. 2,3\n[27] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. DreamBooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration. In Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 22500–22510, 2023. 1,2,5,6\n[28] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon, Ross Wightman, Mehdi Cherti, Theo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, et al. LAION-5B: An open large-scale dataset for train-\ning next generation image-text models. Advances in Neu-\nral Information Processing Systems (NeurIPS), 35:25278–\n25294, 2022. 6\n[29] Karen Simonyan and Andrew Zisserman. Very deep convo-\nlutional networks for large-scale image recognition. arXiv\npreprint arXiv:1409.1556, 2014. 5,8\n[30] Jiaming Song, Chenlin Meng, and Stefano Ermon.\nDenoising diffusion implicit models. arXiv preprint\narXiv:2010.02502, 2020. 2,3\n[31] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali\nDekel. Plug-and-play diffusion features for text-driven\nimage-to-image translation. In Conference on Computer\nVision and Pattern Recognition (CVPR), pages 1921–1930,\n2023. 2,3,6\n[32] Bram Wallace, Akash Gokul, and Nikhil Naik. EDICT:\nExact diffusion inversion via coupled transformations. In\nConference on Computer Vision and Pattern Recognition\n(CVPR), pages 22532–22541, 2023. 2,6\n[33] Qiucheng Wu, Yujian Liu, Handong Zhao, Ajinkya Kale,\nTrung Bui, Tong Yu, Zhe Lin, Yang Zhang, and Shiyu\nChang. Uncovering the disentanglement capability in text-\nto-image diffusion models. In Conference on Computer\nVision and Pattern Recognition (CVPR), pages 1900–1910,\n2023. 2,3,4,5,6\n[34] Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin\nChen, Xiaoyan Sun, Dong Chen, and Fang Wen. Paint by Ex-\nample: Exemplar-based image editing with diffusion mod-\nels. In Conference on Computer Vision and Pattern Recog-\nnition (CVPR), pages 18381–18391, 2023. 2,7\n[35] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models. In In-\nternational Conference on Computer Vision (ICCV), pages\n3836–3847, 2023. 2\n[36] Zhixing Zhang, Ligong Han, Arnab Ghosh, Dimitris N\nMetaxas, and Jian Ren. SINE: Single image editing with\ntext-to-image diffusion models. In Conference on Computer\nVision and Pattern Recognition (CVPR), pages 6027–6037,\n2023. 2,3,6\n6346'}, 'dist': 0.9286905527114868}
Result 29: {'text': 'data', 'metadata': {'created_at': 1736932570, 'modified_at': 1736932570, 'owner': 'sayandeep', 'path': 'RAG_CONTENT/conferences/cvpr_ocr/Baltatzis_Neural_Sign_Actors_A_Diffusion_Model_for_3D_Sign_Language_CVPR_2024_paper.txt', 'size': 51804, 'seen_at': 1737191136, 'data': "Neural Sign Actors: A diffusion model for 3D sign language production from text\nVasileios Baltatzis1, Rolandos Alexandros Potamias1, Evangelos Ververas1,\nGuanxiong Sun2, Jiankang Deng1, Stefanos Zafeiriou1\n1Imperial College London,2Queen’s University Belfast\n{vasileios.baltatzis18, r.potamias, e.ververas16, j.deng16, s.zafeiriou }@imperial.ac.uk ,\ngsun02@qub.ac.uk\nFigure 1. The proposed method takes raw text as input and generates a realistic and coherent motion of its corresponding sign language\ntranslation. From top to bottom: the input text, the ground truth sign language video (shown just for reference), and the generated motion.\nAbstract\nSign Languages (SL) serve as the primary mode of com-\nmunication for the Deaf and Hard of Hearing communities.\nDeep learning methods for SL recognition and translation\nhave achieved promising results. However, Sign Language\nProduction (SLP) poses a challenge as the generated mo-\ntions must be realistic and have precise semantic meaning.\nMost SLP methods rely on 2D data, which hinders their re-\nalism. In this work, a diffusion-based SLP model is trained\non a curated large-scale dataset of 4D signing avatars and\ntheir corresponding text transcripts. The proposed method\ncan generate dynamic sequences of 3D avatars from an un-\nconstrained domain of discourse using a diffusion process\nformed on a novel and anatomically informed graph neu-\nral network defined on the SMPL-X body skeleton. Through\nquantitative and qualitative experiments, we show that the\nproposed method considerably outperforms previous meth-\nods of SLP . This work makes an important step towards re-\nalistic neural sign avatars, bridging the communication gap\nbetween Deaf and hearing communities.1\n1Project page: https://baltatzisv.github.io/neural-sign-actors/1. Introduction\nSign language (SL) is a form of language in which visual-\nmanual modalities are used instead of spoken words to con-\nvey meaning. It is the predominant form of communication\nfor more than 70 million Deaf and Hard of Hearing peo-\nple around the world. Akin to verbal languages, SLs have\nextremely rich vocabulary and grammar, yet the complexi-\nties differ drastically [55]. To enable effective visual com-\nmunication, they consist of both manual and non-manual\ncomponents [35]. The manual modality encompasses hand\narticulation, orientation, position, and motion, while non-\nmanual elements include arm movements and facial expres-\nsions [6]. Whilst it is possible to convey some meaning us-\ning just hand articulations, expressiveness is limited since\nnon-manual elements often convey emotions [3, 55].\nRecently, several methods have been proposed to bridge\nthe domain gap between sign and spoken languages. Most\nmethods focus on Sign Language Recognition (SLR) which\nincludes the translation of a specific sign to its correspond-\ning meaning, as well as Sign Language Translation (SLT)\nthat extends SLR to the translation of a sign sequence to\nits spoken word equivalent. This is usually tackled using\nglosses [10, 14, 15, 28], which are simplified mid-level\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n1985\nrepresentations that relate each sign with a corresponding\nmeaning. However, even though glosses have provided a\nsubstantial enhancement to SLT methods, they have a pre-\ndefined informative bottleneck, which limits the translation\naccuracy, and they usually fail to provide long-range depen-\ndencies and contextual information [10, 32, 64].\nDespite the significant number of individuals with hear-\ning difficulties, only ∼5% of television programs are inter-\npreted into sign language, which shows the vital need for\n3D signing avatars. Compared to SLR and SLT, only a\nsmall number of methods have attempted to tackle the task\nof Sign Language Production (SLP), either by using directly\nstand-alone glosses [53, 62] or by training a network to map\ntext to glosses [50]. In the SLP setting, a network is given\na text sentence and attempts to generate a motion that re-\nflects the corresponding sign language translation. Usually,\nthis is done using 2D and 3D joints to represent the human\nbody [32, 49, 53]. However, joints provide an unrealistic\nrepresentation of the animation, limiting their practical use\nin real-world avatar actor applications. Recently, Stoll et\nal. [54] proposed to extend SLP to 3D meshes using an op-\ntimization step that fits a SMPL-X [41] model to the pre-\ndicted 2D joints. In contrast, the proposed method directly\nregresses the poses of SMPL-X [41] model to generate an\nanimatable 3D signing avatar.\nAside from its challenging nature, SLP remains rela-\ntively unexplored due to the absence of large-scale avail-\nable datasets with a sufficient vocabulary size. In particu-\nlar, the majority of current SLP methods rely on German\nsign language datasets composed of only a few thousand\nwords [9, 24] and use 2D landmarks detected from off-the-\nshelf pose estimation methods [11]. In contrast, we employ\na hybrid regression-optimization method to accurately an-\nnotate, with SMPL-X pose and shape parameters [41], a\nlarge-scale video dataset [19] composed of over 16k word\ntokens. Using the acquired 3D pose annotations, we train a\ndynamic diffusion model to learn SLP from English texts.\nOur method directly translates text to signs without using\nany intermediate representation [49], which increases the\ngenerative capacity of the network. Given that sign lan-\nguage cannot be translated word-for-word [35], we utilize\nan off-the-shelf sentence encoder [43] which also enables\nout-of-distribution generalization. To sum up, the contribu-\ntions of this study can be summarized as:\n• We introduce the task of direct 3D signing avatar genera-\ntion from text, without relying on 2D fitting optimizations\nor any intermediate gloss representations. In this paper,\nwe aim to make a step towards neural sign avatars to aid\nthe Deaf and Hard of Hearing community [40].\n• We derive the first large-scale 3D dataset of American\nSign Language by designing a state-of-the-art pipeline to\nannotate the How2Sign dataset [19].\n• We propose a text-conditioned dynamic diffusion modelfounded on a novel, anatomically inspired graph neu-\nral network that facilitates SLP. The proposed model\nachieves remarkable results that outperform the current\nstate-of-the-art models, by a large margin.\n2. Related Work\n2.1. Sign Language Production\nDespite nearly two decades of research [16, 39], the devel-\nopment of highly effective sign language production meth-\nods remains challenging. Stoll et al. [53] proposed the first\nneural SLP method forming a seq2seq architecture to map\ntext to glosses. To decode poses to 2D joint locations they\nproposed an empirical lookup table paradigm. To avoid the\ntwo-stage generation, Zelinka et al. [62] utilized OpenPose\n[11] to extract joint locations from Czech weather forecast-\ning videos and train a network to directly regress the 2D\njoint poses. Recently, several methods [30, 49] proposed\ntransformer-based architectures to tackle German sign lan-\nguage production [10]. However, their generations suf-\nfer from under-articulation and limited expressiveness in\nhand and body motion. Follow-up works attempted to im-\nprove the generation quality using adversarial training [48],\nmixture density networks [50] and dictionary representa-\ntions [51]. However, most of the aforementioned methods,\napart from being contingent to intermediate glosses repre-\nsentation, rely on the regression of 2D/3D joint positions,\na process that inherently encounters difficulties in realis-\ntically conveying meanings. In an attempt to tackle such\nlimitations, Stoll et al. [54] proposed the application of a\npost-regression SMPL-X [41] fitting to lift 2D joints to 3D\nmeshes. On the contrary, we make a step towards realistic\nsigning avatars and propose a diffusion pipeline that directly\nregresses SMPL-X poses from an unconstrained domain of\ndiscourse, without relying on any intermediate representa-\ntions such as glosses.\n2.2. Sign Language Datasets\nA major contributing factor to the slow-paced advance-\nments in sign language research is the absence of large-\nscale datasets [8]. Earlier datasets were designed with a\nfocus on sign language recognition using isolated signs [4,\n31, 33, 59, 60], containing a limited vocabulary. To address\nthe challenges of sign language recognition and translation\nwithin the context of complete sentences, several contin-\nuous sign language datasets have been introduced. More\nspecifically, RWTH-BOSTON-50 [61], Dreuw et al. [18],\nSIGNUM [1], and BSL [52] along with DictaSign Cor-\npus, which was developed in several languages [7, 20, 21],\nwere among the first datasets with sentence level annota-\ntions. While additional datasets featuring an expanded set\nof signs have been introduced [12, 31, 33], it is crucial\nto emphasize the importance of continuous sign language\n1986\npose body  θb\nexpressions  ψ\ncamera  Kpose hands  θh\nSMPL-X\nFitting Optimization\nMediapipe OSXθh\nSMPL-X\nDetected Joints\nPCA Pose Prior\nFrame  f 3D Reconstruction\nℙθh\nSMPL-X\nshape  β\nFigure 2. Overview of the fitting pipeline. A set of input frames Fare first processed by OSX [36] to obtain an initial set of pose\nparameters pinit\n1:F. Then, using the Mediapipe algorithm [37], we fine-tune the predicted hand poses to match the detected joints Jwhile\nconstraining the hand poses θhto lie in the space of plausible poses. Finally, using a temporal coherence loss, we acquire smooth and\nhigh-fidelity annotations of 3D signing avatars.\nfor the purposes of translation and production. S-pot [58]\nwas among the first large-scale continuous sign language\ndatasets with a vocabulary of over 1K signs of Finnish sign\nlanguage collected in a constrained environment. To en-\nforce the robustness of sign language translation methods,\nRWTH-Phoenix [9] contained a collection of TV clips with\nGerman sign language, remaining amongst one of the most\npopular datasets used for sign language translation [10] and\nproduction [49]. Similarly, BSL-1K, as presented in [2], cu-\nrated a dataset featuring British sign language (BSL) used\nin casual conversations, encompassing a total vocabulary of\n1K signs. Recently, Duarte et al. [19] collected How2Sign,\na large-scale dataset of American Sign Language (ASL),\nthat is aligned with speech signals from the How2 dataset\n[46]. How2Sign is equipped with a vocabulary of over 16K\nsigns, captured in a total of seventy-nine hours of continu-\nous sign language. A major limitation of the above datasets,\nis the lack of available 3D annotations, that not only aid the\ntranslation tasks, but are also essential for training realis-\ntic 3D signing avatars. In this work, we have extended the\nHow2Sign dataset by incorporating high-quality SMPL-X\n[41] annotations, thereby establishing it as the first publicly\navailable 3D sign language dataset.\n3. Dataset\nTo train a high-fidelity SLP method, capable of generating\nrealistic sign actors, we curate a large-scale dataset of 3D\ndynamic ASL sign sequences paired with their correspond-\ning text transcripts. To do so, we devise a robust 4D recon-\nstruction pipeline, crafted specifically for hand gestures, to\nestimate dynamic hand and body poses of signing avatars\nin the SMPL-X format [41]. How2Sign dataset [19] pro-\nvides the optimum candidate since it is composed of 35K\nhigh-resolution clips of co-articulated ASL with a substan-\ntial vocabulary size featuring over 16K word tokens.\nTo acquire high-fidelity 4D reconstructions of How2Sign\nclips, we build our pipeline upon the powerful OSX [36].\nSpecifically, we initialize our fitting optimization using the\nSMPL-X pose and shape parameters acquired from OSXfor each one of the Fframes of the clip as:\npinit\n1:F= [θb||θh||ψ||β], (1)\nwhere θb, θh, ψ, β denote the body pose, hand pose, ex-\npression, and shape parameters respectively, and ||the con-\ncatenation symbol.\nRecognizing that hand poses constitute the pivotal com-\nponent in conveying SL, we adopt an optimization proce-\ndure to enhance the precision of hand poses and rectify\nany potential misalignments by leveraging body and hand\njoints detected from the Mediapipe framework [37]. More\nspecifically, we optimize the initial pose parameters pinit\n1:Fto\nminimize the re-projection loss Lrecbetween the regressed\njoints ˆJ1:Fand the joints predicted from Mediapipe J1:F:\nLrec=||J1:F−ΠK(ˆJ1:F)||1, (2)\nwhere ΠKis the intrinsic camera projection matrix. Follow-\ning extensive experimentation, we observed that optimiza-\ntion is only necessary for the arm and hand joints, as the\nOSX-regressed body joints are sufficiently accurate. Fitting\nhand poses using 2D keypoints is an exceptionally chal-\nlenging task, primarily due to the numerous articulations\nand the inherent ambiguities within the solutions. While\nseveral methods have been proposed to constrain SMPL to\nfeasible body poses [17, 41, 57], pose prior models for the\nhand models [42, 44] remain unexplored. To constrain the\noptimization to plausible human and hand poses, we pro-\npose a simple but intuitive approach using Principal Com-\nponent Analysis (PCA) to model the subspace of anatom-\nically feasible poses. Specifically, we trained a PCA pose\nprior model on two large datasets of human body [38] and\nhand [22] poses, to model the distribution of feasible arm\nand hand poses. To formulate the prior loss we measure\nthe reconstruction error of a mesh Xprojected and recon-\nstructed from the PCA space Uas:\nLprior=||X−[(X−µ)UT]U+µ||2, (3)\nwhere U∈RN·3×dis the eigenvector basis of dcompo-\nnents and µis the mean mesh. Intuitively, realistic poses\n1987\nwill result in smaller reconstruction errors compared to in-\nfeasible articulations.\nFinally, a common issue with blurry videos is that the\nOSX reconstruction and the Mediapipe detections may in-\nclude jittering and spatio-temporal noise. To tackle this, we\nenforce temporal coherence using a loss function on both\nvertex and joint space that enforces smooth transitions be-\ntween adjacent frames f,f−1:\nLtemp=||Xf−Xf−1||2+||Jf−Jf−1||2. (4)\nThe overall loss function can be defined as:\nL=Lrec+λpriorLprior+λtempLtemp, (5)\nwhere λprior, λtemp are hyperparameters. An overview of\nthe proposed fitting pipeline is depicted in Fig. 2.\n4. Method\nWe propose Neural Sign Actors, a diffusion-based gener-\native model that generates motion sequences conditioned\non text transcripts. Similar to traditional diffusion archi-\ntectures, our method is composed of the deterministic for-\nward diffusion process that gradually adds noise to the input\ndistributions and the reverse denoising model ϵθ(·)that pre-\ndicts the noise introduced by the forward process at each\ntime-step. To reduce the computational requirements and\nfacilitate the generation quality, we train a diffusion model\non the low-dimensional pose space defined by SMPL-X\n[41] model instead of the vertex space. Given that sign lan-\nguage is solely related to hand motion and facial expres-\nsions, we focus on modeling the pose and the expression\nparameters on the canonical shape. An overview of the pro-\nposed approach can be found in Fig. 3.\n4.1. Forward Diffusion Process\nDuring the forward diffusion process, noise sampled from\na Gaussian distribution N(µ, σI)is gradually added to the\nsequence of SMPL-X parameters p1:F, which consist of the\nconcatenated poses θ1:F, and expressions ψ1:F. This pro-\ncess iterates a total of Ttimes as a Markov chain, ultimately\ntransforming the poses into a Gaussian distribution N(0,I).\nIn line with the approach delineated in [29], we establish the\nforward diffusion process as follows:\nq(pt\n1:F|pt−1\n1:F) =N(pt\n1:F|√αtpt−1\n1:F,(1−αt)I)(6)\nwhere αtis the variance schedule parameter that controls\nthe noise scheduling of the process.\n4.2. Reverse Diffusion Process\nFollowing the forward process, the goal of the denoising\nmodule ϵΘis to learn the reverse process, i.e. learn a map-\nping from the noised distribution to the real pose spacepθ(pt−1\n1:F|pt\n1:F). Following the reparameterization trick of\n[29], we train a denoising model ϵθthat predicts the time\nconditioned noise ϵtas:\nLt=||ϵt−ϵΘ(pt\n1:F, t,w1:F)||2, (7)\nwhere ϵtis the noise added at time-step tof the forward dif-\nfusion process and w1:Fdenotes the target text transcript.\nTo further enforce the generation of accurate hand artic-\nulations, we modify Ltto double the weighting factor of\nthe hand poses. The proposed denoising module can be\ndivided into three main components: the anatomically in-\nformed pose and expression encoders, the text encoder, and\nthe auto-regressive decoder.\nAnatomically Informed Encoder. Previous methods for\nhuman motion generation attempted to model poses and\njoint rotations independently, using permutation equivari-\nant layers such as MLPs [13, 27]. We observed that such\nequivariance limits the generative ability of the network\nand results in mild motion intensities. To tackle this lim-\nitation, we propose to break the permutation equivariance\nusing a novel, anatomically inspired, graph neural network\n(GNN) combined with a pose embedding layer. In partic-\nular, for a joint i, we build a message passing layer that\nupdates the joint ifeatures fibased on the relative features\nof the SMPL-X kinematic tree K. Additionally, to break\nthe permutation equivariance of the proposed message pass-\ning layer, we introduce a pose embedding that encodes joint\nindex iinto a unique token feature Pi. With this formula-\ntion, the network learns to disentangle the joint distributions\nsince each joint is uniquely defined by its token feature P.\nThe update function of the proposed message passing layer\ncan be defined as:\nf′\ni=γ\uf8eb\n\uf8edX\nj∈K igij(fj−fi) +Pi\uf8f6\n\uf8f8, (8)\nwhere fjdenotes the features of joint jwhich is anatomi-\ncally connected to joint i, in the kinematic tree K,gijis an\nanisotropic function between the joints i, j,Pirefers to the\npositional encoding of joint i, and γis a non-linearity. We\nestablish the anisotropy of kernel gijby assigning a differ-\nent set of learnable weights to each set of neighbors.\nSimilarly, to break the permutation equivariance of the\nexpression encoder layers, we append each of the expres-\nsion parameters with a learnable expression token E. Given\nthat expression blendshapes cannot be represented in graph\nform, we utilize an MLP to encode their latent features as:\ng′\ni=γ(MLP(gi+Ei)), (9)\nwhere gidenotes the latent features of expression parameter\niandEirefers to its corresponding expression embedding.\nText Encoding. Sign language is not merely a direct word-\nfor-word translation of spoken language, rather it possesses\n1988\n|| GNNPose EncoderPose Embedding\nExpression EmbeddingMLP\nExpression Encoder||\nFrames...Forward\nDiffusionReverse  \nDiffusion  ...\nRNN\nReshapeMLPRegression HeadLearnable Module\nNon-Learnable Module\nAuto-regressive\nGround T ruth Sign\n Generated SignFigure 3. Overview of the proposed method. We employ a diffusion model to learn a mapping between text scripts and 3D sign language.\nThe proposed framework consists of an auto-regressive denoising module ϵΘthat is founded on the novel anatomically informed pose\nencoder to model the sign motions.\nTable 1. Mean per vertex error (mm) of the proposed and the base-\nline methods on the SGNify mocap dataset [25].\nMethod Body Left Hand Right Hand\nFrankMoCap[45] 78.07 20.47 19.62\nPIXIE[23] 60.11 25.02 22.42\nPyMAF-X [63] 68.61 21.46 19.19\nSMPLify-X [41] 56.07 22.23 18.83\nSGNify [25] 55.63 19.22 17.50\nOSX [36] 47.32 18.34 18.12\nProposed 46.42 16.17 15.23\nits own unique grammar, semantic structure, and distinct\nlanguage logic [34]. Contingent upon this, we avoid us-\ning a sequence of word embeddings to condition the motion\ngeneration and propose to utilize CLIP [43] as a powerful\nsentence encoder that is able to generalize to arbitrary text\nprompts. We condition pose and expression encoders on the\ntext embedding using a gating approach described in [26].\nAuto-regressive Decoder. Considering that motion can be\nconceptualized as a sequence of poses where each pose is\ncontingent upon its predecessor, we constructed our motion\ngenerative network utilizing an auto-regressive model. As\nwe experimentally illustrate in Sec. 5.4, we utilize a Long-\nShort-Term-Memory (LSTM) model as our pose decoder\nsince it has less memory requirement than transformer ar-\nchitectures and provides better auto-regressive capabilities.\nFinally, we map the output of the autoregressive model back\nto the pose space using an MLP layer.\n5. Experiments\n5.1. Dataset Evaluation\nGiven that accurate 3D annotations are a requisite for the\ntraining of a potent SLP model, we quantitatively and qual-\nitatively evaluated the performance of the pipeline intro-\nduced in Sec. 3 on the task of sign language reconstruc-\ntion from videos. To assess the fitting quality, we apply theproposed pipeline to the SGNify mocap dataset [25] that\ncontains ground truth annotations. In Tab. 1, we report the\nGround Truth Video SGNify OSX ProposedSGNify-MoCap Dataset How2Sign Dataset\nFigure 4. Qualitative comparison between the proposed and the\nbaseline fitting frameworks on SGNify [25] and How2Sign [19].\nreconstruction error of the proposed pipeline and compare\nit with OSX [36] and the SGNify method, which is the cur-\nrent state-of-the-art method for 3D fitting from SL videos.\nIt must be noted that unlike the SGNify method, the pro-\nposed fitting pipeline achieves a smaller reconstruction er-\nror despite not including any SL-driven losses. The power-\nful prior model that guides the fitting optimization leads our\nmethod to valid poses and articulation Fig. 4.\n5.2. Sign Language Production\nBaselines. To evaluate the performance of our method we\nselected the current state-of-the-art methods for text-driven\nsign language generation, i.e. Saunders et al. [49], Saunders\n1989\nTable 2. Quantitative evaluation of the proposed and the baseline methods on the How2Sign test dataset.\nBody Left Hand Right Hand Back-Translation\nMethod MPVPE ↓MPJPE ↓FID↓DTW↓MPVPE MPJPE FID DTW MPVPE MPJPE FID DTW BLEU-4 ↑BLEU-3 ↑BLEU-2 ↑BLEU-1 ↑ROUGE ↑\nSaunders et al. [49] 67.21 70.06 4.71 14.15 73.49 74.13 0.68 11.21 75.57 77.47 0.75 11.93 2.75 5.87 8.21 13.82 29.87\nSaunders et al. [48] 63.19 65.25 3.98 13.78 71.43 72.39 0.59 11.02 68.54 70.14 0.51 11.32 6.21 8.98 12.01 18.22 32.33\nHwang et al. [30] 62.74 63.25 4.45 13.94 78.95 70.34 0.63 11.33 68.65 69.59 0.60 12.26 5.75 8.21 11.62 17.55 31.98\nStoll et al. [54] 55.02 60.32 4.96 13.99 68.48 69.45 0.56 11.59 60.18 62.73 0.64 12.29 7.51 10.72 13.92 19.56 33.17\nProposed 31.47 35.87 1.56 7.83 36.24 38.82 0.24 6.74 39.68 40.56 0.36 7.91 13.12 18.25 25.44 41.31 47.55\nAnother technique is you can braid the hair on first and then start wrapping cause some people have \nreally short hair and we can create the look with adding the hair end.\nI really like that. Ground Truth Stoll et al. Proposed Ground Truth Stoll et al. Proposed\nNow, the front squat develops your teardrop.\n I think we've probably solved our problems.\nFigure 5. Qualitative comparison of generated signs conditioned on the text transcript between the proposed and Stoll et al. [54] methods.\nThe ground truth video is given for reference.\net al. [48], Hwang et al. [30] and Stoll et al. [54]. Given\nthat all methods have been trained on the German Sign Lan-\nguage RWTH-Phoenix dataset [9], we retrained the models\nusing the same training set-up as the proposed method.\nImplementation Details. To train the diffusion model we\nfollowed the implementation details of [29]. We imple-\nmented pose and expression embedding layers using a sim-\nple linear projection. Pose and expression encoders are\ncomposed of 4 stacked GNN and MLP layers, respectively,\nwith an increasing number of channels. We employed the\nCLIP-ViT-L-14 model as our text encoder. The RNN de-\ncoder consists of 4 LSTM layers. We trained our model\nfor 2K epochs using the Adam optimizer with a linearly de-creasing learning rate from 10−3to10−6.\nEvaluation Metrics. We quantitatively evaluate the gen-\neration quality of the proposed and the baseline methods\nunder a set of metrics. The first two were Mean Per Vertex\nPosition Error (MPVPE) and Mean Per Joint Position Error\n(MPJPE). Given that the motions generated from the pro-\nposed and baseline methods may not be correctly aligned\nwith the ground truth annotations, we used Dynamic Time\nWarping (DTW) [5] to measure the similarity between the\ngenerated and the original sign sequences. To evaluate the\nquality of the generated poses we measured the Fr ´echet in-\nception distance (FID) score between the generated and the\nground truth poses. Finally, following [49], we trained a\n1990\nTransformer-based back-translation network to map pose\nsequences back to text. For additional details, we refer the\nreader to the supplementary material.\nEvaluation of Generated Signs. In the first three columns\nof Tab. 2 we quantitatively compare the proposed and the\nbaseline methods on the test set of the curated dataset. The\nproposed method manages to outperform the baselines un-\nder all metrics, even by a large margin. Specifically, the\ngenerated signs not only demonstrate low reconstruction er-\nror across the entire upper body but also exhibit significant\nimprovements on the hands region. Additionally, the pro-\nposed method is able to generate articulations that match the\nground truth signs, which is translated to low FID scores.\nThis can be also validated in Fig. 5, where the proposed\nmethod is able to generate signs with high-frequency artic-\nulations that match the ground truth videos. In contrast,\ncurrent state-of-the-art SLP methods fail to model high-\nfrequency articulations and can only generate small devi-\nations around the canonical pose. This is quantified in\nFig. 6, where we report the average per-frame pose devi-\nations. The proposed method not only produces a larger\nvariety of poses, compared to the small deviations of Stoll\net al. [54], but also follows the ground truth pose distribu-\ntion. To enhance readability, we focus on Stoll et al. for\nqualitative comparisons, as the best performing prior work.\n0 20 40 60 80\nFrame Number0.100.150.20Pose DeviationProposed\nGround Truth\nStoll et al.\nFigure 6. Mean and Standard Deviation of the absolute pose value\nacross the sequence.\n3D Pose Back-Translation. We additionally evaluated the\noverall quality of the generated pose sequences using back-\ntranslation, which measures how much information of the\ninput sentences has been maintained in the model’s output.\nIn particular, we trained a Transformer-based architecture\non our curated How2Sign dataset to learn a mapping from\npose sequences back to the original text transcripts. We then\ntranslated the generated 3D pose sequences of How2Sign\nback to spoken language using our back-translation net-\nwork. To comply with the evaluations in [49, 54] we re-\nport BLEU n-grams from 1 to 4 and ROUGE scores. We\nrepeated the above process for sequences generated by our\nmodel, as well as the baseline models [30, 48, 49, 54] and\nsummarize the results in the last column of Tab. 2. Our\nmodel produces the highest back translation scores across\nall metrics, with BLEU-4 being at the level of BLEU-2 of\nthe second best performing method (Stoll et al. [54]).5.3. User-Study\nAlthough evaluation metrics can provide insights into a net-\nwork’s performance, the most critical benchmark lies in\nthe perceptual evaluation from Hard of Hearing individu-\nals. Notably, we further assess the realism of the generated\nsigns by designing a user study where 15 ASL fluent sub-\njects, with ages ranging from 29 to 62, evaluated the gener-\nated signs. We divided the perceptual study into two parts,\nto assess: (a) how aligned the generated signs are with re-\nspect to the text transcripts and (b) the fidelity and readabil-\nity of the proposed generations. For the first part of the user\nstudy, we presented 15 different generated signs from both\nthe proposed and baseline methods, alongside the ground\ntruth video and its corresponding fitting. Participants were\nasked to assign a value between 1-10 rating the alignment\nof each method with the corresponding text transcript.\nTo avoid potential biases between the methods, all videos\nwere shown in a random order. In Fig. 7, we report the re-\nsults of the first part of the user-study. As expected, the\nground truth videos achieve the best average score of 8.7\nwhile the fittings achieve slightly less with 8.1, which quan-\ntifies the high quality of the generated annotations. The pro-\nposed method achieves an average score of 5.8 whereas the\nmethod of Stoll et al. [54] fails to achieve reasonable results.\n0 1 2 3 4 5 6 7 8 9 10\nAlignment ScoreVideo3D FittingProposedStoll et al.\nFigure 7. Human Evaluation of the alignment between the gener-\nated signs and the text transcript.\nThe second part of the perceptual study aimed to as-\nsess the fidelity of the generated signs. Each participant\nwas shown 15 rendered videos with signs generated by\nthe proposed method and was asked to rank five candidate\ntext translations, from most likely to least likely transla-\ntion. Apart from the ground truth, the candidate translations\nincluded both similar to the ground truth sentences with\nslightly modified meanings, i.e. by masking words, crop-\nping sentences, or changing the word order, and also unre-\nlated sentences from different topics and domains. Measur-\ning the cumulative accuracy, the generated signs attained a\ntop-1 accuracy of 40% and a top-2 accuracy of 80%, affirm-\ning the realism and fidelity of the generated signs.\n5.4. Ablation\nThe proposed method consists of four main components:\nthe anatomically inspired pose encoder, the pose and ex-\npression embeddings, the autoregressive decoder, and the\n1991\nTable 3. Evaluation of individual components in the proposed\nmethod. Every row refers to a different ablated module. We in-\nclude the performance of our method for reference.\nMethod MPVPE MPJPE FID DTW\nw/o GNN 37.51 38.23 2.85 9.19\nw/o Pose and Expression Embedding 66.56 68.34 6.65 11.98\nw/o LSTM 36.17 39.46 2.12 10.82\nw. Transformer 32.73 35.41 1.58 8.17\nw/o CLIP 69.12 71.42 5.36 12.84\nw. BERT 45.32 47.11 2.23 9.21\nTevet et al. [56] 36.11 38.23 2.45 8.92\nChen et al. [13] 35.23 37.12 2.15 8.26\nProposed 31.47 35.87 1.56 7.83\ntext encoding. In this section, we evaluate the contribution\nof each component to the final generations of the model.\nEffect of Pose Encoder and Embedding Layers. Initially,\nwe ablate the novel pose encoder and we substitute it with\nan MLP layer ( w/o GNN ), similar to the expression encoder.\nUnder this setting, the generated motions are smoother and\nmainly deviate around the mean pose. Aligned with our hy-\npothesis, updating poses in an anatomically inspired man-\nner enables the network to learn higher frequencies, such as\nrare articulations, without an increase in the network’s ca-\npacity. Additionally, as mentioned in Sec. 4.2, in the tradi-\ntional setting of motion diffusion models, during the denois-\ning step, poses and expressions are sampled from a Gaus-\nsian distribution and are then decoded back to their original\nspace using a permutation equivariant network, such as an\nMLP. Such permutation equivariance, treats poses under a\nuniform setting that limits the generative power of the net-\nwork. As shown in Tab. 3, without pose and expression em-\nbedding layer ( w/o Pose and Expression Embedding ), the\nmodel fails to produce any reasonable sign, resulting in a\nperformance drop under all metrics.\nEffect of LSTM Encoder. A core part of the proposed\nmethod is the autoregressive LSTM decoder. We initially\nevaluate its contribution compared to a simple frame po-\nsitional encoding to transform the network to a frame-\nconditioned generative model, without having any temporal\nmodule ( w/o LSTM ). As expected, this results in poor DTW\nperformance and generations that present increased jittering\nand lack of temporal coherence. Furthermore, we substi-\ntute the LSTM layer with a Transformer encoder layer ( w.\nTransformer ). Interestingly, the LSTM layer achieves simi-\nlar performance to the Transformer layer while having 75%\nfewer parameters (1M vs. 4.5M).\nEffect of Text Encoding. The text encoding module has\na pivotal effect on motion generation. Firstly, we substi-\ntute the CLIP encoder with a word-level embedding layer\nthat is trained with the rest of the method in an end-to-\nend fashion. We set the embedded size to 256 although\nwe did not observe significant differences in performance.\nFollowing this, we utilized the pretrained DistilBERT [47],whose parameters remain frozen throughout training. As\ndepicted in Tab. 3, training word embeddings from scratch\nstrongly affects the generalization of the network, leading to\nlarge MPVPE and MPJPE metrics. In contrast, DistilBERT\nachieves better performance than learnable word embed-\ndings, although it does not outperform CLIP embeddings.\nThis is aligned with our assumption that sentence embed-\ndings could provide better insights regarding the meaning of\na sentence compared to word-level embeddings. Especially\nin the task of SLP, where there is not an explicit one-to-one\nmapping between words and poses, sentence level embed-\ndings provide a more powerful text encoding solution.\nComparison with Human Motion Diffusion Models. Fi-\nnally, we compare our model with state-of-the-art methods\non human motion modeling [13, 56], which can be con-\nsidered as deviations from the proposed framework. Un-\nlike our anatomically inspired approach, both models rely\non linear layers to handle pose motions, constraining their\ncapacity to encode intricate hand movements with high-\nfrequency details. In particular, although both methods can\nachieve smooth body motions, they fail to produce accu-\nrate hand articulations that match the ground truth distribu-\ntion, which can be validated from the reconstruction errors\n(MPVPE, MPVJE), along with the FID measure.\n6. Conclusion\nNeural 3D sign language production is an important chal-\nlenge that aims to aid the Deaf and Hard of Hearing com-\nmunity and can effectively increase their inclusion in any\nsocial environment. In this work, we made a step towards\nhigh fidelity 3D SLP, by deriving a large-scale 3D dataset\nto train a text conditioned diffusion-based model. The re-\nlease of additional relevant databases will enable the train-\ning of even more robust architectures. We initially introduce\na precise 3D sign language reconstruction pipeline that out-\nperforms previous SL reconstruction methods. Then, we\ntrain a motion generative model using an autoregressive\ndiffusion model. The core of our method is founded on\na novel, anatomically inspired, graph neural network that\nlearns the pose distribution and enables highly detailed ar-\nticulations. Importantly, leveraging the powerful CLIP text\nembeddings, the proposed model can generalize to out-of-\ndistribution samples. Extensive experiments on sign lan-\nguage generation tasks, including a perceptual study with\nASL fluent subjects, demonstrate the superiority of the pro-\nposed method compared to the previous approaches.\nAcknowledgements. S. Zafeiriou was supported\nby EPSRC Project DEFORM (EP/S010203/1) and\nGNOMON (EP/X011364). R.A. Potamias was sup-\nported by EPSRC Project GNOMON (EP/X011364).\n1992\nReferences\n[1] Ulrich von Agris and Karl-Friedrich Kraiss. Signum\ndatabase: Video corpus for signer-independent continuous\nsign language recognition. In sign-lang@ LREC 2010 ,\npages 243–246. European Language Resources Association\n(ELRA), 2010. 2\n[2] Samuel Albanie, G ¨ul Varol, Liliane Momeni, Triantafyllos\nAfouras, Joon Son Chung, Neil Fox, and Andrew Zisserman.\nBsl-1k: Scaling up co-articulated sign language recognition\nusing mouthing cues. In Computer Vision–ECCV 2020: 16th\nEuropean Conference, Glasgow, UK, August 23–28, 2020,\nProceedings, Part XI 16 , pages 35–53. Springer, 2020. 3\n[3] Epameinondas Antonakos, Anastasios Roussos, and Ste-\nfanos Zafeiriou. A survey on mouth modeling and analy-\nsis for sign language recognition. In 2015 11th IEEE Inter-\nnational Conference and Workshops on Automatic Face and\nGesture Recognition (FG) , pages 1–7. IEEE, 2015. 1\n[4] Vassilis Athitsos, Carol Neidle, Stan Sclaroff, Joan Nash,\nAlexandra Stefan, Quan Yuan, and Ashwin Thangali. The\namerican sign language lexicon video dataset. In 2008 IEEE\nComputer Society Conference on Computer Vision and Pat-\ntern Recognition Workshops , pages 1–8. IEEE, 2008. 2\n[5] Donald J Berndt and James Clifford. Using dynamic time\nwarping to find patterns in time series. In Proceedings of\nthe 3rd international conference on knowledge discovery and\ndata mining , pages 359–370, 1994. 6\n[6] P Boyes Braem and RL Sutton-Spence. The Hands Are The\nHead of The Mouth. The Mouth as Articulator in Sign Lan-\nguages . Hamburg: Signum Press, 2001. 1\n[7] Annelies Braffort, Laurence Bolot, Emilie Ch ´etelat-Pel ´e,\nAnnick Choisier, Maxime Delorme, Michael Filhol, J ´er´emie\nSegouat, Cyril Verrecchia, Flora Badin, and Nad `ege Devos.\nSign language corpora for analysis, processing and evalua-\ntion. In LREC , 2010. 2\n[8] Danielle Bragg, Oscar Koller, Mary Bellard, Larwan Berke,\nPatrick Boudreault, Annelies Braffort, Naomi Caselli, Matt\nHuenerfauth, Hernisa Kacorri, Tessa Verhoef, et al. Sign\nlanguage recognition, generation, and translation: An inter-\ndisciplinary perspective. In Proceedings of the 21st Inter-\nnational ACM SIGACCESS Conference on Computers and\nAccessibility , pages 16–31, 2019. 2\n[9] Necati Cihan Camgoz, Simon Hadfield, Oscar Koller, Her-\nmann Ney, and Richard Bowden. Neural sign language trans-\nlation. In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pages 7784–7793, 2018. 2,\n3, 6\n[10] Necati Cihan Camgoz, Oscar Koller, Simon Hadfield, and\nRichard Bowden. Sign language transformers: Joint end-to-\nend sign language recognition and translation. In Proceed-\nings of the IEEE/CVF conference on computer vision and\npattern recognition , pages 10023–10033, 2020. 1, 2, 3\n[11] Z. Cao, G. Hidalgo Martinez, T. Simon, S. Wei, and Y . A.\nSheikh. Openpose: Realtime multi-person 2d pose estima-\ntion using part affinity fields. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence , 2019. 2\n[12] Xiujuan Chai, Hanjie Wang, and Xilin Chen. The devisign\nlarge vocabulary of chinese sign language database and base-line evaluations. In Technical report VIPL-TR-14-SLR-001.\nKey Lab of Intelligent Information Processing of Chinese\nAcademy of Sciences (CAS) . Institute of Computing Tech-\nnology, 2014. 2\n[13] Xin Chen, Biao Jiang, Wen Liu, Zilong Huang, Bin Fu, Tao\nChen, and Gang Yu. Executing your commands via motion\ndiffusion in latent space. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\npages 18000–18010, 2023. 4, 8\n[14] Yutong Chen, Fangyun Wei, Xiao Sun, Zhirong Wu, and\nStephen Lin. A simple multi-modality transfer learning\nbaseline for sign language translation. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 5120–5130, 2022. 1\n[15] Yutong Chen, Ronglai Zuo, Fangyun Wei, Yu Wu, Shujie\nLiu, and Brian Mak. Two-stream network for sign language\nrecognition and translation. Advances in Neural Information\nProcessing Systems , 35:17043–17056, 2022. 1\n[16] Stephen Cox, Michael Lincoln, Judy Tryggvason, Melanie\nNakisa, Mark Wells, Marcus Tutt, and Sanja Abbott. Tessa, a\nsystem to aid communication with deaf people. In Proceed-\nings of the fifth international ACM conference on Assistive\ntechnologies , pages 205–212, 2002. 2\n[17] Andrey Davydov, Anastasia Remizova, Victor Constantin,\nSina Honari, Mathieu Salzmann, and Pascal Fua. Adversarial\nparametric pose prior. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition , pages\n10997–11005, 2022. 3\n[18] Philippe Dreuw, David Rybach, Thomas Deselaers, Morteza\nZahedi, and Hermann Ney. Speech recognition techniques\nfor a sign language recognition system. hand , 60:80, 2007.\n2\n[19] Amanda Duarte, Shruti Palaskar, Lucas Ventura, Deepti\nGhadiyaram, Kenneth DeHaan, Florian Metze, Jordi Torres,\nand Xavier Giro-i Nieto. How2sign: a large-scale multi-\nmodal dataset for continuous american sign language. In\nProceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition , pages 2735–2744, 2021. 2, 3,\n5\n[20] Eleni Efthimiou, Stavroula-Evita Fotinea, Thomas Hanke,\nJohn Glauert, Richard Bowden, Annelies Braffort,\nChristophe Collet, Petros Maragos, and Franc ¸ois Goude-\nnove. Dicta-sign: sign language recognition, generation\nand modelling with application in deaf communication. In\nsign-lang@ LREC 2010 , pages 80–83. European Language\nResources Association (ELRA), 2010. 2\n[21] Eleni Efthimiou, Stavroula-Evita Fotinea, Thomas\nHanke, John Glauert, Richard Bowden, Annelies Braf-\nfort, Christophe Collet, Petros Maragos, and Franc ¸ois\nLefebvre-Albaret. The dicta-sign wiki: Enabling web\ncommunication for the deaf. In Computers Helping People\nwith Special Needs: 13th International Conference, ICCHP\n2012, Linz, Austria, July 11-13, 2012, Proceedings, Part II\n13, pages 205–212. Springer, 2012. 2\n[22] Zicong Fan, Omid Taheri, Dimitrios Tzionas, Muhammed\nKocabas, Manuel Kaufmann, Michael J. Black, and Otmar\nHilliges. ARCTIC: A dataset for dexterous bimanual hand-\n1993\nobject manipulation. In Proceedings IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR) , 2023. 3\n[23] Yao Feng, Vasileios Choutas, Timo Bolkart, Dimitrios\nTzionas, and Michael Black. Collaborative regression of ex-\npressive bodies using moderation. In International Confer-\nence on 3D Vision (3DV) , pages 792–804, 2021. 5\n[24] Jens Forster, Christoph Schmidt, Oscar Koller, Martin Bell-\ngardt, and Hermann Ney. Extensions of the sign language\nrecognition and translation corpus rwth-phoenix-weather. In\nLREC , pages 1911–1916, 2014. 2\n[25] Maria-Paola Forte, Peter Kulits, Chun-Hao Paul Huang,\nVasileios Choutas, Dimitrios Tzionas, Katherine J. Kuchen-\nbecker, and Michael J. Black. Reconstructing signing avatars\nfrom video using linguistic priors. In IEEE/CVF Conf. on\nComputer Vision and Pattern Recognition (CVPR) , pages\n12791–12801, 2023. 5\n[26] Will Grathwohl, Ricky TQ Chen, Jesse Bettencourt, Ilya\nSutskever, and David Duvenaud. Ffjord: Free-form con-\ntinuous dynamics for scalable reversible generative models.\narXiv preprint arXiv:1810.01367 , 2018. 5\n[27] Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji,\nXingyu Li, and Li Cheng. Generating diverse and natural 3d\nhuman motions from text. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\npages 5152–5161, 2022. 4\n[28] Aiming Hao, Yuecong Min, and Xilin Chen. Self-mutual dis-\ntillation learning for continuous sign language recognition.\nInProceedings of the IEEE/CVF International Conference\non Computer Vision , pages 11303–11312, 2021. 1\n[29] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. Advances in neural information\nprocessing systems , 33:6840–6851, 2020. 4, 6\n[30] Eui Jun Hwang, Jung-Ho Kim, and Jong C. Park. Non-\nautoregressive sign language production with gaussian\nspace. In The 32nd British Machine Vision Conference\n(BMVC 21) . British Machine Vision Conference (BMVC),\n2021. 2, 6, 7\n[31] Hamid Reza Vaezi Joze and Oscar Koller. Ms-asl: A large-\nscale data set and benchmark for understanding american\nsign language. arXiv preprint arXiv:1812.01053 , 2018. 2\n[32] Parul Kapoor, Rudrabha Mukhopadhyay, Sindhu B Hegde,\nVinay Namboodiri, and CV Jawahar. Towards auto-\nmatic speech to sign language generation. arXiv preprint\narXiv:2106.12790 , 2021. 2\n[33] Dongxu Li, Cristian Rodriguez, Xin Yu, and Hongdong Li.\nWord-level deep sign language recognition from video: A\nnew large-scale dataset and methods comparison. In Pro-\nceedings of the IEEE/CVF winter conference on applications\nof computer vision , pages 1459–1469, 2020. 2\n[34] Dongxu Li, Chenchen Xu, Xin Yu, Kaihao Zhang, Benjamin\nSwift, Hanna Suominen, and Hongdong Li. Tspnet: Hier-\narchical feature learning via temporal semantic pyramid for\nsign language translation. Advances in Neural Information\nProcessing Systems , 33:12034–12045, 2020. 5\n[35] Zeyu Liang, Huailing Li, and Jianping Chai. Sign language\ntranslation: A survey of approaches and techniques. Elec-\ntronics , 12(12):2678, 2023. 1, 2[36] Jing Lin, Ailing Zeng, Haoqian Wang, Lei Zhang, and Yu Li.\nOne-stage 3d whole-body mesh recovery with component\naware transformer. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition , pages\n21159–21168, 2023. 3, 5\n[37] Camillo Lugaresi, Jiuqiang Tang, Hadon Nash, Chris Mc-\nClanahan, Esha Uboweja, Michael Hays, Fan Zhang, Chuo-\nLing Chang, Ming Guang Yong, Juhyun Lee, et al. Medi-\napipe: A framework for building perception pipelines. arXiv\npreprint arXiv:1906.08172 , 2019. 3\n[38] Naureen Mahmood, Nima Ghorbani, Nikolaus F. Troje, Ger-\nard Pons-Moll, and Michael J. Black. Amass: Archive of\nmotion capture as surface shapes. In The IEEE International\nConference on Computer Vision (ICCV) , 2019. 3\n[39] John McDonald, Rosalee Wolfe, Jerry Schnepp, Julie\nHochgesang, Diana Gorman Jamrozik, Marie Stumbo, Lar-\nwan Berke, Melissa Bialek, and Farah Thomas. An auto-\nmated technique for real-time production of lifelike anima-\ntions of american sign language. Universal Access in the\nInformation Society , 15:551–566, 2016. 2\n[40] Lucie Naert, Caroline Larboulette, and Sylvie Gibet. A sur-\nvey on the animation of signing avatars: From sign repre-\nsentation to utterance synthesis. Computers & Graphics , 92:\n76–98, 2020. 2\n[41] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani,\nTimo Bolkart, Ahmed AA Osman, Dimitrios Tzionas, and\nMichael J Black. Expressive body capture: 3d hands,\nface, and body from a single image. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition , pages 10975–10985, 2019. 2, 3, 4, 5\n[42] Rolandos Alexandros Potamias, Stylianos Ploumpis,\nStylianos Moschoglou, Vasileios Triantafyllou, and Stefanos\nZafeiriou. Handy: Towards a high fidelity 3d hand shape\nand appearance model. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\npages 4670–4680, 2023. 3\n[43] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning , pages\n8748–8763. PMLR, 2021. 2, 5\n[44] Javier Romero, Dimitrios Tzionas, and Michael J Black. Em-\nbodied hands: Modeling and capturing hands and bodies to-\ngether. arXiv preprint arXiv:2201.02610 , 2022. 3\n[45] Yu Rong, Takaaki Shiratori, and Hanbyul Joo. Frankmocap:\nA monocular 3d whole-body pose estimation system via re-\ngression and integration. In IEEE International Conference\non Computer Vision Workshops , 2021. 5\n[46] Ramon Sanabria, Ozan Caglayan, Shruti Palaskar, Desmond\nElliott, Lo ¨ıc Barrault, Lucia Specia, and Florian Metze.\nHow2: a large-scale dataset for multimodal language under-\nstanding. arXiv preprint arXiv:1811.00347 , 2018. 3\n[47] Victor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. Distilbert, a distilled version of bert: smaller,\nfaster, cheaper and lighter. arXiv preprint arXiv:1910.01108 ,\n2019. 8\n1994\n[48] Ben Saunders, Necati Cihan Camg ¨oz, and Richard Bowden.\nAdversarial training for multi-channel sign language produc-\ntion. In The 31st British Machine Vision Virtual Conference .\nBritish Machine Vision Association. 2, 6, 7\n[49] Ben Saunders, Necati Cihan Camgoz, and Richard Bowden.\nProgressive transformers for end-to-end sign language pro-\nduction. In Computer Vision–ECCV 2020: 16th European\nConference, Glasgow, UK, August 23–28, 2020, Proceed-\nings, Part XI 16 , pages 687–705. Springer, 2020. 2, 3, 5,\n6, 7\n[50] Ben Saunders, Necati Cihan Camgoz, and Richard Bowden.\nMixed signals: Sign language production via a mixture of\nmotion primitives. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision , pages 1919–1929,\n2021. 2\n[51] Ben Saunders, Necati Cihan Camgoz, and Richard Bowden.\nSigning at scale: Learning to co-articulate signs for large-\nscale photo-realistic sign language production. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR) , pages 5141–5151, 2022. 2\n[52] Adam Schembri, Jordan Fenlon, Ramas Rentelis, Sally\nReynolds, and Kearsy Cormier. Building the british sign lan-\nguage corpus. 2013. 2\n[53] Stephanie Stoll, Necati Cihan Camg ¨oz, Simon Hadfield, and\nRichard Bowden. Sign language production using neural\nmachine translation and generative adversarial networks. In\nProceedings of the 29th British Machine Vision Conference\n(BMVC 2018) . British Machine Vision Association, 2018. 2\n[54] Stephanie Stoll, Armin Mustafa, and Jean-Yves Guillemaut.\nThere and back again: 3d sign language generation from text\nusing back-translation. In 2022 International Conference on\n3D Vision (3DV) , pages 187–196. IEEE, 2022. 2, 6, 7\n[55] Rachel Sutton-Spence and Bencie Woll. The linguistics of\nBritish Sign Language: an introduction . Cambridge Univer-\nsity Press, 1999. 1\n[56] Guy Tevet, Sigal Raab, Brian Gordon, Yoni Shafir, Daniel\nCohen-or, and Amit Haim Bermano. Human motion diffu-\nsion model. In The Eleventh International Conference on\nLearning Representations , 2022. 8\n[57] Garvita Tiwari, Dimitrije Antic, Jan Eric Lenssen, Nikolaos\nSarafianos, Tony Tung, and Gerard Pons-Moll. Pose-ndf:\nModeling human pose manifolds with neural distance fields.\nInEuropean Conference on Computer Vision (ECCV) , 2022.\n3\n[58] Ville Viitaniemi, Tommi Jantunen, Leena Savolainen, Matti\nKarppa, and Jorma Laaksonen. S-pot - a benchmark in\nspotting signs within continuous signing. In Proceedings of\nthe Ninth International Conference on Language Resources\nand Evaluation (LREC’14) , pages 1892–1897, Reykjavik,\nIceland, 2014. European Language Resources Association\n(ELRA). 3\n[59] Ulrich V on Agris, Moritz Knorr, and Karl-Friedrich Kraiss.\nThe significance of facial features for automatic sign lan-\nguage recognition. In 2008 8th IEEE international confer-\nence on automatic face & gesture recognition , pages 1–6.\nIEEE, 2008. 2\n[60] Ronnie Wilbur and Avinash C Kak. Purdue rvl-slll american\nsign language database. 2006. 2[61] Morteza Zahedi, Daniel Keysers, Thomas Deselaers, and\nHermann Ney. Combination of tangent distance and an\nimage distortion model for appearance-based sign language\nrecognition. In Pattern Recognition: 27th DAGM Sympo-\nsium, Vienna, Austria, August 31-September 2, 2005. Pro-\nceedings 27 , pages 401–408. Springer, 2005. 2\n[62] Jan Zelinka, Jakub Kanis, and Petr Salajka. Nn-based\nczech sign language synthesis. In Speech and Computer:\n21st International Conference, SPECOM 2019, Istanbul,\nTurkey, August 20–25, 2019, Proceedings 21 , pages 559–\n568. Springer, 2019. 2\n[63] Hongwen Zhang, Yating Tian, Yuxiang Zhang, Mengcheng\nLi, Liang An, Zhenan Sun, and Yebin Liu. Pymaf-x: To-\nwards well-aligned full-body model regression from monoc-\nular images. IEEE Transactions on Pattern Analysis and Ma-\nchine Intelligence , 2023. 5\n[64] Benjia Zhou, Zhigang Chen, Albert Clap ´es, Jun Wan,\nYanyan Liang, Sergio Escalera, Zhen Lei, and Du Zhang.\nGloss-free sign language translation: Improving from visual-\nlanguage pretraining. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision , pages 20871–\n20881, 2023. 2\n1995"}, 'dist': 0.9286905527114868}
Result 30: {'text': 'data', 'metadata': {'created_at': 1736932571, 'modified_at': 1736932571, 'owner': 'sayandeep', 'path': 'RAG_CONTENT/conferences/cvpr_ocr/Liu_SCoFT_Self-Contrastive_Fine-Tuning_for_Equitable_Image_Generation_CVPR_2024_paper.txt', 'size': 50519, 'seen_at': 1737191136, 'data': 'SCoFT: Self-Contrastive Fine-Tuning for Equitable Image Generation\r\nZhixuan Liu1Peter Schaldenbrand1Beverley-Claire Okogwu1Wenxuan Peng2\r\nYoungsik Yun3Andrew Hundt1Jihie Kim3Jean Oh1*\r\n1Carnegie Mellon University2Nanyang Technological University3Dongguk University\r\n⇤Corresponding author\r\nOriginal\tStable\tDiffusionOriginal\tStable\tDiffusion(a)“Photo\tof\ta\ttraditional\tbuilding,\tin\t[Culture]”\r\n(b)“Two\tpeople\twearing\ttraditional\tclothing,\tin\t[Culture]”SCoFT\t(Ours)SCoFT\t(Ours)AmericanCultureNigerian\tCultureKorean\tCultureMexican\tCultureChinese\tCultureIndian\tCulture\r\nStereotypeMisrepresen-tation\r\nFigure 1. Comparison between Stable Diffusion with and without our proposed ﬁne-tuning approach, SCoFT, on our proposed CCUB\r\ndataset. Stable Diffusion perpetuates harmful stereotypes that assume dirty buildings are representative of some nations, and often generates\r\nregionally irrelevant designs. By contrast, our approach decreases stereotypes and improves cultural relevance of generated images.\r\nAbstract\r\nAccurate representation in media is known to improve\r\nthe well-being of the people who consume it. Genera-\r\ntive image models trained on large web-crawled datasets\r\nsuch as LAION are known to produce images with harm-\r\nful stereotypes and misrepresentations of cultures. We im-\r\nprove inclusive representation in generated images by (1)\r\nengaging with communities to collect a culturally repre-\r\nsentative dataset that we call the Cross-Cultural Under-\r\nstanding Benchmark (CCUB) and (2) proposing a novel\r\nSelf-Contrastive Fine-Tuning (SCoFT, pronounced /s ˆoft/)\r\nmethod that leverages the model’s known biases to self-\r\nimprove. SCoFT is designed to prevent overﬁtting on small\r\ndatasets, encode only high-level information from the data,and shift the generated distribution away from misrepresen-\r\ntations encoded in a pretrained model. Our user study con-\r\nducted on 51 participants from 5 different countries based\r\non their self-selected national cultural afﬁliation shows that\r\nﬁne-tuning on CCUB consistently generates images with\r\nhigher cultural relevance and fewer stereotypes when com-\r\npared to the Stable Diffusion baseline, which is further im-\r\nproved with our SCoFT technique. Resources and code are\r\nathttps://ariannaliu.github.io/SCoFT .\r\n1. Introduction\r\nRepresentation matters. In media, studies repeatedly show\r\nthat representation affects the well-being of its viewers [ 8,\r\n12,53]. Representation can positively affect viewers by\r\nproviding them with role models that they identify with,\r\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\r\nExcept for this watermark, it is identical to the accepted version;\r\nthe final published version of the proceedings is available on IEEE Xplore.\r\n10822\r\nbut it can also negatively affect viewers by creating harm-\r\nful, stereotypical understandings of people and culture [ 7].\r\nWhen people are accurately represented in media, it al-\r\nlows people to properly understand cultures without harm-\r\nful stereotypes forming [ 11,37]. To date, unfortunately,\r\nmany media-generating AI models show poor representa-\r\ntion in their results [ 32,39] and have been deployed for\r\ncases with negative impacts on various groups, such as non-\r\nconsensual images [ 34]. Such issues stem from every stage\r\nof an ML system’s lifecycle [ 56], with key steps being al-\r\ngorithms and the large training datasets gathered by crawl-\r\ning the Internet with inadequate ﬁltering supervision. They\r\ncontain or amplify malignant stereotypes and ethnic slurs,\r\namong other problematic content [ 4], and impact a range\r\nof applications [ 22]. Researchers have shown that large\r\ndatasets such as LAION-400M [ 51] used to train many text-\r\nto-image synthesis models, including Stable Diffusion [ 46],\r\ncenter the Global North [ 4,5,32] and struggle to accurately\r\ndepict cultures from the Global South as shown in Figure 1.\r\nTo ensure models better represent culture and more ac-\r\ncurately represent the world, we introduce a new task of\r\nculturally-aware image synthesis with the aim of address-\r\ning representation in image generation: generating visual\r\ncontent that is perceived to be more representative of na-\r\ntional cultural contexts. Our overarching goal is to improve\r\nthe well-being of viewers of the AI-generated images with\r\nparticular attention to those who are from a selection of\r\ngroups marginalized by existing methods.\r\nOur research question is, how can effective, existing text-\r\nto-image models be improved to become more culturally\r\nrepresentative and thus less offensive? Since it may be in-\r\nfeasible to vet billions of training examples for accurate cul-\r\ntural content, we hypothesize that a small dataset that is\r\nveritably representative of a culture can be used to prime\r\npre-trained text-to-image models to guide the model toward\r\nmore culturally accurate content creation. To verify the hy-\r\npothesis, we collected a dataset of image and caption pairs\r\nfor 5 cultures. For each culture, data was collected by peo-\r\nple who self-selectedly afﬁliated with that culture as they\r\nare the people who properly understand it and are most af-\r\nfected by its misrepresentations. We call this the Cross-\r\nCultural Understanding Benchmark (CCUB) dataset which\r\ncomprises 150 - 200 images for each culture each with a\r\nmanually written caption as shown in Figure 2.\r\nTo encode the culturally representative information in\r\nCCUB into a pre-trained model, we propose to ﬁne-tune the\r\nmodel with the new dataset. Existing ﬁne-tuning techniques\r\nwork well for low-level adaptations such as style changes\r\nor introducing new characters to models [ 21], but we show\r\nthat these methods struggle to encode high-level, complex\r\nconcepts such as culture. Additionally, ﬁne-tuning on small\r\ndatasets, such as CCUB, can lead to overﬁtting.\r\nUnlike concept editing tasks [ 15,16] with speciﬁc im-age editing directions, depicting cultural accuracy remains\r\nmore abstract and challenging. We propose a novel ﬁne-\r\ntuning approach, Self-Contrastive Fine-Tuning (SCoFT,\r\npronounced /s ˆoft/), to address these issues. SCoFT lever-\r\nages the pre-trained model’s cultural misrepresentations\r\nagainst itself. We harness the intrinsic biases of large pre-\r\ntrained models as a rich source of counterexamples; shift-\r\ning away from these biases gives the model direction to-\r\nwards more accurate cultural concepts. Image samples from\r\nthe pre-trained model are used as negative examples, and\r\nCCUB images are used as positive examples, to train the\r\nmodel to discern subtle differences. We de-noise latent\r\ncodes in several iterations, project them into the pixel space,\r\nand then compute the contrastive loss. The loss is back-\r\npropagated to the diffusion model UNet and optimized to\r\npush generated samples towards the positive distribution\r\nand away from the negative. This is all done in a percep-\r\ntual feature space so that the model learns more high-level,\r\ncomplex features from the images.\r\nTo evaluate our results we recruited participants who\r\nidentify as being a member of the cultural communities in\r\nthe CCUB dataset to rank images generated by Stable Dif-\r\nfusion with or without the proposed self-contrastive ﬁne-\r\ntuning on CCUB. Fine-tuning on CCUB was found to de-\r\ncrease offensiveness and increase the cultural relevance of\r\ngenerated results based on 51 participants across ﬁve cul-\r\ntures and 766 image comparisons. Our proposed SCoFT\r\napproach further improved these results. We share the ﬁnd-\r\nings from our experiments to provide a basis for an im-\r\nportant aspect of AI-generated imagery: that cultural infor-\r\nmation should be accurately presented and celebrated equi-\r\ntably. Our contributions are as follows:\r\n1.The introduction of culturally-aware text-to-image syn-\r\nthesis as a valuable task within text-to-image synthesis;\r\n2.The Cross-Cultural Understanding Benchmark (CCUB)\r\ndataset of culturally representative image-text pairs\r\nacross 5 countries; and\r\n3.Self-Contrastive Fine-Tuning (SCoFT), a novel tech-\r\nnique for encoding high-level information into a pre-\r\ntrained model using small datasets.\r\n2. Related Work\r\nCultural Datasets Various efforts have been made to eval-\r\nuate and document the impacts of datasets[ 23,50,60]. Dol-\r\nlar Street [ 45] aimed to capture accurate demographic infor-\r\nmation based on socioeconomic features, such as everyday\r\nhousehold items and monthly income of 63 countries world-\r\nwide. However, this dataset offers less diverse scenarios, as\r\nmost of its images are indoor views with limited cultural\r\nfeatures. Likewise, the WIT [ 55] and MLM [ 1] strive for\r\ncultural representation but use Wikipedia/WikiData sources\r\nfor images that are not representative of all aspects of cul-\r\nture and are over-saturated with old, streotypical images.\r\nOther works, capturing the idea of a diverse dataset, aim\r\n10823\r\nto reduce stereotypical bias through self-curation [ 10] or\r\ncultural-driven methods [ 17,36,38], inspiring our data col-\r\nlection methodology.\r\nThe MaRVL dataset [ 30], for example, was curated by\r\npeople who identify as afﬁliated with one of several partic-\r\nular cultures, MaRVL was developed to mitigate biases for\r\nreasoning tasks covering popular concepts and is unsuitable\r\nfor text-to-image synthesis. Our dataset was also designed\r\nto cover a diverse sample of cultures and engage with peo-\r\nple who are native, but it is speciﬁcally curated for vision-\r\nlanguage tasks and diverse cultural categories.\r\nFine-Tuning Text-to-Image Models Fine-tuning pre-\r\ntrained text-to-image synthesis models with a new dataset\r\nis an approach to encode additional new simple concepts\r\nand content into the model [ 21,27,31,41,48]. But culture\r\nis a complex, high-level concept that poses many challenges\r\nwhen attempting to ﬁne-tune a model to understand it.\r\nFine-Tuning on Cultural Data. Prior work in adapting\r\npre-trained models to be culturally relevant found some suc-\r\ncess using millions of culturally relevant text-image pairs,\r\nas in ERNIE-ViLG 2.0. [ 13] and Japanese Stable Diffu-\r\nsion [ 54]. The size of these training datasets leads to bet-\r\nter cultural representations of Japan and China, but they are\r\nnot easy to deploy universally, as these approaches require\r\nmillions of training examples which cannot possibly be met\r\nfor cultures with less internet presence. Besides, these\r\ndatasets are so large that it is infeasible to 100% vet them\r\nfor harmful and stereotypical information [ 43]. We propose\r\na ﬁne-tuning technique that adapts pre-trained models to\r\nlearn complex, elusive concepts, namely culture, from small\r\ndatasets.\r\nFine-Tuning Stable Diffusion in the Pixel Space. Latent\r\ndiffusion models are customarily trained in the latent space,\r\nhowever, the latent codes can be decoded into images dif-\r\nferentiably. Multiple works compute losses on the decoded\r\nimages to optimize over the input latent code [ 59] or the de-\r\ncoder weights [ 2]. DiffusionCLIP [ 25] optimizes UNet pa-\r\nrameters using losses computed on image outputs, however,\r\nthis is performed on a non-latent diffusion model. High pa-\r\nrameter count in latent diffusion models complicates gradi-\r\nent recording through multiple UNet passes, hindering de-\r\ncoding into pixel space. We propose a novel method to re-\r\nduce the computation graph size, facilitating tractable back-\r\npropagation of loss through Stable Diffusion.\r\nPerceptual Similarity. Perceptual metrics, such as\r\nLPIPS [ 62], and recent foundation models for perceptual\r\nsimilarity [ 14,29,40,42], have been shown to align more\r\nclosely with human perception than pixel space Euclidean\r\ndistance [ 49]. Perceptual similarity ignores low-level dif-\r\nferences and captures high-level details, which are more\r\nimportant for complex concepts such as cultures. To our\r\nknowledge, no other work has trained a latent diffusion\r\nmodel using perceptual loss, likely due to the technical chal-Nine\tcultural\tcategoriesExperienced\tResidentsof\tTarget\tCulture“Korean\tbarbecue\tgrilling\tmeat”“a\tKorean\tcouple\tin\thanbok”“the\tEight\tGates\tin\tthe\tFortress\tWall\tof\tSeoul”“Woman\tin\tcostume\tfor\tthe\tDay\tof\tthe\tDead\tholiday”“Puebla\tCholula\tChurch\tand\tPopocatepetl\tVolcano\tin\tMexico”“a\tplate\tof\tbeef\ttacos\twith\tbeans\tand\trice\ton\tthe\tside”CCUBKoreanData\tExamplesCCUB\tMexican\tData\tExamples\r\nCCUBDatasetFigure 2. CCUB dataset cultural image and description samples.\r\nlenges of back-propagating loss through the diffusion pro-\r\ncess and latent decoder, which we address in this paper.\r\n3. CCUB Dataset\r\nCan a CCUB tame a LAION? As opposed to the\r\nLAION [ 52] dataset which scraped images and captions\r\nfrom the internet with minimal supervision leading to a\r\nprominence in harmful content, our CCUB dataset was col-\r\nlected by hand by the people most affected by cultural mis-\r\nrepresentations in text-to-image synthesis.\r\nFollowing the deﬁnition of culture in [ 18], [30], and [ 24],\r\nnine categories are used to represent cultural elements in our\r\ndataset: architecture (interior and exterior), city, clothing,\r\ndance music and visual arts, food and drink, nature, people\r\nand action, religion and festival, utensils and tools. The\r\ncategories are further divided into traditional and modern to\r\nreﬂect how cultural characteristics change over time.\r\nFor each culture, we recruited at least 5 people with\r\nat least 5 years of experience living in one of 5 countries\r\n(China, Korea, India, Mexico, and Nigeria) to each provide\r\n20-30 images and captions. The images were collected ei-\r\nther by collecting image links from Google searches or the\r\ncollectors’ own photographs. Fig. 2has selected samples of\r\nour CCUB dataset. More details are in supplement Sec. 9.\r\n4. Method\r\nGiven our CCUB dataset, our objective is to alter Stable\r\nDiffusion to have a more accurate understanding of a given\r\nculture. We next offer a brief background on training latent\r\ndiffusion models (Sec. 4.1), a modiﬁcation to a regulariza-\r\ntion loss to prevent over-ﬁtting (Sec. 4.2), a novel approach\r\nto computing perceptual loss on decoded images (Sec. 4.3),\r\nand a method to contrastively use Stable Diffusion’s mis-\r\nrepresentations of culture to reﬁne itself (Sec. 4.4).SCoFT.\r\nOur full ﬁne-tuning approach, SCoFT, is a weighted sum of\r\nall of the following loss functions:\r\n4.1. Latent Diffusion Model Loss\r\nDiffusion models are latent variable models that sample\r\nfrom a desired distribution by reversing a forward noising\r\nprocess. The latent noise at any timestep tis given by\r\nzt=p↵tz0+p1\x00↵t✏, where z0is the latent variable\r\n10824\r\nStable\tDiffusion\tUNet\r\nIterative\tDenoising\tProcessVAEDecoder(Frozen)\r\n!!!!"#Denoising\twith\tClassifier-free\tGuidance!!!!!!"#!$Record\tfirst\tgradientRecord\trandom\tgradientPixel\tSpaceStable\tDiffusion\tControlNet(Frozen)\r\n!"!!\r\n!"Self-ContrastivePerceptual\tLossCCUB\tImageDepth\tEstimation\r\nLatent\tSpace\r\nℒ#ℒ$ℒ%&\'ℒ\'\r\nFigure 3. SCoFT Overview. A conventional ﬁne-tuning loss, LLDM, and memorization penalty loss, LM, are computed in the Stable\r\nDiffusion latent space using images and captions from our CCUB dataset. After 20 denoising steps, the latent space is decoded. Perceptual\r\nfeatures are extracted from the generated image and compared contrastively to CCUB images as positive and non-ﬁne-tuned Stable Diffu-\r\nsion images as negative examples to form our Self-Contrastive Perceptual Loss, LC.\r\nencoded from the real image and ↵tis the strength of the\r\ngaussian noise ✏. We are interested in the pretrained denois-\r\ning network ✏✓(zt,c,t), which denoises the noisy latent to\r\ngetzt\x001conditioned on the text c. The training objective\r\nfor the denoiser ✏✓is minimizing the noise prediction:\r\nLLDM(z,c)=E✏,z,c,t[wtk✏\x00✏✓(zt,c,t)k](1)\r\nwhere wtis the weight to control the noise schedules. A\r\npretrained model can be further trained, or ﬁne-tuned, using\r\nthe original objective with a new dataset. We use Low-Rank\r\nAdaptation (LoRA) [ 21] to reduce memory demands and\r\nover-ﬁtting by training new low-rank weight matrices.\r\n4.2. Memorization Loss\r\nDespite CCUB’s rich cultural content, its size still remains\r\nsmall compared to LAION. The challenges of language\r\ndrift and decreased output diversity are prevalent issues en-\r\ncountered during this few-shot ﬁne-tuning [ 47]. Moreover,\r\nCCUB’s text captions can be highly speciﬁc, as shown in\r\nFig.2. Fine-tuning solely on the CCUB dataset using Equa-\r\ntion 1may lead to the undesirable outcome of reproduc-\r\ning training data as shown in Fig. 6. Our approach is in-\r\nspired by [ 26], which proposed a model-based concept ab-\r\nlation by letting the model memorize the mapping between\r\nnewly generated anchor images x?andc⇤, except we focus\r\non preventing memorization during ﬁne-tuning on a small\r\ndataset (e.g., CCUB). We harness the property of BLIP au-\r\ntomatic caption cbliponxccub, which is the naive version of\r\nour cultural text prompt cccub, to regularize the model out-\r\nputs conditioned on cccub, as shown in Fig. 6. To achieve\r\nthis, we introduce a memorization penalty loss leveraging\r\nBLIP [ 28] generated captions of CCUB images. We utilize\r\nmultiple BLIP captions {cblip}to regularize the one-on-onemapping between CCUB images and cultural captions:\r\nLM(xccub,cccub,cblip)=E✏,t[k✏(xccub,cccub,t)\r\n\x00Ei[✏(xccub,ci\r\nblip,t).sg()] k],(2)\r\nwhere .sg() stands for a stop-gradient operation of the cur-\r\nrent network to reduce memory cost.\r\n4.3. Perceptual Loss\r\nLLDM andLMare loss functions that operate on the la-\r\ntent codes within a diffusion model. Operating directly on\r\nthese latent codes is ideal for ﬁne-tuning models for adding\r\nsimple concepts such as adding a character’s appearance to\r\nthe model. For adding more complex, abstract concepts,\r\nlike culture, we propose to decode the latent space into the\r\npixel space in order to utilize pre-trained perceptual sim-\r\nilarity models. We propose to use a perceptual loss, LP,\r\nwhich is computed as the difference in extracted perceptual\r\nfeatures between the decoded, generated image, ˆx, and an\r\nimage from a training set, x:\r\nLP(ˆx, x)=Eˆx,x[S(ˆx, x;f✓)] (3)\r\nwhere Sis some perceptual similarity function and f✓is a\r\npretrained feature extractor.\r\nBackpropagation through sampling. State-of-the-art per-\r\nceptual models typically process inputs in the pixel space.\r\nIn contrast, Stable Diffusion is ﬁne-tuned in a latent space.\r\nTo fulﬁll our objective function, an intuitive strategy en-\r\ntails iteratively denoising latent features and then decoding\r\nthem back into the pixel space for use with perceptual mod-\r\nels. Concurrent work [ 9] denoises the stable diffusion latent\r\nfrom Gaussian noise into the pixel space based solely on\r\ntext prompts and by optimizing the Stable Diffusion UNet\r\n10825\r\nwith a reward function computed using the decoded image.\r\nInstead, our approach starts from the latent code at timestep\r\nt:zt=p↵tz0+p1\x00↵t✏, making it coupled with the Sta-\r\nble Diffusion ﬁne-tuning process. We utilize classiﬁer-free\r\nguidance, iteratively denoising the latent code according to\r\n(1 + w(t))✏(zt,cccub,t)\x00w(t)✏(xt,?,t), where w(t)is\r\nthe guidance scale and ?is a null text embedding. This en-\r\nables the model to generate images conditioned on cultural\r\ntext prompts and unveils its cultural understanding, as illus-\r\ntrated in Fig. 3. In practice, we denoise ztfor 20 timesteps.\r\nStarting from zt, our method ensures that the denoised im-\r\nage aligns with the same pose and structure as the original\r\ntraining image x0. This facilitates a more meaningful com-\r\nparison for perceptual loss and subsequent self-contrastive\r\nperceptual loss, revealing cultural differences.\r\nDirectly backpropagating through the multiple UNet de-\r\nnoising iterations, the latent space decoder, and the percep-\r\ntual model incurs signiﬁcant memory and time costs. To\r\naddress this, we selectively record the gradient on a sin-\r\ngle denoising step and employ stopgrad on other denois-\r\ning steps. Our ﬁndings indicate that recording the gradient\r\nfrom the ﬁrst step has the most signiﬁcant impact on re-\r\nﬁning the model’s cultural understanding. Further compar-\r\nisons on gradient recording and perceptual model backbone\r\nare detailed in Sec. 6.\r\n4.4. Self-Contrastive Perceptual Loss\r\nTo further improve Perceptual Loss, we raise an intriguing\r\nquestion: Can Stable Diffusion leverage its intrinsic biases\r\nto reﬁne its own? We seek to leverage the model’s prior of\r\nits cultural understanding and propose a contrastive learn-\r\ning approach. Utilizing our CCUB dataset, we designate\r\nthe positive examples to be {x+|xccub}representing pre-\r\nferred features. To unveil the cultural biases within Stable\r\nDiffusion, we employ images generated by the model itself\r\nas negative samples.\r\nIt is imperative to ensure that the generated negative\r\nsamples share a high-level similarity with positive samples,\r\nsuch as pose and structure, thereby emphasizing that the\r\nprimary distinctions lie in the diffusion model’s percep-\r\ntion of cultural features. We achieve this by incorporat-\r\ning a pre-trained ControlNet [ 61] module ⇥c, conditioned\r\non the estimated depth of {x+}, into Stable Diffusion.\r\nAs depicted in Fig. 3, negative examples are obtained as\r\n{x\x00\r\ni|⇥c(D(x+),c)}, where Dis the MiDaS [ 6,44] depth\r\nestimator, and crepresents cblipfollowed by a cultural suf-\r\nﬁx (e.g., “in Korea”). In practice, we generate 5 negative\r\nexamples for each positive example, then utilize DreamSim\r\nto ﬁlter out false negatives similar to the positive instances.\r\nTo enhance the cultural ﬁne-tuning process, our ob-\r\njective is to ensure that images generated by the current\r\nmodel, denoted as ˆx0\r\n✓thave closer perceptual distances to\r\npositive examples and farther distances from negative ex-\r\namples. This reinforces the model’s alignment with pre-ferred cultural features, distancing itself from undesirable\r\nbiases in negative examples, indicated by: S(ˆx0\r\n✓t,x+;f✓)>\r\nS(ˆx0\r\n✓t,x\x00\r\ni;f✓), where Sis some perceptual similarity func-\r\ntion and f✓is a pre-trained feature extractor. Thus, we for-\r\nmulate this objective, which we call Self-Contrastive Per-\r\nceptual Loss, using triplet loss:\r\nLC(ˆx, x+,x\x00)=Eˆx,x+,x\x00[max( S(ˆx, x+;f✓)\r\n\x00\x00S(ˆx, x\x00;f✓)+m,0)](4)\r\nwhere \x00denotes the weights on negative examples, mis\r\nthe constant margin between positive and negative exam-\r\nples, and f✓is a feature extractor. We evaluate a variety of\r\nstate-of-the-art perceptual embeddings and report compari-\r\nson results in Section 6.\r\n5. Experiments\r\nUser Survey. Our goal of improving the cultural percep-\r\ntion of generated images is a subjective metric largely de-\r\ntermined by members of a given identity group. To eval-\r\nuate our performance on this criterion, we recruited peo-\r\nple with at least 5 years of cultural experience in each of\r\nthe 5 countries with survey questions speciﬁc to their self-\r\nselected national cultural afﬁliation. A single page of the\r\nsurvey form provides one description (prompt) and one im-\r\nage made by four different generators using a common ran-\r\ndom seed, for four total images. We compare four image\r\ngenerators: Stable Diffusion against three ﬁne-tuned ab-\r\nlations. All ﬁne-tunings were performed with the CCUB\r\ndataset using LLDM along with one or more proposed loss\r\nfunctions, see Tab. 1. For example, SCoFT+M is Stable\r\nDiffusion ﬁne-tuned on CCUB using the sum of LLDMand\r\nLMas a loss function. Each survey page has a total of four\r\nsurvey items (rows that participants respond to) to rank im-\r\nages on (a) Description and Image Alignment, (b) Cultural\r\nRepresentation, (c) Stereotypes, and (d) Offensiveness. Par-\r\nticipants rank the set of randomly ordered images from best\r\nto worst image once for each item. An image labeled rank\r\n1 signiﬁes both best aligned and least offensive, while rank\r\n4 is least aligned and most offensive.\r\nWe quantitatively estimate the subjective perceived per-\r\nformance of each method with Matrix Mean-Subsequence-\r\nReduced (MMSR) [ 33] model in crowd-kit [ 57], an estab-\r\nlished algorithm [ 35] for noisy label aggregation, followed\r\nby a weighted majority vote to aggregate labels across\r\nworkers, and then a simple majority vote aggregating labels\r\ninto rankings, thus MMSR+V ote (see Supp Sec. 11.2).\r\nAutomatic Metrics. In addition to the user survey, we use\r\nKernel Inception Distance (KID) [ 3] and CLIP Score [ 20]\r\nto evaluate the quality of generated images. For automatic\r\nevaluation to ablate SCoFT, we use 10 test prompts for each\r\nculture, generating 20 images for each prompt.\r\n10826\r\nStable\tDiffusionSCoFT+MSCoFT+MPSCoFT+MPCNigerian\tCulture\r\nMexico\tCultureIndian\tCulture\r\nChinese\tCulture\r\nKorean\tCulture\r\nFigure 4. Qualitative comparison of our SCoFT model ablated and compared to Stable Diffusion without ﬁne-tuning.\r\nAutomatic Metrics User Survey Results - Average Rank\r\nModel Name LMLPLCKID- #\r\nCCUBKID- #\r\nCOCOCLIP- "\r\nScoreBest\r\nDescribedMost Culturally\r\nRepresentativeLeast\r\nStereotypicalLeast\r\nOffensive\r\nStable Diffusion[ 46] 30.355 4.396 0.813 3.09 3.07 2.98 3.07\r\nSCoFT+M X 22.643 4.711 0.802 2.57 2.56 2.66 2.59\r\nSCoFT+MP XX 21.360 4.936 0.800 2.33 2.35 2.34 2.30\r\nSCoFT+MPC XXX 19.621 4.819 0.799 1.83 1.84 1.91 1.78\r\nTable 1. We compare our SCoFT ablations to Stable Diffusion using automatic metrics (Sec. 6) and a user survey (Sec. 5). Values in the\r\nuser survey results report average ranking of images across all ﬁve cultures where lower rankings indicate better results (KID is ⇥103)\r\n6. Results\r\nQualitative Comparison. We qualitatively compare our\r\nSCoFT model versus the original Stable Diffusion in Fig. 1.\r\nSCoFT guides Stable Diffusion away from generating\r\nstereotypes and misrepresentations of culture. For example,\r\nmany of the Stable Diffusion results for a “Photo of a tra-\r\nditional building, in ...” depict disheveled structures, which\r\npromote a harmful stereotype that some cultures are poor or\r\nsimple, whereas SCoFT promotes more accurate and less\r\nstereotypical buildings for each nation. To investigate the\r\neffects of each loss function within SCoFT we also qual-\r\nitatively compare each ablation in Fig. 4. We tend to see\r\nthe SCoFT models modernize generated images, which de-\r\ncreases harmful stereotypes.\r\nUser Survey Results. 51 survey participants from ﬁve\r\ncountries ranked images across four ablations by respond-\r\ning to each of the four survey items in Sec. 5on freshly\r\ngenerated images. We had 13 Korean, 11 Chinese, 10 In-dian, 9 Nigerian, and 7 Mexican participants. The average\r\nparticipant rankings are in Table 1.\r\nWe also ran the MMSR [ 33] noisy data labeling al-\r\ngorithm across all responses (see Sec. 5, Supp. Sec.\r\n11.2), ﬁnding a participant consensus ranking of: (1)\r\nSCoFT+MPC (2) SCoFT+MP, (3) SCoFT+M, and ﬁnally\r\n(4) Generic Stable Diffusion. MMSR found that partic-\r\npants reached an identical consensus when separately rank-\r\ning each ablation with respect each of the four survey items.\r\nMMSR also found a participant consensus in which every\r\ncountry individually agreed with the ranking above, with\r\nthe exception of India, which swapped (1) SCoFT+MP and\r\n(2) SCoFT+MPC.\r\nWe convert the rankings into binary comparisons by iso-\r\nlating two ablations and comparing their rankings. This\r\nway, we can compare the effects of each of the loss func-\r\ntions of SCoFT. SCoFT+M was ranked less offensive than\r\nStable Diffusion 63% of the time, SCoFT+MP was less of-\r\n10827\r\nFigure 5. Violin plot of participant rankings across the survey\r\nitems and countries. A wider strip means more answers with that\r\nvalue. Each new loss in our ablation study improved the rankings,\r\nand SCoFT+MPC is best. (Rank 1 is the best; 4, the worst)\r\nfensive than SCoFT+M 56% of the time, and SCoFT+MPC\r\nwas less offensive than SCoFT+MP 62% of the time. We\r\nsee that each loss function contributed signiﬁcantly to de-\r\ncreasing the offensiveness of generated results, and this\r\ntrend continued for the other three survey items. We note\r\nthat the initial addition of ﬁne-tuning and the contrastive\r\nloss produced more dramatic improvements in SCoFT com-\r\npared to adding perceptual loss.\r\nWe compare whole distributions of the rankings in Fig. 5.\r\nAcross survey items, we see very similar distributions. For\r\nexample, Stable Diffusion images were very commonly\r\nranked fourth for both Stereotypes and Cultural Representa-\r\ntion. Participants in the Chinese and Korean surveys ranked\r\nimages with less variance than participants in the Indian and\r\nMexican surveys. This is potentially due to a difference in\r\nthe number of participants for each survey.\r\nAutomatic Metric Results. The automated evaluation re-\r\nsults in Table 1show that our proposed approach achieves\r\nthe highest KID score on the CCUB test set, indicating that\r\nthe ﬁne-tuned model is able to generate images with a sim-\r\nilar quality to the culturally curated data. In contrast, the\r\noriginal Stable Diffusion model scores highest in the CLIP\r\nScore and KID score on the MS COCO dataset. This result\r\nis not surprising as the CLIP model is itself known to be\r\nbiased [ 4] in ways shared with Stable Diffusion [ 32], where\r\nthe number of measurable outputs people perceive as hatred\r\nscales with the training set [ 5]. CLIP biases have also been\r\nquantitatively shown to be passed on to downstream ap-\r\nplications [ 22]. Human evaluators favor our SCoFT+MPC\r\nmethod over Stable Diffusion, suggesting CLIP-Score’s in-\r\nadequacy in assessing cultural competence.\r\nPerceptual Backbones and Gradient Recording. SCoFT\r\nuses a perceptual backbone to compare images in feature\r\nspaces rather than pixel space to avoid overﬁtting to train-\r\n!!!"#:\t“a\twoman\tis\toffering\tChinese\ttea\tto\tanother\tChinese\twoman\tin\tcheongsam”!#$%&:\ttwo\tgirls\tsitting\tat\ta\ttable,\tin\tChina\r\nMemorizing\tand\tOverfitting\r\nCorrect\tand\tNon-memorizing\r\nconditioned\ton\t!!!"#?!!"#Figure 6. Top-right: Fine-tuning Stable Diffusion on CCUB data\r\nusing only a conventional loss ( LLDM, Sec. 4.1) leads to overﬁt-\r\nting on CCUB captions. Bottom-right: Adding memorization loss\r\n(LM, Sec. 4.2) prevents overﬁtting with small datasets by ensuring\r\nimages generated by general captions ( cblip) are similar to those\r\ngenerated using CCUB’s cultural captions.\r\ning images. We test several backbones to extract image fea-\r\ntures, including the output of CLIP [ 42] convolutional lay-\r\ners [58], and the last layer of DreamSim [ 14], DINOv2 [ 40],\r\nBLIP2 [ 29]. For each comparison, we generate 200 im-\r\nages and calculate the KID with CCUB test set with the\r\nKorean, Chinese, and Indian models and see that the CLIP\r\nconvolutional layers and DreamSim output provide the best\r\ngeneralization. For all other experiments, we use the CLIP\r\nconvolutional layers as the SCoFT backbone.\r\nWe also compare the effect of recording the gradient dur-\r\ning the ﬁrst, last, and random iterations of denoising during\r\nﬁne-tuning, as reported in Fig. 7. We see that the best gener-\r\nalization comes when recording the gradient during the ﬁrst\r\niteration of denoising. This is in contrast to concurrent work\r\n[9] which recorded the last gradient, indicating that culture\r\nis a high-level concept where important information is cre-\r\nated early in the diffusion process, as opposed to aesthetics\r\nwhich are low-level and correspond more strongly to later\r\nstages of diffusion.\r\nEffectiveness of the Memorization Loss. We ﬁne-tune\r\nStable Diffusion using the original LLDMwith and without\r\nMemorization Loss, LM, on the CCUB dataset. To evalu-\r\nate the consequence of over-ﬁtting and reproducing training\r\nimages during few-shot ﬁne-tuning, we randomly select 10\r\ntext-image pairs for each culture from the CCUB training\r\nset. For each training text prompt, we generate 20 images.\r\nWe evaluate the process using three metrics: CLIP-Image\r\n(CLIP-I), DINO, and DreamSim. All metrics measure the\r\naverage pairwise cosine similarity between the embeddings\r\nof generated and real training images that share the same\r\ntext prompt. For both metrics, lower values indicate more\r\neffective prevention of overﬁtting and reproduction of train-\r\ning images. Fig. 6depicts qualitative results of generating\r\ncreative images. Quantitative results in Table 2show that\r\n10828\r\nPrompts from Training set Prompts from Test set\r\nMethod CLIP-I #DINO #DreamSim #DIV train "DIV test "CLIPScore "\r\nFinetune 0.912 0.836 0.591 0.302 0.317 0.824\r\nFinetune w/ LM 0.897 0.808 0.550 0.356 0.379 0.814\r\nTable 2. Images generated from Stable Diffusion ﬁne-tuned with and without Memorization Loss, LM, are compared in the feature space\r\nfrom various feature extractors. We see that LMencourages the model to produce images with more diversity (larger feature difference).\r\nFigure 7. Comparison of perceptual models used for feature ex-\r\ntraction in SCoFT. KID between held out CCUB images and gen-\r\nerated images is plotted versus training iterations representing gen-\r\neralization to a validation set, score averaged across three cultures.\r\nFigure 8. Generated images for text prompt: “a photo of a person.”\r\nThe proposed approach is able to generate more diverse images\r\ngiven a generic prompt without a speciﬁc cultural context.\r\nthe memorization loss effectively reduces overﬁtting.\r\nTo quantify output diversity, we randomly select 10\r\ntraining text prompts and 10 CCUB testing text prompts.\r\nFor each text prompt, we generate 20 images. We intro-\r\nduce the diversity metric (DIV), which calculates the av-\r\nerage pair-wise DreamSim cosine distance between gener-\r\nated images with the same text prompt. Higher values indi-\r\ncate enhanced diversity in the generated outputs, reﬂecting\r\na more varied and expressive synthesis of images. We also\r\nreport a comparable CLIP Score on the generated image us-\r\ning CCUB testing text prompts with baseline ﬁne-tuning.\r\nLimitations. To tackle the bias in the data, we aim for\r\ntwo goals: 1) to generate accurate images given a speciﬁccultural context and 2) to generate diverse images given a\r\ngeneric text prompt without any speciﬁc cultural context.\r\nOur current approach is focused on achieving the ﬁrst goal.\r\nOur current model can generate promisingly diverse images\r\nfor some generic prompts as shown in Figure 8when com-\r\npared to the baseline model that generates biased images.\r\nOur CCUB dataset was collected by experienced resi-\r\ndents; however more vigorous veriﬁcation will be needed\r\nto improve the quality of the dataset. (See Supp. Sec. 13)\r\n7. Conclusion\r\nThe biases of generative AI have already led to substantial\r\nand very public impacts [ 19,34], so it is essential that we\r\nensure models generate images that more accurately repre-\r\nsent the diversity of the world. We propose Self-Contrastive\r\nFine-Tuning (SCoFT), which is speciﬁcally designed to\r\nﬁne-tune the model for high-level concepts using a small\r\ndataset, for instance, to pay attention to subtle cultural el-\r\nements. SCoFT has potential to generalize to applications\r\nin other domains, such as, reducing the risk of copyright\r\ninfringement, better respecting cultural and community-\r\ndeﬁned boundaries, and addressing offensiveness across a\r\nbroader range of identity characteristics, amongst other cri-\r\nteria. For example, supplement Fig. 19shows initial results\r\nimproving images for individuals with disabilities who use\r\nmobility aids. We have also conﬁrmed positive associations\r\nwith some automated metrics while demonstrating others\r\nare not a good ﬁt for this task.\r\nOur extensive user survey and metric evaluation quan-\r\ntitatively demonstrate improvements in subjective metrics\r\nwith respect to image and description alignment, more cul-\r\nturally representative image contents, as well as reductions\r\nin stereotyping and offensiveness.\r\nAcknowledgment. We thank Youeun Shin, Lia Coleman, and\r\nall contributors for their contributions to our dataset. This\r\nwork was funded in part by NSF IIS-2112633, the Technol-\r\nogy Innovation Program (20018295, Meta-human: a virtual\r\ncooperation platform for specialized industrial services) of the\r\nMinistry of Trade, Industry & Energy (MOTIE, Korea), the\r\nMinistry of Science and ICT, Korea, under the Information\r\nTechnology Research Center support program (IITP-2024-\r\n2020-0-01789), and the Artiﬁcial Intelligence Convergence\r\nInnovation Human Resources Development (IITP-2024-RS-\r\n2023-00254592) supervised by the Institute for Information &\r\nCommunications Technology Planning & Evaluation, and the\r\nNSF under Grant # 2030859 to the Computing Research Associa-\r\ntion CIFellows Project subaward #2021CIF-CarnegieMellon-72.\r\n10829\r\nReferences\r\n[1]Jason Armitage, Endri Kacupaj, Golsa Tahmasebzadeh,\r\nSwati, Maria Maleshkova, Ralph Ewerth, and Jens Lehmann.\r\nMlm: a benchmark dataset for multitask learning with mul-\r\ntiple languages and modalities. In Proceedings of the 29th\r\nACM International Conference on Information & Knowledge\r\nManagement , pages 2967–2974, 2020. 2\r\n[2]Omri Avrahami, Ohad Fried, and Dani Lischinski. Blended\r\nlatent diffusion. ACM Transactions on Graphics (TOG) , 42\r\n(4):1–11, 2023. 3\r\n[3]Mikołaj Bi ´nkowski, Danica J Sutherland, Michael Arbel, and\r\nArthur Gretton. Demystifying mmd gans. arXiv preprint\r\narXiv:1801.01401 , 2018. 5\r\n[4]Abeba Birhane, Vinay Uday Prabhu, and Emmanuel Kahem-\r\nbwe. Multimodal datasets: misogyny, pornography, and ma-\r\nlignant stereotypes. arXiv preprint arXiv:2110.01963 , 2021.\r\n2,7\r\n[5]Abeba Birhane, Vinay Prabhu, Sang Han, and Vishnu Naresh\r\nBoddeti. On hate scaling laws for data-swamps, 2023. 2,7\r\n[6]Reiner Birkl, Diana Wofk, and Matthias M ¨uller. Midas v3.1\r\n– a model zoo for robust monocular relative depth estimation.\r\narXiv preprint arXiv:2307.14460 , 2023. 5\r\n[7]Mari Casta ˜neda. The power of (mis) representation: Why\r\nracial and ethnic stereotypes in the media matter. Challeng-\r\ning inequalities: Readings in race, ethnicity, and immigra-\r\ntion, 2018. 2\r\n[8]Michelle Caswell, Alda Allina Migoni, Noah Geraci, and\r\nMarika Cifor. ‘to be able to imagine otherwise’: commu-\r\nnity archives and the importance of representation. Archives\r\nand Records , 38(1):5–26, 2017. 1\r\n[9]Kevin Clark, Paul Vicol, Kevin Swersky, and David J Fleet.\r\nDirectly ﬁne-tuning diffusion models on differentiable re-\r\nwards. arXiv preprint arXiv:2309.17400 , 2023. 4,7\r\n[10] Karan Desai, Gaurav Kaul, Zubin Aysola, and Justin John-\r\nson. Redcaps: Web-curated image-text data created by the\r\npeople, for the people. arXiv preprint arXiv:2111.11431 ,\r\n2021. 3\r\n[11] Travis L Dixon and Daniel Linz. Overrepresentation and\r\nunderrepresentation of african americans and latinos as law-\r\nbreakers on television news. Journal of communication , 50\r\n(2):131–154, 2000. 2\r\n[12] Rawan Elbaba. Why on-screen representation matters, ac-\r\ncording to these teens. PBS NewsHour , 14, 2019. 1\r\n[13] Zhida Feng, Zhenyu Zhang, Xintong Yu, Yewei Fang,\r\nLanxin Li, Xuyi Chen, Yuxiang Lu, Jiaxiang Liu, Weichong\r\nYin, Shikun Feng, et al. Ernie-vilg 2.0: Improving text-to-\r\nimage diffusion model with knowledge-enhanced mixture-\r\nof-denoising-experts. arXiv preprint arXiv:2210.15257 ,\r\n2022. 3\r\n[14] Stephanie Fu, Netanel Tamir, Shobhita Sundaram, Lucy\r\nChai, Richard Zhang, Tali Dekel, and Phillip Isola. Dream-\r\nsim: Learning new dimensions of human visual similar-\r\nity using synthetic data. arXiv preprint arXiv:2306.09344 ,\r\n2023. 3,7\r\n[15] Rohit Gandikota, Joanna Materzynska, Tingrui Zhou, Anto-\r\nnio Torralba, and David Bau. Concept sliders: Lora adap-tors for precise control in diffusion models. arXiv preprint\r\narXiv:2311.12092 , 2023. 2\r\n[16] Rohit Gandikota, Hadas Orgad, Yonatan Belinkov, Joanna\r\nMaterzy ´nska, and David Bau. Uniﬁed concept editing in dif-\r\nfusion models. In Proceedings of the IEEE/CVF Winter Con-\r\nference on Applications of Computer Vision (WACV) , pages\r\n5111–5120, 2024. 2\r\n[17] Noa Garcia, Yusuke Hirota, Yankun Wu, and Yuta\r\nNakashima. Uncurated image-text datasets: Shedding light\r\non demographic bias. In Proceedings of the IEEE/CVF Con-\r\nference on Computer Vision and Pattern Recognition , pages\r\n6957–6966, 2023. 3\r\n[18] Ben Halpern. The dynamic elements of culture. Ethics , 65\r\n(4):235–249, 1955. 3\r\n[19] Melissa Heikkil ¨a. The viral ai avatar app lensa undressed me\r\nwithout my consent, 2022. Accessed on December 16, 2023.\r\n8\r\n[20] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras,\r\nand Yejin Choi. CLIPScore: a reference-free evaluation met-\r\nric for image captioning. In EMNLP , 2021. 5\r\n[21] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\r\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\r\nLora: Low-rank adaptation of large language models. arXiv\r\npreprint arXiv:2106.09685 , 2021. 2,3,4\r\n[22] Andrew Hundt, William Agnew, Vicky Zeng, Severin Ka-\r\ncianka, and Matthew Gombolay. Robots enact malignant\r\nstereotypes. FAccT , page 743–756, 2022. 2,7\r\n[23] Ben Hutchinson, Andrew Smart, Alex Hanna, Emily Den-\r\nton, Christina Greer, Oddur Kjartansson, Parker Barnes,\r\nand Margaret Mitchell. Towards accountability for machine\r\nlearning datasets: Practices from software engineering and\r\ninfrastructure. In Proceedings of the 2021 ACM Confer-\r\nence on Fairness, Accountability, and Transparency , page\r\n560–575, New York, NY , USA, 2021. Association for Com-\r\nputing Machinery. 2\r\n[24] Mary Ritchie Key and Bernard Comrie, editors. IDS. Max\r\nPlanck Institute for Evolutionary Anthropology, Leipzig,\r\n2021. 3\r\n[25] Gwanghyun Kim, Taesung Kwon, and Jong Chul Ye. Dif-\r\nfusionclip: Text-guided diffusion models for robust image\r\nmanipulation. In Proceedings of the IEEE/CVF Conference\r\non Computer Vision and Pattern Recognition , pages 2426–\r\n2435, 2022. 3\r\n[26] Nupur Kumari, Bingliang Zhang, Sheng-Yu Wang, Eli\r\nShechtman, Richard Zhang, and Jun-Yan Zhu. Ablating con-\r\ncepts in text-to-image diffusion models. In ICCV , 2023. 4,\r\n1\r\n[27] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli\r\nShechtman, and Jun-Yan Zhu. Multi-concept customization\r\nof text-to-image diffusion. In Proceedings of the IEEE/CVF\r\nConference on Computer Vision and Pattern Recognition\r\n(CVPR) , pages 1931–1941, 2023. 3\r\n[28] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.\r\nBlip: Bootstrapping language-image pre-training for uni-\r\nﬁed vision-language understanding and generation. arXiv\r\npreprint arXiv:2201.12086 , 2022. 4,1\r\n[29] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\r\nBlip-2: Bootstrapping language-image pre-training with\r\n10830\r\nfrozen image encoders and large language models. arXiv\r\npreprint arXiv:2301.12597 , 2023. 3,7,1\r\n[30] Fangyu Liu, Emanuele Bugliarello, E. Ponti, Siva\r\nReddy, Nigel Collier, and Desmond Elliott. Visually\r\ngrounded reasoning across languages and cultures. ArXiv ,\r\nabs/2109.13238, 2021. 3\r\n[31] Haoming Lu, Hazarapet Tunanyan, Kai Wang, Shant\r\nNavasardyan, Zhangyang Wang, and Humphrey Shi. Spe-\r\ncialist diffusion: Plug-and-play sample-efﬁcient ﬁne-tuning\r\nof text-to-image diffusion models to learn any unseen style.\r\nInProceedings of the IEEE/CVF Conference on Computer\r\nVision and Pattern Recognition , pages 14267–14276, 2023.\r\n3\r\n[32] Sasha Luccioni, Christopher Akiki, Margaret Mitchell, and\r\nYacine Jernite. Stable bias: Evaluating societal representa-\r\ntions in diffusion models. In Thirty-seventh Conference on\r\nNeural Information Processing Systems Datasets and Bench-\r\nmarks Track , 2023. 2,7\r\n[33] Qianqian Ma and Alex Olshevsky. Adversarial crowdsourc-\r\ning through robust rank-one matrix completion. In Advances\r\nin Neural Information Processing Systems , pages 21841–\r\n21852. Curran Associates, Inc., 2020. 5,6\r\n[34] Emanuel Maiberg. Inside the ai porn marketplace where ev-\r\nerything and everyone is for sale. 2,8\r\n[35] Mohammad S. Majdi and Jeffrey J. Rodriguez. Crowd-\r\ncertain: Label aggregation in crowdsourced and ensemble\r\nlearning classiﬁcation, 2023. 5\r\n[36] Abhishek Mandal, Susan Leavy, and Suzanne Little. Dataset\r\ndiversity: measuring and mitigating geographical bias in im-\r\nage search and retrieval. In Proceedings of the 1st Interna-\r\ntional Workshop on Trustworthy AI for Multimedia Comput-\r\ning, pages 19–25, 2021. 3\r\n[37] Dana E Mastro and Bradley S Greenberg. The portrayal of\r\nracial minorities on prime time television. Journal of Broad-\r\ncasting & Electronic Media , 44(4):690–703, 2000. 2\r\n[38] Marc Miquel-Rib ´e and David Laniado. Wikipedia cultural\r\ndiversity dataset: A complete cartography for 300 language\r\neditions. In Proceedings of the International AAAI Confer-\r\nence on Web and Social Media , pages 620–629, 2019. 3\r\n[39] Eirini Ntoutsi, Pavlos Fafalios, Ujwal Gadiraju, Vasileios\r\nIosiﬁdis, Wolfgang Nejdl, Maria-Esther Vidal, Salvatore\r\nRuggieri, Franco Turini, Symeon Papadopoulos, Emmanouil\r\nKrasanakis, et al. Bias in data-driven artiﬁcial intelligence\r\nsystems—an introductory survey. Wiley Interdisciplinary\r\nReviews: Data Mining and Knowledge Discovery , 10(3):\r\ne1356, 2020. 2\r\n[40] Maxime Oquab, Timoth ´ee Darcet, Theo Moutakanni, Huy V .\r\nV o, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,\r\nDaniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Rus-\r\nsell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, Shang-\r\nWen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nico-\r\nlas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou,\r\nJulien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bo-\r\njanowski. Dinov2: Learning robust visual features without\r\nsupervision, 2023. 3,7\r\n[41] Justin Pinkney. Text to pokemon generator, 2022. 3\r\n[42] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\r\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\r\ntransferable visual models from natural language supervi-\r\nsion. In International conference on machine learning , pages\r\n8748–8763. PMLR, 2021. 3,7\r\n[43] Deborah Raji, Emily Denton, Emily M. Bender, Alex Hanna,\r\nand Amandalynne Paullada. Ai and the everything in the\r\nwhole wide world benchmark. In Proceedings of the Neu-\r\nral Information Processing Systems Track on Datasets and\r\nBenchmarks . Curran, 2021. 3\r\n[44] Ren´e Ranftl, Katrin Lasinger, David Hafner, Konrad\r\nSchindler, and Vladlen Koltun. Towards robust monocular\r\ndepth estimation: Mixing datasets for zero-shot cross-dataset\r\ntransfer. IEEE Transactions on Pattern Analysis and Ma-\r\nchine Intelligence , 44(3), 2022. 5\r\n[45] William Gaviria Rojas, Sudnya Diamos, Keertan Ranjan\r\nKini, David Kanter, and Vijay Janapa Reddi. The dollar\r\nstreet dataset: Images representing the geographic and so-\r\ncioeconomic diversity of the world. 2022. 2\r\n[46] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\r\nPatrick Esser, and Bj ¨orn Ommer. High-resolution image syn-\r\nthesis with latent diffusion models, 2021. 2,6\r\n[47] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\r\nMichael Rubinstein, and Kﬁr Aberman. Dreambooth: Fine\r\ntuning text-to-image diffusion models for subject-driven\r\ngeneration. arXiv preprint arXiv:2208.12242 , 2022. 4\r\n[48] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\r\nMichael Rubinstein, and Kﬁr Aberman. Dreambooth: Fine\r\ntuning text-to-image diffusion models for subject-driven\r\ngeneration. In Proceedings of the IEEE/CVF Conference\r\non Computer Vision and Pattern Recognition , pages 22500–\r\n22510, 2023. 3\r\n[49] Peter Schaldenbrand, James McCann, and Jean Oh.\r\nFrida: A collaborative robot painter with a differentiable,\r\nreal2sim2real planning environment. In 2023 IEEE Inter-\r\nnational Conference on Robotics and Automation (ICRA) ,\r\npages 11712–11718. IEEE, 2023. 3\r\n[50] Morgan Klaus Scheuerman, Alex Hanna, and Emily Denton.\r\nDo datasets have politics? disciplinary values in computer\r\nvision dataset development. Proc. ACM Hum.-Comput. In-\r\nteract. , 5(CSCW2), 2021. 2\r\n[51] Christoph Schuhmann, Richard Vencu, Romain Beaumont,\r\nRobert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo\r\nCoombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m:\r\nOpen dataset of clip-ﬁltered 400 million image-text pairs.\r\narXiv preprint arXiv:2111.02114 , 2021. 2\r\n[52] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\r\nCade Gordon, Ross Wightman, Mehdi Cherti, Theo\r\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\r\nman, et al. Laion-5b: An open large-scale dataset for\r\ntraining next generation image-text models. arXiv preprint\r\narXiv:2210.08402 , 2022. 3\r\n[53] Adrienne Shaw. Identity, identiﬁcation, and media represen-\r\ntation in video game play: An audience reception study . PhD\r\nthesis, University of Pennsylvania, 2010. 1\r\n[54] Makoto Shing and Kei Sawada. Japanese stable diffusion.\r\n2022. 3\r\n[55] Krishna Srinivasan, Karthik Raman, Jiecao Chen, Michael\r\nBendersky, and Marc Najork. Wit: Wikipedia-based image\r\n10831\r\ntext dataset for multimodal multilingual machine learning.\r\nInProceedings of the 44th International ACM SIGIR Confer-\r\nence on Research and Development in Information Retrieval ,\r\npages 2443–2449, 2021. 2\r\n[56] Harini Suresh and John Guttag. A framework for understand-\r\ning sources of harm throughout the machine learning life cy-\r\ncle. In Proceedings of the 1st ACM Conference on Equity\r\nand Access in Algorithms, Mechanisms, and Optimization ,\r\nNew York, NY , USA, 2021. Association for Computing Ma-\r\nchinery. 2\r\n[57] Dmitry Ustalov, Nikita Pavlichenko, and Boris Tseitlin.\r\nLearning from crowds with crowd-kit, 2023. 5\r\n[58] Yael Vinker, Ehsan Pajouheshgar, Jessica Y Bo, Ro-\r\nman Christian Bachmann, Amit Haim Bermano, Daniel\r\nCohen-Or, Amir Zamir, and Ariel Shamir. Clipasso:\r\nSemantically-aware object sketching. ACM Transactions on\r\nGraphics (TOG) , 41(4):1–11, 2022. 7\r\n[59] Bram Wallace, Akash Gokul, Stefano Ermon, and Nikhil\r\nNaik. End-to-end diffusion latent optimization improves\r\nclassiﬁer guidance. arXiv preprint arXiv:2303.13703 , 2023.\r\n3\r\n[60] Angelina Wang, Solon Barocas, Kristen Laird, and Hanna\r\nWallach. Measuring representational harms in image cap-\r\ntioning. In Proceedings of the 2022 ACM Conference on\r\nFairness, Accountability, and Transparency , page 324–335,\r\nNew York, NY , USA, 2022. Association for Computing Ma-\r\nchinery. 2\r\n[61] Lvmin Zhang and Maneesh Agrawala. Adding conditional\r\ncontrol to text-to-image diffusion models. arXiv preprint\r\narXiv:2302.05543 , 2023. 5\r\n[62] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-\r\nman, and Oliver Wang. The unreasonable effectiveness of\r\ndeep features as a perceptual metric. In Proceedings of the\r\nIEEE conference on computer vision and pattern recogni-\r\ntion, pages 586–595, 2018. 3\r\n10832'}, 'dist': 0.9286905527114868}
